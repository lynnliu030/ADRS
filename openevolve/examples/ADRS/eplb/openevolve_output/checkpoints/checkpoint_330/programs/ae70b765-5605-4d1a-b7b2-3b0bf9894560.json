{"id": "ae70b765-5605-4d1a-b7b2-3b0bf9894560", "code": "# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort groups by weight in descending order.\n    # 'weight' is already float and on CPU from rebalance_experts,\n    # so no need for .float().cpu() here.\n    indices = weight.sort(-1, descending=True).indices\n\n    # Initialize pack_index and rank_in_pack.\n    # These will be fully populated, so `empty_like` is sufficient.\n    # They will be on the same device as 'weight' (i.e., CPU).\n    pack_index = torch.empty_like(weight, dtype=torch.int64)\n    rank_in_pack = torch.empty_like(pack_index)\n    \n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These tensors are on the same device as 'weight' (which is CPU).\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id in indices[i]:\n            # Get current state for this layer's packs.\n            current_pack_weights = pack_weights_per_layer[i]\n            current_pack_items = pack_items_per_layer[i]\n\n            # Get current state for this layer's packs.\n            # Create a temporary copy of pack weights for modification during selection.\n            # This avoids modifying the actual cumulative weights before assignment.\n            # Create a temporary tensor for pack weights, setting full packs to infinity\n            # for selection by torch.argmin. This avoids modifying the actual\n            # cumulative weights stored in pack_weights_per_layer.\n            pack_weights_for_selection = torch.where(\n                pack_items_per_layer[i] == groups_per_pack,\n                torch.full_like(pack_weights_per_layer[i], float('inf')),\n                pack_weights_per_layer[i]\n            )\n            \n            # Find the pack with the minimum current weight (among available ones).\n            pack = torch.argmin(pack_weights_for_selection) # pack is a 0-dim tensor\n\n            # Assert that a valid pack was found and it has space.\n            # This check remains important to ensure algorithm correctness.\n            assert current_pack_items[pack] < groups_per_pack\n\n            # Assign the group to the chosen pack.\n            # `group_id` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            # `weight[i, group_id]` is the weight of the current group.\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    # Pre-calculate the indices for optimization.\n    # The core operation is finding the logical expert with the minimum load per replica.\n    # We can optimize the repeated calculation of (weight / logcnt).max(dim=-1).indices.\n    \n    # Initialize a tensor to store the current load per logical expert.\n    current_logical_load = weight.clone()\n\n    for i in range(num_log, num_phy):\n        # Find the logical expert that has the minimum average load (weight / logcnt).\n        # This is the expert that will get the next replica.\n        # Avoid division by zero if logcnt is 0 (though it's initialized to 1).\n        # Ensure we are not dividing by zero if logcnt could become zero.\n        # The current initialization to 1 should prevent this.\n        \n        # Calculate the average load for each logical expert.\n        # Use a small epsilon to avoid division by zero if logcnt could ever be zero.\n        # Given logcnt is initialized to 1 and incremented, this should not be an issue.\n        avg_load = current_logical_load / logcnt\n        \n        # Find the logical expert with the minimum average load.\n        redundant_indices = torch.argmin(avg_load, dim=-1)\n\n        # Assign the chosen logical expert to the current physical expert.\n        phy2log[:, i] = redundant_indices\n        \n        # Record the rank of this replica within the chosen logical expert.\n        # The rank is the current count of replicas for that logical expert.\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        \n        # Increment the replica count for the chosen logical expert.\n        logcnt[arangen, redundant_indices] += 1\n        \n        # Update the current_logical_load to reflect the addition of a replica.\n        # This is not strictly necessary if we recompute avg_load each time,\n        # but it can sometimes be clearer or allow for different optimization strategies.\n        # However, since we are using avg_load directly, this update is not needed here.\n        # The logic here is to pick the expert that *currently* has the lowest average load.\n        # The `current_logical_load` is not updated in the original code either,\n        # it's `weight` that is implicitly used.\n        # The `logcnt` is what changes. So the division `weight / logcnt` is correct.\n        # The optimization here is more about clarity and potentially avoiding redundant\n        # calculations if the compiler can optimize the min operation better.\n        # Let's stick to the original logic's intent of using the `weight` directly.\n        # The original code is: `(weight / logcnt).max(dim=-1).indices`\n        # This is equivalent to `torch.argmin(weight / logcnt, dim=-1)`\n        # So the optimization is primarily in clarity.\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    # Cast weight to float for calculations if not already, and keep on original device.\n    # Moving to CPU adds overhead if data is already on GPU and might slow down ops.\n    weight = weight.float()\n    \n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n", "language": "python", "parent_id": "fc52a6a0-2ab7-46b4-9401-a2de0aaaa405", "generation": 7, "timestamp": 1751056934.6794722, "iteration_found": 116, "metrics": {"balancedness_score": 0.14515957884623945, "speed_score": 0.02457471864110155, "combined_score": 0.0848671487436705}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 15 lines with 18 lines\nChange 2: Replace 5 lines with 46 lines", "parent_metrics": {"balancedness_score": 0.29830835153138974, "speed_score": 0.02426747169913593, "combined_score": 0.16128791161526285}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve the Mixture-of-Expert models Expert Parallelism Load Balancer (MoE EPLB) expert rearrangement algorithm.\nThis algorithm will take the load metrics recorded by the vLLM server, and rearrange the experts to balance the load. It can make replicas of some experts to achieve better load balancing.\nYour goal will be two-fold: 1. Improve the algorithm to achieve better load balancing; while 2. Improve the algorithm to be more efficient, i.e. reduce the execution time of the algorithm itself, since perfect load balancing is NP-hard.\nThe current algorithm is implemented in the `rebalance_experts` function. ", "user": "# Current Program Information\n- Current performance metrics: - balancedness_score: 0.2983\n- speed_score: 0.0243\n- combined_score: 0.1613\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: speed_score, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0830, combined_score: 0.1907\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0833, combined_score: 0.1908\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0835, combined_score: 0.1909\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.1909)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0835), Performs well on combined_score (0.1909)\n\n\n### Program 2 (Score: 0.1908)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0833), Performs well on combined_score (0.1908)\n\n\n### Program 3 (Score: 0.1907)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0830), Performs well on combined_score (0.1907)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.1872)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n\n### Program D2 (Score: 0.1900)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.1909, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 3 lines with 4 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 2 (Score: 0.1630, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 14 lines with 8 lines\nChange 2: Replace 15 lines with 19 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 3 (Score: 0.0000, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 47 lines with 53 lines\nChange 2: Replace 8 lines with 44 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 4 (Score: 0.0872, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 87 lines with 89 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 5 (Score: 0.1908, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 34 lines with 38 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n\n# Current Program\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort groups by weight in descending order.\n    # 'weight' is already float and on CPU from rebalance_experts,\n    # so no need for .float().cpu() here.\n    indices = weight.sort(-1, descending=True).indices\n\n    # Initialize pack_index and rank_in_pack.\n    # These will be fully populated, so `empty_like` is sufficient.\n    # They will be on the same device as 'weight' (i.e., CPU).\n    pack_index = torch.empty_like(weight, dtype=torch.int64)\n    rank_in_pack = torch.empty_like(pack_index)\n    \n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These tensors are on the same device as 'weight' (which is CPU).\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id in indices[i]:\n            # Get current state for this layer's packs.\n            current_pack_weights = pack_weights_per_layer[i]\n            current_pack_items = pack_items_per_layer[i]\n\n            # Get current state for this layer's packs.\n            # Create a temporary copy of pack weights for modification during selection.\n            # This avoids modifying the actual cumulative weights before assignment.\n            # Create a temporary tensor for pack weights, setting full packs to infinity\n            # for selection by torch.argmin. This avoids modifying the actual\n            # cumulative weights stored in pack_weights_per_layer.\n            pack_weights_for_selection = torch.where(\n                pack_items_per_layer[i] == groups_per_pack,\n                torch.full_like(pack_weights_per_layer[i], float('inf')),\n                pack_weights_per_layer[i]\n            )\n            \n            # Find the pack with the minimum current weight (among available ones).\n            pack = torch.argmin(pack_weights_for_selection) # pack is a 0-dim tensor\n\n            # Assert that a valid pack was found and it has space.\n            # This check remains important to ensure algorithm correctness.\n            assert current_pack_items[pack] < groups_per_pack\n\n            # Assign the group to the chosen pack.\n            # `group_id` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            # `weight[i, group_id]` is the weight of the current group.\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    # Cast weight to float for calculations if not already, and keep on original device.\n    # Moving to CPU adds overhead if data is already on GPU and might slow down ops.\n    weight = weight.float()\n    \n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["The current implementation has a `speed_score` of 0.0243 and a `combined_score` of 0.1613. The goal is to improve these scores while maintaining or improving `balancedness_score`.\n\nLooking at the history, previous attempts (1, 2, and 3) achieved much better `speed_score` (around 0.083) and `combined_score` (around 0.190). This suggests that recent changes might have negatively impacted performance. The identified areas for improvement mention simplifying code for readability and maintainability, and that metrics showing regression (speed_score, combined_score) might warrant reverting or revising recent changes.\n\nOne key area to examine is the `balanced_packing` function, as it's called multiple times and involves sorting and iterative assignments, which can be computationally intensive. The current implementation uses a greedy approach to assign items to packs. While this is a common heuristic for bin packing, it's worth exploring if minor optimizations can be made.\n\nAnother observation is the conditional logic:\n```python\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n```\nThis suggests that when `num_groups` is not divisible by `num_nodes`, it defaults to a global policy by setting `num_groups=1` and `num_nodes=1`. This might be an opportunity for optimization if the `rebalance_experts_hierarchical` function can be made more efficient even when these parameters don't align perfectly, or if a simpler global balancing strategy could be employed.\n\nLet's focus on optimizing `balanced_packing` and potentially the conditional logic in `rebalance_experts`.\n\n### Optimization 1: Simplify `balanced_packing` for clarity and potential efficiency\n\nThe `balanced_packing` function has some redundant operations and could be made more concise. Specifically, the way `pack_weights_for_selection` is created and used can be simplified.\n\n<<<<<<< SEARCH\n            # Create a temporary tensor for pack weights, setting full packs to infinity\n            # for selection by torch.argmin. This avoids modifying the actual cumulative weights\n            # stored in pack_weights_per_layer.\n            pack_weights_for_selection = torch.where(\n                pack_items_per_layer[i] == groups_per_pack,\n                torch.full_like(pack_weights_per_layer[i], float('inf')),\n                pack_weights_per_layer[i]\n            )\n            \n            # Find the pack with the minimum current weight (among available ones).\n            pack = torch.argmin(pack_weights_for_selection) # pack is a 0-dim tensor\n\n            # Assert that a valid pack was found and it has space.\n            # This check remains important to ensure algorithm correctness.\n            assert current_pack_items[pack] < groups_per_pack\n=======\n            # Find the pack with the minimum current weight, avoiding full packs.\n            # We can directly modify pack_weights_per_layer for selection purposes,\n            # as the original values are only needed for the next iteration of the outer loop.\n            # However, to avoid modifying the actual cumulative weights before assignment,\n            # it's safer to use a temporary tensor or carefully manage updates.\n            # A more direct approach is to find the minimum among non-full packs.\n            \n            # Temporarily set weights of full packs to infinity for selection\n            temp_pack_weights = pack_weights_per_layer[i].clone()\n            full_pack_mask = pack_items_per_layer[i] == groups_per_pack\n            temp_pack_weights[full_pack_mask] = float('inf')\n\n            # Find the pack with the minimum current weight (among available ones).\n            pack = torch.argmin(temp_pack_weights) # pack is a 0-dim tensor\n\n            # Assert that a valid pack was found and it has space.\n            # This check remains important to ensure algorithm correctness.\n            assert pack_items_per_layer[i][pack] < groups_per_pack\n>>>>>>> REPLACE\n\n**Reasoning:**\nThe original code creates `pack_weights_for_selection` using `torch.where` to replace weights of full packs with infinity. This is a correct approach, but it involves creating a new tensor in each iteration of the inner loop, which might incur some overhead. The suggested change uses `clone()` to create a temporary copy and then modifies it. This might be slightly more efficient by avoiding the `torch.where` operation, although the overall complexity remains the same. The assertion is also slightly adjusted for clarity. This change aims to improve the readability and potentially the performance of this critical section.\n\n### Optimization 2: Refine the `rebalance_experts` conditional logic\n\nThe current `rebalance_experts` function has a conditional that calls `rebalance_experts_hierarchical` twice, with different parameters. If `num_groups % num_nodes != 0`, it forces a global policy by setting `num_groups=1` and `num_nodes=1`. This might not be the most efficient way to handle the \"global\" policy if the hierarchical function is not optimized for these specific values, or if a simpler global strategy exists.\n\nGiven that the previous top-performing programs had much better speed scores, it's possible that the \"global load-balance policy\" implementation in `rebalance_experts_hierarchical` is not as efficient as it could be, or that the original global strategy was better. However, without the specific code for the previous versions, it's hard to revert.\n\nInstead, let's consider if the `rebalance_experts_hierarchical` function can be made more robust or if the call to it can be simplified. The current code uses `rebalance_experts_hierarchical` for both cases. If the intention is a true global balance when `num_groups % num_nodes != 0`, then calling `rebalance_experts_hierarchical` with `num_groups=1` and `num_nodes=1` is a way to achieve that.\n\nHowever, the historical data suggests that the previous `speed_score` was significantly better. This implies that either the `rebalance_experts_hierarchical` function itself has become slower, or the conditions under which it's called are suboptimal.\n\nLet's assume the `rebalance_experts_hierarchical` function is generally intended for hierarchical balancing. When `num_groups` is not divisible by `num_nodes`, a simpler, non-hierarchical approach might be more efficient. The current code uses `rebalance_experts_hierarchical(weight, num_replicas, 1, 1, num_gpus)` which effectively flattens the problem to a single \"node\" and single \"group\" structure, essentially making it a global balancing problem.\n\nIf the goal is to improve the speed score, and the current implementation of `rebalance_experts_hierarchical` is causing a slowdown when `num_groups` is not divisible by `num_nodes`, it might be beneficial to re-examine that path.\n\nGiven the prompt's emphasis on improving efficiency and the regression in speed_score, let's consider a small change to the conditional logic that might slightly simplify the execution path if the parameters don't align.\n\nThe current code:\n```python\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n```\nThis structure is already quite direct. The issue might be *within* `rebalance_experts_hierarchical` when called with `num_groups=1, num_nodes=1`.\n\nLet's consider a minor modification that ensures the `weight` tensor is on the correct device *before* the call, although the current code already seems to handle this by casting to float and keeping on the original device.\n\nA more impactful change would be to revisit the *logic* of `rebalance_experts_hierarchical` itself, but that's a larger refactoring. For targeted improvements, we can try to optimize `balanced_packing` as shown above.\n\nLet's try to optimize the `replicate_experts` function, as it's also a core component. The loop in `replicate_experts` iterates `num_log` to `num_phy` times. Inside the loop, `(weight / logcnt).max(dim=-1).indices` is computed. This involves division and a max operation across the last dimension.\n\n<<<<<<< SEARCH\n    for i in range(num_log, num_phy):\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n=======\n    # Pre-calculate the indices for optimization.\n    # The core operation is finding the logical expert with the minimum load per replica.\n    # We can optimize the repeated calculation of (weight / logcnt).max(dim=-1).indices.\n    \n    # Initialize a tensor to store the current load per logical expert.\n    current_logical_load = weight.clone()\n\n    for i in range(num_log, num_phy):\n        # Find the logical expert that has the minimum average load (weight / logcnt).\n        # This is the expert that will get the next replica.\n        # Avoid division by zero if logcnt is 0 (though it's initialized to 1).\n        # Ensure we are not dividing by zero if logcnt could become zero.\n        # The current initialization to 1 should prevent this.\n        \n        # Calculate the average load for each logical expert.\n        # Use a small epsilon to avoid division by zero if logcnt could ever be zero.\n        # Given logcnt is initialized to 1 and incremented, this should not be an issue.\n        avg_load = current_logical_load / logcnt\n        \n        # Find the logical expert with the minimum average load.\n        redundant_indices = torch.argmin(avg_load, dim=-1)\n\n        # Assign the chosen logical expert to the current physical expert.\n        phy2log[:, i] = redundant_indices\n        \n        # Record the rank of this replica within the chosen logical expert.\n        # The rank is the current count of replicas for that logical expert.\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        \n        # Increment the replica count for the chosen logical expert.\n        logcnt[arangen, redundant_indices] += 1\n        \n        # Update the current_logical_load to reflect the addition of a replica.\n        # This is not strictly necessary if we recompute avg_load each time,\n        # but it can sometimes be clearer or allow for different optimization strategies.\n        # However, since we are using avg_load directly, this update is not needed here.\n        # The logic here is to pick the expert that *currently* has the lowest average load.\n        # The `current_logical_load` is not updated in the original code either,\n        # it's `weight` that is implicitly used.\n        # The `logcnt` is what changes. So the division `weight / logcnt` is correct.\n        # The optimization here is more about clarity and potentially avoiding redundant\n        # calculations if the compiler can optimize the min operation better.\n        # Let's stick to the original logic's intent of using the `weight` directly.\n        # The original code is: `(weight / logcnt).max(dim=-1).indices`\n        # This is equivalent to `torch.argmin(weight / logcnt, dim=-1)`\n        # So the optimization is primarily in clarity.\n>>>>>>> REPLACE\n\n**Reasoning:**\nThe original code `(weight / logcnt).max(dim=-1).indices` is functionally equivalent to `torch.argmin(weight / logcnt, dim=-1)`. The refactored code makes this explicit and adds comments for better understanding. While this might not yield significant performance gains on its own, it improves code clarity. The key performance bottleneck might be the repeated computation of `weight / logcnt` and the subsequent `max` operation. If `weight` is large, this could be a factor. However, PyTorch's `max` and `argmin` are generally well-optimized. The primary goal here is to ensure the logic is clear and to see if any minor performance improvements can be gleaned from a more direct expression.\n\nGiven the historical context of significant speed score regression, the most likely culprit is a change within `rebalance_experts_hierarchical` or `balanced_packing` that has introduced overhead. The suggested changes to `balanced_packing` and `replicate_experts` focus on clarity and minor potential efficiency gains in these core functions. Without access to the previous, higher-performing code versions, it's difficult to pinpoint the exact regression. However, these changes are generally good practices for maintaining performance and readability.\n\nThe `rebalance_experts` function's conditional logic is a bit of a black box. If the `rebalance_experts_hierarchical` function itself is slow when `num_groups` is not a multiple of `num_nodes`, then a dedicated, simpler global balancing algorithm might be needed. However, for this task, we focus on improving the *current* code. The current approach of calling `rebalance_experts_hierarchical` with `num_groups=1, num_nodes=1` is a valid way to implement a global strategy using the existing hierarchical framework. The performance issue is likely in *how* it's implemented within that function for those parameters.\n\nSince the prompt asks for improvements to the *current* program, and the previous performance was much better, the most direct approach is to ensure the core algorithms (`balanced_packing`, `replicate_experts`) are as efficient and clean as possible. The `rebalance_experts_hierarchical` function itself is complex and its performance heavily depends on its internal implementation details, which are not fully provided here. The provided diffs aim to optimize the readily visible parts."]}}}