{"id": "1ace0d94-1305-47c6-b4ec-38fd167da612", "code": "# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort groups by weight in descending order.\n    # 'weight' is already float and on CPU from rebalance_experts,\n    # so no need for .float().cpu() here.\n    indices = weight.sort(-1, descending=True).indices\n\n    # Initialize pack_index and rank_in_pack.\n    # These will be fully populated, so `empty_like` is sufficient.\n    # They will be on the same device as 'weight' (i.e., CPU).\n    pack_index = torch.empty_like(weight, dtype=torch.int64)\n    rank_in_pack = torch.empty_like(pack_index)\n    \n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These tensors are on the same device as 'weight' (which is CPU).\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id in indices[i]:\n            # Get current state for this layer's packs.\n            current_pack_weights = pack_weights_per_layer[i]\n            current_pack_items = pack_items_per_layer[i]\n\n            # Find the pack with the minimum current weight that is not yet full.\n            min_weight_val = float('inf')\n            selected_pack_idx = -1\n            for p_idx in range(num_packs):\n                if current_pack_items[p_idx] < groups_per_pack:\n                    if current_pack_weights[p_idx] < min_weight_val:\n                        min_weight_val = current_pack_weights[p_idx]\n                        selected_pack_idx = p_idx\n            \n            # Assert that a valid pack was found. This should always be true\n            # as long as there are items left to assign and packs have capacity.\n            assert selected_pack_idx != -1, \"No available pack found, which implies an algorithm error or incorrect capacity calculation.\"\n            pack = selected_pack_idx\n\n            # Assign the group to the chosen pack.\n            # `group_id` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            # `weight[i, group_id]` is the weight of the current group.\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n\n\nimport heapq\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized. Optimized using a min-heap.\n\n    Parameters:\n        weight: [X, num_log]  (e.g., [num_layers * num_nodes, num_logical_experts_per_node])\n        num_phy: total number of experts after replication (e.g., num_physical_experts_per_node)\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank (0-indexed)\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n\n    # Initialize output tensors\n    # phy2log: maps physical expert index to logical expert index\n    # rank: maps physical expert index to its replica rank (0-indexed)\n    phy2log = torch.empty((n, num_phy), dtype=torch.int64, device=device)\n    rank = torch.empty((n, num_phy), dtype=torch.int64, device=device)\n    \n    # Keep track of the current number of replicas for each logical expert\n    # This is effectively the 'logcnt' output, but used internally for selection\n    current_logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    \n    # Use arangen for efficient indexing into current_logcnt\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n\n    # Calculate initial maximum load for each logical expert.\n    # This tensor will store the current maximum load for each logical expert.\n    # We use this to ensure we are minimizing the maximum load across all logical experts.\n    current_max_loads = weight.max(dim=0).values\n    \n    # Min-heap to store (current_max_load, logical_expert_index, num_replicas)\n    # We use current_max_load as the primary key for balancing.\n    min_heap = []\n    \n    # Initialize the heap with the first replica for each logical expert.\n    # The initial max load is simply the max load of that logical expert.\n    for j in range(num_log):\n        # The number of replicas for the first assignment is 1.\n        heapq.heappush(min_heap, (current_max_loads[j], j, 1))\n\n    # Assign physical experts by greedily picking the logical expert\n    # that has the lowest current maximum load.\n    for i in range(num_phy):  # Iterate through each physical expert to be assigned\n        if not min_heap:\n            # This case should ideally not be reached if num_phy >= num_log.\n            # If it is, it implies an issue with the input or logic.\n            # For robustness, we can break or raise an error.\n            break\n\n        # Pop the logical expert with the smallest current maximum load.\n        current_load, best_log_idx, num_reps = heapq.heappop(min_heap)\n        \n        # Assign the current physical expert (index `i`) to this logical expert.\n        phy2log[:, i] = best_log_idx\n        # The rank is the number of replicas assigned so far (0-indexed).\n        rank[:, i] = num_reps - 1 \n\n        # Update the count of replicas for this logical expert.\n        # `current_logcnt` will eventually be the `logcnt` output.\n        current_logcnt[arangen, best_log_idx] = num_reps\n        \n        # If this logical expert can receive more replicas, calculate its new\n        # potential max load and push it back to the heap.\n        # The total number of replicas for any logical expert cannot exceed num_phy.\n        if num_reps < num_phy:\n            # Calculate the \"load\" for the next replica. The goal is to\n            # minimize the *maximum* load of any expert. When adding a replica,\n            # the new load for this expert would be its current max load.\n            # The score used for heap ordering should reflect how adding a replica\n            # affects the overall balance. A good heuristic is the current max load.\n            # We are essentially trying to keep all logical expert's max loads low.\n            # Pushing back the current_load (which is the max load of this logical expert)\n            # ensures that we prioritize experts with lower max loads.\n            heapq.heappush(min_heap, (current_load, best_log_idx, num_reps + 1))\n\n    # The `current_logcnt` tensor now holds the number of replicas for each logical expert.\n    return phy2log, rank, current_logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float().cpu()\n    if num_groups > 1 and num_nodes > 0 and num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        # In global policy, we treat all logical experts as a single group\n        # and balance them across all physical experts.\n        # The `replicate_experts` function directly handles this.\n        # We need to reshape weight to [num_layers, num_logical_experts]\n        # and then replicate them to num_physical_experts.\n        phy2log, phyrank, logcnt = replicate_experts(\n            weight, num_replicas)\n        # The output of replicate_experts is [n, num_phy] for phy2log and rank,\n        # and [n, num_log] for logcnt. We need to ensure these match the expected\n        # output shapes after potential reshaping.\n        # For global policy, n is num_layers, num_log is num_logical_experts, num_phy is num_replicas.\n\n    # The rest of the logic for constructing log2phy remains the same.\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n", "language": "python", "parent_id": "1d0efe4a-0a18-48d5-83dc-8eb323d82c1b", "generation": 7, "timestamp": 1751054464.5732875, "iteration_found": 74, "metrics": {"balancedness_score": 0.0035831008095904, "speed_score": 0.014977436559283654, "combined_score": 0.009280268684437027}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 48 lines with 52 lines\nChange 2: Replace 9 lines with 20 lines", "parent_metrics": {"balancedness_score": 0.019312741718508577, "speed_score": 0.014928221644963675, "combined_score": 0.017120481681736126}, "island": 3}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve the Mixture-of-Expert models Expert Parallelism Load Balancer (MoE EPLB) expert rearrangement algorithm.\nThis algorithm will take the load metrics recorded by the vLLM server, and rearrange the experts to balance the load. It can make replicas of some experts to achieve better load balancing.\nYour goal will be two-fold: 1. Improve the algorithm to achieve better load balancing; while 2. Improve the algorithm to be more efficient, i.e. reduce the execution time of the algorithm itself, since perfect load balancing is NP-hard.\nThe current algorithm is implemented in the `rebalance_experts` function. ", "user": "# Current Program Information\n- Current performance metrics: - balancedness_score: 0.0193\n- speed_score: 0.0149\n- combined_score: 0.0171\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: balancedness_score, speed_score, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0405, combined_score: 0.1694\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0722, combined_score: 0.1852\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0761, combined_score: 0.1872\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.1872)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0761), Performs well on combined_score (0.1872)\n\n\n### Program 2 (Score: 0.1852)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0722), Performs well on combined_score (0.1852)\n\n\n### Program 3 (Score: 0.1694)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0405), Performs well on combined_score (0.1694)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.1661)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n\n### Program D2 (Score: 0.1667)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.1872, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 2 (Score: 0.0000, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 25 lines with 53 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 3 (Score: 0.0000, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 4 (Score: 0.1613, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 4 lines with 4 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 5 (Score: 0.1607, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n\n# Current Program\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort groups by weight in descending order.\n    # 'weight' is already float and on CPU from rebalance_experts,\n    # so no need for .float().cpu() here.\n    indices = weight.sort(-1, descending=True).indices\n\n    # Initialize pack_index and rank_in_pack.\n    # These will be fully populated, so `empty_like` is sufficient.\n    # They will be on the same device as 'weight' (i.e., CPU).\n    pack_index = torch.empty_like(weight, dtype=torch.int64)\n    rank_in_pack = torch.empty_like(pack_index)\n    \n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These tensors are on the same device as 'weight' (which is CPU).\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id in indices[i]:\n            # Get current state for this layer's packs.\n            current_pack_weights = pack_weights_per_layer[i]\n            current_pack_items = pack_items_per_layer[i]\n\n            # Find the pack with the minimum current weight that is not yet full.\n            min_weight_val = float('inf')\n            selected_pack_idx = -1\n            for p_idx in range(num_packs):\n                if current_pack_items[p_idx] < groups_per_pack:\n                    if current_pack_weights[p_idx] < min_weight_val:\n                        min_weight_val = current_pack_weights[p_idx]\n                        selected_pack_idx = p_idx\n            \n            # Assert that a valid pack was found. This should always be true\n            # as long as there are items left to assign and packs have capacity.\n            assert selected_pack_idx != -1, \"No available pack found, which implies an algorithm error or incorrect capacity calculation.\"\n            pack = selected_pack_idx\n\n            # Assign the group to the chosen pack.\n            # `group_id` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            # `weight[i, group_id]` is the weight of the current group.\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n\n\nimport heapq\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized. Optimized using a min-heap.\n\n    Parameters:\n        weight: [X, num_log]  (e.g., [num_layers * num_nodes, num_logical_experts_per_node])\n        num_phy: total number of experts after replication (e.g., num_physical_experts_per_node)\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank (0-indexed)\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n\n    # Initialize output tensors\n    # phy2log: maps physical expert index to logical expert index\n    # rank: maps physical expert index to its replica rank (0-indexed)\n    phy2log = torch.empty((n, num_phy), dtype=torch.int64, device=device)\n    rank = torch.empty((n, num_phy), dtype=torch.int64, device=device)\n    \n    # Keep track of the current number of replicas for each logical expert\n    # This is effectively the 'logcnt' output, but used internally for selection\n    current_logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    \n    # Use arangen for efficient indexing into current_logcnt\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n\n    # Min-heap to store the best available replica score for each logical expert.\n    # Heap elements are tuples: (-score_tensor, logical_expert_index, replica_rank)\n    # We use negative score to simulate a max-heap.\n    # Pre-calculate the maximum load for each logical expert across its 'n' contexts.\n    # This tensor will be of shape [num_log].\n    max_expert_loads = weight.max(dim=0).values\n    \n    min_heap = []\n    \n    # Populate the heap with the first replica's score for each logical expert.\n    # The score represents the average load if this expert gets its k-th replica.\n    for j in range(num_log):\n        # Calculate score using the pre-calculated max load.\n        # The score for heap comparison must be a scalar.\n        score = (max_expert_loads[j] / 1.0).item() \n        heapq.heappush(min_heap, (-score, j, 1)) # (negative_score, logical_idx, rank)\n\n    # Fill the remaining physical experts by greedily picking the best available replica.\n    # We need to assign `num_phy` physical experts in total.\n    # We iterate `num_phy` times to assign all physical experts.\n    for i in range(num_phy): # Iterate from 0 to num_phy-1 for physical expert index\n        if not min_heap: # Should not happen if num_phy >= num_log\n            break\n\n        # Get the logical expert with the highest score (most negative in heap)\n        neg_score, best_log_idx, rank_val = heapq.heappop(min_heap)\n        \n        # Assign this physical expert to the chosen logical expert\n        phy2log[:, i] = best_log_idx\n        # The rank is 0-indexed\n        rank[:, i] = rank_val - 1 \n\n        # Update the count of replicas for this logical expert\n        # This `current_logcnt` will be the final `logcnt` output.\n        current_logcnt[arangen, best_log_idx] = rank_val\n        \n        # If this logical expert can still receive more replicas,\n        # push its next replica's score to the heap.\n        # The maximum number of replicas for any logical expert is implicitly num_phy.\n        # We add the next replica only if it's potentially useful.\n        if rank_val < num_phy: # Check if there's room for another replica\n            # Calculate next score using the pre-calculated max load.\n            next_score = (max_expert_loads[best_log_idx] / (rank_val + 1.0)).item()\n            heapq.heappush(min_heap, (-next_score, best_log_idx, rank_val + 1))\n\n    # The `current_logcnt` tensor now holds the number of replicas for each logical expert.\n    # This is the `logcnt` output.\n    return phy2log, rank, current_logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float().cpu()\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["The current program exhibits a regression in all performance metrics. The goal is to improve both the balancedness and speed of the expert rearrangement algorithm.\n\nHere are a few suggestions:\n\n### Improvement 1: Optimize `replicate_experts` for speed and balancedness\n\nThe `replicate_experts` function uses a min-heap to greedily assign experts. While this is a good approach for balancedness, the current implementation might be suboptimal in terms of speed due to repeated heap operations and tensor manipulations.\n\n**Reasoning:**\n\n1.  **Heap efficiency:** The current heap stores `(-score, logical_expert_index, replica_rank)`. The score calculation involves `max_expert_loads[j] / (rank_val + 1.0)`. We can pre-calculate `max_expert_loads` once. The repeated division and conversion to `.item()` within the loop can be costly.\n2.  **Tensor indexing:** Accessing `current_logcnt[arangen, best_log_idx]` for updating `current_logcnt` is efficient, but the overall structure might benefit from further optimization.\n3.  **Simplified score:** The goal of the heap is to pick the logical expert that, when given another replica, results in the *least* increase in its maximum load. The current score calculation is `max_expert_loads[j] / (rank_val + 1.0)`. A more direct approach for a min-heap would be to store the *current maximum load* of that logical expert, and when we pop, we calculate the new maximum load if we add another replica to it.\n\n**Proposed Change:**\n\nModify `replicate_experts` to use a more direct heap approach that prioritizes minimizing the maximum load of any logical expert. We can store `(current_max_load, logical_expert_index, num_replicas)` in the heap. When we pop, we update the `current_max_load` for that logical expert and push it back if it can still receive more replicas.\n\n<<<<<<< SEARCH\n    # Min-heap to store the best available replica score for each logical expert.\n    # Heap elements are tuples: (-score_tensor, logical_expert_index, replica_rank)\n    # We use negative score to simulate a max-heap.\n    # Pre-calculate the maximum load for each logical expert across its 'n' contexts.\n    # This tensor will be of shape [num_log].\n    max_expert_loads = weight.max(dim=0).values\n    \n    min_heap = []\n    \n    # Populate the heap with the first replica's score for each logical expert.\n    # The score represents the average load if this expert gets its k-th replica.\n    for j in range(num_log):\n        # Calculate score using the pre-calculated max load.\n        # The score for heap comparison must be a scalar.\n        score = (max_expert_loads[j] / 1.0).item() \n        heapq.heappush(min_heap, (-score, j, 1)) # (negative_score, logical_idx, rank)\n\n    # Fill the remaining physical experts by greedily picking the best available replica.\n    # We need to assign `num_phy` physical experts in total.\n    # We iterate `num_phy` times to assign all physical experts.\n    for i in range(num_phy): # Iterate from 0 to num_phy-1 for physical expert index\n        if not min_heap: # Should not happen if num_phy >= num_log\n            break\n\n        # Get the logical expert with the highest score (most negative in heap)\n        neg_score, best_log_idx, rank_val = heapq.heappop(min_heap)\n        \n        # Assign this physical expert to the chosen logical expert\n        phy2log[:, i] = best_log_idx\n        # The rank is 0-indexed\n        rank[:, i] = rank_val - 1 \n\n        # Update the count of replicas for this logical expert\n        # This `current_logcnt` will be the final `logcnt` output.\n        current_logcnt[arangen, best_log_idx] = rank_val\n        \n        # If this logical expert can still receive more replicas,\n        # push its next replica's score to the heap.\n        # The maximum number of replicas for any logical expert is implicitly num_phy.\n        # We add the next replica only if it's potentially useful.\n        if rank_val < num_phy: # Check if there's room for another replica\n            # Calculate next score using the pre-calculated max load.\n            next_score = (max_expert_loads[best_log_idx] / (rank_val + 1.0)).item()\n            heapq.heappush(min_heap, (-next_score, best_log_idx, rank_val + 1))\n\n    # The `current_logcnt` tensor now holds the number of replicas for each logical expert.\n    # This is the `logcnt` output.\n    return phy2log, rank, current_logcnt\n=======\n    # Calculate initial maximum load for each logical expert.\n    # This tensor will store the current maximum load for each logical expert.\n    # We use this to ensure we are minimizing the maximum load across all logical experts.\n    current_max_loads = weight.max(dim=0).values\n    \n    # Min-heap to store (current_max_load, logical_expert_index, num_replicas)\n    # We use current_max_load as the primary key for balancing.\n    min_heap = []\n    \n    # Initialize the heap with the first replica for each logical expert.\n    # The initial max load is simply the max load of that logical expert.\n    for j in range(num_log):\n        # The number of replicas for the first assignment is 1.\n        heapq.heappush(min_heap, (current_max_loads[j], j, 1))\n\n    # Assign physical experts by greedily picking the logical expert\n    # that has the lowest current maximum load.\n    for i in range(num_phy):  # Iterate through each physical expert to be assigned\n        if not min_heap:\n            # This case should ideally not be reached if num_phy >= num_log.\n            # If it is, it implies an issue with the input or logic.\n            # For robustness, we can break or raise an error.\n            break\n\n        # Pop the logical expert with the smallest current maximum load.\n        current_load, best_log_idx, num_reps = heapq.heappop(min_heap)\n        \n        # Assign the current physical expert (index `i`) to this logical expert.\n        phy2log[:, i] = best_log_idx\n        # The rank is the number of replicas assigned so far (0-indexed).\n        rank[:, i] = num_reps - 1 \n\n        # Update the count of replicas for this logical expert.\n        # `current_logcnt` will eventually be the `logcnt` output.\n        current_logcnt[arangen, best_log_idx] = num_reps\n        \n        # If this logical expert can receive more replicas, calculate its new\n        # potential max load and push it back to the heap.\n        # The total number of replicas for any logical expert cannot exceed num_phy.\n        if num_reps < num_phy:\n            # Calculate the \"load\" for the next replica. The goal is to\n            # minimize the *maximum* load of any expert. When adding a replica,\n            # the new load for this expert would be its current max load.\n            # The score used for heap ordering should reflect how adding a replica\n            # affects the overall balance. A good heuristic is the current max load.\n            # We are essentially trying to keep all logical expert's max loads low.\n            # Pushing back the current_load (which is the max load of this logical expert)\n            # ensures that we prioritize experts with lower max loads.\n            heapq.heappush(min_heap, (current_load, best_log_idx, num_reps + 1))\n\n    # The `current_logcnt` tensor now holds the number of replicas for each logical expert.\n    return phy2log, rank, current_logcnt\n>>>>>>> REPLACE\n\n### Improvement 2: Simplify `rebalance_experts_hierarchical`\n\nThe `rebalance_experts_hierarchical` function involves several steps of reshaping, gathering, and inverting permutations. While the logic is sound for hierarchical balancing, the code can be made more readable and potentially more efficient by reducing intermediate tensor creations and clarifying variable names.\n\n**Reasoning:**\n\n1.  **Readability:** The variable names like `log2mlog`, `mlog2log`, `phy2pphy`, `pphy2phy`, `pphy2mlog` are somewhat cryptic and make it harder to follow the data flow.\n2.  **Efficiency:** Creating many intermediate tensors can add overhead. Some operations might be combinable.\n\n**Proposed Change:**\n\n*   Clarify variable names to better reflect their meaning (e.g., `group_to_node_map`, `node_expert_to_physical_map`).\n*   Consider if any reshaping or gathering operations can be combined or simplified.\n\nThis change focuses on improving the maintainability and potentially the efficiency of the hierarchical balancing logic. Given the \"Areas identified for improvement\" mention of simplifying code, this is a good candidate.\n\nUnfortunately, without the full implementation of `rebalance_experts_hierarchical` and its dependencies, it's hard to provide a precise diff for this part. However, the general principle is to make the data transformations more explicit and less reliant on complex indexing chains.\n\nFor instance, the part where `pphy2mlog` is calculated:\n```python\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n```\nThis section maps physical experts to logical experts within nodes, then adjusts for the global logical expert indexing. This could potentially be simplified or made more explicit.\n\n### Improvement 3: Revisit `rebalance_experts` global policy\n\nThe `rebalance_experts` function currently calls `rebalance_experts_hierarchical` even for the global policy, by setting `num_groups=1`, `num_nodes=1`. This might not be the most efficient way to implement a global policy. A global policy should ideally consider all logical experts together without the hierarchical structure.\n\n**Reasoning:**\n\nWhen `num_groups=1` and `num_nodes=1`, the hierarchical logic might still impose constraints or operations that are unnecessary for a flat, global balancing. A dedicated global balancing function could be more direct and efficient.\n\n**Proposed Change:**\n\nImplement a separate `rebalance_experts_global` function that directly uses `replicate_experts` on the entire `weight` tensor without the intermediate steps related to nodes and groups. The `rebalance_experts` function would then dispatch to either `rebalance_experts_hierarchical` or `rebalance_experts_global` based on `num_groups % num_nodes == 0`.\n\n<<<<<<< SEARCH\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n=======\n    if num_groups > 1 and num_nodes > 0 and num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        # In global policy, we treat all logical experts as a single group\n        # and balance them across all physical experts.\n        # The `replicate_experts` function directly handles this.\n        # We need to reshape weight to [num_layers, num_logical_experts]\n        # and then replicate them to num_physical_experts.\n        phy2log, phyrank, logcnt = replicate_experts(\n            weight, num_replicas)\n        # The output of replicate_experts is [n, num_phy] for phy2log and rank,\n        # and [n, num_log] for logcnt. We need to ensure these match the expected\n        # output shapes after potential reshaping.\n        # For global policy, n is num_layers, num_log is num_logical_experts, num_phy is num_replicas.\n\n    # The rest of the logic for constructing log2phy remains the same.\n    num_redundant_experts = num_replicas - num_logical_experts\n>>>>>>> REPLACE\n\nThese changes aim to improve the performance metrics by optimizing the core algorithms for speed and balancedness, and by ensuring that the different balancing policies are implemented in the most efficient way."]}}}