{"id": "b6e3e7a7-b729-4e92-895e-4e3152f8efc6", "code": "# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    indices = weight.float().sort(-1, descending=True).indices  # Keep on original device\n    pack_index = torch.full_like(weight,\n                                 fill_value=-1,\n                                 dtype=torch.int64,\n                                 device=weight.device)  # Use weight's device\n    rank_in_pack = torch.full_like(pack_index,\n                                   fill_value=-1,\n                                   device=weight.device)  # Use weight's device\n\n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These are on the same device as the input 'weight'.\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Pre-create a tensor for infinity, to avoid recreating in loop\n    inf_tensor = torch.tensor(float('inf'), device=weight.device, dtype=weight.dtype)\n\n    # Create a tensor for layer indices for advanced indexing\n    layer_indices = torch.arange(num_layers, device=weight.device)\n\n    # Iterate over the rank of experts (from heaviest to lightest) for all layers in parallel\n    for k in range(num_groups):  # k represents the rank (0 = heaviest, num_groups-1 = lightest)\n        # current_group_ids: [num_layers] - the actual expert ID for the k-th heaviest expert in each layer\n        current_group_ids = indices[:, k]\n\n        # Get current state for all layers' packs.\n        # These are slices of the larger tensors, directly modifiable.\n        current_pack_weights = pack_weights_per_layer\n        current_pack_items = pack_items_per_layer\n\n        # Find available packs (those not yet full) for all layers.\n        available_mask = (current_pack_items < groups_per_pack)  # [num_layers, num_packs]\n\n        # Conditionally set weights to infinity for unavailable packs.\n        candidate_weights = torch.where(available_mask, current_pack_weights, inf_tensor)\n\n        # Find the pack with the minimum current weight for each layer.\n        # packs_to_assign: [num_layers]\n        packs_to_assign = torch.argmin(candidate_weights, dim=-1)\n\n        # Assign the group to the chosen pack for all layers.\n        # pack_index[layer_indices, current_group_ids] is a direct assignment to the correct locations.\n        pack_index[layer_indices, current_group_ids] = packs_to_assign\n        # rank_in_pack is the count *before* incrementing pack_items_per_layer\n        rank_in_pack[layer_indices, current_group_ids] = current_pack_items[layer_indices, packs_to_assign]\n\n        # Update pack weights and item counts for the chosen packs for all layers.\n        # weights_to_add: [num_layers] - the actual weight of the current group for each layer\n        weights_to_add = weight[layer_indices, current_group_ids]\n\n        # Update pack weights by adding weights_to_add to the chosen packs.\n        # Use scatter_add for efficient batch update.\n        # Needs to be unsqueezed to match scatter_add's dim requirements\n        pack_weights_per_layer.scatter_add_(\n            1, packs_to_assign.unsqueeze(-1), weights_to_add.unsqueeze(-1)\n        )\n\n        # Increment pack_items_per_layer for the chosen packs.\n        pack_items_per_layer.scatter_add_(\n            1, packs_to_assign.unsqueeze(-1), torch.ones_like(weights_to_add, dtype=torch.int64).unsqueeze(-1)\n        )\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        # To minimize the maximum load, we should add replicas to the\n        # logical experts with the minimum current average load.\n        # Reverting to original behavior as suggested by performance metrics.\n        # This strategy aims to reduce the load of the currently most loaded experts.\n        # To minimize the maximum load, we should add replicas to the\n        # logical experts with the minimum current average load.\n        # The previous implementation used `max`, which might have been incorrect\n        # for minimizing the maximum load. Changing to `min` aligns with the\n        # goal of distributing load to the least loaded experts.\n        redundant_indices = (weight / logcnt).min(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float()  # Ensure float, but keep on original device (e.g., GPU)\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    # Ensure log2phy is created on the same device as other tensors\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=weight.device,  # Use weight's device\n    )\n    # The scatter_ operation and arange should also use the correct device\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=weight.device).expand(num_layers, -1),  # Use weight's device\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n", "language": "python", "parent_id": "1f5cbf0e-985f-4f6d-a33c-2d36edf7f4ea_migrant_1_migrant_2_migrant_1", "generation": 6, "timestamp": 1751063794.8669982, "iteration_found": 224, "metrics": {"balancedness_score": 0.14515957884623945, "speed_score": 0.08343971771153484, "combined_score": 0.11429964827888714}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace redundant_indices = (weight / logcnt).max(dim=-1).indices with 6 lines", "parent_metrics": {"balancedness_score": 0.29830835153138974, "speed_score": 0.08339019090906938, "combined_score": 0.19084927122022954}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve the Mixture-of-Expert models Expert Parallelism Load Balancer (MoE EPLB) expert rearrangement algorithm.\nThis algorithm will take the load metrics recorded by the vLLM server, and rearrange the experts to balance the load. It can make replicas of some experts to achieve better load balancing.\nYour goal will be two-fold: 1. Improve the algorithm to achieve better load balancing; while 2. Improve the algorithm to be more efficient, i.e. reduce the execution time of the algorithm itself, since perfect load balancing is NP-hard.\nThe current algorithm is implemented in the `rebalance_experts` function. ", "user": "# Current Program Information\n- Current performance metrics: - balancedness_score: 0.2983\n- speed_score: 0.0834\n- combined_score: 0.1908\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: balancedness_score, speed_score, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1103, combined_score: 0.2315\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1103, combined_score: 0.2315\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1103, combined_score: 0.2315\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.2315)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1103), Performs well on combined_score (0.2315)\n\n\n### Program 2 (Score: 0.2315)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1103), Performs well on combined_score (0.2315)\n\n\n### Program 3 (Score: 0.2315)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1103), Performs well on combined_score (0.2315)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.2315)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n\n### Program D2 (Score: 0.2315)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.2315, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 32 lines with 32 lines, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 2 (Score: 0.1920, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 3 (Score: 0.0000, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 43 lines with 37 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 4 (Score: 0.1623, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 5 (Score: 0.1628, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 6 lines with 8 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n\n# Current Program\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    indices = weight.float().sort(-1, descending=True).indices  # Keep on original device\n    pack_index = torch.full_like(weight,\n                                 fill_value=-1,\n                                 dtype=torch.int64,\n                                 device=weight.device)  # Use weight's device\n    rank_in_pack = torch.full_like(pack_index,\n                                   fill_value=-1,\n                                   device=weight.device)  # Use weight's device\n\n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These are on the same device as the input 'weight'.\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Pre-create a tensor for infinity, to avoid recreating in loop\n    inf_tensor = torch.tensor(float('inf'), device=weight.device, dtype=weight.dtype)\n\n    # Create a tensor for layer indices for advanced indexing\n    layer_indices = torch.arange(num_layers, device=weight.device)\n\n    # Iterate over the rank of experts (from heaviest to lightest) for all layers in parallel\n    for k in range(num_groups):  # k represents the rank (0 = heaviest, num_groups-1 = lightest)\n        # current_group_ids: [num_layers] - the actual expert ID for the k-th heaviest expert in each layer\n        current_group_ids = indices[:, k]\n\n        # Get current state for all layers' packs.\n        # These are slices of the larger tensors, directly modifiable.\n        current_pack_weights = pack_weights_per_layer\n        current_pack_items = pack_items_per_layer\n\n        # Find available packs (those not yet full) for all layers.\n        available_mask = (current_pack_items < groups_per_pack)  # [num_layers, num_packs]\n\n        # Conditionally set weights to infinity for unavailable packs.\n        candidate_weights = torch.where(available_mask, current_pack_weights, inf_tensor)\n\n        # Find the pack with the minimum current weight for each layer.\n        # packs_to_assign: [num_layers]\n        packs_to_assign = torch.argmin(candidate_weights, dim=-1)\n\n        # Assign the group to the chosen pack for all layers.\n        # pack_index[layer_indices, current_group_ids] is a direct assignment to the correct locations.\n        pack_index[layer_indices, current_group_ids] = packs_to_assign\n        # rank_in_pack is the count *before* incrementing pack_items_per_layer\n        rank_in_pack[layer_indices, current_group_ids] = current_pack_items[layer_indices, packs_to_assign]\n\n        # Update pack weights and item counts for the chosen packs for all layers.\n        # weights_to_add: [num_layers] - the actual weight of the current group for each layer\n        weights_to_add = weight[layer_indices, current_group_ids]\n\n        # Update pack weights by adding weights_to_add to the chosen packs.\n        # Use scatter_add for efficient batch update.\n        # Needs to be unsqueezed to match scatter_add's dim requirements\n        pack_weights_per_layer.scatter_add_(\n            1, packs_to_assign.unsqueeze(-1), weights_to_add.unsqueeze(-1)\n        )\n\n        # Increment pack_items_per_layer for the chosen packs.\n        pack_items_per_layer.scatter_add_(\n            1, packs_to_assign.unsqueeze(-1), torch.ones_like(weights_to_add, dtype=torch.int64).unsqueeze(-1)\n        )\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        # To minimize the maximum load, we should add replicas to the\n        # logical experts with the minimum current average load.\n        # Reverting to original behavior as suggested by performance metrics.\n        # This strategy aims to reduce the load of the currently most loaded experts.\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float()  # Ensure float, but keep on original device (e.g., GPU)\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    # Ensure log2phy is created on the same device as other tensors\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=weight.device,  # Use weight's device\n    )\n    # The scatter_ operation and arange should also use the correct device\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=weight.device).expand(num_layers, -1),  # Use weight's device\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["The current program has a `rebalance_experts` function that calls `rebalance_experts_hierarchical` or a simplified version of it. The performance scores indicate a regression. Let's analyze the code and identify potential areas for improvement.\n\n**Observations and Potential Improvements:**\n\n1.  **`replicate_experts` Function:**\n    *   The current implementation of `replicate_experts` has a loop that iterates `num_log` to `num_phy`. Inside this loop, it calculates `(weight / logcnt).max(dim=-1).indices`. This operation involves division and then finding the maximum, which can be computationally intensive, especially if `num_log` and `num_phy` are large.\n    *   The comment \"Reverting to original behavior as suggested by performance metrics. This strategy aims to reduce the load of the currently most loaded experts\" suggests that the previous implementation might have been more efficient or effective. However, the current implementation's logic of picking the minimum average load might not be optimal for minimizing the *maximum* load. A greedy approach that always picks the least loaded expert might lead to a suboptimal global solution.\n    *   **Improvement Idea:** Consider a more direct approach to distributing replicas. Instead of repeatedly finding the minimum average load, we could sort the experts by their current load and distribute replicas to the least loaded ones in a round-robin fashion until `num_redundant` replicas are assigned. This could potentially be more efficient and lead to better load balancing.\n\n2.  **`balanced_packing` Function:**\n    *   The `balanced_packing` function uses `torch.where` and `torch.argmin` within a loop. While this is vectorized across layers, the inner loop iterates through `num_groups`.\n    *   The use of `inf_tensor` is correct for masking.\n    *   **Improvement Idea:** For `balanced_packing`, the core idea is to assign items to bins to minimize the maximum bin sum. This is a variation of the multiway number partitioning problem, which is NP-hard. The current greedy approach (assigning the next heaviest item to the currently lightest bin) is a common heuristic. However, we can explore if there are ways to optimize the selection process or data structures used. For instance, using a min-heap for `pack_weights_per_layer` could potentially speed up finding the minimum weight pack, though PyTorch's `argmin` is often highly optimized. Given the current performance regression, it might be that the complexity of `inf_tensor` creation or `torch.where` is contributing to slowdowns.\n\n3.  **`rebalance_experts_hierarchical` Function:**\n    *   This function orchestrates the process, breaking it down into packing groups to nodes, replicating experts within nodes, and packing physical experts to GPUs.\n    *   The `inverse` function is a helper to get the inverse permutation.\n    *   The logic involving `gather`, `view`, and `flatten` is standard for tensor manipulation.\n    *   **Improvement Idea:** The overall structure seems reasonable for a hierarchical approach. The performance issues might stem from the underlying `replicate_experts` and `balanced_packing` calls. However, there might be opportunities to optimize the data flow or reduce redundant computations. For example, the calculation of `log2mlog` and `mlog2log` involves several tensor operations.\n\n4.  **`rebalance_experts` Entry Point:**\n    *   This function acts as a dispatcher. The conditional call to `rebalance_experts_hierarchical` is based on `num_groups % num_nodes == 0`.\n    *   The creation of `log2phy` and the subsequent `scatter_` operation are crucial for mapping.\n    *   **Improvement Idea:** The `weight = weight.float()` conversion is done at the beginning. This is good for consistency. The device placement seems to be handled correctly by using `weight.device`.\n\n**Focusing on the Regression and Efficiency:**\n\nThe prompt specifically mentions that the current scores are regressed and that the algorithm needs to be more efficient. The `replicate_experts` function seems like a prime candidate for optimization due to its iterative nature and the `max(dim=-1)` operation.\n\nLet's try to improve `replicate_experts` by using a more direct replica assignment strategy. The goal is to minimize the maximum load. This can be achieved by iteratively assigning replicas to the experts that currently have the *least* total weight.\n\n**Proposed Change Strategy:**\n\nModify `replicate_experts` to use a more efficient greedy assignment strategy for distributing replicas. Instead of repeatedly calculating the average load and finding the maximum, we can maintain the current load of each logical expert and iteratively assign replicas to the least loaded ones.\n\nHere's a breakdown of the proposed change for `replicate_experts`:\n\n1.  **Initialization:**\n    *   `logcnt` is initialized to 1 for all logical experts.\n    *   `phy2log` and `rank` are initialized.\n2.  **Iterative Assignment:**\n    *   For each of the `num_redundant` replicas to be added:\n        *   Calculate the current load of each logical expert by `weight / logcnt`.\n        *   Find the logical expert with the *minimum* current load.\n        *   Assign the next physical expert to this least loaded logical expert.\n        *   Update `logcnt` for that logical expert.\n        *   Update `phy2log` and `rank` for the newly assigned physical expert.\n\nThe current implementation already does something similar by finding `(weight / logcnt).max(dim=-1).indices`, which is actually selecting the expert with the *highest* average load if we were minimizing maximum load. The comment \"This strategy aims to reduce the load of the currently most loaded experts\" implies this.\n\nHowever, the prompt also states \"Reverting to original behavior as suggested by performance metrics.\" This is confusing. If the current behavior is a regression, and the \"original behavior\" was better, then the goal is to bring back that better behavior.\n\nLet's re-examine the `replicate_experts` function with the goal of improving efficiency and potentially balancedness. The current implementation:\n\n```python\n    for i in range(num_log, num_phy):\n        # To minimize the maximum load, we should add replicas to the\n        # logical experts with the minimum current average load.\n        # Reverting to original behavior as suggested by performance metrics.\n        # This strategy aims to reduce the load of the currently most loaded experts.\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n```\n\nThe comment is contradictory: \"minimize the maximum load... add replicas to the logical experts with the minimum current average load.\" but then it says \"This strategy aims to reduce the load of the currently most loaded experts.\" and `(weight / logcnt).max(dim=-1).indices` selects the *most* loaded expert (based on average load).\n\nTo truly minimize the maximum load, we should add replicas to the experts that are *least* loaded. This is a greedy approach.\n\nLet's assume the goal is to minimize the maximum load. The current code seems to be trying to reduce the load of the *most* loaded experts, which is a valid strategy for load balancing, but the comment is confusing.\n\nIf we want to *minimize the maximum load*, we should add replicas to the *least* loaded experts.\n\nLet's assume the \"original behavior\" was indeed to add replicas to the least loaded experts. The current code might have been intended for something else or the comment is misleading.\n\nTo make it more efficient and potentially better balanced, let's try to implement the \"add to least loaded\" strategy more directly.\n\nConsider the `replicate_experts` function. We want to distribute `num_redundant` replicas. Each logical expert starts with 1 replica. We should add the remaining replicas to the logical experts that have the smallest current load *per replica*.\n\nThe current calculation `weight / logcnt` gives the current load per replica for each logical expert. Finding the `max(dim=-1).indices` selects the logical expert with the *highest* load per replica. If we want to minimize the maximum load, we should select the logical expert with the *lowest* load per replica.\n\nLet's try changing `max` to `min` and see if that aligns with the goal of minimizing maximum load and potentially improves scores.\n\n```python\n# Current:\nredundant_indices = (weight / logcnt).max(dim=-1).indices\n# Proposed change:\nredundant_indices = (weight / logcnt).min(dim=-1).indices\n```\n\nThis change directly addresses the goal of minimizing the maximum load by always adding replicas to the least loaded logical experts. This *should* improve the `balancedness_score` and potentially the `combined_score`.\n\nRegarding efficiency, the `max(dim=-1)` and `min(dim=-1)` operations are generally efficient. The primary concern might be the repeated calculation if `num_redundant` is large. However, without specific profiling, it's hard to say if this is the bottleneck.\n\nLet's consider another angle: the `balanced_packing` function. The current implementation iterates through `num_groups`. If `num_groups` is large, this loop can be slow.\n\nIn `balanced_packing`:\n`indices = weight.float().sort(-1, descending=True).indices`\nThis sorts the experts within each layer.\n\n`for k in range(num_groups):`\nThis loop iterates through the sorted experts.\n\n`available_mask = (current_pack_items < groups_per_pack)`\n`candidate_weights = torch.where(available_mask, current_pack_weights, inf_tensor)`\n`packs_to_assign = torch.argmin(candidate_weights, dim=-1)`\n\nThe `torch.where` and `torch.argmin` are applied to `num_layers * num_packs` elements.\n\nIf we can avoid the explicit loop over `k` for `num_groups`, that would be an improvement. However, the logic of assigning the *k*-th heaviest item to the currently lightest pack is inherently iterative.\n\nLet's reconsider the `replicate_experts` function. The current strategy is to pick the expert with the highest average load and add a replica. This is a common greedy strategy to reduce the maximum load. However, the comment \"Reverting to original behavior as suggested by performance metrics\" implies the previous version was better. The performance metrics *did* regress.\n\nIf the \"original behavior\" was to add replicas to the *least* loaded experts, then changing `max` to `min` would be the correct approach to improve `balancedness_score`.\n\nLet's assume the \"original behavior\" referred to a more efficient way of distributing replicas, rather than necessarily picking the least loaded.\n\nConsider the total number of replicas needed: `num_replicas`.\nWe have `num_logical_experts`.\nEach logical expert starts with 1 replica.\nWe need to distribute `num_replicas - num_logical_experts` additional replicas.\n\nIf we simply want to distribute these additional replicas as evenly as possible to minimize the maximum load, we can think of it as a load balancing problem.\n\nThe `replicate_experts` function is called with `tokens_per_mlog` which is `weight.gather(-1, mlog2log).view(-1, num_logical_experts // num_nodes)`. This means it's operating on experts grouped by node.\n\nLet's focus on the `replicate_experts` function's core logic. The current approach is:\n1. Calculate current load per replica: `weight / logcnt`.\n2. Find the logical expert with the maximum load per replica: `(weight / logcnt).max(dim=-1).indices`.\n3. Assign the next physical expert to this expert.\n4. Increment the replica count for that expert.\n\nThis is a greedy approach to reduce the maximum load. The problem might be that this greedy choice is not always optimal, or the implementation is inefficient.\n\nGiven the performance regression, the most likely culprit for a slowdown is the repeated calculation within the loop.\n\nLet's look at the `rebalance_experts_hierarchical` function again.\nStep 1: `tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)`\nStep 2: `tokens_per_mlog = weight.gather(-1, mlog2log).view(-1, num_logical_experts // num_nodes)`\nStep 3: `tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)`\n\nThe `replicate_experts` function is called in Step 2.\n\nThe prompt asks for improvements to achieve *better load balancing* and *more efficiency*.\n\n**Hypothesis:** The current `replicate_experts` function is inefficient because it recomputes `weight / logcnt` and `max(dim=-1)` in each iteration of the loop. A more efficient way would be to use a data structure that allows faster selection of the minimum/maximum.\n\nA potential optimization for `replicate_experts` without changing the fundamental greedy strategy (reducing the load of the currently most loaded experts) would be to use a min-heap (or max-heap in this case, as we want the max load) to keep track of the loads of logical experts.\n\nHowever, implementing a heap efficiently in PyTorch within this context can be tricky.\n\nLet's consider a simpler change that might improve efficiency and potentially balance, based on the idea of distributing replicas more evenly.\n\nIf we want to distribute `N` items (replicas) into `K` bins (logical experts) to minimize the maximum bin sum, and each bin already has some initial load, we can do the following:\n1. Calculate the total load of each logical expert.\n2. Calculate the current number of replicas for each logical expert (`logcnt`).\n3. For each of the remaining replicas:\n    a. Find the logical expert that currently has the minimum load *per replica*.\n    b. Assign the replica to that expert.\n    c. Update the load and replica count for that expert.\n\nThis is exactly what the comment in `replicate_experts` suggests: \"add replicas to the logical experts with the minimum current average load.\" But the code uses `max`! This is a direct contradiction.\n\nIf the goal is to *minimize the maximum load*, then we should add replicas to the *least* loaded experts.\n\nLet's propose changing `max` to `min` in `replicate_experts` to align with the stated goal of minimizing maximum load and potentially recover lost performance.\n\n```python\n# Current Program Analysis:\n# The current implementation of `replicate_experts` aims to reduce the load of\n# the currently most loaded experts by adding replicas to them. However, the\n# comment within the function states that to \"minimize the maximum load, we\n# should add replicas to the logical experts with the minimum current average\n# load.\" This is a contradiction.\n#\n# To achieve better load balancing (minimizing the maximum load), we should\n# indeed add replicas to the logical experts that are currently least loaded.\n# The current code uses `(weight / logcnt).max(dim=-1).indices`, which selects\n# the *most* loaded experts. Changing this to `min` will align the implementation\n# with the stated goal of minimizing maximum load. This change is also likely\n# to improve the balancedness_score.\n#\n# Regarding efficiency, while the `max`/`min` operations are efficient, the\n# repeated calculation of `weight / logcnt` in each loop iteration might be\n# a minor overhead. However, without profiling, it's hard to pinpoint. The\n# primary issue is likely the strategy itself if it's not correctly implemented\n# for the desired outcome.\n#\n# We will focus on correcting the strategy in `replicate_experts` to align with\n# minimizing the maximum load, which is a common goal for load balancing.\n\n```\n\nHere is the proposed change:\n\n<<<<<<< SEARCH\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n=======\n        # To minimize the maximum load, we should add replicas to the\n        # logical experts with the minimum current average load.\n        # The previous implementation used `max`, which might have been incorrect\n        # for minimizing the maximum load. Changing to `min` aligns with the\n        # goal of distributing load to the least loaded experts.\n        redundant_indices = (weight / logcnt).min(dim=-1).indices\n>>>>>>> REPLACE\n\n**Reasoning for the change:**\n\nThe core objective of load balancing in MoE is to distribute the workload evenly across experts. When replicating experts, a common strategy to minimize the *maximum* load is to add replicas to the experts that are currently experiencing the *least* load per replica. The original code, however, uses `(weight / logcnt).max(dim=-1).indices`, which selects the logical expert with the *highest* current load per replica. This strategy aims to reduce the peak load but might not be the most effective for overall minimization of the maximum load in a greedy sense.\n\nBy changing `max` to `min`, we are adopting a greedy strategy that prioritizes balancing the load by always assigning the next replica to the least loaded logical expert. This is a more direct approach to minimizing the maximum load and is expected to improve the `balancedness_score`.\n\nThe comment within the function itself supports this change: \"To minimize the maximum load, we should add replicas to the logical experts with the minimum current average load.\" The original code contradicted this comment. This change rectifies that contradiction.\n\nWhile the efficiency of `min` vs. `max` is similar, aligning the strategy with the stated goal is crucial for improving the `balancedness_score`. If further efficiency gains are needed, profiling would be required to identify other bottlenecks, potentially in tensor operations or data movement. However, fixing the fundamental logic is the priority given the performance regression."]}}}