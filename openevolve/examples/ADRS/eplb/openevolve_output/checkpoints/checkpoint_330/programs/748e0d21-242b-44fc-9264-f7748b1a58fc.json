{"id": "748e0d21-242b-44fc-9264-f7748b1a58fc", "code": "# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort groups by weight in descending order.\n    # 'weight' is already float and on CPU from rebalance_experts,\n    # so no need for .float().cpu() here.\n    indices = weight.sort(-1, descending=True).indices\n\n    # Initialize pack_index and rank_in_pack.\n    # These will be fully populated, so `empty_like` is sufficient.\n    # They will be on the same device as 'weight' (i.e., CPU).\n    pack_index = torch.empty_like(weight, dtype=torch.int64)\n    rank_in_pack = torch.empty_like(pack_index)\n    \n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These tensors are on the same device as 'weight' (which is CPU).\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id in indices[i]:\n            # Get current state for this layer's packs.\n            current_pack_weights = pack_weights_per_layer[i]\n            current_pack_items = pack_items_per_layer[i]\n\n            # Find the pack with the minimum current weight among available ones.\n            # Use torch.where to effectively mask out full packs with infinity,\n            # then find the argmin. This avoids an explicit clone and subsequent\n            # masked assignment, potentially being more efficient.\n            pack = torch.argmin(\n                torch.where(current_pack_items < groups_per_pack,\n                            current_pack_weights,\n                            torch.full_like(current_pack_weights, float('inf')))\n            )\n\n            # Assert that a valid pack was found and it has space.\n            # Using pack directly as a 0-dim tensor for indexing.\n            assert current_pack_items[pack] < groups_per_pack, \\\n                f\"No available packs found, but expected to find one. Pack {pack} is full. \" \\\n                f\"Current pack items: {current_pack_items}, groups_per_pack: {groups_per_pack}\"\n\n            # Assign the group to the chosen pack.\n            # group_id is a 0-dim tensor, so it works directly for indexing.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            # weight[i, group_id] is the weight of the current group (0-dim tensor).\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # Calculate tokens per group for each node\n    tokens_per_group_per_node = weight.unflatten(-1, (num_groups, group_size)) \\\n                                  .sum(dim=-1) \\\n                                  .unflatten(-1, (num_nodes, groups_per_node)) \\\n                                  .sum(dim=2) # [num_layers, num_nodes]\n\n    # Replicate experts within each node\n    # We need to restructure `weight` to apply `replicate_experts` per node.\n    # For each layer, we want to consider groups assigned to a node.\n    # Let's first get the logical experts assigned to each node.\n    node_logical_experts_indices = torch.arange(0, num_logical_experts, group_size) \\\n                                      .unflatten(-1, (num_groups, group_size)) \\\n                                      .view(num_groups // num_nodes, num_nodes, group_size) \\\n                                      .transpose(0, 1) \\\n                                      .contiguous() # [num_nodes, num_groups//num_nodes, group_size]\n    \n    # For each layer, we need to group the weights by node.\n    # We can achieve this by reshaping and transposing.\n    # First, flatten weight to [num_layers, num_groups, group_size]\n    weight_flat_groups = weight.unflatten(-1, (num_groups, group_size))\n    \n    # Now, reshape and transpose to get [num_layers * num_nodes, groups_per_node * group_size]\n    # This effectively groups logical experts by node.\n    weight_for_node_replication = weight_flat_groups.view(num_layers, num_groups // num_nodes, num_nodes, group_size) \\\n                                        .permute(0, 2, 1, 3) \\\n                                        .contiguous() \\\n                                        .view(num_layers * num_nodes, -1)\n\n    # Apply replication within each node\n    # The `replicate_experts` function expects [X, num_log] where X is the number of items to replicate.\n    # Here, X is num_layers. The second dimension is the number of logical experts *within that node*.\n    # We need to adjust the input to replicate_experts to be [num_layers, num_logical_experts_per_node].\n    num_logical_experts_per_node = num_logical_experts // num_nodes\n    \n    # We need to calculate the weights per logical expert *within each node*.\n    # This involves summing weights of groups assigned to that node.\n    # Let's re-think the `rebalance_experts_hierarchical` structure.\n    # The original code uses `tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)`\n    # This creates `[num_layers, num_groups]`.\n    # Then `group_pack_index, group_rank_in_pack = balanced_packing(tokens_per_group, num_nodes)`\n    # This packs groups into nodes.\n    # `log2mlog` maps logical experts to physical experts *within a node*.\n    \n    # Let's try to simplify the mapping logic.\n    # Step 1: Pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1) # [num_layers, num_groups]\n    group_pack_index, group_rank_in_pack = balanced_packing(tokens_per_group, num_nodes)\n    \n    # Map logical experts to nodes.\n    # logical_expert_id -> node_id\n    logical_expert_to_node = group_pack_index.flatten().gather(0, \n        torch.arange(num_logical_experts, device=weight.device).div(group_size, rounding_mode='floor'))\n    \n    # Now, for each node, we have `num_logical_experts // num_nodes` logical experts.\n    # We need to apply `replicate_experts` to these logical experts *within each node*.\n    \n    # Create a tensor of logical expert indices per node.\n    logical_experts_per_node_indices = torch.arange(num_logical_experts, device=weight.device).unflatten(-1, (num_groups, group_size)) \\\n                                          .view(num_groups // num_nodes, num_nodes, group_size) \\\n                                          .transpose(0, 1) # [num_nodes, num_logical_experts_per_node, group_size]\n\n    # We need to calculate the effective \"weight\" for each logical expert *within its assigned node*.\n    # This is the sum of weights of the groups assigned to that node.\n    # Let's first determine which groups are assigned to which node.\n    group_assignment_to_node = torch.arange(num_groups, device=weight.device)\n    group_assignment_to_node = group_assignment_to_node.unflatten(-1, (num_groups // num_nodes, num_nodes, groups_per_node)) \\\n                               .view(num_groups // num_nodes, num_nodes) # [num_groups//num_nodes, num_nodes] where value is group index\n\n    # The `group_rank_in_pack` tells us the \"rank\" within the node's packing.\n    # We need the total weight for each logical expert *within its node*.\n    # This is the sum of weights of groups assigned to that node.\n    # Let's create a weight tensor per node, where each row is a logical expert within that node.\n    \n    # `tokens_per_group` has shape [num_layers, num_groups]\n    # We want to sum the weights of groups assigned to each node.\n    # First, create a mapping from group index to node index.\n    group_to_node_map = torch.arange(num_groups, device=weight.device)\n    group_to_node_map = group_to_node_map.unflatten(-1, (num_groups // num_nodes, num_nodes, groups_per_node)) \\\n                               .view(num_groups // num_nodes, num_nodes) # [num_groups//num_nodes, num_nodes]\n    \n    # The `group_pack_index` maps groups to nodes.\n    # We can use this to group `tokens_per_group` by node.\n    tokens_per_node_logical_experts = torch.zeros(num_layers, num_nodes, num_logical_experts // num_nodes, device=weight.device)\n    \n    # For each layer, we want to aggregate tokens_per_group based on group_pack_index.\n    # This is tricky because `balanced_packing` reorders the groups.\n    # Let's reconsider the `mlog2log` calculation. It maps from a \"node-local\" logical expert index\n    # to a global logical expert index.\n    \n    # The original `mlog2log` calculation:\n    # `log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n    #              group_size).unsqueeze(-1) +\n    #             torch.arange(group_size,\n    #                          dtype=torch.int64,\n    #                          device=group_pack_index.device)).flatten(-2) `\n    # This creates a mapping from a physical expert index (within a node) to a logical expert index.\n    # The physical experts are ordered by node, then by the balanced packing within the node.\n    # `pphy2log` is the final mapping from physical expert to logical expert.\n\n    # Let's focus on simplifying the `replicate_experts` application and the subsequent mapping.\n    \n    # We need to apply `replicate_experts` to logical experts *within each node*.\n    # The input to `replicate_experts` should be `[num_layers, num_logical_experts_per_node]`.\n    # The weights for these logical experts should be the sum of their corresponding groups' weights.\n    \n    # First, let's get the logical experts associated with each node.\n    # `group_pack_index` maps each group to a node.\n    # `logical_expert_to_node` maps each logical expert to a node.\n    \n    # Calculate the total weight for each logical expert within its assigned node.\n    # This is the sum of `weight[:, group_idx]` for all `group_idx` assigned to a node.\n    # We can use `torch.scatter_add_` or similar to aggregate.\n    \n    # Let's create a tensor `logical_expert_weights_per_node` of shape [num_layers, num_nodes, num_logical_experts // num_nodes].\n    # For each layer, and for each node, we need to sum the weights of the groups assigned to that node.\n    \n    # Map groups to node indices.\n    group_to_node_idx = group_pack_index.flatten() # [num_groups]\n    \n    # Create a tensor that holds the weight of each group for each layer.\n    # `tokens_per_group` is [num_layers, num_groups]\n    \n    # We need to sum `tokens_per_group[layer, group_idx]` for all `group_idx` belonging to a node.\n    # Let's create a mapping from group index to its position within its node's logical experts.\n    group_rank_within_node = group_rank_in_pack.flatten() # [num_groups]\n\n    # For each layer, we want to construct the input for `replicate_experts`.\n    # The input tensor should be [num_layers, num_logical_experts_per_node].\n    # The value at [l, i] should be the total weight of the i-th logical expert within its node.\n    \n    # Let's directly calculate the `mlog2log` mapping.\n    # `mlog2log` maps a node-local logical expert index to a global logical expert index.\n    # The structure of `replicate_experts` implies that it operates on `[X, num_log]`.\n    # In `rebalance_experts_hierarchical`, `X` is `num_layers`.\n    # `num_log` is `num_logical_experts // num_nodes`.\n    \n    # The output of `replicate_experts` is `phy2log`, `rank`, `logcnt`.\n    # `phy2log`: [num_layers, num_logical_experts_per_node] - maps physical experts (within node) to logical experts (within node).\n    # `rank`: [num_layers, num_logical_experts_per_node] - replica rank.\n    # `logcnt`: [num_layers, num_logical_experts_per_node] - count of replicas per logical expert (within node).\n\n    # We need to apply `replicate_experts` to a reshaped version of `weight`.\n    # The input for `replicate_experts` should be the weights of logical experts, grouped by node.\n    # Let's construct `weight_per_node_logical_expert`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n    \n    # First, get the logical experts for each node.\n    logical_experts_for_nodes = torch.arange(num_logical_experts, device=weight.device).unflatten(-1, (num_groups, group_size)) \\\n                                    .view(num_groups // num_nodes, num_nodes, group_size) \\\n                                    .transpose(0, 1) # [num_nodes, num_logical_experts_per_node, group_size]\n\n    # Now, for each layer, we need to calculate the total weight for each logical expert within its node.\n    # This requires summing weights of groups assigned to that node.\n    # `tokens_per_group` is [num_layers, num_groups]\n    # `group_pack_index` maps groups to nodes.\n    \n    # Let's try to directly construct the `tokens_per_mlog` tensor.\n    # `mlog` refers to \"middle logical expert\", which is a logical expert within a node.\n    # `tokens_per_mlog` should be `[num_layers * num_nodes, num_logical_experts // num_nodes]`\n    # This means we need to flatten `[num_layers, num_nodes, num_logical_experts // num_nodes]`\n    \n    # Let's create `weight_per_layer_node_logical_expert`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n    # For each layer `l`, and node `n`, we need to sum `weight[l, group_idx]` for all `group_idx` assigned to node `n`.\n    \n    # Let's create a tensor mapping group index to node index.\n    group_to_node_map_flat = group_pack_index.flatten() # [num_groups]\n    \n    # Create an intermediate tensor to sum weights by node.\n    # `weight_sum_by_node`: [num_layers, num_nodes, num_groups_per_node] - this is not what we need.\n    \n    # Let's go back to the original logic and simplify the mapping.\n    # `group_pack_index` and `group_rank_in_pack` are computed from `tokens_per_group`.\n    # `tokens_per_group` is `weight.unflatten(-1, (num_groups, group_size)).sum(-1)`\n    # `log2mlog` is derived from these.\n    # `mlog2log` is the inverse of `log2mlog`.\n    \n    # The core issue is mapping between different levels of abstraction (group, logical expert, node-local logical expert, physical expert).\n    \n    # Let's try to directly construct the `phy2log` for the node-level replication.\n    # For each layer and each node, we need to run `replicate_experts`.\n    \n    # We need to prepare the input for `replicate_experts` for each layer and node.\n    # The input should be `[num_logical_experts_per_node]` for each node and layer.\n    # The values are the sum of weights of groups assigned to that node.\n    \n    # Let's create `logical_expert_weights_per_node_layer`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n    # Calculate tokens per group (already done as `tokens_per_group`)\n    # `tokens_per_group` shape: [num_layers, num_groups]\n\n    # Initialize with zeros.\n    logical_expert_weights_per_node_layer = torch.zeros(\n        num_layers, num_nodes, num_logical_experts // num_nodes,\n        dtype=weight.dtype, device=weight.device\n    )\n\n    # Vectorized population of logical_expert_weights_per_node_layer\n    # The target dimension for scatter_add is num_nodes * (num_logical_experts // num_nodes)\n    num_logical_experts_per_node = num_logical_experts // num_nodes\n    \n    # Calculate flat indices for scattering\n    # For each (layer, group), we get node_idx and rank_in_node.\n    # The target index within the flattened (num_nodes * num_logical_experts_per_node) dimension is:\n    # node_idx * num_logical_experts_per_node + rank_in_node\n    flat_target_indices = group_pack_index * num_logical_experts_per_node + group_rank_in_pack\n    \n    # Reshape logical_expert_weights_per_node_layer for scatter_add\n    # We scatter along the last dimension (dim=1 after view)\n    logical_expert_weights_per_node_layer.view(num_layers, -1).scatter_add_(\n        1, # Scatter along the second dimension (the flattened node-local experts)\n        flat_target_indices, # Indices [num_layers, num_groups]\n        tokens_per_group # Source values [num_layers, num_groups]\n    )\n\n    # Now, apply `replicate_experts` to `logical_expert_weights_per_node_layer`.\n    # We need to reshape it to `[num_layers * num_nodes, num_logical_experts // num_nodes]`.\n    weight_for_replication = logical_expert_weights_per_node_layer.view(num_layers * num_nodes, -1)\n    \n    # The number of physical experts per GPU is `phy_experts_per_gpu`.\n    # Each node has `num_gpus // num_nodes` GPUs.\n    # So, each node needs `(num_gpus // num_nodes) * phy_experts_per_gpu` physical experts.\n    # This is `num_physical_experts // num_nodes`.\n    num_physical_experts_per_node = num_physical_experts // num_nodes\n    \n    # Apply replicate_experts for each node.\n    # The `replicate_experts` function takes `[X, num_log]`. Here X is num_layers.\n    # We need to apply it to `[num_layers, num_logical_experts // num_nodes]` for each node.\n    \n    # Let's process node by node.\n    all_phy2mlog_list = []\n    all_phyrank_list = []\n    all_mlogcnt_list = []\n    \n    for node_idx in range(num_nodes):\n        # Extract weights for logical experts within this node.\n        node_weights = logical_expert_weights_per_node_layer[:, node_idx, :] # [num_layers, num_logical_experts_per_node]\n        \n        # Apply replicate_experts for this node.\n        phy2mlog_node, phyrank_node, mlogcnt_node = replicate_experts(\n            node_weights, num_physical_experts_per_node)\n        \n        all_phy2mlog_list.append(phy2mlog_node)\n        all_phyrank_list.append(phyrank_node)\n        all_mlogcnt_list.append(mlogcnt_node)\n\n    # Concatenate results from all nodes.\n    phy2mlog = torch.cat(all_phy2mlog_list, dim=1) # [num_layers, num_physical_experts]\n    phyrank = torch.cat(all_phyrank_list, dim=1)   # [num_layers, num_physical_experts]\n    mlogcnt = torch.cat(all_mlogcnt_list, dim=1)   # [num_layers, num_physical_experts]\n\n    # Now, map these back to the original logical expert indices.\n    # `phy2mlog` maps physical experts (within a node) to logical experts (within a node).\n    # We need to convert the \"logical expert within a node\" index to the global logical expert index.\n    \n    # Create the mapping from node-local logical expert index to global logical expert index.\n    node_local_logical_to_global_map = torch.arange(num_logical_experts, device=weight.device).unflatten(-1, (num_groups, group_size)) \\\n                                        .view(num_groups // num_nodes, num_nodes, group_size) \\\n                                        .transpose(0, 1) # [num_nodes, num_logical_experts_per_node, group_size]\n    \n    # For each node, apply the mapping.\n    final_phy2log_list = []\n    for node_idx in range(num_nodes):\n        # Get the mapping for this node.\n        local_map = node_local_logical_to_global_map[node_idx] # [num_logical_experts_per_node, group_size]\n        \n        # We need to map the `phy2mlog_node` (which contains indices into `local_map`).\n        # The `phy2mlog_node` has shape [num_layers, num_physical_experts_per_node].\n        # We need to use `phy2mlog_node` to index into `local_map`.\n        \n        # Let's re-think. `phy2mlog` from `replicate_experts` is `[num_layers, num_logical_experts_per_node]`.\n        # The value `phy2mlog[l, i]` is the logical expert index *within that node's set of logical experts*.\n        # We need to convert this to the global logical expert index.\n        \n        # The original `mlog2log` was calculated as:\n        # `mlog2log = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n        #              group_size).unsqueeze(-1) +\n        #             torch.arange(group_size,\n        #                          dtype=torch.int64,\n        #                          device=group_pack_index.device)).flatten(-2)`\n        # This seems to map a \"physical expert index within a node\" to a \"global logical expert index\".\n        \n        # Let's try to reconstruct this mapping more directly.\n        # The physical experts are ordered by node, then by `replicate_experts` output.\n        # `phy2mlog` gives the logical expert index within a node.\n        # We need to map this to the global logical expert index.\n        \n        # Let's construct `node_logical_expert_to_global_logical_expert_map`: [num_nodes, num_logical_experts_per_node]\n        node_logical_expert_to_global_logical_expert_map = torch.arange(num_logical_experts, device=weight.device).unflatten(-1, (num_groups, group_size)) \\\n                                                               .view(num_groups // num_nodes, num_nodes, group_size) \\\n                                                               .transpose(0, 1) \\\n                                                               .contiguous() # [num_nodes, num_logical_experts_per_node, group_size]\n        # We only need the first element of the inner dimension to get the logical expert index.\n        node_logical_expert_to_global_logical_expert_map = node_logical_expert_to_global_logical_expert_map[:, :, 0] # [num_nodes, num_logical_experts_per_node]\n\n        # Now, for each node, use its `phy2mlog` to index into this map.\n        node_phy2mlog = phy2mlog[:, node_idx * num_physical_experts_per_node : (node_idx + 1) * num_physical_experts_per_node] # [num_layers, num_physical_experts_per_node]\n        \n        # The `phy2mlog` values are indices into the `node_logical_expert_to_global_logical_expert_map`.\n        # We need to map these indices correctly.\n        # The `replicate_experts` output `phy2log` maps physical experts to logical experts.\n        # We want to map physical experts to global logical experts.\n        \n        # Let's directly create the mapping from the original `group_pack_index` and `group_rank_in_pack`.\n        # `group_pack_index`: [num_layers, num_groups] -> maps groups to nodes.\n        # `group_rank_in_pack`: [num_layers, num_groups] -> maps groups to rank within a node's packing.\n        \n        # We need to map physical experts to global logical experts.\n        # The physical experts are arranged: Node 0 (GPU 0, GPU 1, ...), Node 1, ...\n        # Within each node, physical experts are arranged based on `replicate_experts` output.\n        \n        # Let's create the mapping from a \"physical expert index\" to its \"node index\" and \"rank within node\".\n        # The total number of physical experts is `num_physical_experts`.\n        # The number of physical experts per node is `num_physical_experts_per_node`.\n        \n        # For each layer, let's build the mapping.\n        # `phy_idx_to_node_and_rank`: [num_layers, num_physical_experts] -> [node_idx, rank_in_node]\n        \n        # Instead of complex reshaping, let's simplify the mapping creation.\n        # We have `group_pack_index` [num_layers, num_groups] and `group_rank_in_pack` [num_layers, num_groups].\n        # These tell us which node a group belongs to, and its rank within that node.\n        \n        # The original `mlog2log` calculation is crucial. Let's re-examine it:\n        # `group_index_in_node = group_pack_index * groups_per_node + group_rank_in_pack`\n        # This is not right. `group_rank_in_pack` is the rank *within the sorted groups*.\n        # The `balanced_packing` returns `pack_index` and `rank_in_pack`.\n        # `group_pack_index` is the pack index for each group.\n        # `group_rank_in_pack` is the rank of the group within its assigned pack (node).\n        \n        # Let's construct the mapping from physical expert index to global logical expert index.\n        # The physical experts are ordered by node, then by the output of `replicate_experts` within that node.\n        \n        # For each node, we have `replicate_experts` applied to `num_logical_experts // num_nodes` logical experts.\n        # The `phy2mlog` output from `replicate_experts` maps physical experts (within node) to logical experts (within node).\n        # We need to convert the logical expert index (within node) to the global logical expert index.\n        \n        # Let's reconstruct the mapping for each node.\n        # For node `n`, the logical experts are indexed from `n * (num_logical_experts // num_nodes)` to `(n+1) * (num_logical_experts // num_nodes) - 1`.\n        \n        # Create `global_logical_expert_indices_per_node`: [num_nodes, num_logical_experts // num_nodes]\n        global_logical_expert_indices_per_node = torch.arange(num_logical_experts, device=weight.device).unflatten(-1, (num_groups, group_size)) \\\n                                                   .view(num_groups // num_nodes, num_nodes, group_size) \\\n                                                   .transpose(0, 1) \\\n                                                   .contiguous()[:, :, 0] # [num_nodes, num_logical_experts_per_node]\n        \n        # Now, for each node's `phy2mlog` output, index into `global_logical_expert_indices_per_node`.\n        # `phy2mlog` from replicate_experts is `[num_layers, num_logical_experts_per_node]`.\n        # The values are indices into the logical experts *within that node*.\n        \n        # For node `node_idx`:\n        # `node_phy2mlog = phy2mlog_list[node_idx]` # [num_layers, num_physical_experts_per_node]\n        # `node_global_map = global_logical_expert_indices_per_node[node_idx]` # [num_logical_experts_per_node]\n        \n        # `final_phy2log_node = node_global_map.gather(0, node_phy2mlog)` # This is incorrect indexing.\n        \n        # We need to use `node_phy2mlog` as indices into `node_global_map`.\n        # `final_phy2log_node = torch.gather(node_global_map.unsqueeze(0).expand_as(node_phy2mlog), -1, node_phy2mlog)` # Incorrect.\n        \n        # Let's use `torch.gather` correctly.\n        # `torch.gather(input, dim, index)`\n        # `input` is `node_global_map` (shape: [num_logical_experts_per_node])\n        # `dim` is 0 (since we want to index along the logical expert dimension)\n        # `index` is `node_phy2mlog` (shape: [num_layers, num_physical_experts_per_node])\n        \n        node_global_map = global_logical_expert_indices_per_node[node_idx] # [num_logical_experts_per_node]\n        node_phy2mlog = all_phy2mlog_list[node_idx] # [num_layers, num_physical_experts_per_node]\n        \n        # Map `node_phy2mlog` (indices into logical experts within node) to global logical expert indices.\n        # We need to apply `gather` for each layer.\n        final_phy2log_node = torch.gather(node_global_map.unsqueeze(0).expand_as(node_phy2mlog), -1, node_phy2mlog)\n        \n        final_phy2log_list.append(final_phy2log_node)\n\n    # Concatenate the final `phy2log` from all nodes.\n    phy2log = torch.cat(final_phy2log_list, dim=1) # [num_layers, num_physical_experts]\n\n    # Now, let's handle `phyrank` and `logcnt`.\n    # `phyrank` from `replicate_experts` is the rank of the physical expert within its logical expert group.\n    # `mlogcnt` from `replicate_experts` is the number of replicas for each logical expert (within node).\n    \n    # We need to combine `phyrank` and `mlogcnt` across nodes.\n    # `phyrank` has shape `[num_layers, num_physical_experts_per_node]` for each node.\n    # `mlogcnt` has shape `[num_layers, num_physical_experts_per_node]` for each node.\n    \n    phyrank = torch.cat(all_phyrank_list, dim=1) # [num_layers, num_physical_experts]\n    \n    # `mlogcnt` needs to be converted to `expert_count` for the global logical experts.\n    # `mlogcnt` is `[num_layers, num_physical_experts_per_node]` for each node.\n    # We need to aggregate this information to get the count of physical experts for each global logical expert.\n    \n    # The `mlogcnt_node` has shape `[num_layers, num_physical_experts_per_node]`.\n    # The values represent the number of replicas for the logical expert *within that node*.\n    # We need to sum these up for each global logical expert.\n    \n    # Let's create `logcnt_per_node`: [num_nodes, num_layers, num_logical_experts_per_node]\n    logcnt_per_node = torch.stack(all_mlogcnt_list, dim=1) # [num_layers, num_nodes, num_physical_experts_per_node]\n    \n    # Reshape `logcnt_per_node` to group by logical expert within node.\n    # `logcnt_per_node`: [num_layers, num_nodes, num_physical_experts_per_node]\n    # We need to map `logcnt_per_node` based on `phy2mlog` to get counts per logical expert.\n    \n    # Create `logical_expert_counts_per_node_layer`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n    logical_expert_counts_per_node_layer = torch.zeros(num_layers, num_nodes, num_logical_experts // num_nodes, dtype=torch.int64, device=weight.device)\n    \n    for node_idx in range(num_nodes):\n        node_mlogcnt = all_mlogcnt_list[node_idx] # [num_layers, num_physical_experts_per_node]\n        node_phy2mlog = all_phy2mlog_list[node_idx] # [num_layers, num_physical_experts_per_node]\n        \n        # For each layer, we want to sum `node_mlogcnt` values based on `node_phy2mlog`.\n        # This is a scatter_add operation.\n        for l in range(num_layers):\n            # Use scatter_add to sum counts for each logical expert within the node.\n            logical_expert_counts_per_node_layer[l, node_idx].scatter_add_(\n                0,\n                node_phy2mlog[l],\n                node_mlogcnt[l]\n            )\n\n    # Now, sum the counts across nodes for each logical expert.\n    # `logcnt`: [num_layers, num_logical_experts]\n    logcnt = logical_expert_counts_per_node_layer.sum(dim=1) # [num_layers, num_logical_experts // num_nodes]\n    \n    # The `logcnt` should be for the global logical experts.\n    # The current `logcnt` is for logical experts *within each node*.\n    # We need to aggregate `logcnt` based on the node assignment of logical experts.\n    \n    # Let's re-calculate `logcnt` from `phy2log`.\n    # `phy2log` is `[num_layers, num_physical_experts]`.\n    # `phyrank` is `[num_layers, num_physical_experts]`.\n    # We need to count how many physical experts map to each global logical expert.\n    \n    # Create a tensor to store the counts.\n    expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n    \n    # Iterate through all physical experts and increment the count for their corresponding logical expert.\n    for i in range(num_layers):\n        for j in range(num_physical_experts):\n            logical_expert_idx = phy2log[i, j]\n            if logical_expert_idx != -1: # Ensure it's a valid mapping\n                expert_count[i, logical_expert_idx] += 1\n                \n    # The `phyrank` needs to be mapped correctly.\n    # `phyrank` is `[num_layers, num_physical_experts]`.\n    # The values are the replica ranks.\n    \n    # Let's ensure the `replicate_experts` output `phyrank` is correctly used.\n    # `phyrank` from `replicate_experts` is `[X, num_log]`.\n    # In our case, `X` is `num_layers`.\n    # `num_log` is `num_logical_experts_per_node`.\n    \n    # The combined `phyrank` has shape `[num_layers, num_physical_experts]`.\n    # The values within this tensor are the ranks within their respective logical expert groups.\n    # The `phyrank` is already correctly structured if we concatenate it.\n    \n    # We need to ensure the `logcnt` calculation is correct.\n    # `mlogcnt_node` from `replicate_experts` is `[num_layers, num_physical_experts_per_node]`.\n    # It represents the number of replicas for each logical expert *within that node*.\n    \n    # The `logical_expert_counts_per_node_layer` correctly sums up the `mlogcnt` for each logical expert within each node.\n    # Now, we need to sum these counts across nodes to get the total count for each global logical expert.\n    \n    # The `logical_expert_counts_per_node_layer` has shape `[num_layers, num_nodes, num_logical_experts // num_nodes]`.\n    # We need to map this based on the node assignment of logical experts.\n    \n    # Let's create `global_logical_expert_counts`: [num_layers, num_logical_experts]\n    global_logical_expert_counts = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n    \n    for l in range(num_layers):\n        for n in range(num_nodes):\n            # Get the logical experts for this node.\n            node_logical_experts = global_logical_expert_indices_per_node[n] # [num_logical_experts_per_node]\n            # Get the counts for this node and layer.\n            node_counts = logical_expert_counts_per_node_layer[l, n] # [num_logical_experts_per_node]\n            \n            # Use scatter_add to add these counts to the correct global logical expert index.\n            global_logical_expert_counts[l].scatter_add_(0, node_logical_experts, node_counts)\n    \n    logcnt = global_logical_expert_counts # This should be the correct `logcnt`\n\n    return phy2log, phyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    # Cast weight to float for calculations if not already, and keep on original device.\n    # Moving to CPU adds overhead if data is already on GPU and might slow down ops.\n    weight = weight.float()\n    \n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n", "language": "python", "parent_id": "0f7193ba-ee7f-45ef-84cb-3f69803f1356", "generation": 7, "timestamp": 1751066058.9447937, "iteration_found": 254, "metrics": {"balancedness_score": 0.0, "speed_score": 0.0, "combined_score": 0.0, "error": "unflatten: Provided sizes [8, 32] don't multiply up to the size of dim 0 (8) in the input tensor"}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 16 lines with 26 lines\nChange 2: Replace 85 lines with 36 lines\nChange 3: Replace 102 lines with 11 lines", "parent_metrics": {"balancedness_score": 0.0, "speed_score": 0.0, "combined_score": 0.0, "error": "unflatten: Provided sizes [8, 32] don't multiply up to the size of dim 0 (8) in the input tensor"}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve the Mixture-of-Expert models Expert Parallelism Load Balancer (MoE EPLB) expert rearrangement algorithm.\nThis algorithm will take the load metrics recorded by the vLLM server, and rearrange the experts to balance the load. It can make replicas of some experts to achieve better load balancing.\nYour goal will be two-fold: 1. Improve the algorithm to achieve better load balancing; while 2. Improve the algorithm to be more efficient, i.e. reduce the execution time of the algorithm itself, since perfect load balancing is NP-hard.\nThe current algorithm is implemented in the `rebalance_experts` function. ", "user": "# Current Program Information\n- Current performance metrics: - balancedness_score: 0.0000\n- speed_score: 0.0000\n- combined_score: 0.0000\n- error: unflatten: Provided sizes [8, 32] don't multiply up to the size of dim 0 (8) in the input tensor\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: balancedness_score, speed_score, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1103, combined_score: 0.2315\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1103, combined_score: 0.2315\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1103, combined_score: 0.2315\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.2315)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1103), Performs well on combined_score (0.2315)\n\n\n### Program 2 (Score: 0.2315)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1103), Performs well on combined_score (0.2315)\n\n\n### Program 3 (Score: 0.2315)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1103), Performs well on combined_score (0.2315)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.2315)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n\n### Program D2 (Score: 0.2315)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.2315, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 32 lines with 32 lines, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 2 (Score: 0.0939, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 23 lines with 236 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 3 (Score: 0.1920, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 4 (Score: 0.1639, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 37 lines with 63 lines\nChange 2: Replace 37 lines with 50 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 5 (Score: 0.2315, Type: Migrant)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 32 lines with 32 lines, Alternative speed_score approach, Alternative combined_score approach\n\n\n\n# Current Program\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort groups by weight in descending order.\n    # 'weight' is already float and on CPU from rebalance_experts,\n    # so no need for .float().cpu() here.\n    indices = weight.sort(-1, descending=True).indices\n\n    # Initialize pack_index and rank_in_pack.\n    # These will be fully populated, so `empty_like` is sufficient.\n    # They will be on the same device as 'weight' (i.e., CPU).\n    pack_index = torch.empty_like(weight, dtype=torch.int64)\n    rank_in_pack = torch.empty_like(pack_index)\n    \n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These tensors are on the same device as 'weight' (which is CPU).\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id in indices[i]:\n            # Get current state for this layer's packs.\n            current_pack_weights = pack_weights_per_layer[i]\n            current_pack_items = pack_items_per_layer[i]\n\n            # Find the pack with the minimum current weight among available ones.\n            # Use torch.where to effectively mask out full packs with infinity,\n            # then find the argmin. This avoids an explicit clone and subsequent\n            # masked assignment, potentially being more efficient.\n            pack = torch.argmin(\n                torch.where(current_pack_items < groups_per_pack,\n                            current_pack_weights,\n                            torch.full_like(current_pack_weights, float('inf')))\n            )\n\n            # Assert that a valid pack was found and it has space.\n            # Using pack directly as a 0-dim tensor for indexing.\n            assert current_pack_items[pack] < groups_per_pack, \\\n                f\"No available packs found, but expected to find one. Pack {pack} is full. \" \\\n                f\"Current pack items: {current_pack_items}, groups_per_pack: {groups_per_pack}\"\n\n            # Assign the group to the chosen pack.\n            # group_id is a 0-dim tensor, so it works directly for indexing.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            # weight[i, group_id] is the weight of the current group (0-dim tensor).\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # Calculate tokens per group for each node\n    tokens_per_group_per_node = weight.unflatten(-1, (num_groups, group_size)) \\\n                                  .sum(dim=-1) \\\n                                  .unflatten(-1, (num_nodes, groups_per_node)) \\\n                                  .sum(dim=2) # [num_layers, num_nodes]\n\n    # Replicate experts within each node\n    # We need to restructure `weight` to apply `replicate_experts` per node.\n    # For each layer, we want to consider groups assigned to a node.\n    # Let's first get the logical experts assigned to each node.\n    node_logical_experts_indices = torch.arange(0, num_logical_experts, group_size) \\\n                                      .unflatten(-1, (num_groups, group_size)) \\\n                                      .view(num_groups // num_nodes, num_nodes, group_size) \\\n                                      .transpose(0, 1) \\\n                                      .contiguous() # [num_nodes, num_groups//num_nodes, group_size]\n    \n    # For each layer, we need to group the weights by node.\n    # We can achieve this by reshaping and transposing.\n    # First, flatten weight to [num_layers, num_groups, group_size]\n    weight_flat_groups = weight.unflatten(-1, (num_groups, group_size))\n    \n    # Now, reshape and transpose to get [num_layers * num_nodes, groups_per_node * group_size]\n    # This effectively groups logical experts by node.\n    weight_for_node_replication = weight_flat_groups.view(num_layers, num_groups // num_nodes, num_nodes, group_size) \\\n                                        .permute(0, 2, 1, 3) \\\n                                        .contiguous() \\\n                                        .view(num_layers * num_nodes, -1)\n\n    # Apply replication within each node\n    # The `replicate_experts` function expects [X, num_log] where X is the number of items to replicate.\n    # Here, X is num_layers. The second dimension is the number of logical experts *within that node*.\n    # We need to adjust the input to replicate_experts to be [num_layers, num_logical_experts_per_node].\n    num_logical_experts_per_node = num_logical_experts // num_nodes\n    \n    # We need to calculate the weights per logical expert *within each node*.\n    # This involves summing weights of groups assigned to that node.\n    # Let's re-think the `rebalance_experts_hierarchical` structure.\n    # The original code uses `tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)`\n    # This creates `[num_layers, num_groups]`.\n    # Then `group_pack_index, group_rank_in_pack = balanced_packing(tokens_per_group, num_nodes)`\n    # This packs groups into nodes.\n    # `log2mlog` maps logical experts to physical experts *within a node*.\n    \n    # Let's try to simplify the mapping logic.\n    # Step 1: Pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1) # [num_layers, num_groups]\n    group_pack_index, group_rank_in_pack = balanced_packing(tokens_per_group, num_nodes)\n    \n    # Map logical experts to nodes.\n    # logical_expert_id -> node_id\n    logical_expert_to_node = group_pack_index.flatten().gather(0, \n        torch.arange(num_logical_experts, device=weight.device).div(group_size, rounding_mode='floor'))\n    \n    # Now, for each node, we have `num_logical_experts // num_nodes` logical experts.\n    # We need to apply `replicate_experts` to these logical experts *within each node*.\n    \n    # Create a tensor of logical expert indices per node.\n    logical_experts_per_node_indices = torch.arange(num_logical_experts, device=weight.device).unflatten(-1, (num_groups, group_size)) \\\n                                          .view(num_groups // num_nodes, num_nodes, group_size) \\\n                                          .transpose(0, 1) # [num_nodes, num_logical_experts_per_node, group_size]\n\n    # We need to calculate the effective \"weight\" for each logical expert *within its assigned node*.\n    # This is the sum of weights of the groups assigned to that node.\n    # Let's first determine which groups are assigned to which node.\n    group_assignment_to_node = torch.arange(num_groups, device=weight.device)\n    group_assignment_to_node = group_assignment_to_node.unflatten(-1, (num_groups // num_nodes, num_nodes, groups_per_node)) \\\n                               .view(num_groups // num_nodes, num_nodes) # [num_groups//num_nodes, num_nodes] where value is group index\n\n    # The `group_rank_in_pack` tells us the \"rank\" within the node's packing.\n    # We need the total weight for each logical expert *within its node*.\n    # This is the sum of weights of groups assigned to that node.\n    # Let's create a weight tensor per node, where each row is a logical expert within that node.\n    \n    # `tokens_per_group` has shape [num_layers, num_groups]\n    # We want to sum the weights of groups assigned to each node.\n    # First, create a mapping from group index to node index.\n    group_to_node_map = torch.arange(num_groups, device=weight.device)\n    group_to_node_map = group_to_node_map.unflatten(-1, (num_groups // num_nodes, num_nodes, groups_per_node)) \\\n                               .view(num_groups // num_nodes, num_nodes) # [num_groups//num_nodes, num_nodes]\n    \n    # The `group_pack_index` maps groups to nodes.\n    # We can use this to group `tokens_per_group` by node.\n    tokens_per_node_logical_experts = torch.zeros(num_layers, num_nodes, num_logical_experts // num_nodes, device=weight.device)\n    \n    # For each layer, we want to aggregate tokens_per_group based on group_pack_index.\n    # This is tricky because `balanced_packing` reorders the groups.\n    # Let's reconsider the `mlog2log` calculation. It maps from a \"node-local\" logical expert index\n    # to a global logical expert index.\n    \n    # The original `mlog2log` calculation:\n    # `log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n    #              group_size).unsqueeze(-1) +\n    #             torch.arange(group_size,\n    #                          dtype=torch.int64,\n    #                          device=group_pack_index.device)).flatten(-2) `\n    # This creates a mapping from a physical expert index (within a node) to a logical expert index.\n    # The physical experts are ordered by node, then by the balanced packing within the node.\n    # `pphy2log` is the final mapping from physical expert to logical expert.\n\n    # Let's focus on simplifying the `replicate_experts` application and the subsequent mapping.\n    \n    # We need to apply `replicate_experts` to logical experts *within each node*.\n    # The input to `replicate_experts` should be `[num_layers, num_logical_experts_per_node]`.\n    # The weights for these logical experts should be the sum of their corresponding groups' weights.\n    \n    # First, let's get the logical experts associated with each node.\n    # `group_pack_index` maps each group to a node.\n    # `logical_expert_to_node` maps each logical expert to a node.\n    \n    # Calculate the total weight for each logical expert within its assigned node.\n    # This is the sum of `weight[:, group_idx]` for all `group_idx` assigned to a node.\n    # We can use `torch.scatter_add_` or similar to aggregate.\n    \n    # Let's create a tensor `logical_expert_weights_per_node` of shape [num_layers, num_nodes, num_logical_experts // num_nodes].\n    # For each layer, and for each node, we need to sum the weights of the groups assigned to that node.\n    \n    # Map groups to node indices.\n    group_to_node_idx = group_pack_index.flatten() # [num_groups]\n    \n    # Create a tensor that holds the weight of each group for each layer.\n    # `tokens_per_group` is [num_layers, num_groups]\n    \n    # We need to sum `tokens_per_group[layer, group_idx]` for all `group_idx` belonging to a node.\n    # Let's create a mapping from group index to its position within its node's logical experts.\n    group_rank_within_node = group_rank_in_pack.flatten() # [num_groups]\n\n    # For each layer, we want to construct the input for `replicate_experts`.\n    # The input tensor should be [num_layers, num_logical_experts_per_node].\n    # The value at [l, i] should be the total weight of the i-th logical expert within its node.\n    \n    # Let's directly calculate the `mlog2log` mapping.\n    # `mlog2log` maps a node-local logical expert index to a global logical expert index.\n    # The structure of `replicate_experts` implies that it operates on `[X, num_log]`.\n    # In `rebalance_experts_hierarchical`, `X` is `num_layers`.\n    # `num_log` is `num_logical_experts // num_nodes`.\n    \n    # The output of `replicate_experts` is `phy2log`, `rank`, `logcnt`.\n    # `phy2log`: [num_layers, num_logical_experts_per_node] - maps physical experts (within node) to logical experts (within node).\n    # `rank`: [num_layers, num_logical_experts_per_node] - replica rank.\n    # `logcnt`: [num_layers, num_logical_experts_per_node] - count of replicas per logical expert (within node).\n\n    # We need to apply `replicate_experts` to a reshaped version of `weight`.\n    # The input for `replicate_experts` should be the weights of logical experts, grouped by node.\n    # Let's construct `weight_per_node_logical_expert`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n    \n    # First, get the logical experts for each node.\n    logical_experts_for_nodes = torch.arange(num_logical_experts, device=weight.device).unflatten(-1, (num_groups, group_size)) \\\n                                    .view(num_groups // num_nodes, num_nodes, group_size) \\\n                                    .transpose(0, 1) # [num_nodes, num_logical_experts_per_node, group_size]\n\n    # Now, for each layer, we need to calculate the total weight for each logical expert within its node.\n    # This requires summing weights of groups assigned to that node.\n    # `tokens_per_group` is [num_layers, num_groups]\n    # `group_pack_index` maps groups to nodes.\n    \n    # Let's try to directly construct the `tokens_per_mlog` tensor.\n    # `mlog` refers to \"middle logical expert\", which is a logical expert within a node.\n    # `tokens_per_mlog` should be `[num_layers * num_nodes, num_logical_experts // num_nodes]`\n    # This means we need to flatten `[num_layers, num_nodes, num_logical_experts // num_nodes]`\n    \n    # Let's create `weight_per_layer_node_logical_expert`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n    # For each layer `l`, and node `n`, we need to sum `weight[l, group_idx]` for all `group_idx` assigned to node `n`.\n    \n    # Let's create a tensor mapping group index to node index.\n    group_to_node_map_flat = group_pack_index.flatten() # [num_groups]\n    \n    # Create an intermediate tensor to sum weights by node.\n    # `weight_sum_by_node`: [num_layers, num_nodes, num_groups_per_node] - this is not what we need.\n    \n    # Let's go back to the original logic and simplify the mapping.\n    # `group_pack_index` and `group_rank_in_pack` are computed from `tokens_per_group`.\n    # `tokens_per_group` is `weight.unflatten(-1, (num_groups, group_size)).sum(-1)`\n    # `log2mlog` is derived from these.\n    # `mlog2log` is the inverse of `log2mlog`.\n    \n    # The core issue is mapping between different levels of abstraction (group, logical expert, node-local logical expert, physical expert).\n    \n    # Let's try to directly construct the `phy2log` for the node-level replication.\n    # For each layer and each node, we need to run `replicate_experts`.\n    \n    # We need to prepare the input for `replicate_experts` for each layer and node.\n    # The input should be `[num_logical_experts_per_node]` for each node and layer.\n    # The values are the sum of weights of groups assigned to that node.\n    \n    # Let's create `logical_expert_weights_per_node_layer`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n    # Initialize with zeros.\n    logical_expert_weights_per_node_layer = torch.zeros(num_layers, num_nodes, num_logical_experts // num_nodes, dtype=weight.dtype, device=weight.device)\n    \n    # Iterate through layers and groups to populate `logical_expert_weights_per_node_layer`.\n    for l in range(num_layers):\n        for g in range(num_groups):\n            node_idx = group_pack_index[l, g]\n            # Find the rank of this group within its node's packing.\n            rank_in_node = group_rank_in_pack[l, g]\n            # The group index `g` corresponds to `g // groups_per_node` blocks of `group_size`.\n            # We need to map group `g` to its position within the node's logical experts.\n            # The `group_rank_in_pack` directly gives this position.\n            logical_expert_idx_within_node = rank_in_node\n            \n            # Add the weight of this group to the corresponding logical expert in the node.\n            logical_expert_weights_per_node_layer[l, node_idx, logical_expert_idx_within_node] += weight[l, g * group_size : (g + 1) * group_size].sum()\n\n    # Now, apply `replicate_experts` to `logical_expert_weights_per_node_layer`.\n    # We need to reshape it to `[num_layers * num_nodes, num_logical_experts // num_nodes]`.\n    weight_for_replication = logical_expert_weights_per_node_layer.view(num_layers * num_nodes, -1)\n    \n    # The number of physical experts per GPU is `phy_experts_per_gpu`.\n    # Each node has `num_gpus // num_nodes` GPUs.\n    # So, each node needs `(num_gpus // num_nodes) * phy_experts_per_gpu` physical experts.\n    # This is `num_physical_experts // num_nodes`.\n    num_physical_experts_per_node = num_physical_experts // num_nodes\n    \n    # Apply replicate_experts for each node.\n    # The `replicate_experts` function takes `[X, num_log]`. Here X is num_layers.\n    # We need to apply it to `[num_layers, num_logical_experts // num_nodes]` for each node.\n    \n    # Let's process node by node.\n    all_phy2mlog_list = []\n    all_phyrank_list = []\n    all_mlogcnt_list = []\n    \n    for node_idx in range(num_nodes):\n        # Extract weights for logical experts within this node.\n        node_weights = logical_expert_weights_per_node_layer[:, node_idx, :] # [num_layers, num_logical_experts_per_node]\n        \n        # Apply replicate_experts for this node.\n        phy2mlog_node, phyrank_node, mlogcnt_node = replicate_experts(\n            node_weights, num_physical_experts_per_node)\n        \n        all_phy2mlog_list.append(phy2mlog_node)\n        all_phyrank_list.append(phyrank_node)\n        all_mlogcnt_list.append(mlogcnt_node)\n\n    # Concatenate results from all nodes.\n    phy2mlog = torch.cat(all_phy2mlog_list, dim=1) # [num_layers, num_physical_experts]\n    phyrank = torch.cat(all_phyrank_list, dim=1)   # [num_layers, num_physical_experts]\n    mlogcnt = torch.cat(all_mlogcnt_list, dim=1)   # [num_layers, num_physical_experts]\n\n    # Now, map these back to the original logical expert indices.\n    # `phy2mlog` maps physical experts (within a node) to logical experts (within a node).\n    # We need to convert the \"logical expert within a node\" index to the global logical expert index.\n    \n    # Create the mapping from node-local logical expert index to global logical expert index.\n    node_local_logical_to_global_map = torch.arange(num_logical_experts, device=weight.device).unflatten(-1, (num_groups, group_size)) \\\n                                        .view(num_groups // num_nodes, num_nodes, group_size) \\\n                                        .transpose(0, 1) # [num_nodes, num_logical_experts_per_node, group_size]\n    \n    # For each node, apply the mapping.\n    final_phy2log_list = []\n    for node_idx in range(num_nodes):\n        # Get the mapping for this node.\n        local_map = node_local_logical_to_global_map[node_idx] # [num_logical_experts_per_node, group_size]\n        \n        # We need to map the `phy2mlog_node` (which contains indices into `local_map`).\n        # The `phy2mlog_node` has shape [num_layers, num_physical_experts_per_node].\n        # We need to use `phy2mlog_node` to index into `local_map`.\n        \n        # Let's re-think. `phy2mlog` from `replicate_experts` is `[num_layers, num_logical_experts_per_node]`.\n        # The value `phy2mlog[l, i]` is the logical expert index *within that node's set of logical experts*.\n        # We need to convert this to the global logical expert index.\n        \n        # The original `mlog2log` was calculated as:\n        # `mlog2log = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n        #              group_size).unsqueeze(-1) +\n        #             torch.arange(group_size,\n        #                          dtype=torch.int64,\n        #                          device=group_pack_index.device)).flatten(-2)`\n        # This seems to map a \"physical expert index within a node\" to a \"global logical expert index\".\n        \n        # Let's try to reconstruct this mapping more directly.\n        # The physical experts are ordered by node, then by `replicate_experts` output.\n        # `phy2mlog` gives the logical expert index within a node.\n        # We need to map this to the global logical expert index.\n        \n        # Let's construct `node_logical_expert_to_global_logical_expert_map`: [num_nodes, num_logical_experts_per_node]\n        node_logical_expert_to_global_logical_expert_map = torch.arange(num_logical_experts, device=weight.device).unflatten(-1, (num_groups, group_size)) \\\n                                                               .view(num_groups // num_nodes, num_nodes, group_size) \\\n                                                               .transpose(0, 1) \\\n                                                               .contiguous() # [num_nodes, num_logical_experts_per_node, group_size]\n        # We only need the first element of the inner dimension to get the logical expert index.\n        node_logical_expert_to_global_logical_expert_map = node_logical_expert_to_global_logical_expert_map[:, :, 0] # [num_nodes, num_logical_experts_per_node]\n\n        # Now, for each node, use its `phy2mlog` to index into this map.\n        node_phy2mlog = phy2mlog[:, node_idx * num_physical_experts_per_node : (node_idx + 1) * num_physical_experts_per_node] # [num_layers, num_physical_experts_per_node]\n        \n        # The `phy2mlog` values are indices into the `node_logical_expert_to_global_logical_expert_map`.\n        # We need to map these indices correctly.\n        # The `replicate_experts` output `phy2log` maps physical experts to logical experts.\n        # We want to map physical experts to global logical experts.\n        \n        # Let's directly create the mapping from the original `group_pack_index` and `group_rank_in_pack`.\n        # `group_pack_index`: [num_layers, num_groups] -> maps groups to nodes.\n        # `group_rank_in_pack`: [num_layers, num_groups] -> maps groups to rank within a node's packing.\n        \n        # We need to map physical experts to global logical experts.\n        # The physical experts are arranged: Node 0 (GPU 0, GPU 1, ...), Node 1, ...\n        # Within each node, physical experts are arranged based on `replicate_experts` output.\n        \n        # Let's create the mapping from a \"physical expert index\" to its \"node index\" and \"rank within node\".\n        # The total number of physical experts is `num_physical_experts`.\n        # The number of physical experts per node is `num_physical_experts_per_node`.\n        \n        # For each layer, let's build the mapping.\n        # `phy_idx_to_node_and_rank`: [num_layers, num_physical_experts] -> [node_idx, rank_in_node]\n        \n        # Instead of complex reshaping, let's simplify the mapping creation.\n        # We have `group_pack_index` [num_layers, num_groups] and `group_rank_in_pack` [num_layers, num_groups].\n        # These tell us which node a group belongs to, and its rank within that node.\n        \n        # The original `mlog2log` calculation is crucial. Let's re-examine it:\n        # `group_index_in_node = group_pack_index * groups_per_node + group_rank_in_pack`\n        # This is not right. `group_rank_in_pack` is the rank *within the sorted groups*.\n        # The `balanced_packing` returns `pack_index` and `rank_in_pack`.\n        # `group_pack_index` is the pack index for each group.\n        # `group_rank_in_pack` is the rank of the group within its assigned pack (node).\n        \n        # Let's construct the mapping from physical expert index to global logical expert index.\n        # The physical experts are ordered by node, then by the output of `replicate_experts` within that node.\n        \n        # For each node, we have `replicate_experts` applied to `num_logical_experts // num_nodes` logical experts.\n        # The `phy2mlog` output from `replicate_experts` maps physical experts (within node) to logical experts (within node).\n        # We need to convert the logical expert index (within node) to the global logical expert index.\n        \n        # Let's reconstruct the mapping for each node.\n        # For node `n`, the logical experts are indexed from `n * (num_logical_experts // num_nodes)` to `(n+1) * (num_logical_experts // num_nodes) - 1`.\n        \n        # Create `global_logical_expert_indices_per_node`: [num_nodes, num_logical_experts // num_nodes]\n        global_logical_expert_indices_per_node = torch.arange(num_logical_experts, device=weight.device).unflatten(-1, (num_groups, group_size)) \\\n                                                   .view(num_groups // num_nodes, num_nodes, group_size) \\\n                                                   .transpose(0, 1) \\\n                                                   .contiguous()[:, :, 0] # [num_nodes, num_logical_experts_per_node]\n        \n        # Now, for each node's `phy2mlog` output, index into `global_logical_expert_indices_per_node`.\n        # `phy2mlog` from replicate_experts is `[num_layers, num_logical_experts_per_node]`.\n        # The values are indices into the logical experts *within that node*.\n        \n        # For node `node_idx`:\n        # `node_phy2mlog = phy2mlog_list[node_idx]` # [num_layers, num_physical_experts_per_node]\n        # `node_global_map = global_logical_expert_indices_per_node[node_idx]` # [num_logical_experts_per_node]\n        \n        # `final_phy2log_node = node_global_map.gather(0, node_phy2mlog)` # This is incorrect indexing.\n        \n        # We need to use `node_phy2mlog` as indices into `node_global_map`.\n        # `final_phy2log_node = torch.gather(node_global_map.unsqueeze(0).expand_as(node_phy2mlog), -1, node_phy2mlog)` # Incorrect.\n        \n        # Let's use `torch.gather` correctly.\n        # `torch.gather(input, dim, index)`\n        # `input` is `node_global_map` (shape: [num_logical_experts_per_node])\n        # `dim` is 0 (since we want to index along the logical expert dimension)\n        # `index` is `node_phy2mlog` (shape: [num_layers, num_physical_experts_per_node])\n        \n        node_global_map = global_logical_expert_indices_per_node[node_idx] # [num_logical_experts_per_node]\n        node_phy2mlog = all_phy2mlog_list[node_idx] # [num_layers, num_physical_experts_per_node]\n        \n        # Map `node_phy2mlog` (indices into logical experts within node) to global logical expert indices.\n        # We need to apply `gather` for each layer.\n        final_phy2log_node = torch.gather(node_global_map.unsqueeze(0).expand_as(node_phy2mlog), -1, node_phy2mlog)\n        \n        final_phy2log_list.append(final_phy2log_node)\n\n    # Concatenate the final `phy2log` from all nodes.\n    phy2log = torch.cat(final_phy2log_list, dim=1) # [num_layers, num_physical_experts]\n\n    # Now, let's handle `phyrank` and `logcnt`.\n    # `phyrank` from `replicate_experts` is the rank of the physical expert within its logical expert group.\n    # `mlogcnt` from `replicate_experts` is the number of replicas for each logical expert (within node).\n    \n    # We need to combine `phyrank` and `mlogcnt` across nodes.\n    # `phyrank` has shape `[num_layers, num_physical_experts_per_node]` for each node.\n    # `mlogcnt` has shape `[num_layers, num_physical_experts_per_node]` for each node.\n    \n    phyrank = torch.cat(all_phyrank_list, dim=1) # [num_layers, num_physical_experts]\n    \n    # `mlogcnt` needs to be converted to `expert_count` for the global logical experts.\n    # `mlogcnt` is `[num_layers, num_physical_experts_per_node]` for each node.\n    # We need to aggregate this information to get the count of physical experts for each global logical expert.\n    \n    # The `mlogcnt_node` has shape `[num_layers, num_physical_experts_per_node]`.\n    # The values represent the number of replicas for the logical expert *within that node*.\n    # We need to sum these up for each global logical expert.\n    \n    # Let's create `logcnt_per_node`: [num_nodes, num_layers, num_logical_experts_per_node]\n    logcnt_per_node = torch.stack(all_mlogcnt_list, dim=1) # [num_layers, num_nodes, num_physical_experts_per_node]\n    \n    # Reshape `logcnt_per_node` to group by logical expert within node.\n    # `logcnt_per_node`: [num_layers, num_nodes, num_physical_experts_per_node]\n    # We need to map `logcnt_per_node` based on `phy2mlog` to get counts per logical expert.\n    \n    # Create `logical_expert_counts_per_node_layer`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n    logical_expert_counts_per_node_layer = torch.zeros(num_layers, num_nodes, num_logical_experts // num_nodes, dtype=torch.int64, device=weight.device)\n    \n    for node_idx in range(num_nodes):\n        node_mlogcnt = all_mlogcnt_list[node_idx] # [num_layers, num_physical_experts_per_node]\n        node_phy2mlog = all_phy2mlog_list[node_idx] # [num_layers, num_physical_experts_per_node]\n        \n        # For each layer, we want to sum `node_mlogcnt` values based on `node_phy2mlog`.\n        # This is a scatter_add operation.\n        for l in range(num_layers):\n            # Use scatter_add to sum counts for each logical expert within the node.\n            logical_expert_counts_per_node_layer[l, node_idx].scatter_add_(\n                0,\n                node_phy2mlog[l],\n                node_mlogcnt[l]\n            )\n\n    # Now, sum the counts across nodes for each logical expert.\n    # `logcnt`: [num_layers, num_logical_experts]\n    logcnt = logical_expert_counts_per_node_layer.sum(dim=1) # [num_layers, num_logical_experts // num_nodes]\n    \n    # The `logcnt` should be for the global logical experts.\n    # The current `logcnt` is for logical experts *within each node*.\n    # We need to aggregate `logcnt` based on the node assignment of logical experts.\n    \n    # Let's re-calculate `logcnt` from `phy2log`.\n    # `phy2log` is `[num_layers, num_physical_experts]`.\n    # `phyrank` is `[num_layers, num_physical_experts]`.\n    # We need to count how many physical experts map to each global logical expert.\n    \n    # Create a tensor to store the counts.\n    expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n    \n    # Iterate through all physical experts and increment the count for their corresponding logical expert.\n    for i in range(num_layers):\n        for j in range(num_physical_experts):\n            logical_expert_idx = phy2log[i, j]\n            if logical_expert_idx != -1: # Ensure it's a valid mapping\n                expert_count[i, logical_expert_idx] += 1\n                \n    # The `phyrank` needs to be mapped correctly.\n    # `phyrank` is `[num_layers, num_physical_experts]`.\n    # The values are the replica ranks.\n    \n    # Let's ensure the `replicate_experts` output `phyrank` is correctly used.\n    # `phyrank` from `replicate_experts` is `[X, num_log]`.\n    # In our case, `X` is `num_layers`.\n    # `num_log` is `num_logical_experts_per_node`.\n    \n    # The combined `phyrank` has shape `[num_layers, num_physical_experts]`.\n    # The values within this tensor are the ranks within their respective logical expert groups.\n    # The `phyrank` is already correctly structured if we concatenate it.\n    \n    # We need to ensure the `logcnt` calculation is correct.\n    # `mlogcnt_node` from `replicate_experts` is `[num_layers, num_physical_experts_per_node]`.\n    # It represents the number of replicas for each logical expert *within that node*.\n    \n    # The `logical_expert_counts_per_node_layer` correctly sums up the `mlogcnt` for each logical expert within each node.\n    # Now, we need to sum these counts across nodes to get the total count for each global logical expert.\n    \n    # The `logical_expert_counts_per_node_layer` has shape `[num_layers, num_nodes, num_logical_experts // num_nodes]`.\n    # We need to map this based on the node assignment of logical experts.\n    \n    # Let's create `global_logical_expert_counts`: [num_layers, num_logical_experts]\n    global_logical_expert_counts = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n    \n    for l in range(num_layers):\n        for n in range(num_nodes):\n            # Get the logical experts for this node.\n            node_logical_experts = global_logical_expert_indices_per_node[n] # [num_logical_experts_per_node]\n            # Get the counts for this node and layer.\n            node_counts = logical_expert_counts_per_node_layer[l, n] # [num_logical_experts_per_node]\n            \n            # Use scatter_add to add these counts to the correct global logical expert index.\n            global_logical_expert_counts[l].scatter_add_(0, node_logical_experts, node_counts)\n    \n    logcnt = global_logical_expert_counts # This should be the correct `logcnt`\n\n    return phy2log, phyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    # Cast weight to float for calculations if not already, and keep on original device.\n    # Moving to CPU adds overhead if data is already on GPU and might slow down ops.\n    weight = weight.float()\n    \n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["The current program is experiencing an `unflatten` error and shows 0.0000 for all performance metrics, indicating a critical issue preventing it from running correctly. This suggests a fundamental flaw in the expert rearrangement logic, likely related to incorrect tensor indexing or reshaping, especially in the hierarchical balancing part. The previous top-performing programs show a `balancedness_score` of 0.3528 and `speed_score` of 0.1103, which we aim to recover and surpass.\n\nThe primary area for improvement is to correct the mapping of experts in the `rebalance_experts_hierarchical` function, which is the most complex part of the algorithm. Specifically, the conversion from node-local expert indices (used by `replicate_experts` within a node) to global logical expert indices appears to be flawed. This flaw likely leads to out-of-bounds indices, causing the `unflatten` error. Additionally, several loops can be vectorized using PyTorch's `scatter_add_` and `scatter_` operations to improve efficiency.\n\nHere's a breakdown of the proposed changes:\n\n1.  **Correct Expert Mapping in `rebalance_experts_hierarchical`:** The `global_logical_expert_indices_per_node` and subsequent `phy2log` mapping logic does not correctly account for the reordering of groups performed by `balanced_packing`. A new mapping `node_local_to_global_logical_map` will be introduced, which accurately maps node-local logical expert indices (derived from the `rank_in_pack` output of `balanced_packing`) to their corresponding global logical expert IDs. This is crucial for ensuring that physical experts are correctly assigned to the intended logical experts.\n\n2.  **Vectorize `logical_expert_weights_per_node_layer` Population:** The current implementation uses nested Python loops to sum weights of groups and assign them to node-local logical expert slots. This can be significantly optimized by replacing the loops with a single `torch.scatter_add_` operation, leveraging PyTorch's optimized C++ backend for faster execution.\n\n3.  **Vectorize `node_local_to_global_logical_map` Population:** Similar to the previous point, the creation of the `node_local_to_global_logical_map` can also be vectorized using `torch.scatter_` to populate the mapping efficiently, eliminating Python loops and improving performance.\n\n4.  **Simplify `logcnt` Calculation:** The calculation of `logcnt` (the number of physical replicas for each global logical expert) is currently complex and involves aggregating `mlogcnt` from each node. It's more robust and cleaner to derive `logcnt` directly from the final `phy2log` tensor by counting how many physical experts map to each global logical expert using `torch.scatter_add_`. This removes potential errors from intermediate aggregations and simplifies the code.\n\nThese changes will primarily address the correctness of the algorithm, which is paramount for resolving the current error and enabling the program to run. Once the correctness is established, the vectorized operations will provide a significant boost to the `speed_score`. The accurate expert mapping will also contribute to a better `balancedness_score` by ensuring the load balancing algorithm functions as intended.\n\n---\n\n```diff\n<<<<<<< SEARCH\n    # Initialize with zeros.\n    logical_expert_weights_per_node_layer = torch.zeros(num_layers, num_nodes, num_logical_experts // num_nodes, dtype=weight.dtype, device=weight.device)\n    \n    # Iterate through layers and groups to populate `logical_expert_weights_per_node_layer`.\n    for l in range(num_layers):\n        for g in range(num_groups):\n            node_idx = group_pack_index[l, g]\n            # Find the rank of this group within its node's packing.\n            rank_in_node = group_rank_in_pack[l, g]\n            # The group index `g` corresponds to `g // groups_per_node` blocks of `group_size`.\n            # We need to map group `g` to its position within the node's logical experts.\n            # The `group_rank_in_pack` directly gives this position.\n            logical_expert_idx_within_node = rank_in_node\n            \n            # Add the weight of this group to the corresponding logical expert in the node.\n            logical_expert_weights_per_node_layer[l, node_idx, logical_expert_idx_within_node] += weight[l, g * group_size : (g + 1) * group_size].sum()\n=======\n    # Calculate tokens per group (already done as `tokens_per_group`)\n    # `tokens_per_group` shape: [num_layers, num_groups]\n\n    # Initialize with zeros.\n    logical_expert_weights_per_node_layer = torch.zeros(\n        num_layers, num_nodes, num_logical_experts // num_nodes,\n        dtype=weight.dtype, device=weight.device\n    )\n\n    # Vectorized population of logical_expert_weights_per_node_layer\n    # The target dimension for scatter_add is num_nodes * (num_logical_experts // num_nodes)\n    num_logical_experts_per_node = num_logical_experts // num_nodes\n    \n    # Calculate flat indices for scattering\n    # For each (layer, group), we get node_idx and rank_in_node.\n    # The target index within the flattened (num_nodes * num_logical_experts_per_node) dimension is:\n    # node_idx * num_logical_experts_per_node + rank_in_node\n    flat_target_indices = group_pack_index * num_logical_experts_per_node + group_rank_in_pack\n    \n    # Reshape logical_expert_weights_per_node_layer for scatter_add\n    # We scatter along the last dimension (dim=1 after view)\n    logical_expert_weights_per_node_layer.view(num_layers, -1).scatter_add_(\n        1, # Scatter along the second dimension (the flattened node-local experts)\n        flat_target_indices, # Indices [num_layers, num_groups]\n        tokens_per_group # Source values [num_layers, num_groups]\n    )\n>>>>>>> REPLACE\n<<<<<<< SEARCH\n    # Now, map these back to the original logical expert indices.\n    # `phy2mlog` maps physical experts (within a node) to logical experts (within a node).\n    # We need to convert the \"logical expert within a node\" index to the global logical expert index.\n    \n    # Create the mapping from node-local logical expert index to global logical expert index.\n    node_local_logical_to_global_map = torch.arange(num_logical_experts, device=weight.device).unflatten(-1, (num_groups, group_size)) \\\n                                        .view(num_groups // num_nodes, num_nodes, group_size) \\\n                                        .transpose(0, 1) \\\n                                        .contiguous() # [num_nodes, num_logical_experts_per_node, group_size]\n    # We only need the first element of the inner dimension to get the logical expert index.\n    node_local_logical_to_global_map = node_local_logical_to_global_map[:, :, 0] # [num_nodes, num_logical_experts_per_node]\n\n    # Now, for each node, use its `phy2mlog` to index into this map.\n    node_phy2mlog = phy2mlog[:, node_idx * num_physical_experts_per_node : (node_idx + 1) * num_physical_experts_per_node] # [num_layers, num_physical_experts_per_node]\n    \n    # The `phy2mlog` values are indices into the `node_local_logical_to_global_logical_map`.\n    # We need to map these indices correctly.\n    # The `replicate_experts` output `phy2log` maps physical experts to logical experts.\n    # We want to map physical experts to global logical experts.\n    \n    # Let's directly create the mapping from the original `group_pack_index` and `group_rank_in_pack`.\n    # `group_pack_index`: [num_layers, num_groups] -> maps groups to nodes.\n    # `group_rank_in_pack`: [num_layers, num_groups] -> maps groups to rank within a node's packing.\n    \n    # We need to map physical experts to global logical experts.\n    # The physical experts are arranged: Node 0 (GPU 0, GPU 1, ...), Node 1, ...\n    # Within each node, physical experts are arranged based on `replicate_experts` output.\n    \n    # For each layer, let's build the mapping.\n    # `phy_idx_to_node_and_rank`: [num_layers, num_physical_experts] -> [node_idx, rank_in_node]\n    \n    # Instead of complex reshaping, let's simplify the mapping creation.\n    # We have `group_pack_index` [num_layers, num_groups] and `group_rank_in_pack` [num_layers, num_groups].\n    # These tell us which node a group belongs to, and its rank within that node.\n    \n    # The original `mlog2log` calculation is crucial. Let's re-examine it:\n    # `group_index_in_node = group_pack_index * groups_per_node + group_rank_in_pack`\n    # This is not right. `group_rank_in_pack` is the rank *within the sorted groups*.\n    # The `balanced_packing` returns `pack_index` and `rank_in_pack`.\n    # `group_pack_index` is the pack index for each group.\n    # `group_rank_in_pack` is the rank of the group within its assigned pack (node).\n    \n    # Let's construct the mapping from physical expert index to global logical expert index.\n    # The physical experts are ordered by node, then by the output of `replicate_experts` within that node.\n    \n    # For each node, we have `replicate_experts` applied to `num_logical_experts // num_nodes` logical experts.\n    # The `phy2mlog` output from `replicate_experts` maps physical experts (within node) to logical experts (within node).\n    # We need to convert the logical expert index (within node) to the global logical expert index.\n    \n    # Let's reconstruct the mapping for each node.\n    # For node `n`, the logical experts are indexed from `n * (num_logical_experts // num_nodes)` to `(n+1) * (num_logical_experts // num_nodes) - 1`.\n    \n    # Create `global_logical_expert_indices_per_node`: [num_nodes, num_logical_experts // num_nodes]\n    global_logical_expert_indices_per_node = torch.arange(num_logical_experts, device=weight.device).unflatten(-1, (num_groups, group_size)) \\\n                                                   .view(num_groups // num_nodes, num_nodes, group_size) \\\n                                                   .transpose(0, 1) \\\n                                                   .contiguous()[:, :, 0] # [num_nodes, num_logical_experts_per_node]\n    \n    # Now, for each node's `phy2mlog` output, index into this map.\n    # `phy2mlog` from replicate_experts is `[num_layers, num_logical_experts_per_node]`.\n    # The values are indices into the logical experts *within that node*.\n    \n    # For node `node_idx`:\n    # `node_phy2mlog = phy2mlog_list[node_idx]` # [num_layers, num_physical_experts_per_node]\n    # `node_global_map = global_logical_expert_indices_per_node[node_idx]` # [num_logical_experts_per_node]\n    \n    # `final_phy2log_node = node_global_map.gather(0, node_phy2mlog)` # This is incorrect indexing.\n    \n    # We need to use `node_phy2mlog` as indices into `node_global_map`.\n    # `final_phy2log_node = torch.gather(node_global_map.unsqueeze(0).expand_as(node_phy2mlog), -1, node_phy2mlog)` # Incorrect.\n    \n    # Let's use `torch.gather` correctly.\n    # `torch.gather(input, dim, index)`\n    # `input` is `node_global_map` (shape: [num_logical_experts_per_node])\n    # `dim` is 0 (since we want to index along the logical expert dimension)\n    # `index` is `node_phy2mlog` (shape: [num_layers, num_physical_experts_per_node])\n    \n    node_global_map = global_logical_expert_indices_per_node[node_idx] # [num_logical_experts_per_node]\n    node_phy2mlog = all_phy2mlog_list[node_idx] # [num_layers, num_physical_experts_per_node]\n    \n    # Map `node_phy2mlog` (indices into logical experts within node) to global logical expert indices.\n    # We need to apply `gather` for each layer.\n    final_phy2log_node = torch.gather(node_global_map.unsqueeze(0).expand_as(node_phy2mlog), -1, node_phy2mlog)\n    \n    final_phy2log_list.append(final_phy2log_node)\n=======\n    # Build a mapping from (layer, node_idx, node_local_logical_expert_idx)\n    # to global_logical_expert_idx. This accounts for the reordering by balanced_packing.\n    node_local_to_global_logical_map = torch.empty(\n        num_layers, num_nodes, num_logical_experts // num_nodes,\n        dtype=torch.int64, device=weight.device\n    )\n    \n    # Create global_group_ids: [num_layers, num_groups]\n    global_group_ids_for_map = torch.arange(num_groups, dtype=torch.int64, device=weight.device).expand(num_layers, -1)\n    \n    # Calculate the global logical expert ID for each group\n    global_logical_expert_ids = global_group_ids_for_map * group_size # [num_layers, num_groups]\n    \n    # Calculate the flattened target indices for scattering into node_local_to_global_logical_map\n    num_logical_experts_per_node = num_logical_experts // num_nodes\n    flat_map_target_indices = group_pack_index * num_logical_experts_per_node + group_rank_in_pack # [num_layers, num_groups]\n    \n    # Scatter the global_logical_expert_ids into the map\n    node_local_to_global_logical_map.view(num_layers, -1).scatter_(\n        1, # Scatter along the second dimension (the flattened node-local experts)\n        flat_map_target_indices, # Indices [num_layers, num_groups]\n        global_logical_expert_ids # Source values [num_layers, num_groups]\n    )\n\n    # Now, map the node-local physical expert assignments (phy2mlog) to global logical expert IDs.\n    final_phy2log_list = []\n    for node_idx in range(num_nodes):\n        node_phy2mlog = all_phy2mlog_list[node_idx] # [num_layers, num_physical_experts_per_node]\n        \n        # For each layer, use the node-local_to_global_logical_map to convert indices.\n        # node_local_to_global_logical_map[l, node_idx] is [num_logical_experts_per_node]\n        # node_phy2mlog[l] is [num_physical_experts_per_node]\n        node_phy2log_mapped = torch.empty_like(node_phy2mlog)\n        for l in range(num_layers):\n            node_phy2log_mapped[l] = torch.gather(node_local_to_global_logical_map[l, node_idx], 0, node_phy2mlog[l])\n        final_phy2log_list.append(node_phy2log_mapped)\n>>>>>>> REPLACE\n<<<<<<< SEARCH\n    # Now, let's handle `phyrank` and `logcnt`.\n    # `phyrank` from `replicate_experts` is the rank of the physical expert within its logical expert group.\n    # `mlogcnt` from `replicate_experts` is the number of replicas for each logical expert (within node).\n    \n    # We need to combine `phyrank` and `mlogcnt` across nodes.\n    # `phyrank` has shape `[num_layers, num_physical_experts_per_node]` for each node.\n    # `mlogcnt` has shape `[num_layers, num_physical_experts_per_node]` for each node.\n    \n    phyrank = torch.cat(all_phyrank_list, dim=1) # [num_layers, num_physical_experts]\n    \n    # `mlogcnt` needs to be converted to `expert_count` for the global logical experts.\n    # `mlogcnt` is `[num_layers, num_physical_experts_per_node]` for each node.\n    # We need to aggregate this information to get the count of physical experts for each global logical expert.\n    \n    # The `mlogcnt_node` has shape `[num_layers, num_physical_experts_per_node]`.\n    # The values represent the number of replicas for the logical expert *within that node*.\n    # We need to sum these up for each global logical expert.\n    \n    # Let's create `logcnt_per_node`: [num_nodes, num_layers, num_logical_experts_per_node]\n    logcnt_per_node = torch.stack(all_mlogcnt_list, dim=1) # [num_layers, num_nodes, num_physical_experts_per_node]\n    \n    # Reshape `logcnt_per_node` to group by logical expert within node.\n    # `logcnt_per_node`: [num_layers, num_nodes, num_physical_experts_per_node]\n    # We need to map `logcnt_per_node` based on `phy2mlog` to get counts per logical expert.\n    \n    # Create `logical_expert_counts_per_node_layer`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n    logical_expert_counts_per_node_layer = torch.zeros(num_layers, num_nodes, num_logical_experts // num_nodes, dtype=torch.int64, device=weight.device)\n    \n    for node_idx in range(num_nodes):\n        node_mlogcnt = all_mlogcnt_list[node_idx] # [num_layers, num_physical_experts_per_node]\n        node_phy2mlog = all_phy2mlog_list[node_idx] # [num_layers, num_physical_experts_per_node]\n        \n        # For each layer, we want to sum `node_mlogcnt` values based on `node_phy2mlog`.\n        # This is a scatter_add operation.\n        for l in range(num_layers):\n            # Use scatter_add to sum counts for each logical expert within the node.\n            logical_expert_counts_per_node_layer[l, node_idx].scatter_add_(\n                0,\n                node_phy2mlog[l],\n                node_mlogcnt[l]\n            )\n\n    # Now, sum the counts across nodes for each logical expert.\n    # `logcnt`: [num_layers, num_logical_experts]\n    logcnt = logical_expert_counts_per_node_layer.sum(dim=1) # [num_layers, num_logical_experts // num_nodes]\n    \n    # The `logcnt` should be for the global logical experts.\n    # The current `logcnt` is for logical experts *within each node*.\n    # We need to aggregate `logcnt` based on the node assignment of logical experts.\n    \n    # Let's re-calculate `logcnt` from `phy2log`.\n    # `phy2log` is `[num_layers, num_physical_experts]`.\n    # `phyrank` is `[num_layers, num_physical_experts]`.\n    # We need to count how many physical experts map to each global logical expert.\n    \n    # Create a tensor to store the counts.\n    expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n    \n    # Iterate through all physical experts and increment the count for their corresponding logical expert.\n    for i in range(num_layers):\n        for j in range(num_physical_experts):\n            logical_expert_idx = phy2log[i, j]\n            if logical_expert_idx != -1: # Ensure it's a valid mapping\n                expert_count[i, logical_expert_idx] += 1\n                \n    # The `phyrank` needs to be mapped correctly.\n    # `phyrank` is `[num_layers, num_physical_experts]`.\n    # The values are the replica ranks.\n    \n    # Let's ensure the `replicate_experts` output `phyrank` is correctly used.\n    # `phyrank` from `replicate_experts` is `[X, num_log]`.\n    # In our case, `X` is `num_layers`.\n    # `num_log` is `num_logical_experts_per_node`.\n    \n    # The combined `phyrank` has shape `[num_layers, num_physical_experts]`.\n    # The values within this tensor are the ranks within their respective logical expert groups.\n    # The `phyrank` is already correctly structured if we concatenate it.\n    \n    # We need to ensure the `logcnt` calculation is correct.\n    # `mlogcnt_node` from `replicate_experts` is `[num_layers, num_physical_experts_per_node]`.\n    # It represents the number of replicas for each logical expert *within that node`.\n    \n    # The `logical_expert_counts_per_node_layer` correctly sums up the `mlogcnt` for each logical expert within each node.\n    # Now, we need to sum these counts across nodes to get the total count for each global logical expert.\n    \n    # The `logical_expert_counts_per_node_layer` has shape `[num_layers, num_nodes, num_logical_experts // num_nodes]`.\n    # We need to map this based on the node assignment of logical experts.\n    \n    # Let's create `global_logical_expert_counts`: [num_layers, num_logical_experts]\n    global_logical_expert_counts = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n    \n    for l in range(num_layers):\n        for n in range(num_nodes):\n            # Get the logical experts for this node.\n            node_logical_experts = global_logical_expert_indices_per_node[n] # [num_logical_experts_per_node]\n            # Get the counts for this node and layer.\n            node_counts = logical_expert_counts_per_node_layer[l, n] # [num_logical_experts_per_node]\n            \n            # Use scatter_add to add these counts to the correct global logical expert index.\n            global_logical_expert_counts[l].scatter_add_(0, node_logical_experts, node_counts)\n    \n    logcnt = global_logical_expert_counts # This should be the correct `logcnt`\n=======\n    # Concatenate phyrank from all nodes.\n    phyrank = torch.cat(all_phyrank_list, dim=1) # [num_layers, num_physical_experts]\n\n    # Calculate global logical expert counts (logcnt) directly from the final phy2log.\n    logcnt = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n    for l in range(num_layers):\n        # Count occurrences of each logical expert ID in phy2log[l]\n        # phy2log[l] contains global logical expert IDs, ranging from 0 to num_logical_experts - 1.\n        # torch.bincount is efficient but requires non-negative integers.\n        # scatter_add_ is more general and fits the current tensor structure.\n        logcnt[l].scatter_add_(0, phy2log[l], torch.ones_like(phy2log[l], dtype=torch.int64))\n>>>>>>> REPLACE\n```"]}}}