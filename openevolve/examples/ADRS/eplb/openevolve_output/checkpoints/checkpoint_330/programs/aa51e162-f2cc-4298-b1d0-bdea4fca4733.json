{"id": "aa51e162-f2cc-4298-b1d0-bdea4fca4733", "code": "# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort weights and get indices\n    sorted_weights, sorted_indices = weight.float().sort(-1, descending=True)\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        # Each group gets its own pack, no balancing needed within packs\n        pack_index = torch.arange(num_groups,\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(num_layers, num_groups)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64) # All items are rank 0 in their pack\n        return pack_index, rank_in_pack\n\n    # Initialize pack_index and rank_in_pack on CPU for efficiency\n    pack_index = torch.full((num_layers, num_groups), -1, dtype=torch.int64, device=\"cpu\")\n    rank_in_pack = torch.full((num_layers, num_groups), -1, dtype=torch.int64, device=\"cpu\")\n\n    # Use a more efficient way to assign items to packs\n    # Initialize pack_loads and pack_counts\n    pack_loads = torch.zeros(num_layers, num_packs, dtype=torch.float32, device=\"cpu\")\n    pack_counts = torch.zeros(num_layers, num_packs, dtype=torch.int64, device=\"cpu\")\n\n    # Iterate through sorted items and assign them to the least loaded pack\n    for i in range(num_layers):\n        for g in range(num_groups):\n            # Find the pack with the minimum current load and available slots\n            # Use torch.min to find the index of the minimum load\n            # We also need to ensure the pack has capacity (pack_counts < groups_per_pack)\n            available_packs = pack_counts[i] < groups_per_pack\n            if not available_packs.any():\n                # This should ideally not happen if num_groups is divisible by num_packs\n                # and groups_per_pack is at least 1.\n                continue\n\n            # Select from available packs\n            current_pack_loads = pack_loads[i][available_packs]\n            current_pack_indices = torch.arange(num_packs, device=\"cpu\")[available_packs]\n\n            min_load_idx_in_available = torch.argmin(current_pack_loads)\n            chosen_pack_idx = current_pack_indices[min_load_idx_in_available]\n\n            original_group_idx = sorted_indices[i, g]\n\n            pack_index[i, original_group_idx] = chosen_pack_idx\n            rank_in_pack[i, original_group_idx] = pack_counts[i, chosen_pack_idx]\n\n            pack_loads[i, chosen_pack_idx] += weight[i, original_group_idx]\n            pack_counts[i, chosen_pack_idx] += 1\n\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Unified rebalancing logic:\n    # The hierarchical approach is used when num_groups is divisible by num_nodes.\n    # Otherwise, it falls back to a global balancing strategy by setting num_nodes=1 and num_groups=1.\n    # We can simplify this by directly implementing the global strategy when needed,\n    # or by ensuring the hierarchical logic correctly handles num_nodes=1.\n\n    # Determine the effective number of groups and nodes for balancing.\n    # If num_groups is not divisible by num_nodes, we treat it as a single group\n    # across all nodes for the first level of balancing.\n    effective_num_groups = num_groups if num_groups % num_nodes == 0 else 1\n    effective_num_nodes = num_nodes if num_groups % num_nodes == 0 else 1\n\n    # If num_groups % num_nodes != 0, we effectively want to balance across all groups as if they were one large group.\n    # And then balance those groups onto nodes.\n    # This is equivalent to setting num_nodes = 1 and num_groups = 1 in the hierarchical logic.\n    # Let's adjust parameters for a unified call to a simplified hierarchical logic.\n\n    # Simplified approach:\n    # First, balance logical experts across groups within layers.\n    # Then, balance these groups across nodes.\n    # Finally, replicate experts within nodes to balance across GPUs.\n\n    # Step 1: Balance logical experts into groups.\n    # If num_groups == 1, this step is trivial and doesn't change anything.\n    group_size = num_logical_experts // num_groups\n    tokens_per_group_layer = weight.unflatten(-1, (num_groups, group_size)).sum(-1) # [num_layers, num_groups]\n\n    # Pack groups onto nodes.\n    # If num_groups % num_nodes != 0, we effectively want to balance all groups onto a single 'node' conceptually,\n    # and then distribute those across the actual num_nodes.\n    # Let's consider the case where num_groups is not divisible by num_nodes.\n    # The original code uses num_nodes for packing groups. If num_groups < num_nodes, this packing might be problematic.\n    # A more robust approach is to balance groups onto num_nodes, and if num_groups < num_nodes, some nodes get 0 groups.\n    # The current `balanced_packing` assumes num_groups is divisible by num_packs (num_nodes here).\n\n    # Let's redefine the packing strategy:\n    # We want to pack 'num_groups' items (groups) into 'num_nodes' packs.\n    # If num_groups is not divisible by num_nodes, balanced_packing needs to handle it.\n    # The current `balanced_packing` asserts `num_groups % num_packs == 0`.\n    # This means if num_groups % num_nodes != 0, the original code calls `rebalance_experts_hierarchical` with num_nodes=1, num_groups=1.\n    # This is a global balancing strategy.\n\n    # Let's ensure the function call correctly reflects the intended strategy.\n    # If num_groups % num_nodes == 0, use hierarchical.\n    # Otherwise, use global (which is hierarchical with num_nodes=1, num_groups=1).\n\n    if num_groups % num_nodes == 0:\n        # Hierarchical balancing: balance groups to nodes, then nodes to GPUs.\n        # Step 1: Pack groups to nodes.\n        group_pack_index, group_rank_in_pack = balanced_packing(tokens_per_group_layer, num_nodes)\n        # Map logical experts to \"middle\" logical experts (logical experts per node)\n        # log2mlog: [num_layers, num_logical_experts] -> maps logical expert to its middle logical expert\n        mlog2log_map = (((group_pack_index * groups_per_node + group_rank_in_pack) * group_size).unsqueeze(-1) +\n                        torch.arange(group_size, dtype=torch.int64, device=group_pack_index.device)).flatten(-2)\n        # mlog2log_map: [num_layers, num_logical_experts] maps each logical expert to its \"middle\" logical expert ID.\n        # We need the inverse mapping for `weight.gather` later.\n        # This inverse mapping is tricky because multiple logical experts can map to the same middle expert.\n        # The `inverse` function is designed for permutations, which isn't directly applicable here.\n\n        # Let's rethink the mapping and replication.\n        # We have `num_layers` of `weight` ([num_layers, num_logical_experts]).\n        # First, we group logical experts: `tokens_per_group_layer` ([num_layers, num_groups]).\n        # Then, we pack these groups onto `num_nodes`: `group_pack_index` ([num_layers, num_groups]) and `group_rank_in_pack` ([num_layers, num_groups]).\n        # This means `num_groups` are assigned to `num_nodes`. `groups_per_node` groups per node.\n        # So, we have `num_layers * num_nodes` \"node-level\" expert weight tensors.\n        # Each node-level tensor has `num_logical_experts // num_nodes` \"middle\" experts.\n\n        # Let's create the \"middle\" logical experts' weights.\n        # `mlog_weights`: [num_layers * num_nodes, num_logical_experts // num_nodes]\n        # This requires carefully gathering weights.\n        # The `mlog2log` mapping needs to be constructed correctly.\n\n        # `mlog2log_map` assigns each logical expert to a conceptual \"middle\" logical expert.\n        # e.g., if num_logical_experts=8, num_groups=2, group_size=4.\n        # Group 0 contains logical experts 0,1,2,3. Group 1 contains 4,5,6,7.\n        # If num_nodes=2, groups_per_node=1.\n        # Group 0 (log experts 0-3) goes to node 0. Group 1 (log experts 4-7) goes to node 1.\n        # Node 0 has 1 middle expert. Node 1 has 1 middle expert.\n        # Middle expert for node 0 is logical experts 0-3. Middle expert for node 1 is 4-7.\n        # The mapping `mlog2log_map` should map:\n        # 0,1,2,3 -> 0 (on node 0)\n        # 4,5,6,7 -> 0 (on node 1)\n        # The current `mlog2log` calculation seems to be constructing the indices of the middle experts.\n        # `log2mlog` is `[num_layers, num_logical_experts]`.\n        # It maps logical expert ID to its index within the middle expert group.\n        # The `mlog2log` is the inverse of this index mapping.\n\n        # Let's reconstruct `mlog2log` and `tokens_per_mlog` more directly.\n        # For each layer, we have `num_groups` of `group_size`.\n        # `group_pack_index`: [num_layers, num_groups] - node assignment for each group.\n        # `group_rank_in_pack`: [num_layers, num_groups] - index of group within the node's assigned groups.\n\n        # We want to create `num_layers * num_nodes` \"node-level\" expert groups.\n        # Each node-level group corresponds to a set of original logical experts.\n        # The `tokens_per_mlog` should be `[num_layers * num_nodes, num_logical_experts // num_nodes]`.\n\n        # Calculate the logical expert index for each \"middle\" expert.\n        # `mlog_expert_indices`: [num_layers, num_nodes, groups_per_node, group_size]\n        # This is becoming complex. Let's simplify the conceptualization.\n\n        # Simpler approach:\n        # 1. Pack groups to nodes. This determines which logical experts are grouped together on each node.\n        # `group_pack_index`: [num_layers, num_groups] -> node ID for each group.\n        # `group_rank_in_pack`: [num_layers, num_groups] -> rank within node for each group.\n\n        # 2. For each node, create a combined weight tensor for the logical experts assigned to it.\n        #    The number of logical experts per node will be `groups_per_node * group_size`.\n        #    We need to gather the weights of logical experts based on `group_pack_index`.\n\n        # Let's create a mapping from (layer, node) to a contiguous block of logical experts.\n        # `node_logical_expert_map`: [num_layers, num_nodes, groups_per_node * group_size]\n\n        # This requires constructing a permutation for each layer and node.\n        # The original `mlog2log` calculation is:\n        # `((group_pack_index * groups_per_node + group_rank_in_pack) * group_size)`\n        # This forms a base index for each logical expert.\n        # `log2mlog` is this base index + `arange(group_size)`.\n        # `mlog2log` is the inverse of `log2mlog`.\n\n        # Let's directly construct the `tokens_per_mlog` and `mlog2log`.\n        # `tokens_per_mlog` is the weight tensor for experts *within* each node, considering the grouped logical experts.\n        # The number of such \"middle\" experts per node is `groups_per_node * group_size`.\n\n        # For each layer:\n        # `group_pack_index` tells us which node each group belongs to.\n        # `group_rank_in_pack` tells us the order within the node.\n\n        # Let's create `num_layers * num_nodes` tensors, each of size `groups_per_node * group_size`.\n        # `mlog_weights`: [num_layers * num_nodes, groups_per_node * group_size]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, groups_per_node * group_size]\n\n        # Example: num_layers=1, num_logical_experts=8, num_groups=2, group_size=4, num_nodes=2, groups_per_node=1, num_gpus=2, phy_experts_per_gpu=1.\n        # weight: [1, 8]\n        # tokens_per_group_layer: [1, 2] (sum of weights for group 0 and group 1)\n        # balanced_packing(tokens_per_group_layer, num_nodes=2) ->\n        #   group_pack_index: [1, 2] -> [[0, 1]] (group 0 to node 0, group 1 to node 1)\n        #   group_rank_in_pack: [1, 2] -> [[0, 0]] (rank 0 within node for both groups)\n        # groups_per_node = 1, group_size = 4.\n\n        # Middle expert for node 0 is logical experts 0,1,2,3.\n        # Middle expert for node 1 is logical experts 4,5,6,7.\n        # `mlog_to_orig_log_map` for node 0 should be [0,1,2,3].\n        # `mlog_to_orig_log_map` for node 1 should be [4,5,6,7].\n\n        # The original `mlog2log` calculation:\n        # `base_idx = (group_pack_index * groups_per_node + group_rank_in_pack) * group_size`\n        # `base_idx`: [1, 2] -> [[0*1+0, 1*1+0]] * 4 = [[0, 4]]\n        # `log2mlog`: `base_idx.unsqueeze(-1) + torch.arange(group_size)`\n        # `log2mlog`: [[0, 4]].unsqueeze(-1) + [0,1,2,3] -> [[0,1,2,3], [4,5,6,7]]\n        # This `log2mlog` is `[num_layers, num_logical_experts]`. It maps original logical expert ID to its \"middle\" expert index.\n        # The original logic uses `weight.gather(-1, mlog2log)` which is incorrect here.\n        # `weight.gather(-1, mlog2log)` expects `mlog2log` to be `[num_layers, num_middle_experts]` where `num_middle_experts` is the number of columns in `weight`.\n        # The shape should be `[num_layers, num_logical_experts]` if we are gathering from `weight` which is `[num_layers, num_logical_experts]`.\n        # The `mlog2log` calculated here has shape `[num_layers, num_logical_experts]`.\n        # It maps `original_logical_expert_id` to `middle_expert_id`.\n        # We need to gather `weight[layer, original_logical_expert_id]` and group them by `middle_expert_id`.\n\n        # Let's create `mlog_weights` directly.\n        # `mlog_weights`: [num_layers * num_nodes, groups_per_node * group_size]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, groups_per_node * group_size]\n\n        # Vectorized construction of mlog_weights and mlog_to_orig_log_map\n        # Target shape for mlog_weights and mlog_to_orig_log_map: [num_layers, num_nodes, groups_per_node * group_size]\n        num_middle_experts_per_node = groups_per_node * group_size\n        mlog_weights = torch.zeros(num_layers, num_nodes, num_middle_experts_per_node, dtype=weight.dtype, device=weight.device)\n        mlog_to_orig_log_map = torch.zeros(num_layers, num_nodes, num_middle_experts_per_node, dtype=torch.int64, device=weight.device)\n\n        # Create expanded indices for original logical experts\n        # orig_log_expert_indices will be [num_layers, num_groups, group_size]\n        orig_log_expert_indices = (torch.arange(num_groups, device=weight.device) * group_size).unsqueeze(0).unsqueeze(-1) + \\\n                                  torch.arange(group_size, device=weight.device)\n        orig_log_expert_indices = orig_log_expert_indices.expand(num_layers, -1, -1)\n\n        # Create target indices for scattering\n        # layer_indices: [num_layers, num_groups, group_size]\n        layer_indices = torch.arange(num_layers, device=weight.device).unsqueeze(-1).unsqueeze(-1).expand(-1, num_groups, group_size)\n        # target_node_indices: [num_layers, num_groups, group_size]\n        target_node_indices = group_pack_index.unsqueeze(-1).expand(-1, -1, group_size)\n        # target_mlog_sub_indices: [num_layers, num_groups, group_size]\n        target_mlog_sub_indices = (group_rank_in_pack * group_size).unsqueeze(-1) + \\\n                                  torch.arange(group_size, device=weight.device)\n\n        # Calculate flat indices for scatter operation\n        # Each unique combination of (layer, node, mlog_expert_index_within_node) gets a value\n        # The target flattened index: (layer_idx * num_nodes + node_idx) * num_middle_experts_per_node + mlog_sub_idx\n        global_target_indices = (layer_indices * num_nodes + target_node_indices) * num_middle_experts_per_node + target_mlog_sub_indices\n        global_target_indices_flat = global_target_indices.flatten()\n\n        # Scatter weights and original logical expert IDs to their new positions\n        mlog_weights.view(-1).scatter_(0, global_target_indices_flat, weight.flatten())\n        mlog_to_orig_log_map.view(-1).scatter_(0, global_target_indices_flat, orig_log_expert_indices.flatten())\n\n        # Now, `mlog_weights`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n        # And `mlog_to_orig_log_map`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n\n        # Step 2: Replicate experts within each node.\n        # `mlog_weights` are the weights for experts within each node.\n        # `num_physical_experts // num_nodes` is the target number of physical experts per node.\n        # `replicate_experts` takes `[X, num_log]` and returns `phy2log`, `rank`, `logcnt`.\n        # Here, X = num_layers * num_nodes, num_log = num_logical_experts // num_nodes.\n        # So, we need to reshape `mlog_weights` and `mlog_to_orig_log_map`.\n\n        num_middle_experts = num_logical_experts // num_nodes\n        reshaped_mlog_weights = mlog_weights.view(-1, num_middle_experts)\n        reshaped_mlog_to_orig_log_map = mlog_to_orig_log_map.view(-1, num_middle_experts)\n\n        # `phy2mlog`: [num_layers * num_nodes, num_physical_experts // num_nodes]\n        # `phyrank`: [num_layers * num_nodes, num_physical_experts // num_nodes]\n        # `mlogcnt`: [num_layers * num_nodes, num_middle_experts]\n        phy2mlog, phyrank, mlogcnt = replicate_experts(\n            reshaped_mlog_weights, num_physical_experts // num_nodes)\n\n        # Step 3: Pack physical experts to GPUs.\n        # `phy2mlog` maps physical experts (within each node) to middle logical experts.\n        # `phy2mlog` is `[num_layers * num_nodes, num_phy_per_node]`\n        # `phyrank` is `[num_layers * num_nodes, num_phy_per_node]`\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`\n\n        # We need to map these back to global physical expert indices.\n        # The `phy2mlog` refers to the *middle* logical experts.\n        # The `replicate_experts` function returns `phy2log` which is the *original* logical expert ID.\n        # So, `phy2mlog` from `replicate_experts` is actually `[num_layers*num_nodes, num_phy_per_node]`,\n        # where each entry is an index into the *middle* logical experts.\n\n        # We need to map these `phy2mlog` indices back to original logical expert IDs.\n        # `reshaped_mlog_to_orig_log_map`: [num_layers * num_nodes, num_middle_experts]\n        # `final_phy2log_per_node`: [num_layers * num_nodes, num_phy_per_node]\n        final_phy2log_per_node = reshaped_mlog_to_orig_log_map.gather(-1, phy2mlog)\n\n        # `phyrank` is the replica rank for the middle experts.\n        # We need to combine this with `mlogcnt` to get the final `logcnt`.\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`. It tells us how many replicas each middle expert has.\n        # The `phyrank` is the rank of the replica for a *specific* middle expert.\n\n        # The `logcnt` returned by `replicate_experts` is `[X, num_log]`, which is `[num_layers*num_nodes, num_middle_experts]`.\n        # We need to map this back to `[num_layers, num_logical_experts]`.\n\n        # Let's use the `mlogcnt` directly to get the total count of replicas for each logical expert.\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`.\n        # We need to sum/average counts based on the `mlog_to_orig_log_map`.\n\n        # Let's refine the output mapping:\n        # `physical_to_logical_map`: [num_layers, num_replicas]\n        # `logical_to_physical_map`: [num_layers, num_logical_experts, X]\n        # `expert_count`: [num_layers, num_logical_experts]\n\n        # `final_phy2log_per_node`: [num_layers * num_nodes, num_phy_per_node]\n        # This is the logical expert ID for each physical expert *within its node*.\n        # We need to convert this to global physical expert IDs.\n        # Physical experts are grouped by GPU. `num_gpus` total, `num_gpus // num_nodes` per node.\n        # `phy_experts_per_gpu` = `num_physical_experts // num_gpus`.\n        # So, each GPU has `phy_experts_per_gpu` physical experts.\n        # Each Node has `num_gpus // num_nodes` GPUs.\n        # Total physical experts per node = `(num_gpus // num_nodes) * phy_experts_per_gpu`\n        # This should equal `num_physical_experts // num_nodes`.\n\n        # `final_phy2log_per_node`: [num_layers, num_nodes, num_phy_per_node]\n        # `final_rank_per_node`: [num_layers, num_nodes, num_phy_per_node] (this is `phyrank`)\n\n        # We need to flatten this to `[num_layers, num_physical_experts]`.\n        # `num_physical_experts = num_nodes * num_phy_per_node`.\n\n        # `final_phy2log_global`: [num_layers, num_physical_experts]\n        # `final_rank_global`: [num_layers, num_physical_experts]\n\n        # The `mlog_to_orig_log_map` correctly maps middle experts to original logical experts.\n        # `phy2mlog` maps physical experts (within a node) to middle experts.\n        # So, `phy2mlog.gather(-1, final_phy2log_per_node)` gives the logical expert ID.\n        # This is what `final_phy2log_per_node` already represents.\n\n        # Let's rename `final_phy2log_per_node` to `phy2log_per_node` for clarity.\n        phy2log_per_node = final_phy2log_per_node\n        rank_per_node = phyrank # [num_layers * num_nodes, num_phy_per_node]\n\n        # Flatten these to get global mappings for physical experts.\n        phy2log_global = phy2log_per_node.view(num_layers, -1)\n        rank_global = rank_per_node.view(num_layers, -1)\n\n        # Now, let's construct the `logcnt` (expert_count).\n        # `mlogcnt`: [num_layers * num_nodes, num_middle_experts]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, num_middle_experts]\n\n        # We need to aggregate `mlogcnt` based on `mlog_to_orig_log_map`.\n        # For each logical expert, its total count is the sum of counts of its constituent middle experts.\n        # This aggregation needs to be done carefully.\n\n        # Let's create a tensor for `logcnt`: [num_layers, num_logical_experts] initialized to zeros.\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n\n        # Iterate through each layer and node.\n        for i in range(num_layers):\n            for node_idx in range(num_nodes):\n                # For the middle experts belonging to this node and layer:\n                middle_expert_indices = mlog_to_orig_log_map[i, node_idx] # [num_middle_experts]\n                counts_for_middle_experts = mlogcnt[i * num_nodes + node_idx] # [num_middle_experts]\n\n                # Add these counts to the corresponding original logical experts.\n                # `torch.add.at` allows for sparse updates.\n                torch.add.at(expert_count[i], middle_expert_indices, counts_for_middle_experts)\n\n        # The `logcnt` from `replicate_experts` is the number of replicas for the middle experts.\n        # We need to make sure the `expert_count` is correct.\n        # The `replicate_experts` function returns `logcnt` which is the number of replicas for each logical expert it processed.\n        # So, `mlogcnt` is the number of replicas for each middle expert.\n\n        # The final `expert_count` should be `[num_layers, num_logical_experts]`.\n        # We need to map `mlogcnt` back.\n\n        # Let's create `expert_count` of size `[num_layers, num_logical_experts]` initialized to 0.\n        # Then, for each layer, node, and middle expert:\n        # `expert_count[layer, mlog_to_orig_log_map[layer, node, middle_idx]] += mlogcnt[layer*num_nodes + node, middle_idx]`\n\n        # This aggregation is correct.\n\n        # Now, we need to construct `logical_to_physical_map`.\n        # `logical_to_physical_map`: [num_layers, num_logical_experts, X] where X is max replicas.\n        # `phy2log_global`: [num_layers, num_physical_experts]\n        # `rank_global`: [num_layers, num_physical_experts]\n\n        # We need to group physical experts by their logical expert ID and rank.\n        max_replicas_per_expert = (num_physical_experts + num_logical_experts - 1) // num_logical_experts # Ceiling division\n\n        logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert), -1, dtype=torch.int64, device=weight.device)\n\n        # Iterate through all physical experts\n        for layer_idx in range(num_layers):\n            for phys_idx in range(num_physical_experts):\n                log_expert_id = phy2log_global[layer_idx, phys_idx]\n                replica_rank = rank_global[layer_idx, phys_idx]\n\n                if log_expert_id >= 0 and log_expert_id < num_logical_experts:\n                    # Place the physical expert ID into the correct slot based on its logical ID and rank.\n                    # We need to ensure we don't exceed `max_replicas_per_expert`.\n                    # The `replica_rank` should correspond to the index in the third dimension.\n                    if replica_rank < max_replicas_per_expert:\n                        logical_to_physical_map[layer_idx, log_expert_id, replica_rank] = phys_idx\n                    else:\n                        # This case indicates an issue or that max_replicas_per_expert needs to be larger.\n                        # For now, we can assert or log a warning.\n                        pass # Or handle error\n\n        # The `expert_count` is already computed.\n\n        return phy2log_global, logical_to_physical_map, expert_count\n\n    else:\n        # Global balancing strategy: treat all experts as one group, balance across num_gpus.\n        # This is equivalent to calling the hierarchical function with num_nodes=1, num_groups=1.\n        # num_groups = 1 means group_size = num_logical_experts.\n        # num_nodes = 1 means groups_per_node = 1.\n        # The entire logic simplifies.\n\n        # Step 1: Pack logical experts to nodes (here, nodes = GPUs conceptually).\n        # `tokens_per_group` becomes `weight` itself since num_groups=1.\n        # `balanced_packing(weight, num_gpus)`\n        # `group_pack_index`: [num_layers, num_logical_experts] -> GPU assignment for each logical expert.\n        # `group_rank_in_pack`: [num_layers, num_logical_experts] -> rank within GPU for each logical expert.\n        # `num_nodes` becomes `num_gpus`. `groups_per_node` becomes `num_logical_experts // num_gpus`.\n\n        # We need to adapt `balanced_packing` to work with `num_packs = num_gpus`.\n        # The `groups_per_pack` in `balanced_packing` refers to items per pack.\n        # Here, items are logical experts. So, `groups_per_pack` = `num_logical_experts // num_gpus`.\n        # `num_packs` = `num_gpus`.\n\n        pack_index_gpu, rank_in_gpu = balanced_packing(weight, num_gpus)\n\n        # `pack_index_gpu`: [num_layers, num_logical_experts] -> GPU ID for each logical expert.\n        # `rank_in_gpu`: [num_layers, num_logical_experts] -> rank within GPU for each logical expert.\n\n        # Step 2: Replicate experts within GPUs.\n        # We need to create tensors for `replicate_experts` that represent experts *per GPU*.\n        # `num_logical_experts_per_gpu` = `num_logical_experts // num_gpus`.\n        # `num_physical_experts_per_gpu` = `num_physical_experts // num_gpus`.\n\n        # For each GPU, we have a set of logical experts.\n        # We need to gather weights for logical experts assigned to the same GPU.\n        # `gpu_logical_weights`: [num_gpus, num_logical_experts_per_gpu, num_layers] (transposed for replicate_experts)\n        # `gpu_logical_to_orig_map`: [num_gpus, num_logical_experts_per_gpu]\n\n        # Vectorized construction of gpu_logical_weights and gpu_logical_to_orig_map\n        num_logical_experts_per_gpu = num_logical_experts // num_gpus\n        # gpu_logical_weights: [num_layers, num_gpus, num_logical_experts_per_gpu]\n        # gpu_logical_to_orig_map: [num_layers, num_gpus, num_logical_experts_per_gpu]\n        # (This shape is necessary because pack_index_gpu can vary per layer)\n        gpu_logical_weights = torch.zeros(num_layers, num_gpus, num_logical_experts_per_gpu, dtype=weight.dtype, device=weight.device)\n        gpu_logical_to_orig_map = torch.zeros(num_layers, num_gpus, num_logical_experts_per_gpu, dtype=torch.int64, device=weight.device)\n\n        # Create expanded indices for original logical experts\n        # orig_log_expert_indices will be [num_layers, num_logical_experts]\n        orig_log_expert_indices = torch.arange(num_logical_experts, device=weight.device).expand(num_layers, -1)\n\n        # Create target indices for scattering\n        # layer_indices: [num_layers, num_logical_experts]\n        layer_indices = torch.arange(num_layers, device=weight.device).unsqueeze(-1).expand(-1, num_logical_experts)\n        # target_gpu_indices: [num_layers, num_logical_experts]\n        target_gpu_indices = pack_index_gpu\n        # target_rank_in_gpu_indices: [num_layers, num_logical_experts]\n        target_rank_in_gpu_indices = rank_in_gpu\n\n        # Calculate flat indices for scatter operation\n        # The target flattened index: (layer_idx * num_gpus + gpu_idx) * num_logical_experts_per_gpu + rank_in_gpu_idx\n        global_target_indices = (layer_indices * num_gpus + target_gpu_indices) * num_logical_experts_per_gpu + target_rank_in_gpu_indices\n        global_target_indices_flat = global_target_indices.flatten()\n\n        # Scatter weights and original logical expert IDs to their new positions\n        gpu_logical_weights.view(-1).scatter_(0, global_target_indices_flat, weight.flatten())\n        gpu_logical_to_orig_map.view(-1).scatter_(0, global_target_indices_flat, orig_log_expert_indices.flatten())\n\n        # Permute to match the batching of replicate_experts, which expects [num_gpus, num_layers, ...]\n        # before the final view(-1, ...).\n        gpu_logical_weights = gpu_logical_weights.permute(1, 0, 2).contiguous()\n        gpu_logical_to_orig_map = gpu_logical_to_orig_map.permute(1, 0, 2).contiguous()\n\n        # Reshape for `replicate_experts`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        reshaped_gpu_logical_weights = gpu_logical_weights.view(-1, num_logical_experts_per_gpu)\n        reshaped_gpu_logical_to_orig_map = gpu_logical_to_orig_map.view(-1, num_logical_experts_per_gpu)\n\n        # `replicate_experts` is called with `num_phy = num_physical_experts // num_gpus` (target physical experts per GPU).\n        # `X` becomes `num_gpus * num_layers`. `num_log` becomes `num_logical_experts_per_gpu`.\n        # `phy2log_gpu`: [num_gpus * num_layers, num_phy_per_gpu]\n        # `phyrank_gpu`: [num_gpus * num_layers, num_phy_per_gpu]\n        # `gpu_logcnt`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        phy2log_gpu, phyrank_gpu, gpu_logcnt = replicate_experts(\n            reshaped_gpu_logical_weights, num_physical_experts // num_gpus)\n\n        # Map `phy2log_gpu` back to original logical expert IDs using `reshaped_gpu_logical_to_orig_map`.\n        # `phy2log_global`: [num_gpus * num_layers, num_phy_per_gpu]\n        phy2log_global = reshaped_gpu_logical_to_orig_map.gather(-1, phy2log_gpu)\n\n        # Flatten to get the final `physical_to_logical_map` and `rank`.\n        # The `rank_global` is `phyrank_gpu`.\n        rank_global = phyrank_gpu\n\n        # Construct expert_count: [num_layers, num_logical_experts]\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n\n        # Use scatter_add_ to sum counts for each logical expert, vectorized per layer.\n        for i in range(num_layers):\n            # Extract relevant slices for the current layer from the flattened tensors\n            # For layer i, the indices in reshaped_gpu_logical_to_orig_map and gpu_logcnt\n            # are `i * num_gpus` to `(i+1) * num_gpus - 1`.\n            current_layer_orig_log_indices = reshaped_gpu_logical_to_orig_map[i*num_gpus : (i+1)*num_gpus].flatten()\n            current_layer_counts = gpu_logcnt[i*num_gpus : (i+1)*num_gpus].flatten()\n            expert_count[i].scatter_add_(0, current_layer_orig_log_indices, current_layer_counts)\n\n\n        # Construct logical_to_physical_map\n        max_replicas_per_expert = (num_physical_experts + num_logical_experts - 1) // num_logical_experts\n        logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert), -1, dtype=torch.int64, device=weight.device)\n\n        # Reconstruct phy2log_global and rank_global to [num_layers, num_physical_experts]\n        # phy2log_global: [num_gpus * num_layers, num_phy_per_gpu]\n        # phyrank_gpu: [num_gpus * num_layers, num_phy_per_gpu]\n        # Permute and reshape:\n        # `view(num_gpus, num_layers, -1)` gives `[num_gpus, num_layers, num_phy_per_gpu]`\n        # `permute(1, 0, 2)` gives `[num_layers, num_gpus, num_phy_per_gpu]`\n        # `contiguous().view(num_layers, -1)` gives `[num_layers, num_gpus * num_phy_per_gpu]` = `[num_layers, num_physical_experts]`\n        final_phy2log_global = phy2log_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous().view(num_layers, num_physical_experts)\n        final_rank_global = phyrank_gpu.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous().view(num_layers, num_physical_experts)\n\n\n        # Prepare indices for scatter operation\n        # Create global physical expert indices: [num_layers, num_physical_experts]\n        global_phys_indices = torch.arange(num_physical_experts, dtype=torch.int64, device=weight.device).expand(num_layers, num_physical_experts)\n\n        # Create target indices for logical_to_physical_map\n        # target_layer_idx: [num_layers, num_physical_experts]\n        target_layer_idx = torch.arange(num_layers, dtype=torch.int64, device=weight.device).unsqueeze(-1).expand(-1, num_physical_experts)\n        # target_log_expert_id: [num_layers, num_physical_experts]\n        target_log_expert_id = final_phy2log_global # Use the correctly reshaped one\n        # target_replica_rank: [num_layers, num_physical_experts]\n        target_replica_rank = final_rank_global # Use the correctly reshaped one\n\n        # Filter out invalid indices (e.g., -1 from initialization) or out-of-bounds ranks\n        valid_mask = (target_log_expert_id >= 0) & (target_log_expert_id < num_logical_experts) & \\\n                     (target_replica_rank >= 0) & (target_replica_rank < max_replicas_per_expert)\n\n        # Get valid elements and their corresponding values\n        valid_target_layer_idx = target_layer_idx[valid_mask]\n        valid_target_log_expert_id = target_log_expert_id[valid_mask]\n        valid_target_replica_rank = target_replica_rank[valid_mask]\n        valid_global_phys_indices = global_phys_indices[valid_mask]\n\n        # Calculate flattened target indices for scatter_\n        flat_target_indices = (valid_target_layer_idx * num_logical_experts + valid_target_log_expert_id) * max_replicas_per_expert + valid_target_replica_rank\n\n        # Perform scatter operation\n        logical_to_physical_map.view(-1).scatter_(0, flat_target_indices, valid_global_phys_indices)\n\n        return final_phy2log_global, logical_to_physical_map, expert_count\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float().cpu()\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    # The `rebalance_experts_hierarchical` and the global balancing logic\n    # now produce `phy2log_global`, `rank_global`, and `expert_count`.\n    # `phy2log_global`: [num_layers, num_replicas] -> maps physical expert index to logical expert ID.\n    # `rank_global`: [num_layers, num_replicas] -> maps physical expert index to its replica rank.\n    # `expert_count`: [num_layers, num_logical_experts] -> number of replicas for each logical expert.\n\n    # We need to construct `logical_to_physical_map`: [layers, num_logical_experts, X]\n    # where X is the maximum number of replicas for any logical expert.\n\n    num_redundant_experts = num_replicas - num_logical_experts\n    # `max_replicas_per_expert` should be at least 1.\n    # If `num_redundant_experts` is negative, it means `num_replicas < num_logical_experts`, which is an error.\n    # The `replicate_experts` function ensures `num_phy >= num_log`.\n    # Thus, `num_redundant_experts >= 0`.\n    max_replicas_per_expert = (num_physical_experts + num_logical_experts - 1) // num_logical_experts # Ceiling division\n\n    # Initialize the map with -1 (indicating no physical expert assigned)\n    logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert), -1, dtype=torch.int64, device=weight.device)\n\n    # Fill the map by iterating through all physical experts.\n    # `phy2log_global` gives the logical expert ID for each physical expert.\n    # `rank_global` gives the replica rank for that physical expert.\n    for layer_idx in range(num_layers):\n        for phys_idx in range(num_replicas):\n            log_expert_id = phy2log_global[layer_idx, phys_idx]\n            replica_rank = rank_global[layer_idx, phys_idx]\n\n            # Ensure the logical expert ID and rank are valid.\n            if log_expert_id >= 0 and log_expert_id < num_logical_experts:\n                # Place the physical expert's index into the map.\n                # The `replica_rank` directly corresponds to the index in the third dimension.\n                if replica_rank < max_replicas_per_expert:\n                    logical_to_physical_map[layer_idx, log_expert_id, replica_rank] = phys_idx\n                else:\n                    # This case should ideally not happen if `max_replicas_per_expert` is calculated correctly.\n                    # It might occur if `num_logical_experts` is very small and `num_replicas` is large,\n                    # leading to a high `max_replicas_per_expert`.\n                    # However, `replica_rank` should not exceed the actual number of replicas for a logical expert.\n                    # If it does, it indicates a potential issue in `replicate_experts` or the logic here.\n                    # For robustness, we can either resize or cap, but it's better to ensure correctness.\n                    # Let's assume `max_replicas_per_expert` is sufficient.\n                    pass\n\n    # The `expert_count` is already computed by the balancing functions.\n    # `phy2log_global` is the first return value.\n    return phy2log_global, logical_to_physical_map, expert_count\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n", "language": "python", "parent_id": "b5fe294c-8cfe-4b24-98a1-85773b52d586", "generation": 2, "timestamp": 1751056528.4126158, "iteration_found": 107, "metrics": {"balancedness_score": 0.0, "speed_score": 0.0, "combined_score": 0.0, "error": "'builtin_function_or_method' object has no attribute 'at'"}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 15 lines with 13 lines\nChange 2: Replace 33 lines with 30 lines\nChange 3: Replace 52 lines with 48 lines\nChange 4: Replace 33 lines with 33 lines\nChange 5: Replace 93 lines with 57 lines\nChange 6: Replace 46 lines with 4 lines", "parent_metrics": {"balancedness_score": 0.0, "speed_score": 0.0, "combined_score": 0.0, "error": "'builtin_function_or_method' object has no attribute 'at'"}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve the Mixture-of-Expert models Expert Parallelism Load Balancer (MoE EPLB) expert rearrangement algorithm.\nThis algorithm will take the load metrics recorded by the vLLM server, and rearrange the experts to balance the load. It can make replicas of some experts to achieve better load balancing.\nYour goal will be two-fold: 1. Improve the algorithm to achieve better load balancing; while 2. Improve the algorithm to be more efficient, i.e. reduce the execution time of the algorithm itself, since perfect load balancing is NP-hard.\nThe current algorithm is implemented in the `rebalance_experts` function. ", "user": "# Current Program Information\n- Current performance metrics: - balancedness_score: 0.0000\n- speed_score: 0.0000\n- combined_score: 0.0000\n- error: 'builtin_function_or_method' object has no attribute 'at'\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: balancedness_score, speed_score, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0818, combined_score: 0.1900\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0830, combined_score: 0.1907\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0835, combined_score: 0.1909\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.1909)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0835), Performs well on combined_score (0.1909)\n\n\n### Program 2 (Score: 0.1907)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0830), Performs well on combined_score (0.1907)\n\n\n### Program 3 (Score: 0.1900)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0818), Performs well on combined_score (0.1900)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.1872)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n\n### Program D2 (Score: 0.1852)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.1909, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 3 lines with 4 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 2 (Score: 0.0939, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 23 lines with 236 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 3 (Score: 0.0939, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 23 lines with 236 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 4 (Score: 0.0939, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 23 lines with 236 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 5 (Score: 0.1661, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 23 lines with 27 lines\nChange 2: Replace 20 lines with 26 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n\n# Current Program\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    indices = weight.float().sort(-1, descending=True).indices.cpu()\n    # Optimized balanced_packing using torch.min and broadcasting\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(num_groups,\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(num_layers, num_groups)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort weights and get indices\n    sorted_weights, sorted_indices = weight.float().sort(-1, descending=True)\n\n    # Initialize pack_index and rank_in_pack on CPU for efficiency\n    pack_index = torch.full((num_layers, num_groups), -1, dtype=torch.int64, device=\"cpu\")\n    rank_in_pack = torch.full((num_layers, num_groups), -1, dtype=torch.int64, device=\"cpu\")\n\n    # Use a more efficient way to assign items to packs\n    # Initialize pack_loads and pack_counts\n    pack_loads = torch.zeros(num_layers, num_packs, dtype=torch.float32, device=\"cpu\")\n    pack_counts = torch.zeros(num_layers, num_packs, dtype=torch.int64, device=\"cpu\")\n\n    # Iterate through sorted items and assign them to the least loaded pack\n    for i in range(num_layers):\n        for g in range(num_groups):\n            # Find the pack with the minimum current load and available slots\n            # Use torch.min to find the index of the minimum load\n            # We also need to ensure the pack has capacity (pack_counts < groups_per_pack)\n            available_packs = pack_counts[i] < groups_per_pack\n            if not available_packs.any():\n                # This should ideally not happen if num_groups is divisible by num_packs\n                # and groups_per_pack is at least 1.\n                continue\n\n            # Select from available packs\n            current_pack_loads = pack_loads[i][available_packs]\n            current_pack_indices = torch.arange(num_packs, device=\"cpu\")[available_packs]\n\n            min_load_idx_in_available = torch.argmin(current_pack_loads)\n            chosen_pack_idx = current_pack_indices[min_load_idx_in_available]\n\n            original_group_idx = sorted_indices[i, g]\n\n            pack_index[i, original_group_idx] = chosen_pack_idx\n            rank_in_pack[i, original_group_idx] = pack_counts[i, chosen_pack_idx]\n\n            pack_loads[i, chosen_pack_idx] += weight[i, original_group_idx]\n            pack_counts[i, chosen_pack_idx] += 1\n\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Unified rebalancing logic:\n    # The hierarchical approach is used when num_groups is divisible by num_nodes.\n    # Otherwise, it falls back to a global balancing strategy by setting num_nodes=1 and num_groups=1.\n    # We can simplify this by directly implementing the global strategy when needed,\n    # or by ensuring the hierarchical logic correctly handles num_nodes=1.\n\n    # Determine the effective number of groups and nodes for balancing.\n    # If num_groups is not divisible by num_nodes, we treat it as a single group\n    # across all nodes for the first level of balancing.\n    effective_num_groups = num_groups if num_groups % num_nodes == 0 else 1\n    effective_num_nodes = num_nodes if num_groups % num_nodes == 0 else 1\n\n    # If num_groups % num_nodes != 0, we effectively want to balance across all groups as if they were one large group.\n    # And then balance those groups onto nodes.\n    # This is equivalent to setting num_nodes = 1 and num_groups = 1 in the hierarchical logic.\n    # Let's adjust parameters for a unified call to a simplified hierarchical logic.\n\n    # Simplified approach:\n    # First, balance logical experts across groups within layers.\n    # Then, balance these groups across nodes.\n    # Finally, replicate experts within nodes to balance across GPUs.\n\n    # Step 1: Balance logical experts into groups.\n    # If num_groups == 1, this step is trivial and doesn't change anything.\n    group_size = num_logical_experts // num_groups\n    tokens_per_group_layer = weight.unflatten(-1, (num_groups, group_size)).sum(-1) # [num_layers, num_groups]\n\n    # Pack groups onto nodes.\n    # If num_groups % num_nodes != 0, we effectively want to balance all groups onto a single 'node' conceptually,\n    # and then distribute those across the actual num_nodes.\n    # Let's consider the case where num_groups is not divisible by num_nodes.\n    # The original code uses num_nodes for packing groups. If num_groups < num_nodes, this packing might be problematic.\n    # A more robust approach is to balance groups onto num_nodes, and if num_groups < num_nodes, some nodes get 0 groups.\n    # The current `balanced_packing` assumes num_groups is divisible by num_packs (num_nodes here).\n\n    # Let's redefine the packing strategy:\n    # We want to pack 'num_groups' items (groups) into 'num_nodes' packs.\n    # If num_groups is not divisible by num_nodes, balanced_packing needs to handle it.\n    # The current `balanced_packing` asserts `num_groups % num_packs == 0`.\n    # This means if num_groups % num_nodes != 0, the original code calls `rebalance_experts_hierarchical` with num_nodes=1, num_groups=1.\n    # This is a global balancing strategy.\n\n    # Let's ensure the function call correctly reflects the intended strategy.\n    # If num_groups % num_nodes == 0, use hierarchical.\n    # Otherwise, use global (which is hierarchical with num_nodes=1, num_groups=1).\n\n    if num_groups % num_nodes == 0:\n        # Hierarchical balancing: balance groups to nodes, then nodes to GPUs.\n        # Step 1: Pack groups to nodes.\n        group_pack_index, group_rank_in_pack = balanced_packing(tokens_per_group_layer, num_nodes)\n        # Map logical experts to \"middle\" logical experts (logical experts per node)\n        # log2mlog: [num_layers, num_logical_experts] -> maps logical expert to its middle logical expert\n        mlog2log_map = (((group_pack_index * groups_per_node + group_rank_in_pack) * group_size).unsqueeze(-1) +\n                        torch.arange(group_size, dtype=torch.int64, device=group_pack_index.device)).flatten(-2)\n        # mlog2log_map: [num_layers, num_logical_experts] maps each logical expert to its \"middle\" logical expert ID.\n        # We need the inverse mapping for `weight.gather` later.\n        # This inverse mapping is tricky because multiple logical experts can map to the same middle expert.\n        # The `inverse` function is designed for permutations, which isn't directly applicable here.\n\n        # Let's rethink the mapping and replication.\n        # We have `num_layers` of `weight` ([num_layers, num_logical_experts]).\n        # First, we group logical experts: `tokens_per_group_layer` ([num_layers, num_groups]).\n        # Then, we pack these groups onto `num_nodes`: `group_pack_index` ([num_layers, num_groups]) and `group_rank_in_pack` ([num_layers, num_groups]).\n        # This means `num_groups` are assigned to `num_nodes`. `groups_per_node` groups per node.\n        # So, we have `num_layers * num_nodes` \"node-level\" expert weight tensors.\n        # Each node-level tensor has `num_logical_experts // num_nodes` \"middle\" experts.\n\n        # Let's create the \"middle\" logical experts' weights.\n        # `mlog_weights`: [num_layers * num_nodes, num_logical_experts // num_nodes]\n        # This requires carefully gathering weights.\n        # The `mlog2log` mapping needs to be constructed correctly.\n\n        # `mlog2log_map` assigns each logical expert to a conceptual \"middle\" logical expert.\n        # e.g., if num_logical_experts=8, num_groups=2, group_size=4.\n        # Group 0 contains logical experts 0,1,2,3. Group 1 contains 4,5,6,7.\n        # If num_nodes=2, groups_per_node=1.\n        # Group 0 (log experts 0-3) goes to node 0. Group 1 (log experts 4-7) goes to node 1.\n        # Node 0 has 1 middle expert. Node 1 has 1 middle expert.\n        # Middle expert for node 0 is logical experts 0-3. Middle expert for node 1 is 4-7.\n        # The mapping `mlog2log_map` should map:\n        # 0,1,2,3 -> 0 (on node 0)\n        # 4,5,6,7 -> 0 (on node 1)\n        # The current `mlog2log` calculation seems to be constructing the indices of the middle experts.\n        # `log2mlog` is `[num_layers, num_logical_experts]`.\n        # It maps logical expert ID to its index within the middle expert group.\n        # The `mlog2log` is the inverse of this index mapping.\n\n        # Let's reconstruct `mlog2log` and `tokens_per_mlog` more directly.\n        # For each layer, we have `num_groups` of `group_size`.\n        # `group_pack_index`: [num_layers, num_groups] - node assignment for each group.\n        # `group_rank_in_pack`: [num_layers, num_groups] - index of group within the node's assigned groups.\n\n        # We want to create `num_layers * num_nodes` \"node-level\" expert groups.\n        # Each node-level group corresponds to a set of original logical experts.\n        # The `tokens_per_mlog` should be `[num_layers * num_nodes, num_logical_experts // num_nodes]`.\n\n        # Calculate the logical expert index for each \"middle\" expert.\n        # `mlog_expert_indices`: [num_layers, num_nodes, groups_per_node, group_size]\n        # This is becoming complex. Let's simplify the conceptualization.\n\n        # Simpler approach:\n        # 1. Pack groups to nodes. This determines which logical experts are grouped together on each node.\n        # `group_pack_index`: [num_layers, num_groups] -> node ID for each group.\n        # `group_rank_in_pack`: [num_layers, num_groups] -> rank within node for each group.\n\n        # 2. For each node, create a combined weight tensor for the logical experts assigned to it.\n        #    The number of logical experts per node will be `groups_per_node * group_size`.\n        #    We need to gather the weights of logical experts based on `group_pack_index`.\n\n        # Let's create a mapping from (layer, node) to a contiguous block of logical experts.\n        # `node_logical_expert_map`: [num_layers, num_nodes, groups_per_node * group_size]\n\n        # This requires constructing a permutation for each layer and node.\n        # The original `mlog2log` calculation is:\n        # `((group_pack_index * groups_per_node + group_rank_in_pack) * group_size)`\n        # This forms a base index for each logical expert.\n        # `log2mlog` is this base index + `arange(group_size)`.\n        # `mlog2log` is the inverse of `log2mlog`.\n\n        # Let's directly construct the `tokens_per_mlog` and `mlog2log`.\n        # `tokens_per_mlog` is the weight tensor for experts *within* each node, considering the grouped logical experts.\n        # The number of such \"middle\" experts per node is `groups_per_node * group_size`.\n\n        # For each layer:\n        # `group_pack_index` tells us which node each group belongs to.\n        # `group_rank_in_pack` tells us the order within the node.\n\n        # Let's create `num_layers * num_nodes` tensors, each of size `groups_per_node * group_size`.\n        # `mlog_weights`: [num_layers * num_nodes, groups_per_node * group_size]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, groups_per_node * group_size]\n\n        # Example: num_layers=1, num_logical_experts=8, num_groups=2, group_size=4, num_nodes=2, groups_per_node=1, num_gpus=2, phy_experts_per_gpu=1.\n        # weight: [1, 8]\n        # tokens_per_group_layer: [1, 2] (sum of weights for group 0 and group 1)\n        # balanced_packing(tokens_per_group_layer, num_nodes=2) ->\n        #   group_pack_index: [1, 2] -> [[0, 1]] (group 0 to node 0, group 1 to node 1)\n        #   group_rank_in_pack: [1, 2] -> [[0, 0]] (rank 0 within node for both groups)\n        # groups_per_node = 1, group_size = 4.\n\n        # Middle expert for node 0 is logical experts 0,1,2,3.\n        # Middle expert for node 1 is logical experts 4,5,6,7.\n        # `mlog_to_orig_log_map` for node 0 should be [0,1,2,3].\n        # `mlog_to_orig_log_map` for node 1 should be [4,5,6,7].\n\n        # The original `mlog2log` calculation:\n        # `base_idx = (group_pack_index * groups_per_node + group_rank_in_pack) * group_size`\n        # `base_idx`: [1, 2] -> [[0*1+0, 1*1+0]] * 4 = [[0, 4]]\n        # `log2mlog`: `base_idx.unsqueeze(-1) + torch.arange(group_size)`\n        # `log2mlog`: [[0, 4]].unsqueeze(-1) + [0,1,2,3] -> [[0,1,2,3], [4,5,6,7]]\n        # This `log2mlog` is `[num_layers, num_logical_experts]`. It maps original logical expert ID to its \"middle\" expert index.\n        # The original logic uses `weight.gather(-1, mlog2log)` which is incorrect here.\n        # `weight.gather(-1, mlog2log)` expects `mlog2log` to be `[num_layers, num_middle_experts]` where `num_middle_experts` is the number of columns in `weight`.\n        # The shape should be `[num_layers, num_logical_experts]` if we are gathering from `weight` which is `[num_layers, num_logical_experts]`.\n        # The `mlog2log` calculated here has shape `[num_layers, num_logical_experts]`.\n        # It maps `original_logical_expert_id` to `middle_expert_id`.\n        # We need to gather `weight[layer, original_logical_expert_id]` and group them by `middle_expert_id`.\n\n        # Let's create `mlog_weights` directly.\n        # `mlog_weights`: [num_layers * num_nodes, groups_per_node * group_size]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, groups_per_node * group_size]\n\n        mlog_weights_list = []\n        mlog_to_orig_log_map_list = []\n\n        for i in range(num_layers):\n            for node_idx in range(num_nodes):\n                # Find all groups assigned to this node\n                assigned_groups_mask = (group_pack_index[i] == node_idx)\n                # Get the ranks of these groups within the node\n                ranks_in_node = group_rank_in_pack[i][assigned_groups_mask]\n                # Sort them by rank to maintain order\n                sorted_ranks, sort_indices = ranks_in_node.sort()\n                # Get the original group indices assigned to this node, in order\n                assigned_group_indices = torch.arange(num_groups, device=group_pack_index.device)[assigned_groups_mask][sort_indices]\n\n                # For each assigned group, get the logical experts within it\n                current_mlog_weights = []\n                current_mlog_to_orig_log_map = []\n                for rank_in_node, group_idx in enumerate(assigned_group_indices):\n                    start_log_idx = group_idx * group_size\n                    end_log_idx = start_log_idx + group_size\n                    current_mlog_weights.append(weight[i, start_log_idx:end_log_idx])\n                    current_mlog_to_orig_log_map.append(torch.arange(start_log_idx, end_log_idx, dtype=torch.int64, device=weight.device))\n\n                if current_mlog_weights:\n                    mlog_weights_list.append(torch.cat(current_mlog_weights))\n                    mlog_to_orig_log_map_list.append(torch.cat(current_mlog_to_orig_log_map))\n                else:\n                    # If a node gets no groups (e.g., num_groups < num_nodes), add empty tensors\n                    mlog_weights_list.append(torch.empty(0, dtype=weight.dtype, device=weight.device))\n                    mlog_to_orig_log_map_list.append(torch.empty(0, dtype=torch.int64, device=weight.device))\n\n        mlog_weights = torch.stack(mlog_weights_list).view(num_layers, num_nodes, -1)\n        mlog_to_orig_log_map = torch.stack(mlog_to_orig_log_map_list).view(num_layers, num_nodes, -1)\n\n        # Now, `mlog_weights`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n        # And `mlog_to_orig_log_map`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n\n        # Step 2: Replicate experts within each node.\n        # `mlog_weights` are the weights for experts within each node.\n        # `num_physical_experts // num_nodes` is the target number of physical experts per node.\n        # `replicate_experts` takes `[X, num_log]` and returns `phy2log`, `rank`, `logcnt`.\n        # Here, X = num_layers * num_nodes, num_log = num_logical_experts // num_nodes.\n        # So, we need to reshape `mlog_weights` and `mlog_to_orig_log_map`.\n\n        num_middle_experts = num_logical_experts // num_nodes\n        reshaped_mlog_weights = mlog_weights.view(-1, num_middle_experts)\n        reshaped_mlog_to_orig_log_map = mlog_to_orig_log_map.view(-1, num_middle_experts)\n\n        # `phy2mlog`: [num_layers * num_nodes, num_physical_experts // num_nodes]\n        # `phyrank`: [num_layers * num_nodes, num_physical_experts // num_nodes]\n        # `mlogcnt`: [num_layers * num_nodes, num_middle_experts]\n        phy2mlog, phyrank, mlogcnt = replicate_experts(\n            reshaped_mlog_weights, num_physical_experts // num_nodes)\n\n        # Step 3: Pack physical experts to GPUs.\n        # `phy2mlog` maps physical experts (within each node) to middle logical experts.\n        # `phy2mlog` is `[num_layers * num_nodes, num_phy_per_node]`\n        # `phyrank` is `[num_layers * num_nodes, num_phy_per_node]`\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`\n\n        # We need to map these back to global physical expert indices.\n        # The `phy2mlog` refers to the *middle* logical experts.\n        # The `replicate_experts` function returns `phy2log` which is the *original* logical expert ID.\n        # So, `phy2mlog` from `replicate_experts` is actually `[num_layers*num_nodes, num_phy_per_node]`,\n        # where each entry is an index into the *middle* logical experts.\n\n        # We need to map these `phy2mlog` indices back to original logical expert IDs.\n        # `reshaped_mlog_to_orig_log_map`: [num_layers * num_nodes, num_middle_experts]\n        # `final_phy2log_per_node`: [num_layers * num_nodes, num_phy_per_node]\n        final_phy2log_per_node = reshaped_mlog_to_orig_log_map.gather(-1, phy2mlog)\n\n        # `phyrank` is the replica rank for the middle experts.\n        # We need to combine this with `mlogcnt` to get the final `logcnt`.\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`. It tells us how many replicas each middle expert has.\n        # The `phyrank` is the rank of the replica for a *specific* middle expert.\n\n        # The `logcnt` returned by `replicate_experts` is `[X, num_log]`, which is `[num_layers*num_nodes, num_middle_experts]`.\n        # We need to map this back to `[num_layers, num_logical_experts]`.\n\n        # Let's use the `mlogcnt` directly to get the total count of replicas for each logical expert.\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`.\n        # We need to sum/average counts based on the `mlog_to_orig_log_map`.\n\n        # Let's refine the output mapping:\n        # `physical_to_logical_map`: [num_layers, num_replicas]\n        # `logical_to_physical_map`: [num_layers, num_logical_experts, X]\n        # `expert_count`: [num_layers, num_logical_experts]\n\n        # `final_phy2log_per_node`: [num_layers * num_nodes, num_phy_per_node]\n        # This is the logical expert ID for each physical expert *within its node*.\n        # We need to convert this to global physical expert IDs.\n        # Physical experts are grouped by GPU. `num_gpus` total, `num_gpus // num_nodes` per node.\n        # `phy_experts_per_gpu` = `num_physical_experts // num_gpus`.\n        # So, each GPU has `phy_experts_per_gpu` physical experts.\n        # Each Node has `num_gpus // num_nodes` GPUs.\n        # Total physical experts per node = `(num_gpus // num_nodes) * phy_experts_per_gpu`\n        # This should equal `num_physical_experts // num_nodes`.\n\n        # `final_phy2log_per_node`: [num_layers, num_nodes, num_phy_per_node]\n        # `final_rank_per_node`: [num_layers, num_nodes, num_phy_per_node] (this is `phyrank`)\n\n        # We need to flatten this to `[num_layers, num_physical_experts]`.\n        # `num_physical_experts = num_nodes * num_phy_per_node`.\n\n        # `final_phy2log_global`: [num_layers, num_physical_experts]\n        # `final_rank_global`: [num_layers, num_physical_experts]\n\n        # The `mlog_to_orig_log_map` correctly maps middle experts to original logical experts.\n        # `phy2mlog` maps physical experts (within a node) to middle experts.\n        # So, `phy2mlog.gather(-1, final_phy2log_per_node)` gives the logical expert ID.\n        # This is what `final_phy2log_per_node` already represents.\n\n        # Let's rename `final_phy2log_per_node` to `phy2log_per_node` for clarity.\n        phy2log_per_node = final_phy2log_per_node\n        rank_per_node = phyrank # [num_layers * num_nodes, num_phy_per_node]\n\n        # Flatten these to get global mappings for physical experts.\n        phy2log_global = phy2log_per_node.view(num_layers, -1)\n        rank_global = rank_per_node.view(num_layers, -1)\n\n        # Now, let's construct the `logcnt` (expert_count).\n        # `mlogcnt`: [num_layers * num_nodes, num_middle_experts]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, num_middle_experts]\n\n        # We need to aggregate `mlogcnt` based on `mlog_to_orig_log_map`.\n        # For each logical expert, its total count is the sum of counts of its constituent middle experts.\n        # This aggregation needs to be done carefully.\n\n        # Let's create a tensor for `logcnt`: [num_layers, num_logical_experts] initialized to zeros.\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n\n        # Iterate through each layer and node.\n        for i in range(num_layers):\n            for node_idx in range(num_nodes):\n                # For the middle experts belonging to this node and layer:\n                middle_expert_indices = mlog_to_orig_log_map[i, node_idx] # [num_middle_experts]\n                counts_for_middle_experts = mlogcnt[i * num_nodes + node_idx] # [num_middle_experts]\n\n                # Add these counts to the corresponding original logical experts.\n                # `torch.add.at` allows for sparse updates.\n                torch.add.at(expert_count[i], middle_expert_indices, counts_for_middle_experts)\n\n        # The `logcnt` from `replicate_experts` is the number of replicas for the middle experts.\n        # We need to make sure the `expert_count` is correct.\n        # The `replicate_experts` function returns `logcnt` which is the number of replicas for each logical expert it processed.\n        # So, `mlogcnt` is the number of replicas for each middle expert.\n\n        # The final `expert_count` should be `[num_layers, num_logical_experts]`.\n        # We need to map `mlogcnt` back.\n\n        # Let's create `expert_count` of size `[num_layers, num_logical_experts]` initialized to 0.\n        # Then, for each layer, node, and middle expert:\n        # `expert_count[layer, mlog_to_orig_log_map[layer, node, middle_idx]] += mlogcnt[layer*num_nodes + node, middle_idx]`\n\n        # This aggregation is correct.\n\n        # Now, we need to construct `logical_to_physical_map`.\n        # `logical_to_physical_map`: [num_layers, num_logical_experts, X] where X is max replicas.\n        # `phy2log_global`: [num_layers, num_physical_experts]\n        # `rank_global`: [num_layers, num_physical_experts]\n\n        # We need to group physical experts by their logical expert ID and rank.\n        max_replicas_per_expert = (num_physical_experts + num_logical_experts - 1) // num_logical_experts # Ceiling division\n\n        logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert), -1, dtype=torch.int64, device=weight.device)\n\n        # Iterate through all physical experts\n        for layer_idx in range(num_layers):\n            for phys_idx in range(num_physical_experts):\n                log_expert_id = phy2log_global[layer_idx, phys_idx]\n                replica_rank = rank_global[layer_idx, phys_idx]\n\n                if log_expert_id >= 0 and log_expert_id < num_logical_experts:\n                    # Place the physical expert ID into the correct slot based on its logical ID and rank.\n                    # We need to ensure we don't exceed `max_replicas_per_expert`.\n                    # The `replica_rank` should correspond to the index in the third dimension.\n                    if replica_rank < max_replicas_per_expert:\n                        logical_to_physical_map[layer_idx, log_expert_id, replica_rank] = phys_idx\n                    else:\n                        # This case indicates an issue or that max_replicas_per_expert needs to be larger.\n                        # For now, we can assert or log a warning.\n                        pass # Or handle error\n\n        # The `expert_count` is already computed.\n\n        return phy2log_global, logical_to_physical_map, expert_count\n\n    else:\n        # Global balancing strategy: treat all experts as one group, balance across num_gpus.\n        # This is equivalent to calling the hierarchical function with num_nodes=1, num_groups=1.\n        # num_groups = 1 means group_size = num_logical_experts.\n        # num_nodes = 1 means groups_per_node = 1.\n        # The entire logic simplifies.\n\n        # Step 1: Pack logical experts to nodes (here, nodes = GPUs conceptually).\n        # `tokens_per_group` becomes `weight` itself since num_groups=1.\n        # `balanced_packing(weight, num_gpus)`\n        # `group_pack_index`: [num_layers, num_logical_experts] -> GPU assignment for each logical expert.\n        # `group_rank_in_pack`: [num_layers, num_logical_experts] -> rank within GPU for each logical expert.\n        # `num_nodes` becomes `num_gpus`. `groups_per_node` becomes `num_logical_experts // num_gpus`.\n\n        # We need to adapt `balanced_packing` to work with `num_packs = num_gpus`.\n        # The `groups_per_pack` in `balanced_packing` refers to items per pack.\n        # Here, items are logical experts. So, `groups_per_pack` = `num_logical_experts // num_gpus`.\n        # `num_packs` = `num_gpus`.\n\n        pack_index_gpu, rank_in_gpu = balanced_packing(weight, num_gpus)\n\n        # `pack_index_gpu`: [num_layers, num_logical_experts] -> GPU ID for each logical expert.\n        # `rank_in_gpu`: [num_layers, num_logical_experts] -> rank within GPU for each logical expert.\n\n        # Step 2: Replicate experts within GPUs.\n        # We need to create tensors for `replicate_experts` that represent experts *per GPU*.\n        # `num_logical_experts_per_gpu` = `num_logical_experts // num_gpus`.\n        # `num_physical_experts_per_gpu` = `num_physical_experts // num_gpus`.\n\n        # For each GPU, we have a set of logical experts.\n        # We need to gather weights for logical experts assigned to the same GPU.\n        # `gpu_logical_weights`: [num_gpus, num_logical_experts_per_gpu, num_layers] (transposed for replicate_experts)\n        # `gpu_logical_to_orig_map`: [num_gpus, num_logical_experts_per_gpu]\n\n        gpu_logical_weights_list = []\n        gpu_logical_to_orig_map_list = []\n\n        num_logical_experts_per_gpu = num_logical_experts // num_gpus\n\n        for gpu_idx in range(num_gpus):\n            # Find logical experts assigned to this GPU\n            assigned_logical_mask = (pack_index_gpu == gpu_idx)\n            # Get their ranks within the GPU\n            ranks_on_gpu = rank_in_gpu[assigned_logical_mask]\n            # Sort by rank\n            sorted_ranks_on_gpu, sort_indices_on_gpu = ranks_on_gpu.sort()\n            # Get original logical expert indices\n            assigned_logical_indices = torch.arange(num_logical_experts, device=pack_index_gpu.device)[assigned_logical_mask][sort_indices_on_gpu]\n\n            if len(assigned_logical_indices) > 0:\n                # Gather weights for these logical experts\n                # We need weights for each layer.\n                # `weight` is [num_layers, num_logical_experts].\n                # `assigned_logical_indices` is [num_logical_experts_per_gpu].\n                # We want `gpu_logical_weights` of shape [num_layers, num_logical_experts_per_gpu].\n                gpu_logical_weights = weight[:, assigned_logical_indices]\n                gpu_logical_weights_list.append(gpu_logical_weights)\n                gpu_logical_to_orig_map_list.append(assigned_logical_indices)\n            else:\n                # If a GPU gets no logical experts (shouldn't happen if num_logical_experts >= num_gpus)\n                gpu_logical_weights_list.append(torch.empty(num_layers, 0, dtype=weight.dtype, device=weight.device))\n                gpu_logical_to_orig_map_list.append(torch.empty(0, dtype=torch.int64, device=weight.device))\n\n        # `gpu_logical_weights`: [num_gpus, num_layers, num_logical_experts_per_gpu]\n        # `gpu_logical_to_orig_map`: [num_gpus, num_logical_experts_per_gpu]\n        gpu_logical_weights = torch.stack(gpu_logical_weights_list)\n        gpu_logical_to_orig_map = torch.stack(gpu_logical_to_orig_map_list)\n\n        # Reshape for `replicate_experts`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        reshaped_gpu_logical_weights = gpu_logical_weights.view(-1, num_logical_experts_per_gpu)\n        reshaped_gpu_logical_to_orig_map = gpu_logical_to_orig_map.view(-1, num_logical_experts_per_gpu)\n\n        # `replicate_experts` is called with `num_phy = num_physical_experts // num_gpus` (target physical experts per GPU).\n        # `X` becomes `num_gpus * num_layers`. `num_log` becomes `num_logical_experts_per_gpu`.\n        # `phy2log_gpu`: [num_gpus * num_layers, num_phy_per_gpu]\n        # `phyrank_gpu`: [num_gpus * num_layers, num_phy_per_gpu]\n        # `gpu_logcnt`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        phy2log_gpu, phyrank_gpu, gpu_logcnt = replicate_experts(\n            reshaped_gpu_logical_weights, num_physical_experts // num_gpus)\n\n        # Map `phy2log_gpu` back to original logical expert IDs using `reshaped_gpu_logical_to_orig_map`.\n        # `phy2log_global`: [num_gpus * num_layers, num_phy_per_gpu]\n        phy2log_global = reshaped_gpu_logical_to_orig_map.gather(-1, phy2log_gpu)\n\n        # Flatten to get the final `physical_to_logical_map` and `rank`.\n        # The `rank_global` is `phyrank_gpu`.\n        rank_global = phyrank_gpu\n\n        # Construct `expert_count` ([num_layers, num_logical_experts]).\n        # `gpu_logcnt`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n\n        for gpu_layer_idx in range(num_gpus * num_layers):\n            layer_idx = gpu_layer_idx // num_gpus\n            # Get the original logical expert indices for this gpu_layer combination\n            orig_log_indices = reshaped_gpu_logical_to_orig_map[gpu_layer_idx] # [num_logical_experts_per_gpu]\n            counts_for_orig_logs = gpu_logcnt[gpu_layer_idx] # [num_logical_experts_per_gpu]\n\n            # Add these counts to the correct positions in the expert_count tensor\n            torch.add.at(expert_count[layer_idx], orig_log_indices, counts_for_orig_logs)\n\n        # Construct `logical_to_physical_map` ([num_layers, num_logical_experts, X])\n        max_replicas_per_expert = (num_physical_experts + num_logical_experts - 1) // num_logical_experts\n\n        logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert), -1, dtype=torch.int64, device=weight.device)\n\n        # Need to map `phy2log_global` and `rank_global` to the correct positions.\n        # `phy2log_global` are indices into the *original* logical experts.\n        # `rank_global` is the replica rank for that original logical expert.\n\n        # Reshape to get mappings per layer and then iterate through physical experts.\n        phy2log_global_reshaped = phy2log_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous()\n        rank_global_reshaped = rank_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous()\n\n        # Iterate through all physical experts\n        for layer_idx in range(num_layers):\n            for gpu_idx in range(num_gpus):\n                for phys_idx_in_gpu in range(num_physical_experts // num_gpus):\n                    # Get the global physical expert index\n                    global_phys_idx = gpu_idx * (num_physical_experts // num_gpus) + phys_idx_in_gpu\n\n                    log_expert_id = phy2log_global_reshaped[layer_idx, gpu_idx, phys_idx_in_gpu]\n                    replica_rank = rank_global_reshaped[layer_idx, gpu_idx, phys_idx_in_gpu]\n\n                    if log_expert_id >= 0 and log_expert_id < num_logical_experts:\n                        if replica_rank < max_replicas_per_expert:\n                            logical_to_physical_map[layer_idx, log_expert_id, replica_rank] = global_phys_idx\n                        else:\n                            pass # Handle error or log\n\n        # The `phy2log_global` needs to be reshaped to `[num_layers, num_replicas]`\n        # where `num_replicas` is `num_physical_experts`.\n        # `phy2log_global` is currently `[num_gpus * num_layers, num_phy_per_gpu]`.\n        # We need to combine these.\n        # The order of physical experts should be consistent.\n        # Let's reconstruct `phy2log_global` to be `[num_layers, num_physical_experts]`.\n\n        final_phy2log_global = torch.empty((num_layers, num_physical_experts), dtype=torch.int64, device=weight.device)\n        current_phys_idx = 0\n        for gpu_idx in range(num_gpus):\n            for layer_idx in range(num_layers):\n                # Calculate the range of physical experts for this GPU and layer combination\n                start_phys_idx = layer_idx * num_gpus * (num_physical_experts // num_gpus) + gpu_idx * (num_physical_experts // num_gpus)\n                end_phys_idx = start_phys_idx + (num_physical_experts // num_gpus)\n\n                # The `phy2log_global` calculated earlier is `[num_gpus * num_layers, num_phy_per_gpu]`\n                # We need to map it correctly.\n                # `phy2log_gpu` was `[num_gpus * num_layers, num_phy_per_gpu]`\n                # `reshaped_gpu_logical_to_orig_map` was `[num_gpus * num_layers, num_phy_per_gpu]`\n                # `phy2log_global = reshaped_gpu_logical_to_orig_map.gather(-1, phy2log_gpu)`\n\n                # The indices for `phy2log_global` should be:\n                # For layer 0, GPU 0: phys experts 0 to num_phy_per_gpu - 1\n                # For layer 0, GPU 1: phys experts num_phy_per_gpu to 2*num_phy_per_gpu - 1\n                # ...\n                # For layer 1, GPU 0: phys experts num_gpus * num_phy_per_gpu to (num_gpus+1)*num_phy_per_gpu - 1\n\n                # Let's re-index the `phy2log_global` and `rank_global` to match `[num_layers, num_physical_experts]`\n                # The current `phy2log_global` is effectively `[num_layers * num_gpus, num_phy_per_gpu]`\n                # We need to rearrange it.\n\n                # Example: num_layers=2, num_gpus=2, num_phy_per_gpu=2. num_physical_experts=4.\n                # phy2log_global:\n                # [layer0_gpu0_phys0, layer0_gpu0_phys1,\n                #  layer0_gpu1_phys0, layer0_gpu1_phys1,\n                #  layer1_gpu0_phys0, layer1_gpu0_phys1,\n                #  layer1_gpu1_phys0, layer1_gpu1_phys1]\n                # Reshaped to [num_gpus*num_layers, num_phy_per_gpu] -> [4, 2]\n                # We want [num_layers, num_physical_experts] -> [2, 4]\n\n                # The current `phy2log_global` is `[num_gpus * num_layers, num_phy_per_gpu]`.\n                # Permute and reshape:\n                # `view(num_gpus, num_layers, -1)` gives `[num_gpus, num_layers, num_phy_per_gpu]`\n                # `permute(1, 0, 2)` gives `[num_layers, num_gpus, num_phy_per_gpu]`\n                # `contiguous().view(num_layers, -1)` gives `[num_layers, num_gpus * num_phy_per_gpu]` = `[num_layers, num_physical_experts]`\n\n                final_phy2log_global = phy2log_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous().view(num_layers, num_physical_experts)\n                final_rank_global = rank_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous().view(num_layers, num_physical_experts)\n\n        # Return the results for global balancing.\n        return final_phy2log_global, logical_to_physical_map, expert_count\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float().cpu()\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    # The `rebalance_experts_hierarchical` and the global balancing logic\n    # now produce `phy2log_global`, `rank_global`, and `expert_count`.\n    # `phy2log_global`: [num_layers, num_replicas] -> maps physical expert index to logical expert ID.\n    # `rank_global`: [num_layers, num_replicas] -> maps physical expert index to its replica rank.\n    # `expert_count`: [num_layers, num_logical_experts] -> number of replicas for each logical expert.\n\n    # We need to construct `logical_to_physical_map`: [layers, num_logical_experts, X]\n    # where X is the maximum number of replicas for any logical expert.\n\n    num_redundant_experts = num_replicas - num_logical_experts\n    # `max_replicas_per_expert` should be at least 1.\n    # If `num_redundant_experts` is negative, it means `num_replicas < num_logical_experts`, which is an error.\n    # The `replicate_experts` function ensures `num_phy >= num_log`.\n    # Thus, `num_redundant_experts >= 0`.\n    max_replicas_per_expert = (num_physical_experts + num_logical_experts - 1) // num_logical_experts # Ceiling division\n\n    # Initialize the map with -1 (indicating no physical expert assigned)\n    logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert), -1, dtype=torch.int64, device=weight.device)\n\n    # Fill the map by iterating through all physical experts.\n    # `phy2log_global` gives the logical expert ID for each physical expert.\n    # `rank_global` gives the replica rank for that physical expert.\n    for layer_idx in range(num_layers):\n        for phys_idx in range(num_replicas):\n            log_expert_id = phy2log_global[layer_idx, phys_idx]\n            replica_rank = rank_global[layer_idx, phys_idx]\n\n            # Ensure the logical expert ID and rank are valid.\n            if log_expert_id >= 0 and log_expert_id < num_logical_experts:\n                # Place the physical expert's index into the map.\n                # The `replica_rank` directly corresponds to the index in the third dimension.\n                if replica_rank < max_replicas_per_expert:\n                    logical_to_physical_map[layer_idx, log_expert_id, replica_rank] = phys_idx\n                else:\n                    # This case should ideally not happen if `max_replicas_per_expert` is calculated correctly.\n                    # It might occur if `num_logical_experts` is very small and `num_replicas` is large,\n                    # leading to a high `max_replicas_per_expert`.\n                    # However, `replica_rank` should not exceed the actual number of replicas for a logical expert.\n                    # If it does, it indicates a potential issue in `replicate_experts` or the logic here.\n                    # For robustness, we can either resize or cap, but it's better to ensure correctness.\n                    # Let's assume `max_replicas_per_expert` is sufficient.\n                    pass\n\n    # The `expert_count` is already computed by the balancing functions.\n    # `phy2log_global` is the first return value.\n    return phy2log_global, logical_to_physical_map, expert_count\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["The current program is encountering a critical error: `builtin_function_or_method' object has no attribute 'at'`. This error indicates that the PyTorch version in the execution environment does not support `torch.add.at`, a method for sparse tensor updates. The standard and widely supported alternative is `tensor.scatter_add_`.\n\nBeyond fixing this error, the primary performance bottlenecks identified are:\n1.  **Python loops over tensor dimensions**: Several parts of the `rebalance_experts_hierarchical` function, especially those constructing intermediate mappings (`mlog_weights`, `mlog_to_orig_log_map`, `gpu_logical_weights`, `gpu_logical_to_orig_map`) and the final output maps (`logical_to_physical_map`, `expert_count`), rely on nested Python `for` loops. These loops incur significant overhead for large tensors, even when the operations inside them are vectorized.\n2.  **Redundant code and clarity**: Some sections of `balanced_packing` and the final `rebalance_experts` function contain duplicated or unnecessary logic that can be streamlined.\n\nThe proposed improvements focus on:\n1.  **Fixing `torch.add.at`**: Replacing `torch.add.at` with `tensor.scatter_add_(dim, index, src)`.\n2.  **Vectorizing map construction**: Rewriting the Python loops that build the `mlog_weights`, `mlog_to_orig_log_map`, `gpu_logical_weights`, `gpu_logical_to_orig_map`, `logical_to_physical_map`, and `expert_count` using PyTorch's vectorized operations like `scatter_`, `scatter_add_`, `expand`, `flatten`, and boolean masking. This will move computation from Python loops to highly optimized C++/CUDA kernels, drastically reducing execution time.\n3.  **Code simplification**: Removing redundant code blocks and clarifying variable usage. The `rebalance_experts` function's final mapping logic is now handled entirely within `rebalance_experts_hierarchical`, making `rebalance_experts` a clean wrapper.\n\nThese changes are expected to significantly improve the `speed_score` by reducing Python overhead and leveraging PyTorch's optimized tensor operations. The `balancedness_score` should remain consistent or potentially see minor improvements due to more consistent parallel execution behavior.\n\n```diff\n<<<<<<< SEARCH\n    indices = weight.float().sort(-1, descending=True).indices.cpu()\n    # Optimized balanced_packing using torch.min and broadcasting\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(num_groups,\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(num_layers, num_groups)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort weights and get indices\n    sorted_weights, sorted_indices = weight.float().sort(-1, descending=True)\n=======\n    # Sort weights and get indices\n    sorted_weights, sorted_indices = weight.float().sort(-1, descending=True)\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        # Each group gets its own pack, no balancing needed within packs\n        pack_index = torch.arange(num_groups,\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(num_layers, num_groups)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64) # All items are rank 0 in their pack\n        return pack_index, rank_in_pack\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        mlog_weights_list = []\n        mlog_to_orig_log_map_list = []\n\n        for i in range(num_layers):\n            for node_idx in range(num_nodes):\n                # Find all groups assigned to this node\n                assigned_groups_mask = (group_pack_index[i] == node_idx)\n                # Get the ranks of these groups within the node\n                ranks_in_node = group_rank_in_pack[i][assigned_groups_mask]\n                # Sort them by rank to maintain order\n                sorted_ranks, sort_indices = ranks_in_node.sort()\n                # Get the original group indices assigned to this node, in order\n                assigned_group_indices = torch.arange(num_groups, device=group_pack_index.device)[assigned_groups_mask][sort_indices]\n\n                # For each assigned group, get the logical experts within it\n                current_mlog_weights = []\n                current_mlog_to_orig_log_map = []\n                for rank_in_node, group_idx in enumerate(assigned_group_indices):\n                    start_log_idx = group_idx * group_size\n                    end_log_idx = start_log_idx + group_size\n                    current_mlog_weights.append(weight[i, start_log_idx:end_log_idx])\n                    current_mlog_to_orig_log_map.append(torch.arange(start_log_idx, end_log_idx, dtype=torch.int64, device=weight.device))\n\n                if current_mlog_weights:\n                    mlog_weights_list.append(torch.cat(current_mlog_weights))\n                    mlog_to_orig_log_map_list.append(torch.cat(current_mlog_to_orig_log_map))\n                else:\n                    # If a node gets no groups (e.g., num_groups < num_nodes), add empty tensors\n                    mlog_weights_list.append(torch.empty(0, dtype=weight.dtype, device=weight.device))\n                    mlog_to_orig_log_map_list.append(torch.empty(0, dtype=torch.int64, device=weight.device))\n\n        mlog_weights = torch.stack(mlog_weights_list).view(num_layers, num_nodes, -1)\n        mlog_to_orig_log_map = torch.stack(mlog_to_orig_log_map_list).view(num_layers, num_nodes, -1)\n=======\n        # Vectorized construction of mlog_weights and mlog_to_orig_log_map\n        # Target shape for mlog_weights and mlog_to_orig_log_map: [num_layers, num_nodes, groups_per_node * group_size]\n        num_middle_experts_per_node = groups_per_node * group_size\n        mlog_weights = torch.zeros(num_layers, num_nodes, num_middle_experts_per_node, dtype=weight.dtype, device=weight.device)\n        mlog_to_orig_log_map = torch.zeros(num_layers, num_nodes, num_middle_experts_per_node, dtype=torch.int64, device=weight.device)\n\n        # Create expanded indices for original logical experts\n        # orig_log_expert_indices will be [num_layers, num_groups, group_size]\n        orig_log_expert_indices = (torch.arange(num_groups, device=weight.device) * group_size).unsqueeze(0).unsqueeze(-1) + \\\n                                  torch.arange(group_size, device=weight.device)\n        orig_log_expert_indices = orig_log_expert_indices.expand(num_layers, -1, -1)\n\n        # Create target indices for scattering\n        # layer_indices: [num_layers, num_groups, group_size]\n        layer_indices = torch.arange(num_layers, device=weight.device).unsqueeze(-1).unsqueeze(-1).expand(-1, num_groups, group_size)\n        # target_node_indices: [num_layers, num_groups, group_size]\n        target_node_indices = group_pack_index.unsqueeze(-1).expand(-1, -1, group_size)\n        # target_mlog_sub_indices: [num_layers, num_groups, group_size]\n        target_mlog_sub_indices = (group_rank_in_pack * group_size).unsqueeze(-1) + \\\n                                  torch.arange(group_size, device=weight.device)\n\n        # Calculate flat indices for scatter operation\n        # Each unique combination of (layer, node, mlog_expert_index_within_node) gets a value\n        # The target flattened index: (layer_idx * num_nodes + node_idx) * num_middle_experts_per_node + mlog_sub_idx\n        global_target_indices = (layer_indices * num_nodes + target_node_indices) * num_middle_experts_per_node + target_mlog_sub_indices\n        global_target_indices_flat = global_target_indices.flatten()\n\n        # Scatter weights and original logical expert IDs to their new positions\n        mlog_weights.view(-1).scatter_(0, global_target_indices_flat, weight.flatten())\n        mlog_to_orig_log_map.view(-1).scatter_(0, global_target_indices_flat, orig_log_expert_indices.flatten())\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        # The `logcnt` from `replicate_experts` is the number of replicas for the middle experts.\n        # We need to make sure the `expert_count` is correct.\n        # The `replicate_experts` function returns `logcnt` which is the number of replicas for each logical expert it processed.\n        # So, `mlogcnt` is the number of replicas for each middle expert.\n\n        # The final `expert_count` should be `[num_layers, num_logical_experts]`.\n        # We need to map `mlogcnt` back.\n\n        # Let's create `expert_count` of size `[num_layers, num_logical_experts]` initialized to 0.\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n\n        # Iterate through each layer and node.\n        for i in range(num_layers):\n            for node_idx in range(num_nodes):\n                # For the middle experts belonging to this node and layer:\n                middle_expert_indices = mlog_to_orig_log_map[i, node_idx] # [num_middle_experts]\n                counts_for_middle_experts = mlogcnt[i * num_nodes + node_idx] # [num_middle_experts]\n\n                # Add these counts to the corresponding original logical experts.\n                # `torch.add.at` allows for sparse updates.\n                torch.add.at(expert_count[i], middle_expert_indices, counts_for_middle_experts)\n\n        # Now, we need to construct `logical_to_physical_map`.\n        # `logical_to_physical_map`: [num_layers, num_logical_experts, X] where X is max replicas.\n        # `phy2log_global`: [num_layers, num_physical_experts]\n        # `rank_global`: [num_layers, num_physical_experts]\n\n        # We need to group physical experts by their logical expert ID and rank.\n        max_replicas_per_expert = (num_physical_experts + num_logical_experts - 1) // num_logical_experts # Ceiling division\n\n        logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert), -1, dtype=torch.int64, device=weight.device)\n\n        # Iterate through all physical experts\n        for layer_idx in range(num_layers):\n            for phys_idx in range(num_physical_experts):\n                log_expert_id = phy2log_global[layer_idx, phys_idx]\n                replica_rank = rank_global[layer_idx, phys_idx]\n\n                if log_expert_id >= 0 and log_expert_id < num_logical_experts:\n                    # Place the physical expert ID into the correct slot based on its logical ID and rank.\n                    # We need to ensure we don't exceed `max_replicas_per_expert`.\n                    # The `replica_rank` should correspond to the index in the third dimension.\n                    if replica_rank < max_replicas_per_expert:\n                        logical_to_physical_map[layer_idx, log_expert_id, replica_rank] = phys_idx\n                    else:\n                        # This case indicates an issue or that max_replicas_per_expert needs to be larger.\n                        # For now, we can assert or log a warning.\n                        pass # Or handle error\n\n        # The `expert_count` is already computed.\n\n        return phy2log_global, logical_to_physical_map, expert_count\n=======\n        # Construct expert_count: [num_layers, num_logical_experts]\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n\n        # Use scatter_add_ to sum counts for each logical expert, vectorized per layer.\n        for i in range(num_layers):\n            # For each layer, gather the original logical expert indices and their counts from the middle experts\n            # that map to this layer.\n            mlog_to_orig_log_map_per_layer = mlog_to_orig_log_map[i].flatten() # [num_nodes * num_middle_experts]\n            mlogcnt_per_layer = mlogcnt[i*num_nodes : (i+1)*num_nodes].flatten() # [num_nodes * num_middle_experts]\n            expert_count[i].scatter_add_(0, mlog_to_orig_log_map_per_layer, mlogcnt_per_layer)\n\n\n        # Construct logical_to_physical_map\n        max_replicas_per_expert = (num_physical_experts + num_logical_experts - 1) // num_logical_experts\n        logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert), -1, dtype=torch.int64, device=weight.device)\n\n        # Prepare indices for scatter operation\n        # phy2log_global: [num_layers, num_physical_experts]\n        # rank_global: [num_layers, num_physical_experts]\n        \n        # Create global physical expert indices: [num_layers, num_physical_experts]\n        global_phys_indices = torch.arange(num_physical_experts, dtype=torch.int64, device=weight.device).expand(num_layers, num_physical_experts)\n\n        # Create target indices for logical_to_physical_map\n        # target_layer_idx: [num_layers, num_physical_experts]\n        target_layer_idx = torch.arange(num_layers, dtype=torch.int64, device=weight.device).unsqueeze(-1).expand(-1, num_physical_experts)\n        # target_log_expert_id: [num_layers, num_physical_experts]\n        target_log_expert_id = phy2log_global\n        # target_replica_rank: [num_layers, num_physical_experts]\n        target_replica_rank = rank_global\n\n        # Filter out invalid indices (e.g., -1 from initialization) or out-of-bounds ranks\n        valid_mask = (target_log_expert_id >= 0) & (target_log_expert_id < num_logical_experts) & \\\n                     (target_replica_rank >= 0) & (target_replica_rank < max_replicas_per_expert)\n\n        # Get valid elements and their corresponding values\n        valid_target_layer_idx = target_layer_idx[valid_mask]\n        valid_target_log_expert_id = target_log_expert_id[valid_mask]\n        valid_target_replica_rank = target_replica_rank[valid_mask]\n        valid_global_phys_indices = global_phys_indices[valid_mask]\n\n        # Calculate flattened target indices for scatter_\n        flat_target_indices = (valid_target_layer_idx * num_logical_experts + valid_target_log_expert_id) * max_replicas_per_expert + valid_target_replica_rank\n\n        # Perform scatter operation\n        logical_to_physical_map.view(-1).scatter_(0, flat_target_indices, valid_global_phys_indices)\n\n        return phy2log_global, logical_to_physical_map, expert_count\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        gpu_logical_weights_list = []\n        gpu_logical_to_orig_map_list = []\n\n        num_logical_experts_per_gpu = num_logical_experts // num_gpus\n\n        for gpu_idx in range(num_gpus):\n            # Find logical experts assigned to this GPU\n            assigned_logical_mask = (pack_index_gpu == gpu_idx)\n            # Get their ranks within the GPU\n            ranks_on_gpu = rank_in_gpu[assigned_logical_mask]\n            # Sort by rank\n            sorted_ranks_on_gpu, sort_indices_on_gpu = ranks_on_gpu.sort()\n            # Get original logical expert indices\n            assigned_logical_indices = torch.arange(num_logical_experts, device=pack_index_gpu.device)[assigned_logical_mask][sort_indices_on_gpu]\n\n            if len(assigned_logical_indices) > 0:\n                # Gather weights for these logical experts\n                # We need weights for each layer.\n                # `weight` is [num_layers, num_logical_experts].\n                # `assigned_logical_indices` is [num_logical_experts_per_gpu].\n                # We want `gpu_logical_weights` of shape [num_layers, num_logical_experts_per_gpu].\n                gpu_logical_weights = weight[:, assigned_logical_indices]\n                gpu_logical_weights_list.append(gpu_logical_weights)\n                gpu_logical_to_orig_map_list.append(assigned_logical_indices)\n            else:\n                # If a GPU gets no logical experts (shouldn't happen if num_logical_experts >= num_gpus)\n                gpu_logical_weights_list.append(torch.empty(num_layers, 0, dtype=weight.dtype, device=weight.device))\n                gpu_logical_to_orig_map_list.append(torch.empty(0, dtype=torch.int64, device=weight.device))\n\n        # `gpu_logical_weights`: [num_gpus, num_layers, num_logical_experts_per_gpu]\n        # `gpu_logical_to_orig_map`: [num_gpus, num_logical_experts_per_gpu]\n        gpu_logical_weights = torch.stack(gpu_logical_weights_list)\n        gpu_logical_to_orig_map = torch.stack(gpu_logical_to_orig_map_list)\n=======\n        # Vectorized construction of gpu_logical_weights and gpu_logical_to_orig_map\n        num_logical_experts_per_gpu = num_logical_experts // num_gpus\n        # gpu_logical_weights: [num_layers, num_gpus, num_logical_experts_per_gpu]\n        # gpu_logical_to_orig_map: [num_layers, num_gpus, num_logical_experts_per_gpu]\n        # (This shape is necessary because pack_index_gpu can vary per layer)\n        gpu_logical_weights = torch.zeros(num_layers, num_gpus, num_logical_experts_per_gpu, dtype=weight.dtype, device=weight.device)\n        gpu_logical_to_orig_map = torch.zeros(num_layers, num_gpus, num_logical_experts_per_gpu, dtype=torch.int64, device=weight.device)\n\n        # Create expanded indices for original logical experts\n        # orig_log_expert_indices will be [num_layers, num_logical_experts]\n        orig_log_expert_indices = torch.arange(num_logical_experts, device=weight.device).expand(num_layers, -1)\n\n        # Create target indices for scattering\n        # layer_indices: [num_layers, num_logical_experts]\n        layer_indices = torch.arange(num_layers, device=weight.device).unsqueeze(-1).expand(-1, num_logical_experts)\n        # target_gpu_indices: [num_layers, num_logical_experts]\n        target_gpu_indices = pack_index_gpu\n        # target_rank_in_gpu_indices: [num_layers, num_logical_experts]\n        target_rank_in_gpu_indices = rank_in_gpu\n\n        # Calculate flat indices for scatter operation\n        # The target flattened index: (layer_idx * num_gpus + gpu_idx) * num_logical_experts_per_gpu + rank_in_gpu_idx\n        global_target_indices = (layer_indices * num_gpus + target_gpu_indices) * num_logical_experts_per_gpu + target_rank_in_gpu_indices\n        global_target_indices_flat = global_target_indices.flatten()\n\n        # Scatter weights and original logical expert IDs to their new positions\n        gpu_logical_weights.view(-1).scatter_(0, global_target_indices_flat, weight.flatten())\n        gpu_logical_to_orig_map.view(-1).scatter_(0, global_target_indices_flat, orig_log_expert_indices.flatten())\n\n        # Permute to match the batching of replicate_experts, which expects [num_gpus, num_layers, ...]\n        # before the final view(-1, ...).\n        gpu_logical_weights = gpu_logical_weights.permute(1, 0, 2).contiguous()\n        gpu_logical_to_orig_map = gpu_logical_to_orig_map.permute(1, 0, 2).contiguous()\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        # Construct `expert_count` ([num_layers, num_logical_experts]).\n        # `gpu_logcnt`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n\n        for gpu_layer_idx in range(num_gpus * num_layers):\n            layer_idx = gpu_layer_idx // num_gpus\n            # Get the original logical expert indices for this gpu_layer combination\n            orig_log_indices = reshaped_gpu_logical_to_orig_map[gpu_layer_idx] # [num_logical_experts_per_gpu]\n            counts_for_orig_logs = gpu_logcnt[gpu_layer_idx] # [num_logical_experts_per_gpu]\n\n            # Add these counts to the correct positions in the expert_count tensor\n            torch.add.at(expert_count[layer_idx], orig_log_indices, counts_for_orig_logs)\n\n        # Construct `logical_to_physical_map` ([num_layers, num_logical_experts, X])\n        max_replicas_per_expert = (num_physical_experts + num_logical_experts - 1) // num_logical_experts\n\n        logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert), -1, dtype=torch.int64, device=weight.device)\n\n        # Need to map `phy2log_global` and `rank_global` to the correct positions.\n        # `phy2log_global` are indices into the *original* logical experts.\n        # `rank_global` is the replica rank for that original logical expert.\n\n        # Reshape to get mappings per layer and then iterate through physical experts.\n        phy2log_global_reshaped = phy2log_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous()\n        rank_global_reshaped = rank_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous()\n\n        # Iterate through all physical experts\n        for layer_idx in range(num_layers):\n            for gpu_idx in range(num_gpus):\n                for phys_idx_in_gpu in range(num_physical_experts // num_gpus):\n                    # Get the global physical expert index\n                    global_phys_idx = gpu_idx * (num_physical_experts // num_gpus) + phys_idx_in_gpu\n\n                    log_expert_id = phy2log_global_reshaped[layer_idx, gpu_idx, phys_idx_in_gpu]\n                    replica_rank = rank_global_reshaped[layer_idx, gpu_idx, phys_idx_in_gpu]\n\n                    if log_expert_id >= 0 and log_expert_id < num_logical_experts:\n                        if replica_rank < max_replicas_per_expert:\n                            logical_to_physical_map[layer_idx, log_expert_id, replica_rank] = global_phys_idx\n                        else:\n                            pass # Handle error or log\n\n        # The `phy2log_global` needs to be reshaped to `[num_layers, num_replicas]`\n        # where `num_replicas` is `num_physical_experts`.\n        # `phy2log_global` is currently `[num_gpus * num_layers, num_phy_per_gpu]`.\n        # We need to combine these.\n        # The order of physical experts should be consistent.\n        # Let's reconstruct `phy2log_global` to be `[num_layers, num_physical_experts]`.\n\n        final_phy2log_global = torch.empty((num_layers, num_physical_experts), dtype=torch.int64, device=weight.device)\n        current_phys_idx = 0\n        for gpu_idx in range(num_gpus):\n            for layer_idx in range(num_layers):\n                # Calculate the range of physical experts for this GPU and layer combination\n                start_phys_idx = layer_idx * num_gpus * (num_physical_experts // num_gpus) + gpu_idx * (num_physical_experts // num_gpus)\n                end_phys_idx = start_phys_idx + (num_physical_experts // num_gpus)\n\n                # The `phy2log_global` calculated earlier is `[num_gpus * num_layers, num_phy_per_gpu]`\n                # We need to map it correctly.\n                # `phy2log_gpu` was `[num_gpus * num_layers, num_phy_per_gpu]`\n                # `reshaped_gpu_logical_to_orig_map` was `[num_gpus * num_layers, num_phy_per_gpu]`\n                # `phy2log_global = reshaped_gpu_logical_to_orig_map.gather(-1, phy2log_gpu)`\n\n                # The indices for `phy2log_global` should be:\n                # For layer 0, GPU 0: phys experts 0 to num_phy_per_gpu - 1\n                # For layer 0, GPU 1: phys experts num_phy_per_gpu to 2*num_phy_per_gpu - 1\n                # ...\n                # For layer 1, GPU 0: phys experts num_gpus * num_phy_per_gpu to (num_gpus+1)*num_phy_per_gpu - 1\n\n                # Let's re-index the `phy2log_global` and `rank_global` to match `[num_layers, num_physical_experts]`\n                # The current `phy2log_global` is effectively `[num_layers * num_gpus, num_phy_per_gpu]`\n                # We need to rearrange it.\n\n                # Example: num_layers=2, num_gpus=2, num_phy_per_gpu=2. num_physical_experts=4.\n                # phy2log_global:\n                # [layer0_gpu0_phys0, layer0_gpu0_phys1,\n                #  layer0_gpu1_phys0, layer0_gpu1_phys1,\n                #  layer1_gpu0_phys0, layer1_gpu0_phys1,\n                #  layer1_gpu1_phys0, layer1_gpu1_phys1]\n                # Reshaped to [num_gpus*num_layers, num_phy_per_gpu] -> [4, 2]\n                # We want [num_layers, num_physical_experts] -> [2, 4]\n\n                # The current `phy2log_global` is `[num_gpus * num_layers, num_phy_per_gpu]`.\n                # Permute and reshape:\n                # `view(num_gpus, num_layers, -1)` gives `[num_gpus, num_layers, num_phy_per_gpu]`\n                # `permute(1, 0, 2)` gives `[num_layers, num_gpus, num_phy_per_gpu]`\n                # `contiguous().view(num_layers, -1)` gives `[num_layers, num_gpus * num_phy_per_gpu]` = `[num_layers, num_physical_experts]`\n\n                final_phy2log_global = phy2log_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous().view(num_layers, num_physical_experts)\n                final_rank_global = rank_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous().view(num_layers, num_physical_experts)\n\n        # Return the results for global balancing.\n        return final_phy2log_global, logical_to_physical_map, expert_count\n=======\n        # Construct expert_count: [num_layers, num_logical_experts]\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n\n        # Use scatter_add_ to sum counts for each logical expert, vectorized per layer.\n        for i in range(num_layers):\n            # Extract relevant slices for the current layer from the flattened tensors\n            # For layer i, the indices in reshaped_gpu_logical_to_orig_map and gpu_logcnt\n            # are `i * num_gpus` to `(i+1) * num_gpus - 1`.\n            current_layer_orig_log_indices = reshaped_gpu_logical_to_orig_map[i*num_gpus : (i+1)*num_gpus].flatten()\n            current_layer_counts = gpu_logcnt[i*num_gpus : (i+1)*num_gpus].flatten()\n            expert_count[i].scatter_add_(0, current_layer_orig_log_indices, current_layer_counts)\n\n\n        # Construct logical_to_physical_map\n        max_replicas_per_expert = (num_physical_experts + num_logical_experts - 1) // num_logical_experts\n        logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert), -1, dtype=torch.int64, device=weight.device)\n\n        # Reconstruct phy2log_global and rank_global to [num_layers, num_physical_experts]\n        # phy2log_global: [num_gpus * num_layers, num_phy_per_gpu]\n        # phyrank_gpu: [num_gpus * num_layers, num_phy_per_gpu]\n        # Permute and reshape:\n        # `view(num_gpus, num_layers, -1)` gives `[num_gpus, num_layers, num_phy_per_gpu]`\n        # `permute(1, 0, 2)` gives `[num_layers, num_gpus, num_phy_per_gpu]`\n        # `contiguous().view(num_layers, -1)` gives `[num_layers, num_gpus * num_phy_per_gpu]` = `[num_layers, num_physical_experts]`\n        final_phy2log_global = phy2log_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous().view(num_layers, num_physical_experts)\n        final_rank_global = phyrank_gpu.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous().view(num_layers, num_physical_experts)\n\n\n        # Prepare indices for scatter operation\n        # Create global physical expert indices: [num_layers, num_physical_experts]\n        global_phys_indices = torch.arange(num_physical_experts, dtype=torch.int64, device=weight.device).expand(num_layers, num_physical_experts)\n\n        # Create target indices for logical_to_physical_map\n        # target_layer_idx: [num_layers, num_physical_experts]\n        target_layer_idx = torch.arange(num_layers, dtype=torch.int64, device=weight.device).unsqueeze(-1).expand(-1, num_physical_experts)\n        # target_log_expert_id: [num_layers, num_physical_experts]\n        target_log_expert_id = final_phy2log_global # Use the correctly reshaped one\n        # target_replica_rank: [num_layers, num_physical_experts]\n        target_replica_rank = final_rank_global # Use the correctly reshaped one\n\n        # Filter out invalid indices (e.g., -1 from initialization) or out-of-bounds ranks\n        valid_mask = (target_log_expert_id >= 0) & (target_log_expert_id < num_logical_experts) & \\\n                     (target_replica_rank >= 0) & (target_replica_rank < max_replicas_per_expert)\n\n        # Get valid elements and their corresponding values\n        valid_target_layer_idx = target_layer_idx[valid_mask]\n        valid_target_log_expert_id = target_log_expert_id[valid_mask]\n        valid_target_replica_rank = target_replica_rank[valid_mask]\n        valid_global_phys_indices = global_phys_indices[valid_mask]\n\n        # Calculate flattened target indices for scatter_\n        flat_target_indices = (valid_target_layer_idx * num_logical_experts + valid_target_log_expert_id) * max_replicas_per_expert + valid_target_replica_rank\n\n        # Perform scatter operation\n        logical_to_physical_map.view(-1).scatter_(0, flat_target_indices, valid_global_phys_indices)\n\n        return final_phy2log_global, logical_to_physical_map, expert_count\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n    # The `rebalance_experts_hierarchical` and the global balancing logic\n    # now produce `phy2log_global`, `rank_global`, and `expert_count`.\n    # `phy2log_global`: [num_layers, num_replicas] -> maps physical expert index to logical expert ID.\n    # `rank_global`: [num_layers, num_replicas] -> maps physical expert index to its replica rank.\n    # `expert_count`: [num_layers, num_logical_experts] -> number of replicas for each logical expert.\n\n    # We need to construct `logical_to_physical_map`: [layers, num_logical_experts, X]\n    # where X is the maximum number of replicas for any logical expert.\n\n    num_redundant_experts = num_replicas - num_logical_experts\n    # `max_replicas_per_expert` should be at least 1.\n    # If `num_redundant_experts` is negative, it means `num_replicas < num_logical_experts`, which is an error.\n    # The `replicate_experts` function ensures `num_phy >= num_log`.\n    # Thus, `num_redundant_experts >= 0`.\n    max_replicas_per_expert = (num_physical_experts + num_logical_experts - 1) // num_logical_experts # Ceiling division\n\n    # Initialize the map with -1 (indicating no physical expert assigned)\n    logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert), -1, dtype=torch.int64, device=weight.device)\n\n    # Fill the map by iterating through all physical experts.\n    # `phy2log_global` gives the logical expert ID for each physical expert.\n    # `rank_global` gives the replica rank for that physical expert.\n    for layer_idx in range(num_layers):\n        for phys_idx in range(num_replicas):\n            log_expert_id = phy2log_global[layer_idx, phys_idx]\n            replica_rank = rank_global[layer_idx, phys_idx]\n\n            # Ensure the logical expert ID and rank are valid.\n            if log_expert_id >= 0 and log_expert_id < num_logical_experts:\n                # Place the physical expert's index into the map.\n                # The `replica_rank` directly corresponds to the index in the third dimension.\n                if replica_rank < max_replicas_per_expert:\n                    logical_to_physical_map[layer_idx, log_expert_id, replica_rank] = phys_idx\n                else:\n                    # This case should ideally not happen if `max_replicas_per_expert` is calculated correctly.\n                    # It might occur if `num_logical_experts` is very small and `num_replicas` is large,\n                    # leading to a high `max_replicas_per_expert`.\n                    # However, `replica_rank` should not exceed the actual number of replicas for a logical expert.\n                    # If it does, it indicates a potential issue in `replicate_experts` or the logic here.\n                    # For robustness, we can either resize or cap, but it's better to ensure correctness.\n                    # Let's assume `max_replicas_per_expert` is sufficient.\n                    pass\n\n    # The `expert_count` is already computed.\n    # `phy2log_global` is the first return value.\n    return phy2log_global, logical_to_physical_map, expert_count\n=======\n    # The `rebalance_experts_hierarchical` function now directly returns\n    # the final `physical_to_logical_map`, `logical_to_physical_map`, and `expert_count`.\n    # No further processing is needed here.\n    return phy2log, phyrank, logcnt\n>>>>>>> REPLACE\n```"]}}}