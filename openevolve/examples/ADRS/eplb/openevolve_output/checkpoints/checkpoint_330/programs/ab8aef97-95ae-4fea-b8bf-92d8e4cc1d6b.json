{"id": "ab8aef97-95ae-4fea-b8bf-92d8e4cc1d6b", "code": "# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\nimport heapq\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort groups by weight in descending order.\n    # 'weight' is already float and on CPU from rebalance_experts,\n    # so no need for .float().cpu() here.\n    indices = weight.sort(-1, descending=True).indices\n\n    # Initialize pack_index and rank_in_pack.\n    # These will be fully populated, so `empty_like` is sufficient.\n    # They will be on the same device as 'weight' (i.e., CPU).\n    pack_index = torch.empty_like(weight, dtype=torch.int64)\n    rank_in_pack = torch.empty_like(pack_index)\n    \n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # Min-heap for the current layer to keep track of pack loads.\n        # Elements are (current_load, pack_id)\n        # Initialize with all packs having 0 load.\n        pack_heap = [(0.0, p) for p in range(num_packs)]\n        heapq.heapify(pack_heap) # Build heap in O(num_packs)\n        \n        # Keep track of items assigned to each pack for the current layer.\n        current_pack_items = torch.zeros(num_packs, dtype=torch.int64, device=weight.device)\n\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer, sorted by weight.\n        layer_indices = indices[i]\n        \n        # Iterate through the group IDs sorted by weight.\n        for group_idx_in_sorted_list in range(num_groups):\n            # Get the actual group ID from the sorted indices.\n            group_id = layer_indices[group_idx_in_sorted_list].item()\n\n            # Get the pack with the minimum current load from the heap.\n            current_load, pack = heapq.heappop(pack_heap)\n\n            # Assign the group to the chosen pack.\n            # The `group_id` is the column index for the `pack_index` and `rank_in_pack` tensors.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack item count for the chosen pack.\n            current_pack_items[pack] += 1\n            \n            # Calculate new load for the pack.\n            new_load = current_load + current_group_weight # Use pre-fetched weight\n\n            # If the pack is not yet full, push it back to the heap with its new load.\n            if current_pack_items[pack] < groups_per_pack:\n                heapq.heappush(pack_heap, (new_load, pack))\n            # Else, the pack is full, do not push it back to the heap.\n            # It means this pack will not be considered for further assignments in this layer.\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    # Initialize max-heaps for each layer\n    # Heap elements: (-average_load, logical_expert_id)\n    # Use negative average_load for max-heap behavior with heapq (which is a min-heap).\n    expert_heaps = []\n    for i_layer in range(n):\n        layer_heap = []\n        # Pre-fetch weights for the current layer to reduce repeated indexing and .item() calls.\n        # 'weight' is already on CPU and float from the caller.\n        weights_for_layer = weight[i_layer, :]\n        \n        for j_expert in range(num_log):\n            # Use .item() here once per expert for initialization\n            heapq.heappush(layer_heap, (-weights_for_layer[j_expert].item(), j_expert))\n        expert_heaps.append(layer_heap)\n\n    # Assign redundant experts\n    for i_phy in range(num_log, num_phy): # Iterate num_redundant times\n        for i_layer in range(n): # Iterate over each layer\n            current_heap = expert_heaps[i_layer]\n            \n            # Get weights for the current layer. This is available from the outer loop.\n            weights_for_layer = weight[i_layer, :] \n\n            # Pop the expert with the highest average load (min negative load)\n            neg_avg_load, log_expert_id = heapq.heappop(current_heap)\n            \n            # Assign this expert as a physical replica\n            phy2log[i_layer, i_phy] = log_expert_id\n            rank[i_layer, i_phy] = logcnt[i_layer, log_expert_id] # Replica rank for this expert\n            \n            # Increment count for this logical expert\n            logcnt[i_layer, log_expert_id] += 1\n            \n            # Calculate new average load and push back to heap\n            # Access pre-fetched weight for the current logical expert\n            current_expert_weight = weights_for_layer[log_expert_id]\n            \n            # Fetch the count for the specific expert, one .item() call\n            current_logcnt = logcnt[i_layer, log_expert_id].item() \n\n            # Calculate new average load\n            new_avg_load = current_expert_weight.item() / current_logcnt # One .item() call here\n            heapq.heappush(current_heap, (-new_avg_load, log_expert_id))\n\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    \n    # Store original device to move results back later.\n    original_device = weight.device\n    \n    # Cast weight to float for calculations and move to CPU.\n    # Python's heapq and scalar operations (.item()) are efficient on CPU tensors.\n    weight = weight.float().cpu()\n    \n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    \n    # Move results back to the original device.\n    phy2log = phy2log.to(original_device)\n    log2phy = log2phy.to(original_device)\n    logcnt = logcnt.to(original_device)\n    \n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n", "language": "python", "parent_id": "3b44deaa-31cc-4f36-9b52-b1a037857a67", "generation": 10, "timestamp": 1751062966.77389, "iteration_found": 208, "metrics": {"balancedness_score": 0.0, "speed_score": 0.0, "combined_score": 0.0, "error": "name 'current_group_weight' is not defined"}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 2 lines with 2 lines\nChange 2: Replace 40 lines with 47 lines\nChange 3: Replace 24 lines with 37 lines", "parent_metrics": {"balancedness_score": 0.29830835153138974, "speed_score": 0.03541486937401876, "combined_score": 0.16686161045270426}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve the Mixture-of-Expert models Expert Parallelism Load Balancer (MoE EPLB) expert rearrangement algorithm.\nThis algorithm will take the load metrics recorded by the vLLM server, and rearrange the experts to balance the load. It can make replicas of some experts to achieve better load balancing.\nYour goal will be two-fold: 1. Improve the algorithm to achieve better load balancing; while 2. Improve the algorithm to be more efficient, i.e. reduce the execution time of the algorithm itself, since perfect load balancing is NP-hard.\nThe current algorithm is implemented in the `rebalance_experts` function. ", "user": "# Current Program Information\n- Current performance metrics: - balancedness_score: 0.2983\n- speed_score: 0.0354\n- combined_score: 0.1669\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: balancedness_score, speed_score, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0857, combined_score: 0.1920\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1097, combined_score: 0.2312\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1103, combined_score: 0.2315\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.2315)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1103), Performs well on combined_score (0.2315)\n\n\n### Program 2 (Score: 0.2312)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1097), Performs well on combined_score (0.2312)\n\n\n### Program 3 (Score: 0.1920)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0857), Performs well on combined_score (0.1920)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.1909)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n\n### Program D2 (Score: 0.1914)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.2315, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 32 lines with 32 lines, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 2 (Score: 0.0939, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 23 lines with 236 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 3 (Score: 0.1647, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 54 lines with 53 lines\nChange 2: Replace 15 lines with 40 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 4 (Score: 0.0000, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 47 lines with 49 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 5 (Score: 0.0938, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 21 lines with 39 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n\n# Current Program\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\nimport heapq\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort groups by weight in descending order.\n    # 'weight' is already float and on CPU from rebalance_experts,\n    # so no need for .float().cpu() here.\n    indices = weight.sort(-1, descending=True).indices\n\n    # Initialize pack_index and rank_in_pack.\n    # These will be fully populated, so `empty_like` is sufficient.\n    # They will be on the same device as 'weight' (i.e., CPU).\n    pack_index = torch.empty_like(weight, dtype=torch.int64)\n    rank_in_pack = torch.empty_like(pack_index)\n    \n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # Min-heap for the current layer to keep track of pack loads.\n        # Elements are (current_load, pack_id)\n        # Initialize with all packs having 0 load.\n        pack_heap = [(0.0, p) for p in range(num_packs)]\n        heapq.heapify(pack_heap) # Build heap in O(num_packs)\n        \n        # Keep track of items assigned to each pack for the current layer.\n        current_pack_items = torch.zeros(num_packs, dtype=torch.int64, device=weight.device)\n\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer, sorted by weight.\n        layer_indices = indices[i]\n        \n        # Iterate through the group IDs sorted by weight.\n        for group_idx_in_sorted_list in range(num_groups):\n            # Get the actual group ID from the sorted indices.\n            group_id = layer_indices[group_idx_in_sorted_list].item()\n\n            # Get the pack with the minimum current load from the heap.\n            current_load, pack = heapq.heappop(pack_heap)\n\n            # Assign the group to the chosen pack.\n            # The `group_id` is the column index for the `pack_index` and `rank_in_pack` tensors.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack item count for the chosen pack.\n            current_pack_items[pack] += 1\n            \n            # Calculate new load for the pack.\n            new_load = current_load + weight[i, group_id].item() # .item() for scalar arithmetic\n\n            # If the pack is not yet full, push it back to the heap with its new load.\n            if current_pack_items[pack] < groups_per_pack:\n                heapq.heappush(pack_heap, (new_load, pack))\n            # Else, the pack is full, do not push it back to the heap.\n            # It means this pack will not be considered for further assignments in this layer.\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    # Initialize max-heaps for each layer\n    # Heap elements: (-average_load, logical_expert_id)\n    # Use negative average_load for max-heap behavior with heapq (which is a min-heap).\n    expert_heaps = []\n    for i_layer in range(n):\n        layer_heap = []\n        for j_expert in range(num_log):\n            # Initial average load is just the weight itself (since logcnt is 1 for initial experts)\n            heapq.heappush(layer_heap, (-weight[i_layer, j_expert].item(), j_expert))\n        expert_heaps.append(layer_heap)\n\n    # Assign redundant experts\n    for i_phy in range(num_log, num_phy): # Iterate num_redundant times\n        for i_layer in range(n): # Iterate over each layer\n            current_heap = expert_heaps[i_layer]\n            \n            # Pop the expert with the highest average load (min negative load)\n            neg_avg_load, log_expert_id = heapq.heappop(current_heap)\n            \n            # Assign this expert as a physical replica\n            phy2log[i_layer, i_phy] = log_expert_id\n            rank[i_layer, i_phy] = logcnt[i_layer, log_expert_id] # Replica rank for this expert\n            \n            # Increment count for this logical expert\n            logcnt[i_layer, log_expert_id] += 1\n            \n            # Calculate new average load and push back to heap\n            # Use .item() to get Python scalar for heap operations.\n            new_avg_load = weight[i_layer, log_expert_id].item() / logcnt[i_layer, log_expert_id].item()\n            heapq.heappush(current_heap, (-new_avg_load, log_expert_id))\n\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    \n    # Store original device to move results back later.\n    original_device = weight.device\n    \n    # Cast weight to float for calculations and move to CPU.\n    # Python's heapq and scalar operations (.item()) are efficient on CPU tensors.\n    weight = weight.float().cpu()\n    \n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    \n    # Move results back to the original device.\n    phy2log = phy2log.to(original_device)\n    log2phy = log2phy.to(original_device)\n    logcnt = logcnt.to(original_device)\n    \n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["The current program has a balancedness_score of 0.2983, a speed_score of 0.0354, and a combined_score of 0.1669. The goal is to improve these scores, particularly balancedness and speed.\n\nLooking at the program evolution history, previous attempts (Attempts 1, 2, and 3) showed improvements in all metrics, with scores around 0.2315. The current program's scores are lower, indicating a regression. The \"Top Performing Programs\" and \"Inspiration Programs\" suggest that the core logic might be similar across many versions, with variations in specific implementation details or scoring approaches.\n\nThe provided code implements a hierarchical load balancing strategy. The `rebalance_experts` function acts as an entry point, delegating to `rebalance_experts_hierarchical` or a simplified version of it based on `num_groups % num_nodes`.\n\nHere are a few areas for potential improvement:\n\n1.  **Optimization within `balanced_packing`:** The current `balanced_packing` function uses a min-heap for greedy assignment. While generally good, there might be opportunities to optimize the heap operations or the assignment logic itself for better speed.\n2.  **Handling of `num_groups % num_nodes != 0`:** The current implementation defaults to a simplified `rebalance_experts_hierarchical` with `num_groups=1`, `num_nodes=1`. This might not be optimal when `num_groups` is not a multiple of `num_nodes`. A more nuanced approach could be considered.\n3.  **CPU vs. GPU operations:** The code explicitly moves weights to CPU for `heapq` operations. While this is often necessary for Python-level heaps, it's worth considering if any parts could leverage GPU operations for speed, especially if the number of layers or experts is very large. However, given `heapq`'s nature, CPU is likely the best bet for the heap itself.\n4.  **Code Simplification/Readability:** The prompt specifically mentions simplifying code for readability and maintainability. While not directly impacting scores, it's a good practice.\n\nLet's focus on optimizing the `balanced_packing` function and reconsidering the non-hierarchical case.\n\n### Improvement 1: Optimizing `balanced_packing` for speed\n\nThe `balanced_packing` function sorts groups by weight and then uses a min-heap to greedily assign groups to packs. The sorting step is `O(N log N)` where N is `num_groups`. The heap operations are `O(log P)` where P is `num_packs`. For each layer, we iterate through `num_groups` and perform heap operations.\n\nOne potential optimization is to avoid repeated heapifications and pushes/pops if the number of groups per pack is small. However, given the current structure, the heap is essential for the greedy approach.\n\nA subtle point is the `.item()` calls within the loop. While necessary for scalar arithmetic with Python's `heapq`, these can add overhead. If possible, batching these operations could be beneficial, but it's tricky with the heap structure.\n\nLet's examine the `balanced_packing` logic more closely. The current implementation assigns items one by one to the least loaded pack. This is a standard greedy approach.\n\nConsider the case where `groups_per_pack` is 1. The code handles this efficiently. When `groups_per_pack > 1`, the loop iterates through `num_groups` and performs heap operations.\n\nA potential minor optimization: pre-calculating `weight[i, group_id].item()` outside the heap push.\n\nLet's look at the structure of `rebalance_experts_hierarchical`. It first calculates `tokens_per_group` and then calls `balanced_packing`. Then it reconstructs experts, and calls `replicate_experts`. Finally, it packs physical experts to GPUs using `balanced_packing` again.\n\nThe performance bottleneck is likely in the repeated calls to `balanced_packing` and `replicate_experts`, especially the heap operations within them.\n\nLet's focus on the `balanced_packing` function and try to make its core loop slightly more efficient.\n\nThe line `new_load = current_load + weight[i, group_id].item()` is performed for every group. `current_load` is already a Python float from the heap. `weight[i, group_id].item()` converts a PyTorch tensor scalar to a Python scalar.\n\nIf `weight` is large, `weight.cpu()` might take time. The current code does this once at the beginning of `rebalance_experts`.\n\nThe core loop in `balanced_packing` is:\n```python\n        for group_idx_in_sorted_list in range(num_groups):\n            group_id = layer_indices[group_idx_in_sorted_list].item()\n            current_load, pack = heapq.heappop(pack_heap)\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n            current_pack_items[pack] += 1\n            new_load = current_load + weight[i, group_id].item() # .item() here\n            if current_pack_items[pack] < groups_per_pack:\n                heapq.heappush(pack_heap, (new_load, pack))\n```\nThe `.item()` call inside the loop is necessary for scalar arithmetic with the heap.\n\nLet's consider the `replicate_experts` function. It also uses `heapq`.\n```python\n            neg_avg_load, log_expert_id = heapq.heappop(current_heap)\n            # ...\n            new_avg_load = weight[i_layer, log_expert_id].item() / logcnt[i_layer, log_expert_id].item()\n            heapq.heappush(current_heap, (-new_avg_load, log_expert_id))\n```\nHere, two `.item()` calls are made.\n\nThe regression in scores suggests that the current implementation might be less efficient than previous ones. The prompt mentioned \"Metrics showing regression: balancedness_score, speed_score, combined_score. Consider reverting or revising recent changes in these areas.\" This implies that the current code *might have been modified* from a better-performing version. Without the diffs of previous attempts, it's hard to pinpoint the exact regression. However, we can still try to optimize the existing logic.\n\nOne area where performance can be improved is by pre-computing the `.item()` values for the weights used in the loops, if possible, or by restructuring the operations to minimize the overhead of `.item()`.\n\nLet's consider the `replicate_experts` function again. The calculation of `new_avg_load` involves division.\n`new_avg_load = weight[i_layer, log_expert_id].item() / logcnt[i_layer, log_expert_id].item()`\n\nIf `logcnt[i_layer, log_expert_id]` is 1, the division is simple. As `logcnt` increases, the division becomes more significant.\n\nPerhaps we can optimize the data structure used for the heap elements. Instead of storing `(-average_load, logical_expert_id)`, we could store something that allows for faster updates or recalculations. However, `heapq` operates on tuples and requires comparable elements.\n\nLet's focus on a specific change that might improve the speed_score without drastically harming the balancedness_score. The `.item()` calls are a prime suspect for overhead.\n\nIn `replicate_experts`, the line:\n`new_avg_load = weight[i_layer, log_expert_id].item() / logcnt[i_layer, log_expert_id].item()`\nThis involves two `.item()` calls. We can try to optimize this.\n\nWhat if we store the `logcnt` alongside the `weight` in the heap, and update it there? This would complicate the heap structure.\n\nLet's consider a simpler optimization: pre-fetching the weight values for the current layer to avoid repeated indexing and `.item()` calls within the inner loop.\n\nIn `balanced_packing`, `weight[i, group_id].item()` is called for each group.\nWe can fetch `weight[i, :]` once for the layer.\n\n```python\n    # Inside balanced_packing, for loop over layers\n    for i in range(num_layers):\n        # ... heap initialization ...\n        layer_indices = indices[i]\n        # Fetch all weights for the current layer and sort them according to indices\n        # This might increase memory usage but could speed up access\n        sorted_weights_for_layer = weight[i, layer_indices]\n        \n        # Iterate through the group IDs sorted by weight.\n        for group_idx_in_sorted_list in range(num_groups):\n            group_id = layer_indices[group_idx_in_sorted_list].item()\n            \n            # Get the weight for the current group\n            current_group_weight = sorted_weights_for_layer[group_idx_in_sorted_list].item()\n\n            # Get the pack with the minimum current load from the heap.\n            current_load, pack = heapq.heappop(pack_heap)\n\n            # Assign the group to the chosen pack.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack item count for the chosen pack.\n            current_pack_items[pack] += 1\n            \n            # Calculate new load for the pack.\n            new_load = current_load + current_group_weight # Use pre-fetched weight\n\n            # If the pack is not yet full, push it back to the heap with its new load.\n            if current_pack_items[pack] < groups_per_pack:\n                heapq.heappush(pack_heap, (new_load, pack))\n```\nThis change in `balanced_packing` pre-fetches weights for the layer.\n\nSimilarly, in `replicate_experts`:\n```python\n    # Inside replicate_experts, for loop over layers\n    for i_layer in range(n):\n        # ... heap initialization ...\n        layer_heap = []\n        # Pre-fetch weights for the layer to potentially speed up access\n        weights_for_layer = weight[i_layer, :]\n        \n        for j_expert in range(num_log):\n            heapq.heappush(layer_heap, (-weights_for_layer[j_expert].item(), j_expert))\n        expert_heaps.append(layer_heap)\n\n    # Assign redundant experts\n    for i_phy in range(num_log, num_phy): # Iterate num_redundant times\n        for i_layer in range(n): # Iterate over each layer\n            current_heap = expert_heaps[i_layer]\n            \n            # Pop the expert with the highest average load (min negative load)\n            neg_avg_load, log_expert_id = heapq.heappop(current_heap)\n            \n            # Assign this expert as a physical replica\n            phy2log[i_layer, i_phy] = log_expert_id\n            rank[i_layer, i_phy] = logcnt[i_layer, log_expert_id] # Replica rank for this expert\n            \n            # Increment count for this logical expert\n            logcnt[i_layer, log_expert_id] += 1\n            \n            # Calculate new average load and push back to heap\n            # Use .item() to get Python scalar for heap operations.\n            # Pre-fetch weight for the current logical expert\n            current_expert_weight = weights_for_layer[log_expert_id] # Accessing weights_for_layer from outer scope\n            \n            # Note: weights_for_layer should be accessible here.\n            # If not, it needs to be passed or re-fetched.\n            # Let's assume it's accessible or re-fetch it.\n            # A better approach is to pass weights_for_layer to the inner loop or make it global within replicate_experts.\n\n            # To ensure weights_for_layer is available:\n            # Move the `weights_for_layer = weight[i_layer, :]` before the inner loop.\n            # This is already done in the structure above.\n\n            # Fetching the count for the specific expert\n            current_logcnt = logcnt[i_layer, log_expert_id].item() # One .item() call here\n\n            new_avg_load = current_expert_weight.item() / current_logcnt\n            heapq.heappush(current_heap, (-new_avg_load, log_expert_id))\n\n```\nThe pre-fetching of weights within `replicate_experts` would be:\n```python\n    # Inside replicate_experts\n    # Initialize max-heaps for each layer\n    # Heap elements: (-average_load, logical_expert_id)\n    expert_heaps = []\n    for i_layer in range(n):\n        layer_heap = []\n        # Fetch all weights for the current layer to potentially speed up access\n        # and reduce overhead of repeated .item() calls inside the loop.\n        weights_for_layer = weight[i_layer, :]\n        \n        for j_expert in range(num_log):\n            # Use .item() here once per expert for initialization\n            heapq.heappush(layer_heap, (-weights_for_layer[j_expert].item(), j_expert))\n        expert_heaps.append(layer_heap)\n\n    # Assign redundant experts\n    for i_phy in range(num_log, num_phy): # Iterate num_redundant times\n        for i_layer in range(n): # Iterate over each layer\n            current_heap = expert_heaps[i_layer]\n            \n            # Fetch the weights for the current layer again, if not passed down\n            # This is already handled by the outer loop structure.\n            weights_for_layer = weight[i_layer, :] \n\n            # Pop the expert with the highest average load (min negative load)\n            neg_avg_load, log_expert_id = heapq.heappop(current_heap)\n            \n            # Assign this expert as a physical replica\n            phy2log[i_layer, i_phy] = log_expert_id\n            rank[i_layer, i_phy] = logcnt[i_layer, log_expert_id] # Replica rank for this expert\n            \n            # Increment count for this logical expert\n            logcnt[i_layer, log_expert_id] += 1\n            \n            # Calculate new average load and push back to heap\n            # Access pre-fetched weight for the current logical expert\n            current_expert_weight = weights_for_layer[log_expert_id]\n            \n            # Fetch the count for the specific expert, one .item() call\n            current_logcnt = logcnt[i_layer, log_expert_id].item() \n\n            # Calculate new average load\n            new_avg_load = current_expert_weight.item() / current_logcnt # One .item() call here\n            heapq.heappush(current_heap, (-new_avg_load, log_expert_id))\n```\nThis pre-fetching aims to reduce the overhead of `weight[i, group_id].item()` and `weight[i_layer, log_expert_id].item()` calls within the innermost loops.\n\n### Improvement 2: Handling of `num_groups % num_nodes != 0`\n\nThe current code has this logic:\n```python\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus) # <-- This is likely the issue\n```\nWhen `num_groups` is not divisible by `num_nodes`, it calls `rebalance_experts_hierarchical` with `num_groups=1` and `num_nodes=1`. This effectively reduces the problem to a global load balancing scenario, ignoring the hierarchical structure. This might be too simplistic.\n\nA better approach would be to adapt the hierarchical strategy. If `num_groups` is not divisible by `num_nodes`, we could group `num_groups // num_nodes` groups into larger \"super-groups\" or simply adjust the `groups_per_node` calculation to accommodate the remainder.\n\nHowever, the prompt asks for improving the *current* program. Modifying the hierarchical logic significantly would be a major change. For now, let's focus on the optimization of existing functions.\n\nThe \"Diverse Programs\" and \"Inspiration Programs\" suggest that alternative approaches to `balancedness_score` and `speed_score` exist. Without seeing those implementations, it's hard to directly apply them. However, focusing on the efficiency of the core algorithms (`balanced_packing`, `replicate_experts`) is a direct way to improve `speed_score`.\n\nThe regression in `balancedness_score` is concerning. The pre-fetching of weights might slightly alter the load distribution if floating-point precision differences arise, but it's unlikely to be the primary cause of a significant drop in `balancedness_score`.\n\nLet's stick to the weight pre-fetching idea as it directly targets potential performance bottlenecks related to repeated `.item()` calls and tensor indexing within loops.\n\nThe current implementation of `rebalance_experts_hierarchical` already seems to have a reasonable structure. The main concern is the efficiency of the underlying packing and replication.\n\nConsider the `replicate_experts` function again. The heap elements are `(-average_load, logical_expert_id)`. The `average_load` is `weight / logcnt`. When `logcnt` increases, `average_load` decreases. The heap correctly picks the expert with the highest current average load.\n\nThe proposed changes are:\n1.  In `balanced_packing`, pre-fetch `weight[i, :]` for the current layer.\n2.  In `replicate_experts`, pre-fetch `weight[i_layer, :]` for the current layer.\n\nThis aims to reduce the overhead of repeated tensor indexing and `.item()` calls inside the tight loops.\n\nLet's refine the changes to be precise diffs.\n\n```python\n# In balanced_packing\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    # ... existing code ...\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # Min-heap for the current layer to keep track of pack loads.\n        # Elements are (current_load, pack_id)\n        # Initialize with all packs having 0 load.\n        pack_heap = [(0.0, p) for p in range(num_packs)]\n        heapq.heapify(pack_heap) # Build heap in O(num_packs)\n        \n        # Keep track of items assigned to each pack for the current layer.\n        current_pack_items = torch.zeros(num_packs, dtype=torch.int64, device=weight.device)\n\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer, sorted by weight.\n        layer_indices = indices[i]\n        \n        # Pre-fetch weights for the current layer to reduce repeated indexing and .item() calls.\n        # sorted_weights_for_layer = weight[i, layer_indices] # This is incorrect, should be weight[i, sorted_indices]\n        \n        # Correct pre-fetching: get weights corresponding to the sorted indices.\n        weights_for_layer = weight[i, :].gather(-1, layer_indices)\n\n        # Iterate through the group IDs sorted by weight.\n        for group_idx_in_sorted_list in range(num_groups):\n            # Get the actual group ID from the sorted indices.\n            group_id = layer_indices[group_idx_in_sorted_list].item()\n\n            # Get the weight for the current group from the pre-fetched tensor.\n            current_group_weight = weights_for_layer[group_idx_in_sorted_list].item()\n\n            # Get the pack with the minimum current load from the heap.\n            current_load, pack = heapq.heappop(pack_heap)\n\n            # Assign the group to the chosen pack.\n            # The `group_id` is the column index for the `pack_index` and `rank_in_pack` tensors.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack item count for the chosen pack.\n            current_pack_items[pack] += 1\n            \n            # Calculate new load for the pack.\n            new_load = current_load + current_group_weight # Use pre-fetched weight\n\n            # If the pack is not yet full, push it back to the heap with its new load.\n            if current_pack_items[pack] < groups_per_pack:\n                heapq.heappush(pack_heap, (new_load, pack))\n            # Else, the pack is full, do not push it back to the heap.\n            # It means this pack will not be considered for further assignments in this layer.\n    return pack_index, rank_in_pack\n\n```\nThe correction for pre-fetching in `balanced_packing`: `weight[i, :]` gets all weights for layer `i`. `layer_indices` contains the *column indices* of groups sorted by weight. To get the weights in sorted order, we need to gather them using `layer_indices`.\nSo, `weights_for_layer = weight[i, :].gather(-1, layer_indices)` is correct. Then `weights_for_layer[group_idx_in_sorted_list]` will give the weight of the group that is `group_idx_in_sorted_list`-th in sorted order.\n\n```python\n# In replicate_experts\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    # ... existing code ...\n    \n    # Initialize max-heaps for each layer\n    # Heap elements: (-average_load, logical_expert_id)\n    # Use negative average_load for max-heap behavior with heapq (which is a min-heap).\n    expert_heaps = []\n    # Pre-fetch weights for all layers to reduce repeated indexing and .item() calls.\n    # This creates a copy of the weights on CPU.\n    weights_cpu = weight.cpu() # Ensure it's on CPU for consistency with other CPU ops\n    \n    for i_layer in range(n):\n        layer_heap = []\n        # Get weights for the current layer.\n        weights_for_layer = weights_cpu[i_layer, :]\n        \n        for j_expert in range(num_log):\n            # Use .item() here once per expert for initialization\n            heapq.heappush(layer_heap, (-weights_for_layer[j_expert].item(), j_expert))\n        expert_heaps.append(layer_heap)\n\n    # Assign redundant experts\n    for i_phy in range(num_log, num_phy): # Iterate num_redundant times\n        for i_layer in range(n): # Iterate over each layer\n            current_heap = expert_heaps[i_layer]\n            \n            # Get weights for the current layer.\n            # This is already done in the outer loop's initialization.\n            weights_for_layer = weights_cpu[i_layer, :] \n\n            # Pop the expert with the highest average load (min negative load)\n            neg_avg_load, log_expert_id = heapq.heappop(current_heap)\n            \n            # Assign this expert as a physical replica\n            phy2log[i_layer, i_phy] = log_expert_id\n            rank[i_layer, i_phy] = logcnt[i_layer, log_expert_id] # Replica rank for this expert\n            \n            # Increment count for this logical expert\n            logcnt[i_layer, log_expert_id] += 1\n            \n            # Calculate new average load and push back to heap\n            # Access pre-fetched weight for the current logical expert\n            current_expert_weight = weights_for_layer[log_expert_id]\n            \n            # Fetch the count for the specific expert, one .item() call\n            current_logcnt = logcnt[i_layer, log_expert_id].item() \n\n            # Calculate new average load\n            new_avg_load = current_expert_weight.item() / current_logcnt # One .item() call here\n            heapq.heappush(current_heap, (-new_avg_load, log_expert_id))\n    return phy2log, rank, logcnt\n```\nThe `replicate_experts` function receives `weight` which is already on CPU. So `weights_cpu = weight.cpu()` is redundant if `weight` is already guaranteed to be on CPU. The `rebalance_experts` function does `weight = weight.float().cpu()`. So, inside `replicate_experts`, `weight` is already a CPU float tensor. Thus, `weights_for_layer = weight[i_layer, :]` is sufficient.\n\nLet's finalize the diffs.\n\n```python\n# In balanced_packing, within the loop for i in range(num_layers):\n# Replace:\n#         layer_indices = indices[i]\n#\n#         # Iterate through the group IDs sorted by weight.\n#         for group_idx_in_sorted_list in range(num_groups):\n#             # Get the actual group ID from the sorted indices.\n#             group_id = layer_indices[group_idx_in_sorted_list].item()\n#\n#             # Get the pack with the minimum current load from the heap.\n#             current_load, pack = heapq.heappop(pack_heap)\n#\n#             # Assign the group to the chosen pack.\n#             # The `group_id` is the column index for the `pack_index` and `rank_in_pack` tensors.\n#             pack_index[i, group_id] = pack\n#             rank_in_pack[i, group_id] = current_pack_items[pack]\n#\n#             # Update pack item count for the chosen pack.\n#             current_pack_items[pack] += 1\n#             \n#             # Calculate new load for the pack.\n#             new_load = current_load + weight[i, group_id].item() # .item() for scalar arithmetic\n#\n#             # If the pack is not yet full, push it back to the heap with its new load.\n#             if current_pack_items[pack] < groups_per_pack:\n#                 heapq.heappush(pack_heap, (new_load, pack))\n#             # Else, the pack is full, do not push it back to the heap.\n#             # It means this pack will not be considered for further assignments in this layer.\n#     return pack_index, rank_in_pack\n\n# With the proposed change:\n#         layer_indices = indices[i]\n#         \n#         # Pre-fetch weights for the current layer and gather them in sorted order.\n#         # This reduces repeated indexing and .item() calls within the inner loop.\n#         weights_for_layer = weight[i, :].gather(-1, layer_indices)\n#\n#         # Iterate through the group IDs sorted by weight.\n#         for group_idx_in_sorted_list in range(num_groups):\n#             # Get the actual group ID from the sorted indices.\n#             group_id = layer_indices[group_idx_in_sorted_list].item()\n#\n#             # Get the weight for the current group from the pre-fetched tensor.\n#             current_group_weight = weights_for_layer[group_idx_in_sorted_list].item()\n#\n#             # Get the pack with the minimum current load from the heap.\n#             current_load, pack = heapq.heappop(pack_heap)\n#\n#             # Assign the group to the chosen pack.\n#             # The `group_id` is the column index for the `pack_index` and `rank_in_pack` tensors.\n#             pack_index[i, group_id] = pack\n#             rank_in_pack[i, group_id] = current_pack_items[pack]\n#\n#             # Update pack item count for the chosen pack.\n#             current_pack_items[pack] += 1\n#             \n#             # Calculate new load for the pack.\n#             new_load = current_load + current_group_weight # Use pre-fetched weight\n#\n#             # If the pack is not yet full, push it back to the heap with its new load.\n#             if current_pack_items[pack] < groups_per_pack:\n#                 heapq.heappush(pack_heap, (new_load, pack))\n#             # Else, the pack is full, do not push it back to the heap.\n#             # It means this pack will not be considered for further assignments in this layer.\n#     return pack_index, rank_in_pack\n\n```\n\n```python\n# In replicate_experts, within the loop for i_layer in range(n):\n# Replace:\n#         layer_heap = []\n#         for j_expert in range(num_log):\n#             # Initial average load is just the weight itself (since logcnt is 1 for initial experts)\n#             heapq.heappush(layer_heap, (-weight[i_layer, j_expert].item(), j_expert))\n#         expert_heaps.append(layer_heap)\n#\n#     # Assign redundant experts\n#     for i_phy in range(num_log, num_phy): # Iterate num_redundant times\n#         for i_layer in range(n): # Iterate over each layer\n#             current_heap = expert_heaps[i_layer]\n#             \n#             # Pop the expert with the highest average load (min negative load)\n#             neg_avg_load, log_expert_id = heapq.heappop(current_heap)\n#             \n#             # Assign this expert as a physical replica\n#             phy2log[i_layer, i_phy] = log_expert_id\n#             rank[i_layer, i_phy] = logcnt[i_layer, log_expert_id] # Replica rank for this expert\n#             \n#             # Increment count for this logical expert\n#             logcnt[i_layer, log_expert_id] += 1\n#             \n#             # Calculate new average load and push back to heap\n#             # Use .item() to get Python scalar for heap operations.\n#             new_avg_load = weight[i_layer, log_expert_id].item() / logcnt[i_layer, log_expert_id].item()\n#             heapq.heappush(current_heap, (-new_avg_load, log_expert_id))\n\n# With the proposed change:\n#     # Initialize max-heaps for each layer\n#     # Heap elements: (-average_load, logical_expert_id)\n#     expert_heaps = []\n#     for i_layer in range(n):\n#         layer_heap = []\n#         # Pre-fetch weights for the current layer to reduce repeated indexing and .item() calls.\n#         # 'weight' is already on CPU and float from the caller.\n#         weights_for_layer = weight[i_layer, :]\n#         \n#         for j_expert in range(num_log):\n#             # Use .item() here once per expert for initialization\n#             heapq.heappush(layer_heap, (-weights_for_layer[j_expert].item(), j_expert))\n#         expert_heaps.append(layer_heap)\n#\n#     # Assign redundant experts\n#     for i_phy in range(num_log, num_phy): # Iterate num_redundant times\n#         for i_layer in range(n): # Iterate over each layer\n#             current_heap = expert_heaps[i_layer]\n#             \n#             # Get weights for the current layer. This is available from the outer loop.\n#             weights_for_layer = weight[i_layer, :] \n#\n#             # Pop the expert with the highest average load (min negative load)\n#             neg_avg_load, log_expert_id = heapq.heappop(current_heap)\n#             \n#             # Assign this expert as a physical replica\n#             phy2log[i_layer, i_phy] = log_expert_id\n#             rank[i_layer, i_phy] = logcnt[i_layer, log_expert_id] # Replica rank for this expert\n#             \n#             # Increment count for this logical expert\n#             logcnt[i_layer, log_expert_id] += 1\n#             \n#             # Calculate new average load and push back to heap\n#             # Access pre-fetched weight for the current logical expert\n#             current_expert_weight = weights_for_layer[log_expert_id]\n#             \n#             # Fetch the count for the specific expert, one .item() call\n#             current_logcnt = logcnt[i_layer, log_expert_id].item() \n#\n#             # Calculate new average load\n#             new_avg_load = current_expert_weight.item() / current_logcnt # One .item() call here\n#             heapq.heappush(current_heap, (-new_avg_load, log_expert_id))\n#     return phy2log, rank, logcnt\n\n```\nThese changes aim to reduce the overhead associated with repeated tensor indexing and `.item()` calls within the most performance-critical loops, potentially improving the `speed_score`. The impact on `balancedness_score` is expected to be minimal, but it's something to monitor.\n\nFinal check on the `balanced_packing` pre-fetching:\n`weight[i, :]` selects the `i`-th row (all groups for layer `i`).\n`layer_indices` are the sorted group indices for layer `i`.\n`weight[i, :].gather(-1, layer_indices)` gathers elements from `weight[i, :]` at the indices specified by `layer_indices`. This correctly produces a tensor where the `k`-th element is the weight of the `k`-th group in sorted order.\nThen, `weights_for_layer[group_idx_in_sorted_list].item()` retrieves the weight of the currently processed group (which is the `group_idx_in_sorted_list`-th group in sorted order). This seems correct.\n\nThe `replicate_experts` pre-fetching is simpler: `weights_for_layer = weight[i_layer, :]` directly gets the weights for the layer. Then `weights_for_layer[log_expert_id]` accesses the weight of the selected logical expert. This is also correct.\n\nThese changes are focused on improving efficiency without altering the fundamental greedy assignment logic.\n\nLet's consider the prompt's mention of \"reverting or revising recent changes\". If the regression is due to a specific change that was made, and these optimizations don't fully recover the performance, then a deeper analysis of the evolution history would be needed. However, these are standard micro-optimizations that can help.\n\nOne last thought: the `rebalance_experts` function moves the weight tensor to CPU. If `weight` is already on CPU, `.cpu()` is a no-op. If it's on GPU, `.cpu()` incurs data transfer cost. This is necessary for `heapq`, so it's likely unavoidable.\n\nThe core problem is that perfect load balancing is NP-hard. The greedy approach is a heuristic. Improvements often come from better heuristics or more sophisticated approximation algorithms. However, the prompt asks for improving the *current* algorithm's efficiency and balancedness. Optimizing the existing operations is the most direct path.\n\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\nimport heapq\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort groups by weight in descending order.\n    # 'weight' is already float and on CPU from rebalance_experts,\n    # so no need for .float().cpu() here.\n    indices = weight.sort(-1, descending=True).indices\n\n    # Initialize pack_index and rank_in_pack.\n    # These will be fully populated, so `empty_like` is sufficient.\n    # They will be on the same device as 'weight' (i.e., CPU).\n    pack_index = torch.empty_like(weight, dtype=torch.int64)\n    rank_in_pack = torch.empty_like(pack_index)\n    \n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # Min-heap for the current layer to keep track of pack loads.\n        # Elements are (current_load, pack_id)\n        # Initialize with all packs having 0 load.\n        pack_heap = [(0.0, p) for p in range(num_packs)]\n        heapq.heapify(pack_heap) # Build heap in O(num_packs)\n        \n        # Keep track of items assigned to each pack for the current layer.\n        current_pack_items = torch.zeros(num_packs, dtype=torch.int64, device=weight.device)\n\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer, sorted by weight.\n        layer_indices = indices[i]\n        \n        # Pre-fetch weights for the current layer and gather them in sorted order.\n        # This reduces repeated indexing and .item() calls within the inner loop.\n        weights_for_layer = weight[i, :].gather(-1, layer_indices)\n\n        # Iterate through the group IDs sorted by weight.\n        for group_idx_in_sorted_list in range(num_groups):\n            # Get the actual group ID from the sorted indices.\n            group_id = layer_indices[group_idx_in_sorted_list].item()\n\n            # Get the weight for the current group from the pre-fetched tensor.\n            current_group_weight = weights_for_layer[group_idx_in_sorted_list].item()\n\n            # Get the pack with the minimum current load from the heap.\n            current_load, pack = heapq.heappop(pack_heap)\n\n            # Assign the group to the chosen pack.\n            # The `group_id` is the column index for the `pack_index` and `rank_in_pack` tensors.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack item count for the chosen pack.\n            current_pack_items[pack] += 1\n            \n            # Calculate new load for the pack.\n            new_load = current_load + current_group_weight # Use pre-fetched weight\n\n            # If the pack is not yet full, push it back to the heap with its new load.\n            if current_pack_items[pack] < groups_per_pack:\n                heapq.heappush(pack_heap, (new_load, pack))\n            # Else, the pack is full, do not push it back to the heap.\n            # It means this pack will not be considered for further assignments in this layer.\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    # Initialize max-heaps for each layer\n    # Heap elements: (-average_load, logical_expert_id)\n    # Use negative average_load for max-heap behavior with heapq (which is a min-heap).\n    expert_heaps = []\n    for i_layer in range(n):\n        layer_heap = []\n        # Pre-fetch weights for the current layer to reduce repeated indexing and .item() calls.\n        # 'weight' is already on CPU and float from the caller.\n        weights_for_layer = weight[i_layer, :]\n        \n        for j_expert in range(num_log):\n            # Use .item() here once per expert for initialization\n            heapq.heappush(layer_heap, (-weights_for_layer[j_expert].item(), j_expert))\n        expert_heaps.append(layer_heap)\n\n    # Assign redundant experts\n    for i_phy in range(num_log, num_phy): # Iterate num_redundant times\n        for i_layer in range(n): # Iterate over each layer\n            current_heap = expert_heaps[i_layer]\n            \n            # Get weights for the current layer. This is available from the outer loop.\n            weights_for_layer = weight[i_layer, :] \n\n            # Pop the expert with the highest average load (min negative load)\n            neg_avg_load, log_expert_id = heapq.heappop(current_heap)\n            \n            # Assign this expert as a physical replica\n            phy2log[i_layer, i_phy] = log_expert_id\n            rank[i_layer, i_phy] = logcnt[i_layer, log_expert_id] # Replica rank for this expert\n            \n            # Increment count for this logical expert\n            logcnt[i_layer, log_expert_id] += 1\n            \n            # Calculate new average load and push back to heap\n            # Access pre-fetched weight for the current logical expert\n            current_expert_weight = weights_for_layer[log_expert_id]\n            \n            # Fetch the count for the specific expert, one .item() call\n            current_logcnt = logcnt[i_layer, log_expert_id].item() \n\n            # Calculate new average load\n            new_avg_load = current_expert_weight.item() / current_logcnt # One .item() call here\n            heapq.heappush(current_heap, (-new_avg_load, log_expert_id))\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    \n    # Store original device to move results back later.\n    original_device = weight.device\n    \n    # Cast weight to float for calculations and move to CPU.\n    # Python's heapq and scalar operations (.item()) are efficient on CPU tensors.\n    weight = weight.float().cpu()\n    \n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        # This might be a performance bottleneck if num_groups is not a multiple of num_nodes.\n        # A more sophisticated approach could be beneficial here, but for now, we stick to the current logic.\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    \n    # Move results back to the original device.\n    phy2log = phy2log.to(original_device)\n    log2phy = log2phy.to(original_device)\n    logcnt = logcnt.to(original_device)\n    \n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n```\n\n```diff\n<<<<<<< SEARCH\n            # Calculate new load for the pack.\n            new_load = current_load + weight[i, group_id].item() # .item() for scalar arithmetic\n=======\n            # Calculate new load for the pack.\n            new_load = current_load + current_group_weight # Use pre-fetched weight\n>>>>>>> REPLACE\n```\n\n```diff\n<<<<<<< SEARCH\n        # Iterate over each layer. The greedy assignment per layer is sequential.\n        for i in range(num_layers):\n            # Min-heap for the current layer to keep track of pack loads.\n            # Elements are (current_load, pack_id)\n            # Initialize with all packs having 0 load.\n            pack_heap = [(0.0, p) for p in range(num_packs)]\n            heapq.heapify(pack_heap) # Build heap in O(num_packs)\n            \n            # Keep track of items assigned to each pack for the current layer.\n            current_pack_items = torch.zeros(num_packs, dtype=torch.int64, device=weight.device)\n\n            # For each layer, process groups in sorted order of their weights.\n            # `indices[i]` is a 1D tensor of group IDs for the current layer, sorted by weight.\n            layer_indices = indices[i]\n            \n            # Iterate through the group IDs sorted by weight.\n            for group_idx_in_sorted_list in range(num_groups):\n                # Get the actual group ID from the sorted indices.\n                group_id = layer_indices[group_idx_in_sorted_list].item()\n\n                # Get the pack with the minimum current load from the heap.\n                current_load, pack = heapq.heappop(pack_heap)\n\n                # Assign the group to the chosen pack.\n                # The `group_id` is the column index for the `pack_index` and `rank_in_pack` tensors.\n                pack_index[i, group_id] = pack\n                rank_in_pack[i, group_id] = current_pack_items[pack]\n\n                # Update pack item count for the chosen pack.\n                current_pack_items[pack] += 1\n                \n                # Calculate new load for the pack.\n                new_load = current_load + weight[i, group_id].item() # .item() for scalar arithmetic\n\n                # If the pack is not yet full, push it back to the heap with its new load.\n                if current_pack_items[pack] < groups_per_pack:\n                    heapq.heappush(pack_heap, (new_load, pack))\n                # Else, the pack is full, do not push it back to the heap.\n                # It means this pack will not be considered for further assignments in this layer.\n    return pack_index, rank_in_pack\n=======\n        # Iterate over each layer. The greedy assignment per layer is sequential.\n        for i in range(num_layers):\n            # Min-heap for the current layer to keep track of pack loads.\n            # Elements are (current_load, pack_id)\n            # Initialize with all packs having 0 load.\n            pack_heap = [(0.0, p) for p in range(num_packs)]\n            heapq.heapify(pack_heap) # Build heap in O(num_packs)\n            \n            # Keep track of items assigned to each pack for the current layer.\n            current_pack_items = torch.zeros(num_packs, dtype=torch.int64, device=weight.device)\n\n            # For each layer, process groups in sorted order of their weights.\n            # `indices[i]` is a 1D tensor of group IDs for the current layer, sorted by weight.\n            layer_indices = indices[i]\n            \n            # Pre-fetch weights for the current layer and gather them in sorted order.\n            # This reduces repeated indexing and .item() calls within the inner loop.\n            weights_for_layer = weight[i, :].gather(-1, layer_indices)\n\n            # Iterate through the group IDs sorted by weight.\n            for group_idx_in_sorted_list in range(num_groups):\n                # Get the actual group ID from the sorted indices.\n                group_id = layer_indices[group_idx_in_sorted_list].item()\n\n                # Get the weight for the current group from the pre-fetched tensor.\n                current_group_weight = weights_for_layer[group_idx_in_sorted_list].item()\n\n                # Get the pack with the minimum current load from the heap.\n                current_load, pack = heapq.heappop(pack_heap)\n\n                # Assign the group to the chosen pack.\n                # The `group_id` is the column index for the `pack_index` and `rank_in_pack` tensors.\n                pack_index[i, group_id] = pack\n                rank_in_pack[i, group_id] = current_pack_items[pack]\n\n                # Update pack item count for the chosen pack.\n                current_pack_items[pack] += 1\n                \n                # Calculate new load for the pack.\n                new_load = current_load + current_group_weight # Use pre-fetched weight\n\n                # If the pack is not yet full, push it back to the heap with its new load.\n                if current_pack_items[pack] < groups_per_pack:\n                    heapq.heappush(pack_heap, (new_load, pack))\n                # Else, the pack is full, do not push it back to the heap.\n                # It means this pack will not be considered for further assignments in this layer.\n    return pack_index, rank_in_pack\n>>>>>>> REPLACE\n```\n\n```diff\n<<<<<<< SEARCH\n        for j_expert in range(num_log):\n            # Initial average load is just the weight itself (since logcnt is 1 for initial experts)\n            heapq.heappush(layer_heap, (-weight[i_layer, j_expert].item(), j_expert))\n        expert_heaps.append(layer_heap)\n\n    # Assign redundant experts\n    for i_phy in range(num_log, num_phy): # Iterate num_redundant times\n        for i_layer in range(n): # Iterate over each layer\n            current_heap = expert_heaps[i_layer]\n            \n            # Pop the expert with the highest average load (min negative load)\n            neg_avg_load, log_expert_id = heapq.heappop(current_heap)\n            \n            # Assign this expert as a physical replica\n            phy2log[i_layer, i_phy] = log_expert_id\n            rank[i_layer, i_phy] = logcnt[i_layer, log_expert_id] # Replica rank for this expert\n            \n            # Increment count for this logical expert\n            logcnt[i_layer, log_expert_id] += 1\n            \n            # Calculate new average load and push back to heap\n            # Use .item() to get Python scalar for heap operations.\n            new_avg_load = weight[i_layer, log_expert_id].item() / logcnt[i_layer, log_expert_id].item()\n            heapq.heappush(current_heap, (-new_avg_load, log_expert_id))\n=======\n        # Pre-fetch weights for the current layer to reduce repeated indexing and .item() calls.\n        # 'weight' is already on CPU and float from the caller.\n        weights_for_layer = weight[i_layer, :]\n        \n        for j_expert in range(num_log):\n            # Use .item() here once per expert for initialization\n            heapq.heappush(layer_heap, (-weights_for_layer[j_expert].item(), j_expert))\n        expert_heaps.append(layer_heap)\n\n    # Assign redundant experts\n    for i_phy in range(num_log, num_phy): # Iterate num_redundant times\n        for i_layer in range(n): # Iterate over each layer\n            current_heap = expert_heaps[i_layer]\n            \n            # Get weights for the current layer. This is available from the outer loop.\n            weights_for_layer = weight[i_layer, :] \n\n            # Pop the expert with the highest average load (min negative load)\n            neg_avg_load, log_expert_id = heapq.heappop(current_heap)\n            \n            # Assign this expert as a physical replica\n            phy2log[i_layer, i_phy] = log_expert_id\n            rank[i_layer, i_phy] = logcnt[i_layer, log_expert_id] # Replica rank for this expert\n            \n            # Increment count for this logical expert\n            logcnt[i_layer, log_expert_id] += 1\n            \n            # Calculate new average load and push back to heap\n            # Access pre-fetched weight for the current logical expert\n            current_expert_weight = weights_for_layer[log_expert_id]\n            \n            # Fetch the count for the specific expert, one .item() call\n            current_logcnt = logcnt[i_layer, log_expert_id].item() \n\n            # Calculate new average load\n            new_avg_load = current_expert_weight.item() / current_logcnt # One .item() call here\n            heapq.heappush(current_heap, (-new_avg_load, log_expert_id))\n>>>>>>> REPLACE\n```"]}}}