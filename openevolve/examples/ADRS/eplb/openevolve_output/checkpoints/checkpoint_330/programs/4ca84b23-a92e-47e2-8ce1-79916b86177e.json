{"id": "4ca84b23-a92e-47e2-8ce1-79916b86177e", "code": "# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\nimport heapq\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort groups by weight in descending order.\n    # 'weight' is already float and on CPU from rebalance_experts,\n    # so no need for .float().cpu() here.\n    indices = weight.sort(-1, descending=True).indices\n\n    # Initialize pack_index and rank_in_pack.\n    # These will be fully populated, so `empty_like` is sufficient.\n    # They will be on the same device as 'weight' (i.e., CPU).\n    pack_index = torch.empty_like(weight, dtype=torch.int64)\n    rank_in_pack = torch.empty_like(pack_index)\n    \n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These tensors are on the same device as 'weight' (which is CPU).\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer, sorted by weight.\n        layer_indices = indices[i]\n        \n        # Get the current pack loads and counts for this layer.\n        current_pack_weights = pack_weights_per_layer[i]\n        current_pack_items = pack_items_per_layer[i]\n\n        # Iterate through the group IDs sorted by weight.\n        # We are assigning `num_groups` items into `num_packs` packs, with `groups_per_pack` items per pack.\n        # The current loop iterates through each group and finds the best pack.\n        # We optimize this by directly finding the best pack without creating a temporary tensor.\n        \n        # Create a mask for packs that are not yet full.\n        not_full_mask = current_pack_items < groups_per_pack\n        \n        # Find the indices of available packs.\n        available_pack_indices = torch.where(not_full_mask)[0]\n        \n        # If all packs are full, this should not happen given the problem constraints.\n        # Asserting this for debugging.\n        assert torch.any(not_full_mask), \"All packs are full, but we are still assigning items.\"\n\n        # Iterate through the group IDs sorted by weight.\n        for group_idx_in_sorted_list in range(num_groups):\n            # Get the actual group ID from the sorted indices.\n            group_id = layer_indices[group_idx_in_sorted_list].item()\n\n            # Get the weights corresponding to these available pack indices.\n            weights_of_available_packs = current_pack_weights[available_pack_indices]\n            \n            # Find the index within the `available_pack_indices` tensor that corresponds to the minimum weight.\n            best_available_idx = torch.argmin(weights_of_available_packs)\n            \n            # Get the actual pack index (the original index in the `num_packs` dimension).\n            pack = available_pack_indices[best_available_idx]\n\n            # Assign the group to the chosen pack.\n            # The `group_id` is the column index for the `pack_index` and `rank_in_pack` tensors.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            current_pack_weights[pack] += weight[i, group_id]\n            current_pack_items[pack] += 1\n\n            # After assigning an item to a pack, we need to update the `not_full_mask` and `available_pack_indices`\n            # for the next iteration. This is crucial for correctness.\n            # If the pack that was just filled becomes full, we need to remove it from consideration.\n            if current_pack_items[pack] == groups_per_pack:\n                # Find the index of 'pack' in 'available_pack_indices' and remove it.\n                # This is an O(num_packs) operation, which is still a bottleneck if num_packs is large.\n                # A more efficient way to manage available packs would be to use a data structure\n                # that supports efficient deletion, like a linked list or a dynamic array.\n                # However, for simplicity and to target the most obvious inefficiency (tensor creation),\n                # we'll stick with this for now.\n                \n                # Recompute available_pack_indices for the next iteration.\n                # This is a bit inefficient. A better way would be to update the mask directly.\n                not_full_mask = current_pack_items < groups_per_pack\n                available_pack_indices = torch.where(not_full_mask)[0]\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    # Initialize max-heaps for each layer\n    # Heap elements: (-average_load, logical_expert_id)\n    # Use negative average_load for max-heap behavior with heapq (which is a min-heap).\n    expert_heaps = []\n    for i_layer in range(n):\n        layer_heap = []\n        for j_expert in range(num_log):\n            # Initial average load is just the weight itself (since logcnt is 1 for initial experts)\n            heapq.heappush(layer_heap, (-weight[i_layer, j_expert].item(), j_expert))\n        expert_heaps.append(layer_heap)\n\n    # Assign redundant experts\n    for i_phy in range(num_log, num_phy): # Iterate num_redundant times\n        for i_layer in range(n): # Iterate over each layer\n            current_heap = expert_heaps[i_layer]\n            \n            # Pop the expert with the highest average load (min negative load)\n            neg_avg_load, log_expert_id = heapq.heappop(current_heap)\n            \n            # Assign this expert as a physical replica\n            phy2log[i_layer, i_phy] = log_expert_id\n            rank[i_layer, i_phy] = logcnt[i_layer, log_expert_id] # Replica rank for this expert\n            \n            # Increment count for this logical expert\n            logcnt[i_layer, log_expert_id] += 1\n            \n            # Calculate new average load and push back to heap\n            # Use .item() to get Python scalar for heap operations.\n            new_avg_load = weight[i_layer, log_expert_id].item() / logcnt[i_layer, log_expert_id].item()\n            heapq.heappush(current_heap, (-new_avg_load, log_expert_id))\n\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    \n    # Store original device to move results back later.\n    original_device = weight.device\n    \n    # Cast weight to float for calculations and move to CPU.\n    # Python's heapq and scalar operations (.item()) are efficient on CPU tensors.\n    weight = weight.float().cpu()\n    \n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    \n    # Move results back to the original device.\n    phy2log = phy2log.to(original_device)\n    log2phy = log2phy.to(original_device)\n    logcnt = logcnt.to(original_device)\n    \n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n", "language": "python", "parent_id": "58922b52-ed0e-48ee-8168-a2a9bf7b047d", "generation": 9, "timestamp": 1751061612.9488533, "iteration_found": 185, "metrics": {"balancedness_score": 0.29830835153138974, "speed_score": 0.027621027508328382, "combined_score": 0.16296468951985907}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 47 lines with 28 lines", "parent_metrics": {"balancedness_score": 0.29830835153138974, "speed_score": 0.027121205034964135, "combined_score": 0.16271477828317693}, "island": 4}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve the Mixture-of-Expert models Expert Parallelism Load Balancer (MoE EPLB) expert rearrangement algorithm.\nThis algorithm will take the load metrics recorded by the vLLM server, and rearrange the experts to balance the load. It can make replicas of some experts to achieve better load balancing.\nYour goal will be two-fold: 1. Improve the algorithm to achieve better load balancing; while 2. Improve the algorithm to be more efficient, i.e. reduce the execution time of the algorithm itself, since perfect load balancing is NP-hard.\nThe current algorithm is implemented in the `rebalance_experts` function. ", "user": "# Current Program Information\n- Current performance metrics: - balancedness_score: 0.2983\n- speed_score: 0.0271\n- combined_score: 0.1627\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: balancedness_score, speed_score, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0857, combined_score: 0.1920\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1097, combined_score: 0.2312\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1103, combined_score: 0.2315\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.2315)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1103), Performs well on combined_score (0.2315)\n\n\n### Program 2 (Score: 0.2312)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1097), Performs well on combined_score (0.2312)\n\n\n### Program 3 (Score: 0.1920)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0857), Performs well on combined_score (0.1920)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.1909)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n\n### Program D2 (Score: 0.1914)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.2315, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 32 lines with 32 lines, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 2 (Score: 0.1920, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 3 (Score: 0.0000, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 43 lines with 37 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 4 (Score: 0.1831, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 43 lines with 37 lines\nChange 2: Replace 204 lines with 114 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 5 (Score: 0.1627, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 40 lines with 38 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n\n# Current Program\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\nimport heapq\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort groups by weight in descending order.\n    # 'weight' is already float and on CPU from rebalance_experts,\n    # so no need for .float().cpu() here.\n    indices = weight.sort(-1, descending=True).indices\n\n    # Initialize pack_index and rank_in_pack.\n    # These will be fully populated, so `empty_like` is sufficient.\n    # They will be on the same device as 'weight' (i.e., CPU).\n    pack_index = torch.empty_like(weight, dtype=torch.int64)\n    rank_in_pack = torch.empty_like(pack_index)\n    \n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These tensors are on the same device as 'weight' (which is CPU).\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer, sorted by weight.\n        layer_indices = indices[i]\n        \n        # Get the current pack loads and counts for this layer.\n        current_pack_weights = pack_weights_per_layer[i]\n        current_pack_items = pack_items_per_layer[i]\n\n        # Iterate through the group IDs sorted by weight.\n        # We are assigning `num_groups` items into `num_packs` packs, with `groups_per_pack` items per pack.\n        # The current loop iterates through each group and finds the best pack.\n        # We optimize this by directly finding the best pack without creating a temporary tensor.\n        \n        # Create a mask for packs that are not yet full.\n        not_full_mask = current_pack_items < groups_per_pack\n        \n        # Find the indices of available packs.\n        available_pack_indices = torch.where(not_full_mask)[0]\n        \n        # If all packs are full, this should not happen given the problem constraints.\n        # Asserting this for debugging.\n        assert torch.any(not_full_mask), \"All packs are full, but we are still assigning items.\"\n\n        # Iterate through the group IDs sorted by weight.\n        for group_idx_in_sorted_list in range(num_groups):\n            # Get the actual group ID from the sorted indices.\n            group_id = layer_indices[group_idx_in_sorted_list].item()\n\n            # Get the weights corresponding to these available pack indices.\n            weights_of_available_packs = current_pack_weights[available_pack_indices]\n            \n            # Find the index within the `available_pack_indices` tensor that corresponds to the minimum weight.\n            best_available_idx = torch.argmin(weights_of_available_packs)\n            \n            # Get the actual pack index (the original index in the `num_packs` dimension).\n            pack = available_pack_indices[best_available_idx]\n\n            # Assign the group to the chosen pack.\n            # The `group_id` is the column index for the `pack_index` and `rank_in_pack` tensors.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            current_pack_weights[pack] += weight[i, group_id]\n            current_pack_items[pack] += 1\n\n            # After assigning an item to a pack, we need to update the `not_full_mask` and `available_pack_indices`\n            # for the next iteration. This is crucial for correctness.\n            # If the pack that was just filled becomes full, we need to remove it from consideration.\n            if current_pack_items[pack] == groups_per_pack:\n                # Find the index of 'pack' in 'available_pack_indices' and remove it.\n                # This is an O(num_packs) operation, which is still a bottleneck if num_packs is large.\n                # A more efficient way to manage available packs would be to use a data structure\n                # that supports efficient deletion, like a linked list or a dynamic array.\n                # However, for simplicity and to target the most obvious inefficiency (tensor creation),\n                # we'll stick with this for now.\n                \n                # Recompute available_pack_indices for the next iteration.\n                # This is a bit inefficient. A better way would be to update the mask directly.\n                not_full_mask = current_pack_items < groups_per_pack\n                available_pack_indices = torch.where(not_full_mask)[0]\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    # Initialize max-heaps for each layer\n    # Heap elements: (-average_load, logical_expert_id)\n    # Use negative average_load for max-heap behavior with heapq (which is a min-heap).\n    expert_heaps = []\n    for i_layer in range(n):\n        layer_heap = []\n        for j_expert in range(num_log):\n            # Initial average load is just the weight itself (since logcnt is 1 for initial experts)\n            heapq.heappush(layer_heap, (-weight[i_layer, j_expert].item(), j_expert))\n        expert_heaps.append(layer_heap)\n\n    # Assign redundant experts\n    for i_phy in range(num_log, num_phy): # Iterate num_redundant times\n        for i_layer in range(n): # Iterate over each layer\n            current_heap = expert_heaps[i_layer]\n            \n            # Pop the expert with the highest average load (min negative load)\n            neg_avg_load, log_expert_id = heapq.heappop(current_heap)\n            \n            # Assign this expert as a physical replica\n            phy2log[i_layer, i_phy] = log_expert_id\n            rank[i_layer, i_phy] = logcnt[i_layer, log_expert_id] # Replica rank for this expert\n            \n            # Increment count for this logical expert\n            logcnt[i_layer, log_expert_id] += 1\n            \n            # Calculate new average load and push back to heap\n            # Use .item() to get Python scalar for heap operations.\n            new_avg_load = weight[i_layer, log_expert_id].item() / logcnt[i_layer, log_expert_id].item()\n            heapq.heappush(current_heap, (-new_avg_load, log_expert_id))\n\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    \n    # Store original device to move results back later.\n    original_device = weight.device\n    \n    # Cast weight to float for calculations and move to CPU.\n    # Python's heapq and scalar operations (.item()) are efficient on CPU tensors.\n    weight = weight.float().cpu()\n    \n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    \n    # Move results back to the original device.\n    phy2log = phy2log.to(original_device)\n    log2phy = log2phy.to(original_device)\n    logcnt = logcnt.to(original_device)\n    \n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["The current performance metrics show a regression in both `balancedness_score` and `speed_score` compared to previous top-performing programs. The primary goal is to improve both, with a focus on efficiency given that perfect load balancing is NP-hard.\n\nUpon reviewing the code, the `balanced_packing` function contains an inner loop that repeatedly performs `torch.where` and `torch.argmin` operations. While these operations are efficient on GPUs for large tensors, when applied to small tensors (like `available_pack_indices` which has size up to `num_packs`, typically a small number like 8, 16, or 64) repeatedly within a Python loop, the overhead of creating intermediate tensors and calling PyTorch functions can accumulate.\n\nThe current approach to find the best pack is:\n1.  Create a boolean mask for non-full packs (`not_full_mask`).\n2.  Use `torch.where` to get indices of non-full packs (`available_pack_indices`).\n3.  Slice `current_pack_weights` using these indices (`weights_of_available_packs`).\n4.  Use `torch.argmin` on the sliced weights to find the best among available packs.\n5.  Map the index back to the original pack index.\n\nThis sequence of operations is performed `num_groups` times for each layer. For small `num_packs`, a direct Python loop to find the minimum weight among available packs can be more efficient by avoiding these intermediate tensor creations and PyTorch function call overheads. It also makes the logic more straightforward, addressing the \"simplify code for readability and maintainability\" suggestion. The underlying greedy algorithm (Longest Processing Time - LPT heuristic) remains the same, ensuring that the `balancedness_score` should not be negatively impacted, and might even slightly improve due to reduced overhead.\n\nThe proposed change replaces the `torch.where` and `argmin` logic with a direct Python loop that iterates through all possible packs, checks if they are full, and finds the one with the minimum current weight. This should lead to a better `speed_score`.\n\nNo significant changes are proposed for `replicate_experts` or the overall `rebalance_experts` structure, as their current implementations (using `heapq` for `replicate_experts` and the hierarchical approach) are generally sound for the given problem and complexity.\n\n**Detailed plan:**\n\n1.  **Modify `balanced_packing` function:** Replace the `torch.where` and `torch.argmin` sequence with a direct Python loop to find the minimum-weight available pack. This reduces overhead from repeated tensor operations for small `num_packs`.\n\n<<<<<<< SEARCH\n        # Create a mask for packs that are not yet full.\n        not_full_mask = current_pack_items < groups_per_pack\n        \n        # Find the indices of available packs.\n        available_pack_indices = torch.where(not_full_mask)[0]\n        \n        # If all packs are full, this should not happen given the problem constraints.\n        # Asserting this for debugging.\n        assert torch.any(not_full_mask), \"All packs are full, but we are still assigning items.\"\n\n        # Iterate through the group IDs sorted by weight.\n        for group_idx_in_sorted_list in range(num_groups):\n            # Get the actual group ID from the sorted indices.\n            group_id = layer_indices[group_idx_in_sorted_list].item()\n\n            # Get the weights corresponding to these available pack indices.\n            weights_of_available_packs = current_pack_weights[available_pack_indices]\n            \n            # Find the index within the `available_pack_indices` tensor that corresponds to the minimum weight.\n            best_available_idx = torch.argmin(weights_of_available_packs)\n            \n            # Get the actual pack index (the original index in the `num_packs` dimension).\n            pack = available_pack_indices[best_available_idx]\n\n            # Assign the group to the chosen pack.\n            # The `group_id` is the column index for the `pack_index` and `rank_in_pack` tensors.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            current_pack_weights[pack] += weight[i, group_id]\n            current_pack_items[pack] += 1\n\n            # After assigning an item to a pack, we need to update the `not_full_mask` and `available_pack_indices`\n            # for the next iteration. This is crucial for correctness.\n            # If the pack that was just filled becomes full, we need to remove it from consideration.\n            # Find the index of 'pack' in 'available_pack_indices' and remove it.\n            # This is an O(num_packs) operation, which is still a bottleneck if num_packs is large.\n            # A more efficient way to manage available packs would be to use a data structure\n            # that supports efficient deletion, like a linked list or a dynamic array.\n            # However, for simplicity and to target the most obvious inefficiency (tensor creation),\n            # we'll stick with this for now.\n            \n            # Recompute available_pack_indices for the next iteration.\n            # This is a bit inefficient. A better way would be to update the mask directly.\n            not_full_mask = current_pack_items < groups_per_pack\n            available_pack_indices = torch.where(not_full_mask)[0]\n=======\n        # Iterate through the group IDs sorted by weight.\n        for group_idx_in_sorted_list in range(num_groups):\n            # Get the actual group ID from the sorted indices.\n            group_id = layer_indices[group_idx_in_sorted_list].item()\n\n            # Find the pack with the minimum current weight that is not yet full.\n            min_weight_val = float('inf')\n            best_pack_idx = -1\n            for p_idx in range(num_packs):\n                if current_pack_items[p_idx] < groups_per_pack:\n                    if current_pack_weights[p_idx] < min_weight_val:\n                        min_weight_val = current_pack_weights[p_idx]\n                        best_pack_idx = p_idx\n            \n            # Assert that a valid pack was found. This should always be true\n            # given the problem constraints (num_groups is a multiple of num_packs\n            # and each pack is designed to hold groups_per_pack items).\n            assert best_pack_idx != -1, \"Error: No available pack found for assignment.\"\n            \n            pack = best_pack_idx\n\n            # Assign the group to the chosen pack.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            current_pack_weights[pack] += weight[i, group_id]\n            current_pack_items[pack] += 1\n>>>>>>> REPLACE"]}}}