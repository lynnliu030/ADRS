{"id": "45eef517-98f0-4d62-8426-699e5666067d", "code": "# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    indices = weight.float().sort(-1, descending=True).indices  # Keep on original device\n    pack_index = torch.full_like(weight,\n                                 fill_value=-1,\n                                 dtype=torch.int64,\n                                 device=weight.device)  # Use weight's device\n    rank_in_pack = torch.full_like(pack_index,\n                                   fill_value=-1,\n                                   device=weight.device)  # Use weight's device\n\n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These are on the same device as the input 'weight'.\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Pre-calculate a tensor for broadcasting layer indices\n    layer_indices = torch.arange(num_layers, device=weight.device)\n    \n    # Create an infinity tensor once, with the correct device and dtype\n    inf_tensor = weight.new_full((), float('inf'))\n\n    # Iterate over each \"slot\" in the sorted list of groups (from heaviest to lightest)\n    # This processes the k-th heaviest group for ALL layers simultaneously, leveraging PyTorch's vectorization.\n    for k in range(num_groups):\n        # Get the group_ids that are k-th heaviest for each layer\n        current_group_ids = indices[:, k]  # [num_layers]\n\n        # Get the weights of these groups\n        # Use advanced indexing to get weights: weight[layer_idx, group_id_for_that_layer]\n        current_weights = weight[layer_indices, current_group_ids]  # [num_layers]\n\n        # Find available packs for each layer (those not yet full).\n        # available_mask: [num_layers, num_packs]\n        available_mask = (pack_items_per_layer < groups_per_pack)\n\n        # Set unavailable packs to infinity for argmin\n        # candidate_weights: [num_layers, num_packs]\n        candidate_weights = torch.where(available_mask, pack_weights_per_layer, inf_tensor)\n\n        # Find the pack with the minimum current weight for each layer\n        # pack_indices_to_assign: [num_layers]\n        pack_indices_to_assign = torch.argmin(candidate_weights, dim=-1)\n\n        # Assign the current group to the chosen pack for each layer\n        # Using advanced indexing to update pack_index and rank_in_pack\n        pack_index[layer_indices, current_group_ids] = pack_indices_to_assign\n        rank_in_pack[layer_indices, current_group_ids] = \\\n            pack_items_per_layer[layer_indices, pack_indices_to_assign]\n\n        # Update pack weights and item counts for the chosen packs\n        # These updates are also vectorized across layers\n        pack_weights_per_layer[layer_indices, pack_indices_to_assign] += current_weights\n        pack_items_per_layer[layer_indices, pack_indices_to_assign] += 1\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    # Efficiently distribute redundant experts to minimize maximum load.\n    # Calculate the average load for each logical expert.\n    avg_load = weight / logcnt\n\n    # Get the indices of logical experts sorted by their average load in ascending order.\n    # This allows us to greedily assign replicas to the least loaded experts first.\n    sorted_expert_indices = torch.argsort(avg_load, dim=-1) # [n, num_log]\n\n    # For each layer, determine how many replicas each logical expert should get.\n    # We want to distribute `num_redundant` replicas as evenly as possible.\n    # The goal is to make the load distribution as balanced as possible.\n    # We can pre-calculate the target number of replicas for each logical expert.\n    # This is a bit complex to do perfectly with integer division and remainders\n    # across all layers simultaneously in a vectorized way.\n    # A simpler, but potentially slightly less optimal, approach is to iterate\n    # through the sorted experts and assign replicas.\n\n    # Let's refine the assignment to be more directly related to minimizing max load.\n    # Instead of a simple loop, we can try to determine the \"target load\" for each expert.\n    # This is still a heuristic, as perfect load balancing is NP-hard.\n\n    # A more direct approach to minimize the maximum load is to simulate\n    # the assignment of redundant experts.\n    # We can calculate the load each expert would have if it received an additional replica.\n    # Then, we assign the next replica to the expert that results in the smallest\n    # increase in its load, or keeps its load the lowest.\n\n    # Let's use a more systematic approach for distributing `num_redundant` experts.\n    # We can determine how many replicas each expert should ideally have.\n    # This can be approximated by distributing `num_phy` total experts across `num_log` logical experts.\n    total_experts_per_layer = num_phy\n    ideal_replicas_per_expert = total_experts_per_layer / num_log\n\n    # We will assign replicas one by one to the logical experts that have the least current load.\n    # This is a common heuristic for load balancing.\n\n    # Keep track of the current load of each logical expert after assignments.\n    current_expert_loads = weight.clone() # [n, num_log]\n    current_logcnt = logcnt.clone() # [n, num_log]\n\n    # We need to assign `num_redundant` additional experts.\n    for i in range(num_redundant):\n        # For each layer, find the logical expert with the minimum current load.\n        # Use argmin on current_expert_loads, considering only experts that can receive more replicas\n        # (i.e., those that are not already assigned their \"ideal\" number of replicas,\n        # or simply those that are least loaded).\n\n        # A simpler strategy that often works well: iteratively assign to the least loaded expert.\n        # Find the logical expert with the minimum current load across all layers.\n        # This requires careful handling of which expert to pick for each layer.\n\n        # Let's stick to the original loop structure but optimize the selection.\n        # The current strategy is: pick the logical expert with the minimum average load.\n        # To make it more efficient, we can pre-sort or use a data structure.\n\n        # Let's try to keep the original loop but ensure the selection is optimal.\n        # The current code `(weight / logcnt).max(dim=-1).indices` is incorrect.\n        # It should be `(weight / logcnt).min(dim=-1).indices` to find the MINIMUM average load.\n        # Also, `max` should be `min` for finding the least loaded expert.\n\n        # Corrected logic: find the logical expert with the minimum current average load for each layer.\n        avg_load_per_layer = weight / current_logcnt # [n, num_log]\n        \n        # Find the logical expert with the minimum load for each layer.\n        # We need to avoid assigning more replicas than feasible if there's a hard cap,\n        # but here we are distributing a fixed number of redundant experts.\n        \n        # To make it efficient, we can sort the experts by load for each layer.\n        # However, the current structure iterates through physical experts (i from num_log to num_phy).\n        # For each physical expert slot `i`, we decide which logical expert to assign it to.\n\n        # Let's rethink the objective: minimize the maximum load.\n        # This is akin to the multiway number partitioning problem.\n        # A greedy approach that works well is to always assign the next item (replica)\n        # to the bin (logical expert) with the smallest current sum of weights.\n\n        # The current loop iterates through physical expert slots. For each slot,\n        # it picks a logical expert to assign to.\n\n        # Original (buggy) line:\n        # redundant_indices = (weight / logcnt).max(dim=-1).indices\n\n        # Corrected approach to find the least loaded logical expert for each layer:\n        avg_load_per_layer = weight / current_logcnt # [n, num_log]\n        \n        # Find the index of the logical expert with the minimum average load for each layer\n        # This is what `min(dim=-1).indices` does.\n        # We need to ensure we don't assign too many replicas to one expert if it's not necessary.\n        # The current loop structure is assigning `num_redundant` experts to fill `num_phy` slots.\n        \n        # Let's refine the selection of `redundant_indices`.\n        # We want to find the logical expert that, if it receives another replica,\n        # will have the minimum load among all logical experts.\n\n        # A better heuristic:\n        # For each layer, calculate the current load of each logical expert.\n        # Find the logical expert with the minimum load.\n        # Assign the current physical expert slot to this logical expert.\n        # Update the load and replica count for that logical expert.\n\n        # Let's re-implement this part of the loop more carefully.\n        # We are filling `phy2log[:, i]` for `i` from `num_log` to `num_phy - 1`.\n\n        # Calculate the current average load for each logical expert across all layers.\n        # This is not correct, we need to do it per layer.\n        \n        # Let's use the `sorted_expert_indices` to guide the assignment.\n        # For each layer `l`, we have `sorted_expert_indices[l, :]` which gives us\n        # the order of logical experts from least loaded to most loaded.\n\n        # We need to distribute `num_redundant` experts across `num_log` logical experts.\n        # The total number of experts will be `num_phy`.\n        # So, on average, each logical expert will have `num_phy / num_log` replicas.\n\n        # Let's try to determine the target number of replicas for each logical expert first.\n        # This is a distribution problem.\n        \n        # A simpler, greedy approach for distributing `num_redundant` items:\n        # For each of the `num_redundant` slots, pick the logical expert that currently has the minimum load.\n\n        # The current code `(weight / logcnt).max(dim=-1).indices` is selecting the *most* loaded expert\n        # and giving it the *next* replica, which is counter-intuitive for balancing.\n        # It should be `min` to find the least loaded expert.\n\n        # Let's rewrite the loop for clarity and correctness.\n        # We want to fill `num_phy` slots in `phy2log` and `rank`.\n        # The first `num_log` slots are implicitly handled by having one replica per logical expert.\n        # We are adding `num_redundant` more experts.\n\n        # For each layer, we have `num_log` logical experts.\n        # `current_logcnt` tracks how many physical experts are currently assigned to each logical expert.\n        # `current_expert_loads` tracks the total load assigned to each logical expert.\n\n        # To minimize the maximum load, we should assign the next replica to the logical expert\n        # that currently has the minimum load.\n\n        # For each layer `l` from 0 to `n-1`:\n        # Find the logical expert `j` that minimizes `weight[l, :] / current_logcnt[l, :]`.\n        # Assign the `i`-th physical expert (for layer `l`) to this logical expert `j`.\n        # Update `phy2log[l, i] = j`\n        # Update `rank[l, i] = current_logcnt[l, j]`\n        # Update `current_logcnt[l, j] += 1`\n        # Update `current_expert_loads[l, j] += weight[l, j] / logcnt[l, j]` (This is tricky, should be the weight of the assigned expert)\n\n        # Let's use the `sorted_expert_indices` to make this efficient.\n        # For each layer, we want to distribute `num_phy / num_nodes` experts.\n        # The `replicate_experts` function is called with `tokens_per_mlog`, which is `[n * num_nodes, num_log_per_node]`\n        # and `num_phy // num_nodes` is the target number of physical experts per GPU.\n\n        # Let's assume `replicate_experts` is called with `weight_for_replication` and `target_phy_experts`.\n        # `weight_for_replication` is `[N_effective, num_logical_experts_subset]`\n        # `target_phy_experts` is the number of physical experts we need in this subset.\n        # `num_log_subset = num_logical_experts_subset`\n\n        # The goal is to assign `target_phy_experts` total physical experts to `num_log_subset` logical experts.\n        # Initially, each logical expert has 1 physical expert. We need to add `target_phy_experts - num_log_subset` more.\n\n        # Let's simplify the loop logic for `replicate_experts`.\n        # We have `num_redundant = target_phy_experts - num_log_subset` replicas to add.\n        \n        # Create a tensor to track the current number of replicas assigned to each logical expert.\n        current_replica_counts = torch.ones(n, num_log, dtype=torch.int64, device=device) # Initially 1 replica for each logical expert.\n        \n        # For each of the `num_redundant` slots to fill:\n        for i in range(num_redundant):\n            # For each layer, find the logical expert that currently has the minimum load.\n            # The load of a logical expert is its total weight divided by its current replica count.\n            # We should consider the *average* load per replica.\n            \n            # Calculate the average load for each logical expert in each layer.\n            # Avoid division by zero if current_replica_counts is 0 (shouldn't happen here).\n            avg_loads = weight / current_replica_counts # [n, num_log]\n\n            # Find the index of the logical expert with the minimum average load for each layer.\n            # This is the expert that would benefit most from an additional replica.\n            least_loaded_expert_indices = torch.argmin(avg_loads, dim=-1) # [n]\n\n            # Assign the current physical expert slot to this least loaded logical expert.\n            # The `phy2log` tensor is [n, target_phy_experts].\n            # We are filling `phy2log[:, num_log + i]`.\n            # The `rank` tensor is [n, target_phy_experts].\n            # The `logcnt` tensor is [n, num_log].\n\n            # The physical experts are indexed from 0 to `target_phy_experts - 1`.\n            # The first `num_log` are implicit (one replica per logical expert).\n            # We are filling the slots from `num_log` to `target_phy_experts - 1`.\n            \n            # The target index in `phy2log` and `rank` for the current redundant expert is `num_log + i`.\n            target_phy_idx = num_log + i\n\n            # Update `phy2log` for the current layer and physical expert slot.\n            phy2log[arangen, target_phy_idx] = least_loaded_expert_indices\n\n            # Update `rank`. The rank of the new replica is the current count of replicas for that logical expert.\n            rank[arangen, target_phy_idx] = current_replica_counts[arangen, least_loaded_expert_indices]\n\n            # Increment the replica count for the chosen logical expert in each layer.\n            current_replica_counts[arangen, least_loaded_expert_indices] += 1\n\n        # Update the final `logcnt` based on the `current_replica_counts`.\n        # `logcnt` should reflect the number of physical experts assigned to each logical expert.\n        logcnt = current_replica_counts\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float()  # Ensure float, but keep on original device (e.g., GPU)\n    # Determine the load balancing policy based on divisibility.\n    # If num_groups is divisible by num_nodes, use hierarchical balancing.\n    # Otherwise, a global balancing strategy is more appropriate.\n    if num_groups % num_nodes == 0:\n        # Use hierarchical load-balance policy: pack groups to nodes first.\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # Use global load-balance policy: treat all experts as a single pool.\n        # This is achieved by setting num_groups to 1 and num_nodes to 1.\n        # The `rebalance_experts_hierarchical` function can handle these parameters.\n        # When num_groups = 1, it effectively becomes a global balancing.\n        # When num_nodes = 1, it simplifies the node-level packing.\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    # Ensure log2phy is created on the same device as other tensors\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=weight.device,  # Use weight's device\n    )\n    # The scatter_ operation and arange should also use the correct device\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=weight.device).expand(num_layers, -1),  # Use weight's device\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n", "language": "python", "parent_id": "e70129e9-0e75-41b2-a61f-904c687b9653_migrant_1_migrant_2_migrant_3", "generation": 6, "timestamp": 1751066512.0413187, "iteration_found": 262, "metrics": {"balancedness_score": 0.14515957884623945, "speed_score": 0.08210056615135138, "combined_score": 0.11363007249879542}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 9 lines with 202 lines\nChange 2: Replace 8 lines with 15 lines", "parent_metrics": {"balancedness_score": 0.29830835153138974, "speed_score": 0.08334680991298496, "combined_score": 0.19082758072218736}, "island": 3}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve the Mixture-of-Expert models Expert Parallelism Load Balancer (MoE EPLB) expert rearrangement algorithm.\nThis algorithm will take the load metrics recorded by the vLLM server, and rearrange the experts to balance the load. It can make replicas of some experts to achieve better load balancing.\nYour goal will be two-fold: 1. Improve the algorithm to achieve better load balancing; while 2. Improve the algorithm to be more efficient, i.e. reduce the execution time of the algorithm itself, since perfect load balancing is NP-hard.\nThe current algorithm is implemented in the `rebalance_experts` function. ", "user": "# Current Program Information\n- Current performance metrics: - balancedness_score: 0.2983\n- speed_score: 0.0833\n- combined_score: 0.1908\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: balancedness_score, speed_score, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1103, combined_score: 0.2315\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1103, combined_score: 0.2315\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1103, combined_score: 0.2315\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.2315)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1103), Performs well on combined_score (0.2315)\n\n\n### Program 2 (Score: 0.2315)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1103), Performs well on combined_score (0.2315)\n\n\n### Program 3 (Score: 0.2315)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1103), Performs well on combined_score (0.2315)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.2315)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n\n### Program D2 (Score: 0.2315)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.2315, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 32 lines with 32 lines, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 2 (Score: 0.1920, Type: Migrant)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 3 (Score: 0.2315, Type: Migrant)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 32 lines with 32 lines, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 4 (Score: 0.0000, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 5 (Score: 0.1628, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 6 lines with 8 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n\n# Current Program\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    indices = weight.float().sort(-1, descending=True).indices  # Keep on original device\n    pack_index = torch.full_like(weight,\n                                 fill_value=-1,\n                                 dtype=torch.int64,\n                                 device=weight.device)  # Use weight's device\n    rank_in_pack = torch.full_like(pack_index,\n                                   fill_value=-1,\n                                   device=weight.device)  # Use weight's device\n\n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These are on the same device as the input 'weight'.\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Pre-calculate a tensor for broadcasting layer indices\n    layer_indices = torch.arange(num_layers, device=weight.device)\n    \n    # Create an infinity tensor once, with the correct device and dtype\n    inf_tensor = weight.new_full((), float('inf'))\n\n    # Iterate over each \"slot\" in the sorted list of groups (from heaviest to lightest)\n    # This processes the k-th heaviest group for ALL layers simultaneously, leveraging PyTorch's vectorization.\n    for k in range(num_groups):\n        # Get the group_ids that are k-th heaviest for each layer\n        current_group_ids = indices[:, k]  # [num_layers]\n\n        # Get the weights of these groups\n        # Use advanced indexing to get weights: weight[layer_idx, group_id_for_that_layer]\n        current_weights = weight[layer_indices, current_group_ids]  # [num_layers]\n\n        # Find available packs for each layer (those not yet full).\n        # available_mask: [num_layers, num_packs]\n        available_mask = (pack_items_per_layer < groups_per_pack)\n\n        # Set unavailable packs to infinity for argmin\n        # candidate_weights: [num_layers, num_packs]\n        candidate_weights = torch.where(available_mask, pack_weights_per_layer, inf_tensor)\n\n        # Find the pack with the minimum current weight for each layer\n        # pack_indices_to_assign: [num_layers]\n        pack_indices_to_assign = torch.argmin(candidate_weights, dim=-1)\n\n        # Assign the current group to the chosen pack for each layer\n        # Using advanced indexing to update pack_index and rank_in_pack\n        pack_index[layer_indices, current_group_ids] = pack_indices_to_assign\n        rank_in_pack[layer_indices, current_group_ids] = \\\n            pack_items_per_layer[layer_indices, pack_indices_to_assign]\n\n        # Update pack weights and item counts for the chosen packs\n        # These updates are also vectorized across layers\n        pack_weights_per_layer[layer_indices, pack_indices_to_assign] += current_weights\n        pack_items_per_layer[layer_indices, pack_indices_to_assign] += 1\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        # To minimize the maximum load, we should add replicas to the\n        # logical experts with the minimum current average load.\n        # Reverting to original behavior as suggested by performance metrics.\n        # This strategy aims to reduce the load of the currently most loaded experts.\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float()  # Ensure float, but keep on original device (e.g., GPU)\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    # Ensure log2phy is created on the same device as other tensors\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=weight.device,  # Use weight's device\n    )\n    # The scatter_ operation and arange should also use the correct device\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=weight.device).expand(num_layers, -1),  # Use weight's device\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["The current program aims to balance expert loads by first packing groups to nodes, then replicating experts within nodes, and finally packing physical experts to GPUs. However, the performance metrics indicate a regression. Let's analyze and propose improvements.\n\n**Analysis of Current Performance:**\n\nThe current performance metrics (`balancedness_score: 0.2983`, `speed_score: 0.0833`, `combined_score: 0.1908`) are significantly lower than the top-performing programs (`combined_score: 0.2315`). The fact that all scores have regressed suggests that recent changes might have negatively impacted the algorithm's effectiveness or efficiency. The note about \"reverting to original behavior as suggested by performance metrics\" in the `replicate_experts` function implies a potential issue with the replication strategy itself.\n\n**Areas for Improvement:**\n\n1.  **Replication Strategy:** The current `replicate_experts` function aims to minimize the maximum load by adding replicas to logical experts with the minimum current average load. This greedy approach might not always yield the optimal global solution for load balancing. A more sophisticated approach could consider the overall distribution of loads and potential future imbalances.\n\n2.  **Load Balancing within Packs:** The `balanced_packing` function uses a greedy approach to assign items to packs to balance weights. While effective, for very large numbers of items and packs, more advanced bin-packing algorithms or heuristics could potentially offer better balancedness, though at the cost of increased computation.\n\n3.  **Efficiency of `replicate_experts`:** The loop in `replicate_experts` iterates `num_redundant` times. If `num_redundant` is large, this could be a bottleneck. Optimizing this loop or finding a more vectorized approach would improve the `speed_score`.\n\n4.  **Hierarchical vs. Global Policy:** The current implementation uses `rebalance_experts_hierarchical` for both cases (when `num_groups % num_nodes == 0` and when it's not). This might not be optimal. When `num_groups % num_nodes != 0`, a truly global policy might be more appropriate than a slightly modified hierarchical one.\n\n**Proposed Changes:**\n\nLet's focus on improving the `replicate_experts` function for better balancedness and efficiency, and refine the logic for choosing between hierarchical and global policies.\n\n**Improvement 1: Optimizing `replicate_experts` for Efficiency and Balancedness**\n\nThe current `replicate_experts` function uses a loop to iteratively assign replicas. This can be inefficient if `num_redundant` is large. We can improve this by sorting the logical experts by their current load and then distributing the redundant experts greedily to those with the lowest loads. This maintains the greedy strategy but makes it more efficient.\n\n```python\n<<<<<<< SEARCH\n    for i in range(num_log, num_phy):\n        # To minimize the maximum load, we should add replicas to the\n        # logical experts with the minimum current average load.\n        # Reverting to original behavior as suggested by performance metrics.\n        # This strategy aims to reduce the load of the currently most loaded experts.\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n=======\n    # Efficiently distribute redundant experts to minimize maximum load.\n    # Calculate the average load for each logical expert.\n    avg_load = weight / logcnt\n\n    # Get the indices of logical experts sorted by their average load in ascending order.\n    # This allows us to greedily assign replicas to the least loaded experts first.\n    sorted_expert_indices = torch.argsort(avg_load, dim=-1) # [n, num_log]\n\n    # For each layer, determine how many replicas each logical expert should get.\n    # We want to distribute `num_redundant` replicas as evenly as possible.\n    # The goal is to make the load distribution as balanced as possible.\n    # We can pre-calculate the target number of replicas for each logical expert.\n    # This is a bit complex to do perfectly with integer division and remainders\n    # across all layers simultaneously in a vectorized way.\n    # A simpler, but potentially slightly less optimal, approach is to iterate\n    # through the sorted experts and assign replicas.\n\n    # Let's refine the assignment to be more directly related to minimizing max load.\n    # Instead of a simple loop, we can try to determine the \"target load\" for each expert.\n    # This is still a heuristic, as perfect load balancing is NP-hard.\n\n    # A more direct approach to minimize the maximum load is to simulate\n    # the assignment of redundant experts.\n    # We can calculate the load each expert would have if it received an additional replica.\n    # Then, we assign the next replica to the expert that results in the smallest\n    # increase in its load, or keeps its load the lowest.\n\n    # Let's use a more systematic approach for distributing `num_redundant` experts.\n    # We can determine how many replicas each expert should ideally have.\n    # This can be approximated by distributing `num_phy` total experts across `num_log` logical experts.\n    total_experts_per_layer = num_phy\n    ideal_replicas_per_expert = total_experts_per_layer / num_log\n\n    # We will assign replicas one by one to the logical experts that have the least current load.\n    # This is a common heuristic for load balancing.\n\n    # Keep track of the current load of each logical expert after assignments.\n    current_expert_loads = weight.clone() # [n, num_log]\n    current_logcnt = logcnt.clone() # [n, num_log]\n\n    # We need to assign `num_redundant` additional experts.\n    for i in range(num_redundant):\n        # For each layer, find the logical expert with the minimum current load.\n        # Use argmin on current_expert_loads, considering only experts that can receive more replicas\n        # (i.e., those that are not already assigned their \"ideal\" number of replicas,\n        # or simply those that are least loaded).\n\n        # A simpler strategy that often works well: iteratively assign to the least loaded expert.\n        # Find the logical expert with the minimum current load across all layers.\n        # This requires careful handling of which expert to pick for each layer.\n\n        # Let's stick to the original loop structure but optimize the selection.\n        # The current strategy is: pick the logical expert with the minimum average load.\n        # To make it more efficient, we can pre-sort or use a data structure.\n\n        # Let's try to keep the original loop but ensure the selection is optimal.\n        # The current code `(weight / logcnt).max(dim=-1).indices` is incorrect.\n        # It should be `(weight / logcnt).min(dim=-1).indices` to find the MINIMUM average load.\n        # Also, `max` should be `min` for finding the least loaded expert.\n\n        # Corrected logic: find the logical expert with the minimum current average load for each layer.\n        avg_load_per_layer = weight / current_logcnt # [n, num_log]\n        \n        # Find the logical expert with the minimum load for each layer.\n        # We need to avoid assigning more replicas than feasible if there's a hard cap,\n        # but here we are distributing a fixed number of redundant experts.\n        \n        # To make it efficient, we can sort the experts by load for each layer.\n        # However, the current structure iterates through physical experts (i from num_log to num_phy).\n        # For each physical expert slot `i`, we decide which logical expert to assign it to.\n\n        # Let's rethink the objective: minimize the maximum load.\n        # This is akin to the multiway number partitioning problem.\n        # A greedy approach that works well is to always assign the next item (replica)\n        # to the bin (logical expert) with the smallest current sum of weights.\n\n        # The current loop iterates through physical expert slots. For each slot,\n        # it picks a logical expert to assign to.\n\n        # Original (buggy) line:\n        # redundant_indices = (weight / logcnt).max(dim=-1).indices\n\n        # Corrected approach to find the least loaded logical expert for each layer:\n        avg_load_per_layer = weight / current_logcnt # [n, num_log]\n        \n        # Find the index of the logical expert with the minimum average load for each layer\n        # This is what `min(dim=-1).indices` does.\n        # We need to ensure we don't assign too many replicas to one expert if it's not necessary.\n        # The current loop structure is assigning `num_redundant` experts to fill `num_phy` slots.\n        \n        # Let's refine the selection of `redundant_indices`.\n        # We want to find the logical expert that, if it receives another replica,\n        # will have the minimum load among all logical experts.\n\n        # A better heuristic:\n        # For each layer, calculate the current load of each logical expert.\n        # Find the logical expert with the minimum load.\n        # Assign the current physical expert slot to this logical expert.\n        # Update the load and replica count for that logical expert.\n\n        # Let's re-implement this part of the loop more carefully.\n        # We are filling `phy2log[:, i]` for `i` from `num_log` to `num_phy - 1`.\n\n        # Calculate the current average load for each logical expert across all layers.\n        # This is not correct, we need to do it per layer.\n        \n        # Let's use the `sorted_expert_indices` to guide the assignment.\n        # For each layer `l`, we have `sorted_expert_indices[l, :]` which gives us\n        # the order of logical experts from least loaded to most loaded.\n\n        # We need to distribute `num_redundant` experts across `num_log` logical experts.\n        # The total number of experts will be `num_phy`.\n        # So, on average, each logical expert will have `num_phy / num_log` replicas.\n\n        # Let's try to determine the target number of replicas for each logical expert first.\n        # This is a distribution problem.\n        \n        # A simpler, greedy approach for distributing `num_redundant` items:\n        # For each of the `num_redundant` slots, pick the logical expert that currently has the minimum load.\n\n        # The current code `(weight / logcnt).max(dim=-1).indices` is selecting the *most* loaded expert\n        # and giving it the *next* replica, which is counter-intuitive for balancing.\n        # It should be `min` to find the least loaded expert.\n\n        # Let's rewrite the loop for clarity and correctness.\n        # We want to fill `num_phy` slots in `phy2log` and `rank`.\n        # The first `num_log` slots are implicitly handled by having one replica per logical expert.\n        # We are adding `num_redundant` more experts.\n\n        # For each layer, we have `num_log` logical experts.\n        # `current_logcnt` tracks how many physical experts are currently assigned to each logical expert.\n        # `current_expert_loads` tracks the total load assigned to each logical expert.\n\n        # To minimize the maximum load, we should assign the next replica to the logical expert\n        # that currently has the minimum load.\n\n        # For each layer `l` from 0 to `n-1`:\n        # Find the logical expert `j` that minimizes `weight[l, :] / current_logcnt[l, :]`.\n        # Assign the `i`-th physical expert (for layer `l`) to this logical expert `j`.\n        # Update `phy2log[l, i] = j`\n        # Update `rank[l, i] = current_logcnt[l, j]`\n        # Update `current_logcnt[l, j] += 1`\n        # Update `current_expert_loads[l, j] += weight[l, j] / logcnt[l, j]` (This is tricky, should be the weight of the assigned expert)\n\n        # Let's use the `sorted_expert_indices` to make this efficient.\n        # For each layer, we want to distribute `num_phy / num_nodes` experts.\n        # The `replicate_experts` function is called with `tokens_per_mlog`, which is `[n * num_nodes, num_log_per_node]`\n        # and `num_phy // num_nodes` is the target number of physical experts per GPU.\n\n        # Let's assume `replicate_experts` is called with `weight_for_replication` and `target_phy_experts`.\n        # `weight_for_replication` is `[N_effective, num_logical_experts_subset]`\n        # `target_phy_experts` is the number of physical experts we need in this subset.\n        # `num_log_subset = num_logical_experts_subset`\n\n        # The goal is to assign `target_phy_experts` total physical experts to `num_log_subset` logical experts.\n        # Initially, each logical expert has 1 physical expert. We need to add `target_phy_experts - num_log_subset` more.\n\n        # Let's simplify the loop logic for `replicate_experts`.\n        # We have `num_redundant = target_phy_experts - num_log_subset` replicas to add.\n        \n        # Create a tensor to track the current number of replicas assigned to each logical expert.\n        current_replica_counts = torch.ones(n, num_log, dtype=torch.int64, device=device) # Initially 1 replica for each logical expert.\n        \n        # For each of the `num_redundant` slots to fill:\n        for i in range(num_redundant):\n            # For each layer, find the logical expert that currently has the minimum load.\n            # The load of a logical expert is its total weight divided by its current replica count.\n            # We should consider the *average* load per replica.\n            \n            # Calculate the average load for each logical expert in each layer.\n            # Avoid division by zero if current_replica_counts is 0 (shouldn't happen here).\n            avg_loads = weight / current_replica_counts # [n, num_log]\n\n            # Find the index of the logical expert with the minimum average load for each layer.\n            # This is the expert that would benefit most from an additional replica.\n            least_loaded_expert_indices = torch.argmin(avg_loads, dim=-1) # [n]\n\n            # Assign the current physical expert slot to this least loaded logical expert.\n            # The `phy2log` tensor is [n, target_phy_experts].\n            # We are filling `phy2log[:, num_log + i]`.\n            # The `rank` tensor is [n, target_phy_experts].\n            # The `logcnt` tensor is [n, num_log].\n\n            # The physical experts are indexed from 0 to `target_phy_experts - 1`.\n            # The first `num_log` are implicit (one replica per logical expert).\n            # We are filling the slots from `num_log` to `target_phy_experts - 1`.\n            \n            # The target index in `phy2log` and `rank` for the current redundant expert is `num_log + i`.\n            target_phy_idx = num_log + i\n\n            # Update `phy2log` for the current layer and physical expert slot.\n            phy2log[arangen, target_phy_idx] = least_loaded_expert_indices\n\n            # Update `rank`. The rank of the new replica is the current count of replicas for that logical expert.\n            rank[arangen, target_phy_idx] = current_replica_counts[arangen, least_loaded_expert_indices]\n\n            # Increment the replica count for the chosen logical expert in each layer.\n            current_replica_counts[arangen, least_loaded_expert_indices] += 1\n\n        # Update the final `logcnt` based on the `current_replica_counts`.\n        # `logcnt` should reflect the number of physical experts assigned to each logical expert.\n        logcnt = current_replica_counts\n>>>>>>> REPLACE\n```\n\n**Reasoning for Improvement 1:**\n\nThe original `replicate_experts` function had a logical error in selecting the expert to replicate: `(weight / logcnt).max(dim=-1).indices` would select the *most* loaded expert, which is counterproductive for balancing. The corrected logic uses `torch.argmin(avg_loads, dim=-1)` to find the *least* loaded expert for each layer and assigns the next replica to it. This is a standard greedy heuristic for minimizing the maximum load. The loop structure is preserved for clarity, but the selection logic is fundamentally improved. The use of `current_replica_counts` correctly tracks the number of replicas assigned to each logical expert during the distribution process.\n\n**Improvement 2: Refine Policy Selection**\n\nThe current implementation uses `rebalance_experts_hierarchical` for both cases, only changing `num_groups` to 1 when it's not divisible. This might not be ideal. When `num_groups` is not a divisor of `num_nodes`, a truly global strategy (equivalent to `num_nodes=1, num_groups=1`) might be more appropriate and simpler.\n\n```python\n<<<<<<< SEARCH\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n=======\n    # Determine the load balancing policy based on divisibility.\n    # If num_groups is divisible by num_nodes, use hierarchical balancing.\n    # Otherwise, a global balancing strategy is more appropriate.\n    if num_groups % num_nodes == 0:\n        # Use hierarchical load-balance policy: pack groups to nodes first.\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # Use global load-balance policy: treat all experts as a single pool.\n        # This is achieved by setting num_groups to 1 and num_nodes to 1.\n        # The `rebalance_experts_hierarchical` function can handle these parameters.\n        # When num_groups = 1, it effectively becomes a global balancing.\n        # When num_nodes = 1, it simplifies the node-level packing.\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n>>>>>>> REPLACE\n```\n\n**Reasoning for Improvement 2:**\n\nThe logic for selecting the policy remains the same, but the comment is updated to clarify *why* `num_groups=1` and `num_nodes=1` are used to simulate a global policy within the existing `rebalance_experts_hierarchical` framework. This ensures that the code's intent is clear and that the correct strategy is applied when the hierarchical structure is not naturally supported by the group/node configuration. The `rebalance_experts_hierarchical` function is designed to handle `num_groups=1` and `num_nodes=1` as a global case.\n\nThese changes address the identified performance regressions by correcting a fundamental flaw in the replication strategy and ensuring the appropriate balancing policy is chosen. The corrected `replicate_experts` should lead to better `balancedness_score` and potentially `speed_score` by avoiding a flawed selection process. The clarity in policy selection also contributes to maintainability."]}}}