{"id": "e399fcc9-cae3-432a-b4ef-f7c4ed15f437", "code": "# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\nimport heapq # Added for priority queue optimization in replicate_experts\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    indices = weight.float().sort(-1, descending=True).indices.cpu()\n    pack_index = torch.full_like(weight,\n                                 fill_value=-1,\n                                 dtype=torch.int64,\n                                 device=\"cpu\")\n    rank_in_pack = torch.full_like(pack_index, fill_value=-1)\n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These are on CPU because the input 'weight' is moved to CPU in rebalance_experts.\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id in indices[i]:\n            # Get current state for this layer's packs.\n            current_pack_weights = pack_weights_per_layer[i]\n            current_pack_items = pack_items_per_layer[i]\n\n            # Find available packs (those not yet full).\n            available_mask = (current_pack_items < groups_per_pack)\n\n            # Get weights of available packs\n            available_weights = current_pack_weights[available_mask]\n\n            # Find the pack with the minimum current weight among available ones.\n            # torch.min returns the minimum value and its index.\n            # We need to map the index in available_weights back to the original pack index.\n            min_weight, min_idx_in_available = torch.min(available_weights, dim=-1)\n            \n            # Find the original index of the pack with the minimum weight\n            # Optimized: Use torch.nonzero directly to get original index\n            pack = torch.nonzero(available_mask, as_tuple=True)[0][min_idx_in_available].item()\n\n            # Assert that a valid pack was found and it has space.\n            assert current_pack_items[pack] < groups_per_pack\n\n            # Assign the group to the chosen pack.\n            # `group_id` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            # `weight[i, group_id]` is the weight of the current group.\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    \n    # Initialize phy2log and rank\n    phy2log = torch.empty(n, num_phy, dtype=torch.int64, device=device)\n    # The first num_log physical experts map to the first num_log logical experts\n    phy2log[:, :num_log] = torch.arange(num_log, dtype=torch.int64, device=device).expand(n, num_log)\n    \n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    \n    # Initialize min-heaps for each layer\n    # Heap stores (average_load, logical_expert_idx)\n    heaps = []\n    for i in range(n):\n        current_layer_heap = []\n        for j in range(num_log):\n            heapq.heappush(current_layer_heap, (weight[i, j].item() / logcnt[i, j].item(), j))\n        heaps.append(current_layer_heap)\n\n    # Replicate experts for the redundant slots\n    for i_phy in range(num_log, num_phy): # Iterate over physical expert slots to fill\n        for layer_idx in range(n): # Iterate over each layer\n            # Pop the expert with the minimum average load for the current layer\n            min_load, min_load_expert_idx = heapq.heappop(heaps[layer_idx])\n\n            # Assign the current physical expert slot (i_phy) to this logical expert\n            phy2log[layer_idx, i_phy] = min_load_expert_idx\n            \n            # Determine the rank for this new replica.\n            # The rank is the current count of replicas for that logical expert.\n            rank[layer_idx, i_phy] = logcnt[layer_idx, min_load_expert_idx]\n            \n            # Increment the replica count for the chosen logical expert.\n            logcnt[layer_idx, min_load_expert_idx] += 1\n            \n            # Calculate new average load for the replicated expert and push back to heap\n            new_average_load = weight[layer_idx, min_load_expert_idx].item() / logcnt[layer_idx, min_load_expert_idx].item()\n            heapq.heappush(heaps[layer_idx], (new_average_load, min_load_expert_idx))\n            \n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # Reshape weights for replication: [num_layers * num_nodes, groups_per_node * group_size]\n    # where groups_per_node * group_size is the number of logical experts per node.\n    # The shape should be [num_layers, num_nodes, num_logical_experts // num_nodes]\n    # Then flatten to [num_layers * num_nodes, num_logical_experts // num_nodes]\n    num_logical_experts_per_node = num_logical_experts // num_nodes\n    tokens_per_mlog_intermediate = weight.gather(-1, mlog2log).view(\n        num_layers, num_nodes, num_logical_experts_per_node)\n    \n    # Flatten for replicate_experts: [num_layers * num_nodes, num_logical_experts_per_node]\n    tokens_per_mlog = tokens_per_mlog_intermediate.view(-1, num_logical_experts_per_node)\n\n    # Replicate experts within each node's logical experts\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Reshape replication results back to [num_layers, num_nodes, num_physical_experts // num_nodes]\n    phy2mlog_reshaped = phy2mlog.view(num_layers, num_nodes, -1)\n    phyrank_reshaped = phyrank.view(num_layers, num_nodes, -1)\n    mlogcnt_reshaped = mlogcnt.view(num_layers, num_nodes, -1)\n\n    # Step 3: pack physical_experts to GPUs\n    # Calculate the average load for physical experts within each node\n    # `mlogcnt_reshaped` has shape [num_layers, num_nodes, num_logical_experts_per_node]\n    # `phy2mlog_reshaped` maps physical experts to logical experts within nodes.\n    # We need to get the correct mlogcnt for the replicated logical experts.\n    # The average load for each physical expert is `tokens_per_mlog / mlogcnt` for the corresponding logical expert.\n    \n    # Get the count of replicas for each logical expert *within a node*\n    # We need to align mlogcnt with phy2mlog.\n    # `phy2mlog` is [num_layers * num_nodes, num_logical_experts // num_nodes]\n    # `mlogcnt` is [num_layers * num_nodes, num_logical_experts // num_nodes]\n    # `tokens_per_mlog` is [num_layers * num_nodes, num_logical_experts // num_nodes]\n    \n    # The average load for each *physical* expert is `tokens_per_mlog / mlogcnt` after mapping.\n    # We need to use the correct mlogcnt for the physical expert.\n    # The `phyrank` tells us the rank of the physical expert within its logical expert group.\n    # The `mlogcnt` should be aligned with `phy2mlog`.\n    \n    # Let's re-evaluate `tokens_per_phy`:\n    # `tokens_per_mlog`: load of logical experts within a node.\n    # `mlogcnt`: count of replicas for each logical expert within a node.\n    # `phy2mlog`: maps physical experts (within a node) to logical experts (within a node).\n    \n    # To get the load of a physical expert, we need to find which logical expert it maps to,\n    # and then use the load of that logical expert divided by its replica count.\n    # `tokens_per_mlog` is already structured as [num_layers*num_nodes, num_logical_experts_per_node]\n    # `mlogcnt` is also structured similarly.\n    # `phy2mlog` maps physical expert index to logical expert index.\n    \n    # The average load of a physical expert is `tokens_per_mlog[p_idx] / mlogcnt[p_idx]` IF\n    # `p_idx` corresponds to the same logical expert. This is not directly what we have.\n    \n    # Let's use the reshaped tensors for clarity:\n    # `tokens_per_mlog_intermediate` [num_layers, num_nodes, num_logical_experts_per_node]\n    # `mlogcnt_reshaped` [num_layers, num_nodes, num_logical_experts_per_node]\n    # `phy2mlog_reshaped` [num_layers, num_nodes, num_physical_experts // num_nodes]\n    \n    # We need to get the load of the logical expert that each physical expert maps to.\n    # For a physical expert at `[l, n_idx, p_idx]`, it maps to logical expert `phy2mlog_reshaped[l, n_idx, p_idx]`.\n    # The load of this logical expert is `tokens_per_mlog_intermediate[l, n_idx, phy2mlog_reshaped[l, n_idx, p_idx]]`.\n    # The count of replicas for this logical expert is `mlogcnt_reshaped[l, n_idx, phy2mlog_reshaped[l, n_idx, p_idx]]`.\n    \n    # This requires careful indexing. A more direct approach might be to flatten first.\n    \n    # Re-flattening `tokens_per_mlog` and `mlogcnt` for consistency with `phy2mlog`.\n    # `phy2mlog` is [num_layers * num_nodes, num_logical_experts // num_nodes]\n    # `tokens_per_mlog` is [num_layers * num_nodes, num_logical_experts // num_nodes]\n    # `mlogcnt` is [num_layers * num_nodes, num_logical_experts // num_nodes]\n    \n    # The average load for each physical expert is:\n    # first, find the logical expert index for each physical expert: `phy2mlog`\n    # then, get the load of that logical expert: `tokens_per_mlog[idx, logical_expert_idx]`\n    # then, get the count of replicas for that logical expert: `mlogcnt[idx, logical_expert_idx]`\n    \n    # This can be achieved by using `gather` on `tokens_per_mlog` and `mlogcnt` using `phy2mlog`.\n    # This assumes `tokens_per_mlog` and `mlogcnt` are structured to be directly indexed by `phy2mlog`.\n    # Let's assume `tokens_per_mlog` and `mlogcnt` are already aligned to `phy2mlog`'s structure.\n    # The `replicate_experts` function returns `phy2mlog` and `mlogcnt` of the same shape as its input `weight`.\n    # So `tokens_per_mlog` and `mlogcnt` are [num_layers * num_nodes, num_logical_experts_per_node].\n    # `phy2mlog` is [num_layers * num_nodes, num_physical_experts // num_nodes].\n    \n    # To get the load of the logical expert corresponding to each physical expert:\n    # We need to index into `tokens_per_mlog` using `phy2mlog`.\n    # `tokens_per_mlog` has shape [N_total, N_log_per_node], where N_total = num_layers * num_nodes.\n    # `phy2mlog` has shape [N_total, N_phy_per_node].\n    \n    # This requires an advanced indexing operation.\n    # A simpler way to think about `tokens_per_phy`:\n    # It's the load of the physical expert. A physical expert is a replica of a logical expert.\n    # The load of a logical expert is `weight / logcnt`.\n    # So, the load of a physical expert should be `(weight / logcnt) mapped to the physical expert's logical expert`.\n    \n    # Let's use `mlogcnt_reshaped` and `phy2mlog_reshaped` to guide the calculation.\n    # We need the average load for each physical expert.\n    # The average load of a logical expert `le` is `AvgLoad(le) = tokens_per_mlog[le] / mlogcnt[le]`.\n    # A physical expert `pe` maps to logical expert `le_pe = phy2mlog[pe]`.\n    # So, `AvgLoad(pe) = AvgLoad(le_pe)`.\n    \n    # We need to gather `tokens_per_mlog` and `mlogcnt` using `phy2mlog`.\n    # `tokens_per_mlog` is [num_layers * num_nodes, num_logical_experts_per_node]\n    # `mlogcnt` is [num_layers * num_nodes, num_logical_experts_per_node]\n    # `phy2mlog` is [num_layers * num_nodes, num_physical_experts // num_nodes]\n    \n    # Using advanced indexing:\n    # Create indices for `tokens_per_mlog` and `mlogcnt` to align with `phy2mlog`.\n    # This requires careful handling of batch dimensions.\n    \n    # Let's simplify the structure and ensure correct dimensions:\n    # `phy2mlog` is [num_layers * num_nodes, num_logical_experts // num_nodes]\n    # `mlogcnt` is [num_layers * num_nodes, num_logical_experts // num_nodes]\n    # `tokens_per_mlog` is [num_layers * num_nodes, num_logical_experts // num_nodes]\n    \n    # The number of physical experts per GPU is `phy_experts_per_gpu`.\n    # The number of GPUs per node is `num_gpus // num_nodes`.\n    # The total number of physical experts is `num_replicas`.\n    # The number of physical experts per node is `num_replicas // num_nodes`.\n    \n    # `phy2mlog` is structured per node.\n    # `tokens_per_mlog` is structured per node.\n    # `mlogcnt` is structured per node.\n    \n    # The output of `replicate_experts` is `phy2mlog, phyrank, mlogcnt`.\n    # `phy2mlog` maps physical experts to logical experts (within a node).\n    # `mlogcnt` is the count of replicas for the logical experts.\n    \n    # We want the load of the *physical* experts.\n    # A physical expert `pe` is a replica of a logical expert `le`.\n    # The load of `le` is `tokens_per_mlog[le] / mlogcnt[le]`.\n    # So, the load of `pe` is `tokens_per_mlog[phy2mlog[pe]] / mlogcnt[phy2mlog[pe]]`.\n    \n    # This requires advanced indexing.\n    # Let's create indices for `tokens_per_mlog` and `mlogcnt` that match `phy2mlog`.\n    \n    # `phy2mlog` shape: [num_layers * num_nodes, num_physical_experts_per_node]\n    # `tokens_per_mlog` shape: [num_layers * num_nodes, num_logical_experts_per_node]\n    # `mlogcnt` shape: [num_layers * num_nodes, num_logical_experts_per_node]\n    \n    # The indices for `tokens_per_mlog` and `mlogcnt` should be the values in `phy2mlog`.\n    # The first dimension of `phy2mlog` corresponds to the first dimension of `tokens_per_mlog` and `mlogcnt`.\n    # The second dimension of `phy2mlog` needs to be indexed into the second dimension of `tokens_per_mlog` and `mlogcnt`.\n    \n    # This can be done with `torch.gather` or by carefully constructing indices.\n    \n    # Let's re-evaluate the calculation of `tokens_per_phy`:\n    # `tokens_per_mlog` is the load for logical experts within a node.\n    # `mlogcnt` is the count of replicas for logical experts within a node.\n    # `phy2mlog` maps physical experts to logical experts within a node.\n    \n    # For each physical expert, we need its average load.\n    # This is the average load of the logical expert it belongs to.\n    # We can compute `avg_load_per_logical_expert = tokens_per_mlog / mlogcnt`.\n    # Then, for each physical expert, we find its logical expert `le_idx = phy2mlog[pe_idx]`.\n    # The load of physical expert `pe_idx` is `avg_load_per_logical_expert[le_idx]`.\n    \n    # This can be done by creating a tensor of indices for `avg_load_per_logical_expert`.\n    # Let `logical_expert_indices_for_physical_experts = phy2mlog`.\n    # Then `tokens_per_phy = torch.gather(avg_load_per_logical_expert, 1, logical_expert_indices_for_physical_experts)`.\n    # This requires `avg_load_per_logical_expert` to be [num_layers * num_nodes, num_logical_experts_per_node].\n    \n    # Let's ensure dimensions are correct.\n    # `tokens_per_mlog` is [num_layers * num_nodes, num_logical_experts_per_node]\n    # `mlogcnt` is [num_layers * num_nodes, num_logical_experts_per_node]\n    # `phy2mlog` is [num_layers * num_nodes, num_physical_experts // num_nodes]\n    \n    # Calculate average load per logical expert within each node.\n    avg_load_per_logical_expert = tokens_per_mlog / mlogcnt\n    \n    # Now, for each physical expert, find the average load of its corresponding logical expert.\n    # We need to gather `avg_load_per_logical_expert` using `phy2mlog`.\n    # The indices for `gather` should be `phy2mlog`.\n    # `avg_load_per_logical_expert` has shape [num_layers * num_nodes, num_logical_experts_per_node]\n    # `phy2mlog` has shape [num_layers * num_nodes, num_physical_experts // num_nodes]\n    \n    # `torch.gather` requires the index tensor to have the same number of dimensions as the input tensor.\n    # It also requires the size of the dimension along which to gather to match.\n    # This means we need to expand `phy2mlog` to match the dimensions of `avg_load_per_logical_expert`.\n    \n    # Alternatively, we can use advanced indexing directly if `phy2mlog`'s values are valid indices for `avg_load_per_logical_expert`.\n    # Let's use `torch.gather` on the second dimension (index 1).\n    \n    # Ensure `phy2mlog` is on the same device as `avg_load_per_logical_expert`.\n    phy2mlog_device = phy2mlog.device\n    avg_load_per_logical_expert_device = avg_load_per_logical_expert.device\n    \n    # If `phy2mlog` is not on the correct device, move it.\n    if phy2mlog_device != avg_load_per_logical_expert_device:\n        phy2mlog = phy2mlog.to(avg_load_per_logical_expert_device)\n        \n    # The indices for `gather` must be of `torch.long` type.\n    phy2mlog = phy2mlog.long()\n\n    # Calculate the load for each physical expert.\n    tokens_per_phy = torch.gather(\n        avg_load_per_logical_expert,\n        1,  # Gather along dimension 1 (the logical expert dimension)\n        phy2mlog\n    )\n\n    # Now pack these physical experts onto GPUs.\n    # `tokens_per_phy` is [num_layers * num_nodes, num_physical_experts // num_nodes]\n    # `num_gpus // num_nodes` is the number of GPUs available on each node.\n    pack_index, rank_in_pack = balanced_packing(\n        tokens_per_phy, num_gpus // num_nodes)\n    \n    # Map physical experts to physical experts on GPUs.\n    # `phy2pphy` is the mapping from physical expert index to physical expert index on GPU.\n    # The shape is [num_layers * num_nodes, num_physical_experts // num_nodes]\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    \n    # Compute the inverse mapping: physical expert on GPU to physical expert.\n    pphy2phy = inverse(phy2pphy)\n\n    # Now map physical experts on GPUs back to logical experts.\n    # `pphy2phy` has shape [num_layers * num_nodes, num_physical_experts // num_nodes]\n    # We need to gather `phy2mlog` using `pphy2phy` to get the logical expert mapping for each physical expert on GPU.\n    # `phy2mlog` is [num_layers * num_nodes, num_logical_experts_per_node]\n    # `pphy2phy` is [num_layers * num_nodes, num_physical_experts // num_nodes]\n    \n    # We need to gather `phy2mlog` using `pphy2phy`.\n    # The first dimension of `pphy2phy` corresponds to the first dimension of `phy2mlog`.\n    # The second dimension of `pphy2phy` needs to index into the second dimension of `phy2mlog`.\n    \n    # Ensure `pphy2phy` is on the same device as `phy2mlog`.\n    pphy2phy_device = pphy2phy.device\n    phy2mlog_device = phy2mlog.device\n    if pphy2phy_device != phy2mlog_device:\n        pphy2phy = pphy2phy.to(phy2mlog_device)\n        \n    # Ensure `pphy2phy` is of long type for gather.\n    pphy2phy = pphy2phy.long()\n    \n    # Gather `phy2mlog` using `pphy2phy` to get the logical expert index for each physical expert on GPU.\n    pphy2mlog = torch.gather(\n        phy2mlog,\n        1,  # Gather along dimension 1 (physical expert index within node)\n        pphy2phy\n    )\n    \n    # The `pphy2mlog` now contains logical expert indices within each node's context.\n    # We need to convert these to global logical expert indices.\n    # The offset for each node is `num_logical_experts // num_nodes * node_idx`.\n    # A simpler way is to add the start index of logical experts for each node.\n    # The logical experts per node are `num_logical_experts // num_nodes`.\n    # The starting index for logical experts for node `n_idx` is `n_idx * (num_logical_experts // num_nodes)`.\n    \n    # The `pphy2mlog` is already structured as [num_layers, num_nodes, num_physical_experts_per_gpu].\n    # The original code's logic was:\n    # `pphy2mlog.view(num_layers, num_nodes, -1)` reshapes it correctly.\n    # `+ torch.arange(0, num_logical_experts, num_logical_experts // num_nodes, device=group_pack_index.device).view(1, -1, 1)` adds the node offsets.\n    # This `arange` creates the starting indices for logical experts for each node.\n    # `view(1, -1, 1)` makes it broadcastable: [1, num_nodes, 1].\n    \n    node_offsets = torch.arange(0, num_logical_experts, num_logical_experts // num_nodes, device=group_pack_index.device).view(1, -1, 1)\n    pphy2mlog_global = pphy2mlog.view(num_layers, num_nodes, -1) + node_offsets\n    \n    # Flatten back to the required shape for the final mapping.\n    pphy2mlog_final = pphy2mlog_global.flatten(-2) # [num_layers, num_physical_experts]\n\n    # Now, map these global logical expert indices back to the original logical experts.\n    # `pphy2log` is the final mapping from physical experts (on GPUs) to logical experts.\n    # This is essentially the `mlog2log` mapping applied to the `pphy2mlog_final`.\n    # `mlog2log` maps grouped logical experts to original logical experts.\n    # `pphy2mlog_final` gives us the logical expert indices (in the grouped sense) for each physical expert.\n    # We need to gather `mlog2log` using `pphy2mlog_final`.\n    \n    # `mlog2log` is [num_layers, num_logical_experts // num_nodes]\n    # `pphy2mlog_final` is [num_layers, num_physical_experts]\n    \n    # This requires advanced indexing.\n    # Let's ensure `pphy2mlog_final` is on the same device as `mlog2log`.\n    pphy2mlog_final_device = pphy2mlog_final.device\n    mlog2log_device = mlog2log.device\n    if pphy2mlog_final_device != mlog2log_device:\n        pphy2mlog_final = pphy2mlog_final.to(mlog2log_device)\n        \n    # Ensure `pphy2mlog_final` is of long type.\n    pphy2mlog_final = pphy2mlog_final.long()\n\n    # Gather `mlog2log` using `pphy2mlog_final`.\n    # This will give the original logical expert index for each physical expert on GPU.\n    pphy2log = torch.gather(\n        mlog2log,\n        1,  # Gather along dimension 1 (the grouped logical expert index)\n        pphy2mlog_final\n    )\n    \n    # Now, compute the rank for each physical expert on GPU.\n    # `phyrank` is the rank of the physical expert within its logical expert group (within a node).\n    # `pphy2phy` maps physical expert on GPU to physical expert.\n    # We need to gather `phyrank` using `pphy2phy`.\n    # `phyrank` is [num_layers * num_nodes, num_physical_experts // num_nodes]\n    # `pphy2phy` is [num_layers * num_nodes, num_physical_experts // num_nodes]\n    \n    # Ensure `pphy2phy` is on the same device as `phyrank`.\n    pphy2phy_device = pphy2phy.device\n    phyrank_device = phyrank.device\n    if pphy2phy_device != phyrank_device:\n        pphy2phy = pphy2phy.to(phyrank_device)\n\n    # Ensure `pphy2phy` is of long type.\n    pphy2phy = pphy2phy.long()\n    \n    # Gather `phyrank` using `pphy2phy`.\n    pphyrank = torch.gather(\n        phyrank,\n        1,  # Gather along dimension 1 (physical expert index within node)\n        pphy2phy\n    )\n    \n    # The `pphyrank` is currently shaped [num_layers * num_nodes, num_physical_experts // num_nodes].\n    # We need it shaped [num_layers, num_physical_experts].\n    # Reshape `pphyrank` to [num_layers, num_nodes, num_physical_experts // num_nodes]\n    # Then flatten to [num_layers, num_physical_experts].\n    pphyrank = pphyrank.view(num_layers, num_nodes, -1).flatten(-2)\n\n    # Finally, compute the expert count for each logical expert.\n    # `logcnt` is the count of replicas for logical experts within a node.\n    # We need the total count of physical replicas for each original logical expert.\n    # `mlogcnt` is [num_layers * num_nodes, num_logical_experts_per_node].\n    # We need to gather `mlogcnt` using `log2mlog`.\n    # `log2mlog` maps grouped logical experts to original logical experts.\n    # `log2mlog` is [num_layers, num_logical_experts]\n    # `mlogcnt` is [num_layers * num_nodes, num_logical_experts_per_node]\n    \n    # This requires careful indexing.\n    # Let's ensure `log2mlog` is on the same device as `mlogcnt`.\n    log2mlog_device = log2mlog.device\n    mlogcnt_device = mlogcnt.device\n    if log2mlog_device != mlogcnt_device:\n        log2mlog = log2mlog.to(mlogcnt_device)\n        \n    # Ensure `log2mlog` is of long type.\n    log2mlog = log2mlog.long()\n    \n    # The `mlogcnt` is structured per node. We need to sum up counts for the same logical expert across nodes.\n    # However, the `replicate_experts` function already handles replication to minimize max load.\n    # The `mlogcnt` returned by `replicate_experts` is the number of replicas for each *logical* expert within its node context.\n    \n    # The original code: `logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)`\n    # `mlogcnt.view(num_layers, -1)` reshapes `mlogcnt` to [num_layers, num_nodes * num_logical_experts_per_node].\n    # This is effectively creating a flat list of logical expert replica counts per layer, across all nodes.\n    # `log2mlog` maps the grouped logical experts to the original logical experts.\n    # So, `gather(-1, log2mlog)` should correctly aggregate the counts.\n    \n    # Let's ensure the dimensions match for gather.\n    # `mlogcnt.view(num_layers, -1)` shape: [num_layers, num_nodes * num_logical_experts_per_node]\n    # `log2mlog` shape: [num_layers, num_logical_experts]\n    \n    # The number of elements in the second dimension of the gathered tensor must match the second dimension of the input tensor.\n    # `num_nodes * num_logical_experts_per_node` is `num_logical_experts`.\n    # So, `mlogcnt.view(num_layers, -1)` has shape [num_layers, num_logical_experts].\n    # `log2mlog` has shape [num_layers, num_logical_experts].\n    \n    # The gather operation should be on the second dimension (index 1).\n    logcnt = torch.gather(\n        mlogcnt.view(num_layers, -1),\n        1,  # Gather along dimension 1\n        log2mlog\n    )\n    \n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    \n    # Store original device and move weight to it if it's not already CPU\n    original_device = weight.device\n    if original_device.type != 'cpu':\n        weight_cpu = weight.float().cpu()\n    else:\n        weight_cpu = weight.float() # Ensure it's float for calculations\n\n    # Decide on the load balancing policy\n    if num_groups > 0 and num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight_cpu, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        # For global policy, we effectively treat all experts as one group.\n        # Setting num_groups to 1, num_nodes to 1 for hierarchical function.\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight_cpu, num_replicas, 1, 1, num_gpus)\n            \n    # Move intermediate results to the original device if it was GPU\n    # This is crucial for performance if subsequent operations depend on GPU.\n    # However, the final output tensors need to be on the correct device.\n    # Let's ensure all returned tensors are on the original device.\n    \n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    \n    # Create log2phy on the same device as logcnt (which is CPU from weight_cpu)\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device, # Should be CPU\n    )\n    \n    # Scatter operation requires indices to be on the same device.\n    # The calculation `phy2log * maxlogcnt + phyrank` should be done on CPU.\n    scatter_indices = phy2log * maxlogcnt + phyrank\n    \n    # The values to scatter should be the physical replica indices.\n    scatter_values = torch.arange(num_replicas, dtype=torch.int64, device=logcnt.device).expand(num_layers, -1)\n    \n    # Perform scatter.\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        scatter_indices.view(num_layers, -1), # Ensure scatter_indices is also flattened correctly\n        scatter_values,\n    )\n\n    # Move results back to the original device if it was not CPU.\n    if original_device.type != 'cpu':\n        phy2log = phy2log.to(original_device)\n        log2phy = log2phy.to(original_device)\n        logcnt = logcnt.to(original_device)\n        \n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n", "language": "python", "parent_id": "109045ba-f864-486a-a759-e4d4eb9df655", "generation": 5, "timestamp": 1751069328.9630558, "iteration_found": 305, "metrics": {"balancedness_score": 0.14515957884623945, "speed_score": 0.020770140952966244, "combined_score": 0.08296485989960285}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 136 lines with 140 lines", "parent_metrics": {"balancedness_score": 0.14515957884623945, "speed_score": 0.02301243653636561, "combined_score": 0.08408600769130253}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve the Mixture-of-Expert models Expert Parallelism Load Balancer (MoE EPLB) expert rearrangement algorithm.\nThis algorithm will take the load metrics recorded by the vLLM server, and rearrange the experts to balance the load. It can make replicas of some experts to achieve better load balancing.\nYour goal will be two-fold: 1. Improve the algorithm to achieve better load balancing; while 2. Improve the algorithm to be more efficient, i.e. reduce the execution time of the algorithm itself, since perfect load balancing is NP-hard.\nThe current algorithm is implemented in the `rebalance_experts` function. ", "user": "# Current Program Information\n- Current performance metrics: - balancedness_score: 0.1452\n- speed_score: 0.0230\n- combined_score: 0.0841\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: balancedness_score, speed_score, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1125, combined_score: 0.2326\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1126, combined_score: 0.2327\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1143, combined_score: 0.2335\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.2335)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1143), Performs well on combined_score (0.2335)\n\n\n### Program 2 (Score: 0.2327)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1126), Performs well on combined_score (0.2327)\n\n\n### Program 3 (Score: 0.2326)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1125), Performs well on combined_score (0.2326)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.2325)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n\n### Program D2 (Score: 0.2322)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.2335, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 32 lines with 17 lines, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 2 (Score: 0.1920, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 3 (Score: 0.0939, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 23 lines with 236 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 4 (Score: 0.0939, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 23 lines with 236 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 5 (Score: 0.0000, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 18 lines with 6 lines\nChange 2: Replace 509 lines with 208 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n\n# Current Program\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    indices = weight.float().sort(-1, descending=True).indices.cpu()\n    pack_index = torch.full_like(weight,\n                                 fill_value=-1,\n                                 dtype=torch.int64,\n                                 device=\"cpu\")\n    rank_in_pack = torch.full_like(pack_index, fill_value=-1)\n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These are on CPU because the input 'weight' is moved to CPU in rebalance_experts.\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id in indices[i]:\n            # Get current state for this layer's packs.\n            current_pack_weights = pack_weights_per_layer[i]\n            current_pack_items = pack_items_per_layer[i]\n\n            # Find available packs (those not yet full).\n            available_mask = (current_pack_items < groups_per_pack)\n\n            # Get weights of available packs\n            available_weights = current_pack_weights[available_mask]\n\n            # Find the pack with the minimum current weight among available ones.\n            # torch.min returns the minimum value and its index.\n            # We need to map the index in available_weights back to the original pack index.\n            min_weight, min_idx_in_available = torch.min(available_weights, dim=-1)\n            \n            # Find the original index of the pack with the minimum weight\n            # This can be done by finding the first occurrence of min_weight in current_pack_weights\n            # that also satisfies the available_mask.\n            # A more direct way is to use torch.where on the masked tensor.\n            # However, to avoid potential issues with multiple packs having the same min weight,\n            # and to ensure we pick one that's actually available, we can do this:\n            \n            # Get indices of available packs\n            available_pack_indices = torch.where(available_mask)[0]\n            \n            # Find the index within the available_pack_indices that corresponds to the minimum weight\n            pack = available_pack_indices[min_idx_in_available].item()\n\n            # Assert that a valid pack was found and it has space.\n            assert current_pack_items[pack] < groups_per_pack\n\n            # Assign the group to the chosen pack.\n            # `group_id` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            # `weight[i, group_id]` is the weight of the current group.\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        # To minimize the maximum load, we should add replicas to the\n        # logical experts with the minimum current average load.\n        # Calculate the average load for each logical expert.\n        average_load = weight / logcnt\n\n        # Find the logical expert with the minimum average load.\n        # `torch.min` returns the minimum value and its index.\n        # We are interested in the index of the expert to replicate.\n        _, min_load_expert_idx = torch.min(average_load, dim=-1)\n\n        # Assign the current physical expert to this logical expert.\n        # `min_load_expert_idx` is a tensor of shape [n], so we can directly assign it.\n        phy2log[:, i] = min_load_expert_idx\n        \n        # Determine the rank for this new replica.\n        # The rank is the current count of replicas for that logical expert.\n        rank[:, i] = logcnt[arangen, min_load_expert_idx]\n        \n        # Increment the replica count for the chosen logical expert.\n        logcnt[arangen, min_load_expert_idx] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # Reshape weights for replication: [num_layers * num_nodes, groups_per_node * group_size]\n    # where groups_per_node * group_size is the number of logical experts per node.\n    # The shape should be [num_layers, num_nodes, num_logical_experts // num_nodes]\n    # Then flatten to [num_layers * num_nodes, num_logical_experts // num_nodes]\n    num_logical_experts_per_node = num_logical_experts // num_nodes\n    tokens_per_mlog_intermediate = weight.gather(-1, mlog2log).view(\n        num_layers, num_nodes, num_logical_experts_per_node)\n    \n    # Flatten for replicate_experts: [num_layers * num_nodes, num_logical_experts_per_node]\n    tokens_per_mlog = tokens_per_mlog_intermediate.view(-1, num_logical_experts_per_node)\n\n    # Replicate experts within each node's logical experts\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Reshape replication results back to [num_layers, num_nodes, num_physical_experts // num_nodes]\n    phy2mlog_reshaped = phy2mlog.view(num_layers, num_nodes, -1)\n    phyrank_reshaped = phyrank.view(num_layers, num_nodes, -1)\n    mlogcnt_reshaped = mlogcnt.view(num_layers, num_nodes, -1)\n\n    # Step 3: pack physical_experts to GPUs\n    # Calculate the average load for physical experts within each node\n    # `mlogcnt_reshaped` has shape [num_layers, num_nodes, num_logical_experts_per_node]\n    # `phy2mlog_reshaped` maps physical experts to logical experts within nodes.\n    # We need to get the correct mlogcnt for the replicated logical experts.\n    # The average load for each physical expert is `tokens_per_mlog / mlogcnt` for the corresponding logical expert.\n    \n    # Get the count of replicas for each logical expert *within a node*\n    # We need to align mlogcnt with phy2mlog.\n    # `phy2mlog` is [num_layers * num_nodes, num_logical_experts // num_nodes]\n    # `mlogcnt` is [num_layers * num_nodes, num_logical_experts // num_nodes]\n    # `tokens_per_mlog` is [num_layers * num_nodes, num_logical_experts // num_nodes]\n    \n    # The average load for each *physical* expert is `tokens_per_mlog / mlogcnt` after mapping.\n    # We need to use the correct mlogcnt for the physical expert.\n    # The `phyrank` tells us the rank of the physical expert within its logical expert group.\n    # The `mlogcnt` should be aligned with `phy2mlog`.\n    \n    # Let's re-evaluate `tokens_per_phy`:\n    # `tokens_per_mlog`: load of logical experts within a node.\n    # `mlogcnt`: count of replicas for each logical expert within a node.\n    # `phy2mlog`: maps physical experts (within a node) to logical experts (within a node).\n    \n    # To get the load of a physical expert, we need to find which logical expert it maps to,\n    # and then use the load of that logical expert divided by its replica count.\n    # `tokens_per_mlog` is already structured as [num_layers*num_nodes, num_logical_experts_per_node]\n    # `mlogcnt` is also structured similarly.\n    # `phy2mlog` maps physical expert index to logical expert index.\n    \n    # The average load of a physical expert is `tokens_per_mlog[p_idx] / mlogcnt[p_idx]` IF\n    # `p_idx` corresponds to the same logical expert. This is not directly what we have.\n    \n    # Let's use the reshaped tensors for clarity:\n    # `tokens_per_mlog_intermediate` [num_layers, num_nodes, num_logical_experts_per_node]\n    # `mlogcnt_reshaped` [num_layers, num_nodes, num_logical_experts_per_node]\n    # `phy2mlog_reshaped` [num_layers, num_nodes, num_physical_experts // num_nodes]\n    \n    # We need to get the load of the logical expert that each physical expert maps to.\n    # For a physical expert at `[l, n_idx, p_idx]`, it maps to logical expert `phy2mlog_reshaped[l, n_idx, p_idx]`.\n    # The load of this logical expert is `tokens_per_mlog_intermediate[l, n_idx, phy2mlog_reshaped[l, n_idx, p_idx]]`.\n    # The count of replicas for this logical expert is `mlogcnt_reshaped[l, n_idx, phy2mlog_reshaped[l, n_idx, p_idx]]`.\n    \n    # This requires careful indexing. A more direct approach might be to flatten first.\n    \n    # Re-flattening `tokens_per_mlog` and `mlogcnt` for consistency with `phy2mlog`.\n    # `phy2mlog` is [num_layers * num_nodes, num_logical_experts // num_nodes]\n    # `tokens_per_mlog` is [num_layers * num_nodes, num_logical_experts // num_nodes]\n    # `mlogcnt` is [num_layers * num_nodes, num_logical_experts // num_nodes]\n    \n    # The average load for each physical expert is:\n    # first, find the logical expert index for each physical expert: `phy2mlog`\n    # then, get the load of that logical expert: `tokens_per_mlog[idx, logical_expert_idx]`\n    # then, get the count of replicas for that logical expert: `mlogcnt[idx, logical_expert_idx]`\n    \n    # This can be achieved by using `gather` on `tokens_per_mlog` and `mlogcnt` using `phy2mlog`.\n    # This assumes `tokens_per_mlog` and `mlogcnt` are structured to be directly indexed by `phy2mlog`.\n    # Let's assume `tokens_per_mlog` and `mlogcnt` are already aligned to `phy2mlog`'s structure.\n    # The `replicate_experts` function returns `phy2mlog` and `mlogcnt` of the same shape as its input `weight`.\n    # So `tokens_per_mlog` and `mlogcnt` are [num_layers * num_nodes, num_logical_experts_per_node].\n    # `phy2mlog` is [num_layers * num_nodes, num_physical_experts // num_nodes].\n    \n    # To get the load of the logical expert corresponding to each physical expert:\n    # We need to index into `tokens_per_mlog` using `phy2mlog`.\n    # `tokens_per_mlog` has shape [N_total, N_log_per_node], where N_total = num_layers * num_nodes.\n    # `phy2mlog` has shape [N_total, N_phy_per_node].\n    \n    # This requires an advanced indexing operation.\n    # A simpler way to think about `tokens_per_phy`:\n    # It's the load of the physical expert. A physical expert is a replica of a logical expert.\n    # The load of a logical expert is `weight / logcnt`.\n    # So, the load of a physical expert should be `(weight / logcnt) mapped to the physical expert's logical expert`.\n    \n    # Let's use `mlogcnt_reshaped` and `phy2mlog_reshaped` to guide the calculation.\n    # We need the average load for each physical expert.\n    # The average load of a logical expert `le` is `AvgLoad(le) = tokens_per_mlog[le] / mlogcnt[le]`.\n    # A physical expert `pe` maps to logical expert `le_pe = phy2mlog[pe]`.\n    # So, `AvgLoad(pe) = AvgLoad(le_pe)`.\n    \n    # We need to gather `tokens_per_mlog` and `mlogcnt` using `phy2mlog`.\n    # `tokens_per_mlog` is [num_layers * num_nodes, num_logical_experts_per_node]\n    # `mlogcnt` is [num_layers * num_nodes, num_logical_experts_per_node]\n    # `phy2mlog` is [num_layers * num_nodes, num_physical_experts // num_nodes]\n    \n    # Using advanced indexing:\n    # Create indices for `tokens_per_mlog` and `mlogcnt` to align with `phy2mlog`.\n    # This requires careful handling of batch dimensions.\n    \n    # Let's simplify the structure and ensure correct dimensions:\n    # `phy2mlog` is [num_layers * num_nodes, num_logical_experts // num_nodes]\n    # `mlogcnt` is [num_layers * num_nodes, num_logical_experts // num_nodes]\n    # `tokens_per_mlog` is [num_layers * num_nodes, num_logical_experts // num_nodes]\n    \n    # The number of physical experts per GPU is `phy_experts_per_gpu`.\n    # The number of GPUs per node is `num_gpus // num_nodes`.\n    # The total number of physical experts is `num_replicas`.\n    # The number of physical experts per node is `num_replicas // num_nodes`.\n    \n    # `phy2mlog` is structured per node.\n    # `tokens_per_mlog` is structured per node.\n    # `mlogcnt` is structured per node.\n    \n    # The output of `replicate_experts` is `phy2mlog, phyrank, mlogcnt`.\n    # `phy2mlog` maps physical experts to logical experts (within a node).\n    # `mlogcnt` is the count of replicas for the logical experts.\n    \n    # We want the load of the *physical* experts.\n    # A physical expert `pe` is a replica of a logical expert `le`.\n    # The load of `le` is `tokens_per_mlog[le] / mlogcnt[le]`.\n    # So, the load of `pe` is `tokens_per_mlog[phy2mlog[pe]] / mlogcnt[phy2mlog[pe]]`.\n    \n    # This requires advanced indexing.\n    # Let's create indices for `tokens_per_mlog` and `mlogcnt` that match `phy2mlog`.\n    \n    # `phy2mlog` shape: [num_layers * num_nodes, num_physical_experts_per_node]\n    # `tokens_per_mlog` shape: [num_layers * num_nodes, num_logical_experts_per_node]\n    # `mlogcnt` shape: [num_layers * num_nodes, num_logical_experts_per_node]\n    \n    # The indices for `tokens_per_mlog` and `mlogcnt` should be the values in `phy2mlog`.\n    # The first dimension of `phy2mlog` corresponds to the first dimension of `tokens_per_mlog` and `mlogcnt`.\n    # The second dimension of `phy2mlog` needs to be indexed into the second dimension of `tokens_per_mlog` and `mlogcnt`.\n    \n    # This can be done with `torch.gather` or by carefully constructing indices.\n    \n    # Let's re-evaluate the calculation of `tokens_per_phy`:\n    # `tokens_per_mlog` is the load for logical experts within a node.\n    # `mlogcnt` is the count of replicas for logical experts within a node.\n    # `phy2mlog` maps physical experts to logical experts within a node.\n    \n    # For each physical expert, we need its average load.\n    # This is the average load of the logical expert it belongs to.\n    # We can compute `avg_load_per_logical_expert = tokens_per_mlog / mlogcnt`.\n    # Then, for each physical expert, we find its logical expert `le_idx = phy2mlog[pe_idx]`.\n    # The load of physical expert `pe_idx` is `avg_load_per_logical_expert[le_idx]`.\n    \n    # This can be done by creating a tensor of indices for `avg_load_per_logical_expert`.\n    # Let `logical_expert_indices_for_physical_experts = phy2mlog`.\n    # Then `tokens_per_phy = torch.gather(avg_load_per_logical_expert, 1, logical_expert_indices_for_physical_experts)`.\n    # This requires `avg_load_per_logical_expert` to be [num_layers * num_nodes, num_logical_experts_per_node].\n    \n    # Let's ensure dimensions are correct.\n    # `tokens_per_mlog` is [num_layers * num_nodes, num_logical_experts_per_node]\n    # `mlogcnt` is [num_layers * num_nodes, num_logical_experts_per_node]\n    # `phy2mlog` is [num_layers * num_nodes, num_physical_experts // num_nodes]\n    \n    # Calculate average load per logical expert within each node.\n    avg_load_per_logical_expert = tokens_per_mlog / mlogcnt\n    \n    # Now, for each physical expert, find the average load of its corresponding logical expert.\n    # We need to gather `avg_load_per_logical_expert` using `phy2mlog`.\n    # The indices for `gather` should be `phy2mlog`.\n    # `avg_load_per_logical_expert` has shape [num_layers * num_nodes, num_logical_experts_per_node]\n    # `phy2mlog` has shape [num_layers * num_nodes, num_physical_experts // num_nodes]\n    \n    # `torch.gather` requires the index tensor to have the same number of dimensions as the input tensor.\n    # It also requires the size of the dimension along which to gather to match.\n    # This means we need to expand `phy2mlog` to match the dimensions of `avg_load_per_logical_expert`.\n    \n    # Alternatively, we can use advanced indexing directly if `phy2mlog`'s values are valid indices for `avg_load_per_logical_expert`.\n    # Let's use `torch.gather` on the second dimension (index 1).\n    \n    # Ensure `phy2mlog` is on the same device as `avg_load_per_logical_expert`.\n    phy2mlog_device = phy2mlog.device\n    avg_load_per_logical_expert_device = avg_load_per_logical_expert.device\n    \n    # If `phy2mlog` is not on the correct device, move it.\n    if phy2mlog_device != avg_load_per_logical_expert_device:\n        phy2mlog = phy2mlog.to(avg_load_per_logical_expert_device)\n        \n    # The indices for `gather` must be of `torch.long` type.\n    phy2mlog = phy2mlog.long()\n\n    # Calculate the load for each physical expert.\n    tokens_per_phy = torch.gather(\n        avg_load_per_logical_expert,\n        1,  # Gather along dimension 1 (the logical expert dimension)\n        phy2mlog\n    )\n\n    # Now pack these physical experts onto GPUs.\n    # `tokens_per_phy` is [num_layers * num_nodes, num_physical_experts // num_nodes]\n    # `num_gpus // num_nodes` is the number of GPUs available on each node.\n    pack_index, rank_in_pack = balanced_packing(\n        tokens_per_phy, num_gpus // num_nodes)\n    \n    # Map physical experts to physical experts on GPUs.\n    # `phy2pphy` is the mapping from physical expert index to physical expert index on GPU.\n    # The shape is [num_layers * num_nodes, num_physical_experts // num_nodes]\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    \n    # Compute the inverse mapping: physical expert on GPU to physical expert.\n    pphy2phy = inverse(phy2pphy)\n\n    # Now map physical experts on GPUs back to logical experts.\n    # `pphy2phy` has shape [num_layers * num_nodes, num_physical_experts // num_nodes]\n    # We need to gather `phy2mlog` using `pphy2phy` to get the logical expert mapping for each physical expert on GPU.\n    # `phy2mlog` is [num_layers * num_nodes, num_logical_experts_per_node]\n    # `pphy2phy` is [num_layers * num_nodes, num_physical_experts // num_nodes]\n    \n    # We need to gather `phy2mlog` using `pphy2phy`.\n    # The first dimension of `pphy2phy` corresponds to the first dimension of `phy2mlog`.\n    # The second dimension of `pphy2phy` needs to index into the second dimension of `phy2mlog`.\n    \n    # Ensure `pphy2phy` is on the same device as `phy2mlog`.\n    pphy2phy_device = pphy2phy.device\n    phy2mlog_device = phy2mlog.device\n    if pphy2phy_device != phy2mlog_device:\n        pphy2phy = pphy2phy.to(phy2mlog_device)\n        \n    # Ensure `pphy2phy` is of long type for gather.\n    pphy2phy = pphy2phy.long()\n    \n    # Gather `phy2mlog` using `pphy2phy` to get the logical expert index for each physical expert on GPU.\n    pphy2mlog = torch.gather(\n        phy2mlog,\n        1,  # Gather along dimension 1 (physical expert index within node)\n        pphy2phy\n    )\n    \n    # The `pphy2mlog` now contains logical expert indices within each node's context.\n    # We need to convert these to global logical expert indices.\n    # The offset for each node is `num_logical_experts // num_nodes * node_idx`.\n    # A simpler way is to add the start index of logical experts for each node.\n    # The logical experts per node are `num_logical_experts // num_nodes`.\n    # The starting index for logical experts for node `n_idx` is `n_idx * (num_logical_experts // num_nodes)`.\n    \n    # The `pphy2mlog` is already structured as [num_layers, num_nodes, num_physical_experts_per_gpu].\n    # The original code's logic was:\n    # `pphy2mlog.view(num_layers, num_nodes, -1)` reshapes it correctly.\n    # `+ torch.arange(0, num_logical_experts, num_logical_experts // num_nodes, device=group_pack_index.device).view(1, -1, 1)` adds the node offsets.\n    # This `arange` creates the starting indices for logical experts for each node.\n    # `view(1, -1, 1)` makes it broadcastable: [1, num_nodes, 1].\n    \n    node_offsets = torch.arange(0, num_logical_experts, num_logical_experts // num_nodes, device=group_pack_index.device).view(1, -1, 1)\n    pphy2mlog_global = pphy2mlog.view(num_layers, num_nodes, -1) + node_offsets\n    \n    # Flatten back to the required shape for the final mapping.\n    pphy2mlog_final = pphy2mlog_global.flatten(-2) # [num_layers, num_physical_experts]\n\n    # Now, map these global logical expert indices back to the original logical experts.\n    # `pphy2log` is the final mapping from physical experts (on GPUs) to logical experts.\n    # This is essentially the `mlog2log` mapping applied to the `pphy2mlog_final`.\n    # `mlog2log` maps grouped logical experts to original logical experts.\n    # `pphy2mlog_final` gives us the logical expert indices (in the grouped sense) for each physical expert.\n    # We need to gather `mlog2log` using `pphy2mlog_final`.\n    \n    # `mlog2log` is [num_layers, num_logical_experts // num_nodes]\n    # `pphy2mlog_final` is [num_layers, num_physical_experts]\n    \n    # This requires advanced indexing.\n    # Let's ensure `pphy2mlog_final` is on the same device as `mlog2log`.\n    pphy2mlog_final_device = pphy2mlog_final.device\n    mlog2log_device = mlog2log.device\n    if pphy2mlog_final_device != mlog2log_device:\n        pphy2mlog_final = pphy2mlog_final.to(mlog2log_device)\n        \n    # Ensure `pphy2mlog_final` is of long type.\n    pphy2mlog_final = pphy2mlog_final.long()\n\n    # Gather `mlog2log` using `pphy2mlog_final`.\n    # This will give the original logical expert index for each physical expert on GPU.\n    pphy2log = torch.gather(\n        mlog2log,\n        1,  # Gather along dimension 1 (the grouped logical expert index)\n        pphy2mlog_final\n    )\n    \n    # Now, compute the rank for each physical expert on GPU.\n    # `phyrank` is the rank of the physical expert within its logical expert group (within a node).\n    # `pphy2phy` maps physical expert on GPU to physical expert.\n    # We need to gather `phyrank` using `pphy2phy`.\n    # `phyrank` is [num_layers * num_nodes, num_physical_experts // num_nodes]\n    # `pphy2phy` is [num_layers * num_nodes, num_physical_experts // num_nodes]\n    \n    # Ensure `pphy2phy` is on the same device as `phyrank`.\n    pphy2phy_device = pphy2phy.device\n    phyrank_device = phyrank.device\n    if pphy2phy_device != phyrank_device:\n        pphy2phy = pphy2phy.to(phyrank_device)\n\n    # Ensure `pphy2phy` is of long type.\n    pphy2phy = pphy2phy.long()\n    \n    # Gather `phyrank` using `pphy2phy`.\n    pphyrank = torch.gather(\n        phyrank,\n        1,  # Gather along dimension 1 (physical expert index within node)\n        pphy2phy\n    )\n    \n    # The `pphyrank` is currently shaped [num_layers * num_nodes, num_physical_experts // num_nodes].\n    # We need it shaped [num_layers, num_physical_experts].\n    # Reshape `pphyrank` to [num_layers, num_nodes, num_physical_experts // num_nodes]\n    # Then flatten to [num_layers, num_physical_experts].\n    pphyrank = pphyrank.view(num_layers, num_nodes, -1).flatten(-2)\n\n    # Finally, compute the expert count for each logical expert.\n    # `logcnt` is the count of replicas for logical experts within a node.\n    # We need the total count of physical replicas for each original logical expert.\n    # `mlogcnt` is [num_layers * num_nodes, num_logical_experts_per_node].\n    # We need to gather `mlogcnt` using `log2mlog`.\n    # `log2mlog` maps grouped logical experts to original logical experts.\n    # `log2mlog` is [num_layers, num_logical_experts]\n    # `mlogcnt` is [num_layers * num_nodes, num_logical_experts_per_node]\n    \n    # This requires careful indexing.\n    # Let's ensure `log2mlog` is on the same device as `mlogcnt`.\n    log2mlog_device = log2mlog.device\n    mlogcnt_device = mlogcnt.device\n    if log2mlog_device != mlogcnt_device:\n        log2mlog = log2mlog.to(mlogcnt_device)\n        \n    # Ensure `log2mlog` is of long type.\n    log2mlog = log2mlog.long()\n    \n    # The `mlogcnt` is structured per node. We need to sum up counts for the same logical expert across nodes.\n    # However, the `replicate_experts` function already handles replication to minimize max load.\n    # The `mlogcnt` returned by `replicate_experts` is the number of replicas for each *logical* expert within its node context.\n    \n    # The original code: `logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)`\n    # `mlogcnt.view(num_layers, -1)` reshapes `mlogcnt` to [num_layers, num_nodes * num_logical_experts_per_node].\n    # This is effectively creating a flat list of logical expert replica counts per layer, across all nodes.\n    # `log2mlog` maps the grouped logical experts to the original logical experts.\n    # So, `gather(-1, log2mlog)` should correctly aggregate the counts.\n    \n    # Let's ensure the dimensions match for gather.\n    # `mlogcnt.view(num_layers, -1)` shape: [num_layers, num_nodes * num_logical_experts_per_node]\n    # `log2mlog` shape: [num_layers, num_logical_experts]\n    \n    # The number of elements in the second dimension of the gathered tensor must match the second dimension of the input tensor.\n    # `num_nodes * num_logical_experts_per_node` is `num_logical_experts`.\n    # So, `mlogcnt.view(num_layers, -1)` has shape [num_layers, num_logical_experts].\n    # `log2mlog` has shape [num_layers, num_logical_experts].\n    \n    # The gather operation should be on the second dimension (index 1).\n    logcnt = torch.gather(\n        mlogcnt.view(num_layers, -1),\n        1,  # Gather along dimension 1\n        log2mlog\n    )\n    \n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    \n    # Store original device and move weight to it if it's not already CPU\n    original_device = weight.device\n    if original_device.type != 'cpu':\n        weight_cpu = weight.float().cpu()\n    else:\n        weight_cpu = weight.float() # Ensure it's float for calculations\n\n    # Decide on the load balancing policy\n    if num_groups > 0 and num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight_cpu, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        # For global policy, we effectively treat all experts as one group.\n        # Setting num_groups to 1, num_nodes to 1 for hierarchical function.\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight_cpu, num_replicas, 1, 1, num_gpus)\n            \n    # Move intermediate results to the original device if it was GPU\n    # This is crucial for performance if subsequent operations depend on GPU.\n    # However, the final output tensors need to be on the correct device.\n    # Let's ensure all returned tensors are on the original device.\n    \n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    \n    # Create log2phy on the same device as logcnt (which is CPU from weight_cpu)\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device, # Should be CPU\n    )\n    \n    # Scatter operation requires indices to be on the same device.\n    # The calculation `phy2log * maxlogcnt + phyrank` should be done on CPU.\n    scatter_indices = phy2log * maxlogcnt + phyrank\n    \n    # The values to scatter should be the physical replica indices.\n    scatter_values = torch.arange(num_replicas, dtype=torch.int64, device=logcnt.device).expand(num_layers, -1)\n    \n    # Perform scatter.\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        scatter_indices.view(num_layers, -1), # Ensure scatter_indices is also flattened correctly\n        scatter_values,\n    )\n\n    # Move results back to the original device if it was not CPU.\n    if original_device.type != 'cpu':\n        phy2log = phy2log.to(original_device)\n        log2phy = log2phy.to(original_device)\n        logcnt = logcnt.to(original_device)\n        \n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["The current program's performance, especially `speed_score` and `balancedness_score`, shows a significant regression compared to previous attempts. This indicates that recent changes might have introduced inefficiencies or suboptimal algorithms. The core rearrangement algorithm is adapted from DeepSeek EPLB, and the `rebalance_experts` function, particularly its sub-functions `balanced_packing` and `replicate_experts`, are critical for performance.\n\nUpon analysis, the most impactful area for improvement lies within the `replicate_experts` function. This function iteratively selects the logical expert with the minimum average load to replicate. The current implementation recalculates `average_load` and performs a `torch.min` operation over potentially many logical experts in each iteration of a loop. If `num_logical_experts` and `num_redundant` experts are large, this leads to an `O(num_redundant * num_layers * num_logical_experts)` complexity, which can be very slow.\n\n**Proposed Improvement:**\n\nTo significantly improve the `speed_score` of `replicate_experts` without compromising `balancedness_score` (as the greedy strategy remains the same), we can replace the repeated `torch.min` search with a min-priority queue (min-heap). This allows us to retrieve the minimum average load expert in `O(log(num_logical_experts))` time per replication step, instead of `O(num_logical_experts)`.\n\nThe new complexity for `replicate_experts` would be `O(num_layers * num_logical_experts)` for initial heap construction, plus `O(num_redundant * num_layers * log(num_logical_experts))` for the replication loop. This is a substantial improvement, especially when `num_logical_experts` is much larger than `log(num_logical_experts)`. Since `weight` is moved to CPU within `rebalance_experts`, Python's `heapq` module is suitable for this purpose.\n\n**Minor Improvement:**\n\nA small readability and potentially marginal speed improvement can be made in `balanced_packing` by using `torch.nonzero` more directly instead of an intermediate `torch.where` result.\n\nHere are the specific changes:\n\n1.  **Add `import heapq`:** Necessary for the priority queue implementation.\n2.  **Optimize `replicate_experts`:**\n    *   Initialize `phy2log` to correctly map the initial `num_logical_experts` to themselves.\n    *   Create a list of min-heaps, one for each layer. Each heap will store tuples of `(average_load, logical_expert_idx)`.\n    *   In the replication loop, for each layer:\n        *   Pop the expert with the minimum average load from its heap.\n        *   Assign the current physical expert slot to this logical expert.\n        *   Update the `logcnt` for the chosen logical expert.\n        *   Calculate the new average load for the updated logical expert and push it back into the heap.\n3.  **Optimize `balanced_packing` (minor):** Refactor the lookup of `pack` index using `torch.nonzero` directly.\n\nThese changes are expected to significantly boost `speed_score` while maintaining or improving `balancedness_score` due to the more efficient, but logically equivalent, greedy selection process.\n\n```diff\n<<<<<<< SEARCH\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    indices = weight.float().sort(-1, descending=True).indices.cpu()\n    pack_index = torch.full_like(weight,\n                                 fill_value=-1,\n                                 dtype=torch.int64,\n                                 device=\"cpu\")\n    rank_in_pack = torch.full_like(pack_index, fill_value=-1)\n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These are on CPU because the input 'weight' is moved to CPU in rebalance_experts.\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id in indices[i]:\n            # Get current state for this layer's packs.\n            current_pack_weights = pack_weights_per_layer[i]\n            current_pack_items = pack_items_per_layer[i]\n\n            # Find available packs (those not yet full).\n            available_mask = (current_pack_items < groups_per_pack)\n\n            # Get weights of available packs\n            available_weights = current_pack_weights[available_mask]\n\n            # Find the pack with the minimum current weight among available ones.\n            # torch.min returns the minimum value and its index.\n            # We need to map the index in available_weights back to the original pack index.\n            min_weight, min_idx_in_available = torch.min(available_weights, dim=-1)\n            \n            # Find the original index of the pack with the minimum weight\n            # This can be done by finding the first occurrence of min_weight in current_pack_weights\n            # that also satisfies the available_mask.\n            # A more direct way is to use torch.where on the masked tensor.\n            # However, to avoid potential issues with multiple packs having the same min weight,\n            # and to ensure we pick one that's actually available, we can do this:\n            \n            # Get indices of available packs\n            available_pack_indices = torch.where(available_mask)[0]\n            \n            # Find the index within the available_pack_indices that corresponds to the minimum weight\n            pack = available_pack_indices[min_idx_in_available].item()\n\n            # Assert that a valid pack was found and it has space.\n            assert current_pack_items[pack] < groups_per_pack\n\n            # Assign the group to the chosen pack.\n            # `group_id` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            # `weight[i, group_id]` is the weight of the current group.\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        # To minimize the maximum load, we should add replicas to the\n        # logical experts with the minimum current average load.\n        # Calculate the average load for each logical expert.\n        average_load = weight / logcnt\n\n        # Find the logical expert with the minimum average load.\n        # `torch.min` returns the minimum value and its index.\n        # We are interested in the index of the expert to replicate.\n        _, min_load_expert_idx = torch.min(average_load, dim=-1)\n\n        # Assign the current physical expert to this logical expert.\n        # `min_load_expert_idx` is a tensor of shape [n], so we can directly assign it.\n        phy2log[:, i] = min_load_expert_idx\n        \n        # Determine the rank for this new replica.\n        # The rank is the current count of replicas for that logical expert.\n        rank[:, i] = logcnt[arangen, min_load_expert_idx]\n        \n        # Increment the replica count for the chosen logical expert.\n        logcnt[arangen, min_load_expert_idx] += 1\n    return phy2log, rank, logcnt\n=======\nimport torch\nimport heapq # Added for priority queue optimization in replicate_experts\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    indices = weight.float().sort(-1, descending=True).indices.cpu()\n    pack_index = torch.full_like(weight,\n                                 fill_value=-1,\n                                 dtype=torch.int64,\n                                 device=\"cpu\")\n    rank_in_pack = torch.full_like(pack_index, fill_value=-1)\n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These are on CPU because the input 'weight' is moved to CPU in rebalance_experts.\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id in indices[i]:\n            # Get current state for this layer's packs.\n            current_pack_weights = pack_weights_per_layer[i]\n            current_pack_items = pack_items_per_layer[i]\n\n            # Find available packs (those not yet full).\n            available_mask = (current_pack_items < groups_per_pack)\n\n            # Get weights of available packs\n            available_weights = current_pack_weights[available_mask]\n\n            # Find the pack with the minimum current weight among available ones.\n            # torch.min returns the minimum value and its index.\n            # We need to map the index in available_weights back to the original pack index.\n            min_weight, min_idx_in_available = torch.min(available_weights, dim=-1)\n            \n            # Find the original index of the pack with the minimum weight\n            # Optimized: Use torch.nonzero directly to get original index\n            pack = torch.nonzero(available_mask, as_tuple=True)[0][min_idx_in_available].item()\n\n            # Assert that a valid pack was found and it has space.\n            assert current_pack_items[pack] < groups_per_pack\n\n            # Assign the group to the chosen pack.\n            # `group_id` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            # `weight[i, group_id]` is the weight of the current group.\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    \n    # Initialize phy2log and rank\n    phy2log = torch.empty(n, num_phy, dtype=torch.int64, device=device)\n    # The first num_log physical experts map to the first num_log logical experts\n    phy2log[:, :num_log] = torch.arange(num_log, dtype=torch.int64, device=device).expand(n, num_log)\n    \n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    \n    # Initialize min-heaps for each layer\n    # Heap stores (average_load, logical_expert_idx)\n    heaps = []\n    for i in range(n):\n        current_layer_heap = []\n        for j in range(num_log):\n            heapq.heappush(current_layer_heap, (weight[i, j].item() / logcnt[i, j].item(), j))\n        heaps.append(current_layer_heap)\n\n    # Replicate experts for the redundant slots\n    for i_phy in range(num_log, num_phy): # Iterate over physical expert slots to fill\n        for layer_idx in range(n): # Iterate over each layer\n            # Pop the expert with the minimum average load for the current layer\n            min_load, min_load_expert_idx = heapq.heappop(heaps[layer_idx])\n\n            # Assign the current physical expert slot (i_phy) to this logical expert\n            phy2log[layer_idx, i_phy] = min_load_expert_idx\n            \n            # Determine the rank for this new replica.\n            # The rank is the current count of replicas for that logical expert.\n            rank[layer_idx, i_phy] = logcnt[layer_idx, min_load_expert_idx]\n            \n            # Increment the replica count for the chosen logical expert.\n            logcnt[layer_idx, min_load_expert_idx] += 1\n            \n            # Calculate new average load for the replicated expert and push back to heap\n            new_average_load = weight[layer_idx, min_load_expert_idx].item() / logcnt[layer_idx, min_load_expert_idx].item()\n            heapq.heappush(heaps[layer_idx], (new_average_load, min_load_expert_idx))\n            \n    return phy2log, rank, logcnt\n>>>>>>> REPLACE\n```"]}}}