{"id": "ada74b29-e327-41f1-9ad1-3a4e23dc588a", "code": "# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains as close\n    to n/m objects as possible and the weights of all packs are as balanced\n    as possible. Handles cases where n is not divisible by m.\n\n    Parameters:\n        weight: [X, n], the weight of each item (e.g., [num_layers, num_groups])\n        num_packs: number of packs (e.g., num_nodes or num_gpus)\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item within its assigned pack\n    \"\"\"\n    num_layers, num_items = weight.shape\n    device = weight.device\n\n    # Handle trivial case: if num_packs is 1, all items go to pack 0.\n    if num_packs == 1:\n        pack_index = torch.zeros_like(weight, dtype=torch.int64)\n        rank_in_pack = torch.arange(num_items, dtype=torch.int64, device=device).expand_as(weight)\n        return pack_index, rank_in_pack\n\n    # Handle trivial case: if num_items <= num_packs, assign each item to a unique pack.\n    # This might not be strictly \"balanced\" in terms of load, but it distributes items.\n    # For load balancing, we still need to sort.\n    # The original code's `groups_per_pack == 1` check is for when `num_items == num_packs`.\n    # If `num_items < num_packs`, `groups_per_pack` would be 0.\n\n    # Calculate target pack sizes for non-divisible cases\n    base_items_per_pack = num_items // num_packs\n    extra_items = num_items % num_packs\n\n    # Create a tensor for target capacities of each pack for the current layer\n    target_pack_capacities = torch.full((num_packs,), base_items_per_pack, dtype=torch.int64, device=device)\n    if extra_items > 0:\n        target_pack_capacities[:extra_items] = base_items_per_pack + 1\n\n    # Sort weights and get indices for each layer\n    sorted_weights, sorted_indices = weight.float().sort(-1, descending=True)\n\n    # Initialize pack_index and rank_in_pack on the same device as weight\n    pack_index = torch.full((num_layers, num_items), -1, dtype=torch.int64, device=device)\n    rank_in_pack = torch.full((num_layers, num_items), -1, dtype=torch.int64, device=device)\n\n    # Initialize pack_loads and pack_counts on the same device as weight\n    pack_loads = torch.zeros(num_layers, num_packs, dtype=torch.float32, device=device)\n    pack_counts = torch.zeros(num_layers, num_packs, dtype=torch.int64, device=device)\n\n    # Iterate through sorted items and assign them to the least loaded pack that is not yet full.\n    # This loop is inherently sequential for each layer (i) and item (g).\n    # Optimizing this further would require more complex dynamic programming or approximation algorithms.\n    # The key is to avoid Python loops for the inner-most assignment if possible, but the greedy nature makes it hard.\n    # The current implementation is a standard greedy approach for bin packing.\n    for i in range(num_layers):\n        # Reset pack_loads and pack_counts for each layer\n        pack_loads[i].zero_()\n        pack_counts[i].zero_()\n        \n        # Use the dynamically calculated target capacities for this layer\n        current_layer_target_capacities = target_pack_capacities.clone()\n\n        for g in range(num_items):\n            # Find the pack with the minimum current load that has available capacity.\n            # `available_packs_mask` identifies packs that have not reached their target capacity.\n            available_packs_mask = (pack_counts[i] < current_layer_target_capacities)\n\n            # If no packs are available (all are full), this indicates an issue with capacity calculation or distribution logic.\n            # However, with dynamic capacities, this should not happen if num_items >= num_packs.\n            # If num_items < num_packs, some packs might remain empty, which is fine.\n\n            # Use `torch.where` to set loads of unavailable packs to infinity.\n            # This ensures `argmin` picks from the available (not full) packs.\n            current_pack_loads = torch.where(available_packs_mask, pack_loads[i], float('inf'))\n            \n            # Find the index of the least loaded available pack.\n            chosen_pack_idx = torch.argmin(current_pack_loads)\n\n            # Get the original index and weight of the item being assigned.\n            original_item_idx = sorted_indices[i, g]\n            original_item_weight = weight[i, original_item_idx]\n\n            # Assign the item to the chosen pack and record its rank within that pack.\n            pack_index[i, original_item_idx] = chosen_pack_idx\n            rank_in_pack[i, original_item_idx] = pack_counts[i, chosen_pack_idx]\n\n            # Update the load and count for the chosen pack.\n            pack_loads[i, chosen_pack_idx] += original_item_weight\n            pack_counts[i, chosen_pack_idx] += 1\n\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Unified rebalancing logic:\n    # The hierarchical approach is used when num_groups is divisible by num_nodes.\n    # Otherwise, it falls back to a global balancing strategy by setting num_nodes=1 and num_groups=1.\n    # We can simplify this by directly implementing the global strategy when needed,\n    # or by ensuring the hierarchical logic correctly handles num_nodes=1.\n\n    # Determine the effective number of groups and nodes for balancing.\n    # If num_groups is not divisible by num_nodes, we treat it as a single group\n    # across all nodes for the first level of balancing.\n    effective_num_groups = num_groups if num_groups % num_nodes == 0 else 1\n    effective_num_nodes = num_nodes if num_groups % num_nodes == 0 else 1\n\n    # If num_groups % num_nodes != 0, we effectively want to balance across all groups as if they were one large group.\n    # And then balance those groups onto nodes.\n    # This is equivalent to setting num_nodes = 1 and num_groups = 1 in the hierarchical logic.\n    # Let's adjust parameters for a unified call to a simplified hierarchical logic.\n\n    # Simplified approach:\n    # First, balance logical experts across groups within layers.\n    # Then, balance these groups across nodes.\n    # Finally, replicate experts within nodes to balance across GPUs.\n\n    # Step 1: Balance logical experts into groups.\n    # If num_groups == 1, this step is trivial and doesn't change anything.\n    group_size = num_logical_experts // num_groups\n    tokens_per_group_layer = weight.unflatten(-1, (num_groups, group_size)).sum(-1) # [num_layers, num_groups]\n\n    # Pack groups onto nodes.\n    # If num_groups % num_nodes != 0, we effectively want to balance all groups onto a single 'node' conceptually,\n    # and then distribute those across the actual num_nodes.\n    # Let's consider the case where num_groups is not divisible by num_nodes.\n    # The original code uses num_nodes for packing groups. If num_groups < num_nodes, this packing might be problematic.\n    # A more robust approach is to balance groups onto num_nodes, and if num_groups < num_nodes, some nodes get 0 groups.\n    # The current `balanced_packing` assumes num_groups is divisible by num_packs (num_nodes here).\n\n    # Let's redefine the packing strategy:\n    # We want to pack 'num_groups' items (groups) into 'num_nodes' packs.\n    # If num_groups is not divisible by num_nodes, balanced_packing needs to handle it.\n    # The current `balanced_packing` asserts `num_groups % num_packs == 0`.\n    # This means if num_groups % num_nodes != 0, the original code calls `rebalance_experts_hierarchical` with num_nodes=1, num_groups=1.\n    # This is a global balancing strategy.\n\n    # Let's ensure the function call correctly reflects the intended strategy.\n    # If num_groups % num_nodes == 0, use hierarchical.\n    # Otherwise, use global (which is hierarchical with num_nodes=1, num_groups=1).\n\n    if num_groups % num_nodes == 0:\n        # Hierarchical balancing: balance groups to nodes, then nodes to GPUs.\n        # Step 1: Pack groups to nodes.\n        group_pack_index, group_rank_in_pack = balanced_packing(tokens_per_group_layer, num_nodes)\n        # Map logical experts to \"middle\" logical experts (logical experts per node)\n        # log2mlog: [num_layers, num_logical_experts] -> maps logical expert to its middle logical expert\n        mlog2log_map = (((group_pack_index * groups_per_node + group_rank_in_pack) * group_size).unsqueeze(-1) +\n                        torch.arange(group_size, dtype=torch.int64, device=group_pack_index.device)).flatten(-2)\n        # mlog2log_map: [num_layers, num_logical_experts] maps each logical expert to its \"middle\" logical expert ID.\n        # We need the inverse mapping for `weight.gather` later.\n        # This inverse mapping is tricky because multiple logical experts can map to the same middle expert.\n        # The `inverse` function is designed for permutations, which isn't directly applicable here.\n\n        # Let's rethink the mapping and replication.\n        # We have `num_layers` of `weight` ([num_layers, num_logical_experts]).\n        # First, we group logical experts: `tokens_per_group_layer` ([num_layers, num_groups]).\n        # Then, we pack these groups onto `num_nodes`: `group_pack_index` ([num_layers, num_groups]) and `group_rank_in_pack` ([num_layers, num_groups]).\n        # This means `num_groups` are assigned to `num_nodes`. `groups_per_node` groups per node.\n        # So, we have `num_layers * num_nodes` \"node-level\" expert weight tensors.\n        # Each node-level tensor has `num_logical_experts // num_nodes` \"middle\" experts.\n\n        # Let's create the \"middle\" logical experts' weights.\n        # `mlog_weights`: [num_layers * num_nodes, num_logical_experts // num_nodes]\n        # This requires carefully gathering weights.\n        # The `mlog2log` mapping needs to be constructed correctly.\n\n        # `mlog2log_map` assigns each logical expert to a conceptual \"middle\" logical expert.\n        # e.g., if num_logical_experts=8, num_groups=2, group_size=4.\n        # Group 0 contains logical experts 0,1,2,3. Group 1 contains 4,5,6,7.\n        # If num_nodes=2, groups_per_node=1.\n        # Group 0 (log experts 0-3) goes to node 0. Group 1 (log experts 4-7) goes to node 1.\n        # Node 0 has 1 middle expert. Node 1 has 1 middle expert.\n        # Middle expert for node 0 is logical experts 0-3. Middle expert for node 1 is 4-7.\n        # The mapping `mlog2log_map` should map:\n        # 0,1,2,3 -> 0 (on node 0)\n        # 4,5,6,7 -> 0 (on node 1)\n        # The current `mlog2log` calculation seems to be constructing the indices of the middle experts.\n        # `log2mlog` is `[num_layers, num_logical_experts]`.\n        # It maps logical expert ID to its index within the middle expert group.\n        # The `mlog2log` is the inverse of this index mapping.\n\n        # Let's reconstruct `mlog2log` and `tokens_per_mlog` more directly.\n        # For each layer, we have `num_groups` of `group_size`.\n        # `group_pack_index`: [num_layers, num_groups] - node assignment for each group.\n        # `group_rank_in_pack`: [num_layers, num_groups] - index of group within the node's assigned groups.\n\n        # We want to create `num_layers * num_nodes` \"node-level\" expert groups.\n        # Each node-level group corresponds to a set of original logical experts.\n        # The `tokens_per_mlog` should be `[num_layers * num_nodes, num_logical_experts // num_nodes]`.\n\n        # Calculate the logical expert index for each \"middle\" expert.\n        # `mlog_expert_indices`: [num_layers, num_nodes, groups_per_node, group_size]\n        # This is becoming complex. Let's simplify the conceptualization.\n\n        # Simpler approach:\n        # 1. Pack groups to nodes. This determines which logical experts are grouped together on each node.\n        # `group_pack_index`: [num_layers, num_groups] -> node ID for each group.\n        # `group_rank_in_pack`: [num_layers, num_groups] -> rank within node for each group.\n\n        # 2. For each node, create a combined weight tensor for the logical experts assigned to it.\n        #    The number of logical experts per node will be `groups_per_node * group_size`.\n        #    We need to gather the weights of logical experts based on `group_pack_index`.\n\n        # Let's create a mapping from (layer, node) to a contiguous block of logical experts.\n        # `node_logical_expert_map`: [num_layers, num_nodes, groups_per_node * group_size]\n\n        # This requires constructing a permutation for each layer and node.\n        # The original `mlog2log` calculation is:\n        # `((group_pack_index * groups_per_node + group_rank_in_pack) * group_size)`\n        # This forms a base index for each logical expert.\n        # `log2mlog` is this base index + `arange(group_size)`.\n        # `mlog2log` is the inverse of `log2mlog`.\n\n        # Let's directly construct the `tokens_per_mlog` and `mlog2log`.\n        # `tokens_per_mlog` is the weight tensor for experts *within* each node, considering the grouped logical experts.\n        # The number of such \"middle\" experts per node is `groups_per_node * group_size`.\n\n        # For each layer:\n        # `group_pack_index` tells us which node each group belongs to.\n        # `group_rank_in_pack` tells us the order within the node.\n\n        # Let's create `num_layers * num_nodes` tensors, each of size `groups_per_node * group_size`.\n        # `mlog_weights`: [num_layers * num_nodes, groups_per_node * group_size]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, groups_per_node * group_size]\n\n        # Example: num_layers=1, num_logical_experts=8, num_groups=2, group_size=4, num_nodes=2, groups_per_node=1, num_gpus=2, phy_experts_per_gpu=1.\n        # weight: [1, 8]\n        # tokens_per_group_layer: [1, 2] (sum of weights for group 0 and group 1)\n        # balanced_packing(tokens_per_group_layer, num_nodes=2) ->\n        #   group_pack_index: [1, 2] -> [[0, 1]] (group 0 to node 0, group 1 to node 1)\n        #   group_rank_in_pack: [1, 2] -> [[0, 0]] (rank 0 within node for both groups)\n        # groups_per_node = 1, group_size = 4.\n\n        # Middle expert for node 0 is logical experts 0,1,2,3.\n        # Middle expert for node 1 is logical experts 4,5,6,7.\n        # `mlog_to_orig_log_map` for node 0 should be [0,1,2,3].\n        # `mlog_to_orig_log_map` for node 1 should be [4,5,6,7].\n\n        # The original `mlog2log` calculation:\n        # `base_idx = (group_pack_index * groups_per_node + group_rank_in_pack) * group_size`\n        # `base_idx`: [1, 2] -> [[0*1+0, 1*1+0]] * 4 = [[0, 4]]\n        # `log2mlog`: `base_idx.unsqueeze(-1) + torch.arange(group_size)`\n        # `log2mlog`: [[0, 4]].unsqueeze(-1) + [0,1,2,3] -> [[0,1,2,3], [4,5,6,7]]\n        # This `log2mlog` is `[num_layers, num_logical_experts]`. It maps original logical expert ID to its \"middle\" expert index.\n        # The original logic uses `weight.gather(-1, mlog2log)` which is incorrect here.\n        # `weight.gather(-1, mlog2log)` expects `mlog2log` to be `[num_layers, num_middle_experts]` where `num_middle_experts` is the number of columns in `weight`.\n        # The shape should be `[num_layers, num_logical_experts]` if we are gathering from `weight` which is `[num_layers, num_logical_experts]`.\n        # The `mlog2log` calculated here has shape `[num_layers, num_logical_experts]`.\n        # It maps `original_logical_expert_id` to `middle_expert_id`.\n        # We need to gather `weight[layer, original_logical_expert_id]` and group them by `middle_expert_id`.\n\n        # Let's create `mlog_weights` directly.\n        # `mlog_weights`: [num_layers * num_nodes, groups_per_node * group_size]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, groups_per_node * group_size]\n\n        # Vectorized construction of mlog_weights and mlog_to_orig_log_map\n        # 1. Create a combined key for sorting groups within each layer based on node assignment and rank.\n        # This will order the groups such that all groups for node 0 come first, then node 1, etc.,\n        # and within each node, groups are ordered by their rank_in_pack.\n        combined_group_key = group_pack_index * groups_per_node + group_rank_in_pack # [num_layers, num_groups]\n\n        # 2. Get the sorted indices for groups for each layer.\n        sorted_group_indices = combined_group_key.argsort(dim=-1) # [num_layers, num_groups]\n\n        # 3. Create a base tensor for original logical expert IDs within each group.\n        # This tensor represents the original logical expert IDs if groups were laid out sequentially.\n        # Shape: [num_groups, group_size]\n        base_orig_log_expert_ids_per_group = (torch.arange(num_groups, device=weight.device, dtype=torch.int64) * group_size).unsqueeze(-1) + \\\n                                             torch.arange(group_size, device=weight.device, dtype=torch.int64)\n\n        # 4. Use sorted_group_indices to gather original logical expert IDs and their weights.\n        # Expand sorted_group_indices to match the shape of the data being gathered (groups, then experts within groups).\n        sorted_group_indices_expanded = sorted_group_indices.unsqueeze(-1).expand(-1, -1, group_size)\n\n        # Reshape the original weight tensor to [num_layers, num_groups, group_size] for gathering.\n        weight_reshaped_by_group = weight.view(num_layers, num_groups, group_size)\n\n        # Gather the weights based on the sorted group order.\n        # mlog_weights will have shape [num_layers, num_groups, group_size]\n        mlog_weights = weight_reshaped_by_group.gather(1, sorted_group_indices_expanded)\n\n        # Gather the original logical expert IDs based on the sorted group order.\n        # mlog_to_orig_log_map will have shape [num_layers, num_groups, group_size]\n        mlog_to_orig_log_map = base_orig_log_expert_ids_per_group.unsqueeze(0).expand(num_layers, -1, -1).gather(1, sorted_group_indices_expanded)\n\n        # 5. Reshape to [num_layers, num_nodes, groups_per_node * group_size]\n        # This aligns the data such that experts assigned to the same node are contiguous.\n        num_middle_experts_per_node = groups_per_node * group_size\n        mlog_weights = mlog_weights.view(num_layers, num_nodes, num_middle_experts_per_node)\n        mlog_to_orig_log_map = mlog_to_orig_log_map.view(num_layers, num_nodes, num_middle_experts_per_node)\n\n        # Now, `mlog_weights`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n        # And `mlog_to_orig_log_map`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n\n        # Step 2: Replicate experts within each node.\n        # `mlog_weights` are the weights for experts within each node.\n        # `num_physical_experts // num_nodes` is the target number of physical experts per node.\n        # `replicate_experts` takes `[X, num_log]` and returns `phy2log`, `rank`, `logcnt`.\n        # Here, X = num_layers * num_nodes, num_log = num_logical_experts // num_nodes.\n        # So, we need to reshape `mlog_weights` and `mlog_to_orig_log_map`.\n\n        num_middle_experts = num_logical_experts // num_nodes\n        reshaped_mlog_weights = mlog_weights.view(-1, num_middle_experts)\n        reshaped_mlog_to_orig_log_map = mlog_to_orig_log_map.view(-1, num_middle_experts)\n\n        # `phy2mlog`: [num_layers * num_nodes, num_physical_experts // num_nodes]\n        # `phyrank`: [num_layers * num_nodes, num_physical_experts // num_nodes]\n        # `mlogcnt`: [num_layers * num_nodes, num_middle_experts]\n        phy2mlog, phyrank, mlogcnt = replicate_experts(\n            reshaped_mlog_weights, num_physical_experts // num_nodes)\n\n        # Step 3: Pack physical experts to GPUs.\n        # `phy2mlog` maps physical experts (within each node) to middle logical experts.\n        # `phy2mlog` is `[num_layers * num_nodes, num_phy_per_node]`\n        # `phyrank` is `[num_layers * num_nodes, num_phy_per_node]`\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`\n\n        # We need to map these back to global physical expert indices.\n        # The `phy2mlog` refers to the *middle* logical experts.\n        # The `replicate_experts` function returns `phy2log` which is the *original* logical expert ID.\n        # So, `phy2mlog` from `replicate_experts` is actually `[num_layers*num_nodes, num_phy_per_node]`,\n        # where each entry is an index into the *middle* logical experts.\n\n        # We need to map these `phy2mlog` indices back to original logical expert IDs.\n        # `reshaped_mlog_to_orig_log_map`: [num_layers * num_nodes, num_middle_experts]\n        # `final_phy2log_per_node`: [num_layers * num_nodes, num_phy_per_node]\n        final_phy2log_per_node = reshaped_mlog_to_orig_log_map.gather(-1, phy2mlog)\n\n        # `phyrank` is the replica rank for the middle experts.\n        # We need to combine this with `mlogcnt` to get the final `logcnt`.\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`. It tells us how many replicas each middle expert has.\n        # The `phyrank` is the rank of the replica for a *specific* middle expert.\n\n        # The `logcnt` returned by `replicate_experts` is `[X, num_log]`, which is `[num_layers*num_nodes, num_middle_experts]`.\n        # We need to map this back to `[num_layers, num_logical_experts]`.\n\n        # Let's use the `mlogcnt` directly to get the total count of replicas for each logical expert.\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`.\n        # We need to sum/average counts based on the `mlog_to_orig_log_map`.\n\n        # Let's refine the output mapping:\n        # `physical_to_logical_map`: [num_layers, num_replicas]\n        # `logical_to_physical_map`: [num_layers, num_logical_experts, X]\n        # `expert_count`: [num_layers, num_logical_experts]\n\n        # `final_phy2log_per_node`: [num_layers * num_nodes, num_phy_per_node]\n        # This is the logical expert ID for each physical expert *within its node*.\n        # We need to convert this to global physical expert IDs.\n        # Physical experts are grouped by GPU. `num_gpus` total, `num_gpus // num_nodes` per node.\n        # `phy_experts_per_gpu` = `num_physical_experts // num_gpus`.\n        # So, each GPU has `phy_experts_per_gpu` physical experts.\n        # Each Node has `num_gpus // num_nodes` GPUs.\n        # Total physical experts per node = `(num_gpus // num_nodes) * phy_experts_per_gpu`\n        # This should equal `num_physical_experts // num_nodes`.\n\n        # `final_phy2log_per_node`: [num_layers, num_nodes, num_phy_per_node]\n        # `final_rank_per_node`: [num_layers, num_nodes, num_phy_per_node] (this is `phyrank`)\n\n        # We need to flatten this to `[num_layers, num_physical_experts]`.\n        # `num_physical_experts = num_nodes * num_phy_per_node`.\n\n        # `final_phy2log_global`: [num_layers, num_physical_experts]\n        # `final_rank_global`: [num_layers, num_physical_experts]\n\n        # The `mlog_to_orig_log_map` correctly maps middle experts to original logical experts.\n        # `phy2mlog` maps physical experts (within a node) to middle experts.\n        # So, `phy2mlog.gather(-1, final_phy2log_per_node)` gives the logical expert ID.\n        # This is what `final_phy2log_per_node` already represents.\n\n        # Let's rename `final_phy2log_per_node` to `phy2log_per_node` for clarity.\n        phy2log_per_node = final_phy2log_per_node\n        rank_per_node = phyrank # [num_layers * num_nodes, num_phy_per_node]\n\n        # Flatten these to get global mappings for physical experts.\n        phy2log_global = phy2log_per_node.view(num_layers, -1)\n        rank_global = rank_per_node.view(num_layers, -1)\n\n        # Now, let's construct the `logcnt` (expert_count).\n        # `mlogcnt`: [num_layers * num_nodes, num_middle_experts]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, num_middle_experts]\n\n        # We need to aggregate `mlogcnt` based on `mlog_to_orig_log_map`.\n        # For each logical expert, its total count is the sum of counts of its constituent middle experts.\n        # This aggregation needs to be done carefully.\n\n        # Let's create a tensor for `logcnt`: [num_layers, num_logical_experts] initialized to zeros.\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n\n        # Iterate through each layer and node.\n        for i in range(num_layers):\n            for node_idx in range(num_nodes):\n                # For the middle experts belonging to this node and layer:\n                middle_expert_indices = mlog_to_orig_log_map[i, node_idx] # [num_middle_experts]\n                counts_for_middle_experts = mlogcnt[i * num_nodes + node_idx] # [num_middle_experts]\n\n                # Add these counts to the corresponding original logical experts.\n                # `torch.add.at` allows for sparse updates.\n                expert_count[i].scatter_add_(0, middle_expert_indices, counts_for_middle_experts)\n\n        # The `logcnt` from `replicate_experts` is the number of replicas for the middle experts.\n        # We need to make sure the `expert_count` is correct.\n        # The `replicate_experts` function returns `logcnt` which is the number of replicas for each logical expert it processed.\n        # So, `mlogcnt` is the number of replicas for each middle expert.\n\n        # The final `expert_count` should be `[num_layers, num_logical_experts]`.\n        # We need to map `mlogcnt` back.\n\n        # Let's create `expert_count` of size `[num_layers, num_logical_experts]` initialized to 0.\n        # Then, for each layer, node, and middle expert:\n        # `expert_count[layer, mlog_to_orig_log_map[layer, node, middle_idx]] += mlogcnt[layer*num_nodes + node, middle_idx]`\n\n        # This aggregation is correct.\n\n        # Now, we need to construct `logical_to_physical_map`.\n        # `logical_to_physical_map`: [num_layers, num_logical_experts, X] where X is max replicas.\n        # `phy2log_global`: [num_layers, num_physical_experts]\n        # `rank_global`: [num_layers, num_physical_experts]\n\n        # We need to group physical experts by their logical expert ID and rank.\n        max_replicas_per_expert = (num_physical_experts + num_logical_experts - 1) // num_logical_experts # Ceiling division\n\n        logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert), -1, dtype=torch.int64, device=weight.device)\n\n        # Iterate through all physical experts\n        for layer_idx in range(num_layers):\n            for phys_idx in range(num_physical_experts):\n                log_expert_id = phy2log_global[layer_idx, phys_idx]\n                replica_rank = rank_global[layer_idx, phys_idx]\n\n                if log_expert_id >= 0 and log_expert_id < num_logical_experts:\n                    # Place the physical expert ID into the correct slot based on its logical ID and rank.\n                    # We need to ensure we don't exceed `max_replicas_per_expert`.\n                    # The `replica_rank` should correspond to the index in the third dimension.\n                    if replica_rank < max_replicas_per_expert:\n                        logical_to_physical_map[layer_idx, log_expert_id, replica_rank] = phys_idx\n                    else:\n                        # This case indicates an issue or that max_replicas_per_expert needs to be larger.\n                        # For now, we can assert or log a warning.\n                        pass # Or handle error\n\n        # The `expert_count` is already computed.\n\n        return phy2log_global, logical_to_physical_map, expert_count\n\n    else:\n        # Global balancing strategy: treat all experts as one group, balance across num_gpus.\n        # This is equivalent to calling the hierarchical function with num_nodes=1, num_groups=1.\n        # num_groups = 1 means group_size = num_logical_experts.\n        # num_nodes = 1 means groups_per_node = 1.\n        # The entire logic simplifies.\n\n        # Step 1: Pack logical experts to nodes (here, nodes = GPUs conceptually).\n        # `tokens_per_group` becomes `weight` itself since num_groups=1.\n        # `balanced_packing(weight, num_gpus)`\n        # `group_pack_index`: [num_layers, num_logical_experts] -> GPU assignment for each logical expert.\n        # `group_rank_in_pack`: [num_layers, num_logical_experts] -> rank within GPU for each logical expert.\n        # `num_nodes` becomes `num_gpus`. `groups_per_node` becomes `num_logical_experts // num_gpus`.\n\n        # We need to adapt `balanced_packing` to work with `num_packs = num_gpus`.\n        # The `groups_per_pack` in `balanced_packing` refers to items per pack.\n        # Here, items are logical experts. So, `groups_per_pack` = `num_logical_experts // num_gpus`.\n        # `num_packs` = `num_gpus`.\n\n        pack_index_gpu, rank_in_gpu = balanced_packing(weight, num_gpus)\n\n        # `pack_index_gpu`: [num_layers, num_logical_experts] -> GPU ID for each logical expert.\n        # `rank_in_gpu`: [num_layers, num_logical_experts] -> rank within GPU for each logical expert.\n\n        # Step 2: Replicate experts within GPUs.\n        # We need to create tensors for `replicate_experts` that represent experts *per GPU*.\n        # `num_logical_experts_per_gpu` = `num_logical_experts // num_gpus`.\n        # `num_physical_experts_per_gpu` = `num_physical_experts // num_gpus`.\n\n        # For each GPU, we have a set of logical experts.\n        # We need to gather weights for logical experts assigned to the same GPU.\n        # `gpu_logical_weights`: [num_gpus, num_logical_experts_per_gpu, num_layers] (transposed for replicate_experts)\n        # `gpu_logical_to_orig_map`: [num_gpus, num_logical_experts_per_gpu]\n\n        gpu_logical_weights_list = []\n        gpu_logical_to_orig_map_list = []\n\n        num_logical_experts_per_gpu = num_logical_experts // num_gpus\n\n        for gpu_idx in range(num_gpus):\n            # Find logical experts assigned to this GPU\n            assigned_logical_mask = (pack_index_gpu == gpu_idx)\n            # Get their ranks within the GPU\n            ranks_on_gpu = rank_in_gpu[assigned_logical_mask]\n            # Sort by rank\n            sorted_ranks_on_gpu, sort_indices_on_gpu = ranks_on_gpu.sort()\n            # Get original logical expert indices\n            assigned_logical_indices = torch.arange(num_logical_experts, device=pack_index_gpu.device)[assigned_logical_mask][sort_indices_on_gpu]\n\n            if len(assigned_logical_indices) > 0:\n                # Gather weights for these logical experts\n                # We need weights for each layer.\n                # `weight` is [num_layers, num_logical_experts].\n                # `assigned_logical_indices` is [num_logical_experts_per_gpu].\n                # We want `gpu_logical_weights` of shape [num_layers, num_logical_experts_per_gpu].\n                gpu_logical_weights = weight[:, assigned_logical_indices]\n                gpu_logical_weights_list.append(gpu_logical_weights)\n                gpu_logical_to_orig_map_list.append(assigned_logical_indices)\n            else:\n                # If a GPU gets no logical experts (shouldn't happen if num_logical_experts >= num_gpus)\n                gpu_logical_weights_list.append(torch.empty(num_layers, 0, dtype=weight.dtype, device=weight.device))\n                gpu_logical_to_orig_map_list.append(torch.empty(0, dtype=torch.int64, device=weight.device))\n\n        # `gpu_logical_weights`: [num_gpus, num_layers, num_logical_experts_per_gpu]\n        # `gpu_logical_to_orig_map`: [num_gpus, num_logical_experts_per_gpu]\n        gpu_logical_weights = torch.stack(gpu_logical_weights_list)\n        gpu_logical_to_orig_map = torch.stack(gpu_logical_to_orig_map_list)\n\n        # Reshape for `replicate_experts`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        reshaped_gpu_logical_weights = gpu_logical_weights.view(-1, num_logical_experts_per_gpu)\n        reshaped_gpu_logical_to_orig_map = gpu_logical_to_orig_map.view(-1, num_logical_experts_per_gpu)\n\n        # `replicate_experts` is called with `num_phy = num_physical_experts // num_gpus` (target physical experts per GPU).\n        # `X` becomes `num_gpus * num_layers`. `num_log` becomes `num_logical_experts_per_gpu`.\n        # `phy2log_gpu`: [num_gpus * num_layers, num_phy_per_gpu]\n        # `phyrank_gpu`: [num_gpus * num_layers, num_phy_per_gpu]\n        # `gpu_logcnt`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        phy2log_gpu, phyrank_gpu, gpu_logcnt = replicate_experts(\n            reshaped_gpu_logical_weights, num_physical_experts // num_gpus)\n\n        # Map `phy2log_gpu` back to original logical expert IDs using `reshaped_gpu_logical_to_orig_map`.\n        # `phy2log_global`: [num_gpus * num_layers, num_phy_per_gpu]\n        phy2log_global = reshaped_gpu_logical_to_orig_map.gather(-1, phy2log_gpu)\n\n        # Flatten to get the final `physical_to_logical_map` and `rank`.\n        # The `rank_global` is `phyrank_gpu`.\n        rank_global = phyrank_gpu\n\n        # Construct `expert_count` ([num_layers, num_logical_experts]).\n        # `gpu_logcnt`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n\n        for gpu_layer_idx in range(num_gpus * num_layers):\n            layer_idx = gpu_layer_idx // num_gpus\n            # Get the original logical expert indices for this gpu_layer combination\n            orig_log_indices = reshaped_gpu_logical_to_orig_map[gpu_layer_idx] # [num_logical_experts_per_gpu]\n            counts_for_orig_logs = gpu_logcnt[gpu_layer_idx] # [num_logical_experts_per_gpu]\n\n            # Add these counts to the correct positions in the expert_count tensor\n            expert_count[layer_idx].scatter_add_(0, orig_log_indices, counts_for_orig_logs)\n\n        # Construct `logical_to_physical_map` ([num_layers, num_logical_experts, X])\n        max_replicas_per_expert = (num_physical_experts + num_logical_experts - 1) // num_logical_experts\n\n        logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert), -1, dtype=torch.int64, device=weight.device)\n\n        # Need to map `phy2log_global` and `rank_global` to the correct positions.\n        # `phy2log_global` are indices into the *original* logical experts.\n        # `rank_global` is the replica rank for that original logical expert.\n\n        # Reshape to get mappings per layer and then iterate through physical experts.\n        phy2log_global_reshaped = phy2log_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous()\n        rank_global_reshaped = rank_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous()\n\n        # Iterate through all physical experts\n        for layer_idx in range(num_layers):\n            for gpu_idx in range(num_gpus):\n                for phys_idx_in_gpu in range(num_physical_experts // num_gpus):\n                    # Get the global physical expert index\n                    global_phys_idx = gpu_idx * (num_physical_experts // num_gpus) + phys_idx_in_gpu\n\n                    log_expert_id = phy2log_global_reshaped[layer_idx, gpu_idx, phys_idx_in_gpu]\n                    replica_rank = rank_global_reshaped[layer_idx, gpu_idx, phys_idx_in_gpu]\n\n                    if log_expert_id >= 0 and log_expert_id < num_logical_experts:\n                        if replica_rank < max_replicas_per_expert:\n                            logical_to_physical_map[layer_idx, log_expert_id, replica_rank] = global_phys_idx\n                        else:\n                            pass # Handle error or log\n\n        # The `phy2log_global` needs to be reshaped to `[num_layers, num_replicas]`\n        # where `num_replicas` is `num_physical_experts`.\n        # `phy2log_global` is currently `[num_gpus * num_layers, num_phy_per_gpu]`.\n        # We need to combine these.\n        # The order of physical experts should be consistent.\n        # Let's reconstruct `phy2log_global` to be `[num_layers, num_physical_experts]`.\n\n        final_phy2log_global = torch.empty((num_layers, num_physical_experts), dtype=torch.int64, device=weight.device)\n        current_phys_idx = 0\n        for gpu_idx in range(num_gpus):\n            for layer_idx in range(num_layers):\n                # Calculate the range of physical experts for this GPU and layer combination\n                start_phys_idx = layer_idx * num_gpus * (num_physical_experts // num_gpus) + gpu_idx * (num_physical_experts // num_gpus)\n                end_phys_idx = start_phys_idx + (num_physical_experts // num_gpus)\n\n                # The `phy2log_global` calculated earlier is `[num_gpus * num_layers, num_phy_per_gpu]`\n                # We need to map it correctly.\n                # `phy2log_gpu` was `[num_gpus * num_layers, num_phy_per_gpu]`\n                # `reshaped_gpu_logical_to_orig_map` was `[num_gpus * num_layers, num_phy_per_gpu]`\n                # `phy2log_global = reshaped_gpu_logical_to_orig_map.gather(-1, phy2log_gpu)`\n\n                # The indices for `phy2log_global` should be:\n                # For layer 0, GPU 0: phys experts 0 to num_phy_per_gpu - 1\n                # For layer 0, GPU 1: phys experts num_phy_per_gpu to 2*num_phy_per_gpu - 1\n                # ...\n                # For layer 1, GPU 0: phys experts num_gpus * num_phy_per_gpu to (num_gpus+1)*num_phy_per_gpu - 1\n\n                # Let's re-index the `phy2log_global` and `rank_global` to match `[num_layers, num_physical_experts]`\n                # The current `phy2log_global` is effectively `[num_layers * num_gpus, num_phy_per_gpu]`\n                # We need to rearrange it.\n\n                # Example: num_layers=2, num_gpus=2, num_phy_per_gpu=2. num_physical_experts=4.\n                # phy2log_global:\n                # [layer0_gpu0_phys0, layer0_gpu0_phys1,\n                #  layer0_gpu1_phys0, layer0_gpu1_phys1,\n                #  layer1_gpu0_phys0, layer1_gpu0_phys1,\n                #  layer1_gpu1_phys0, layer1_gpu1_phys1]\n                # Reshaped to [num_gpus*num_layers, num_phy_per_gpu] -> [4, 2]\n                # We want [num_layers, num_physical_experts] -> [2, 4]\n\n                # The current `phy2log_global` is `[num_gpus * num_layers, num_phy_per_gpu]`.\n                # Permute and reshape:\n                # `view(num_gpus, num_layers, -1)` gives `[num_gpus, num_layers, num_phy_per_gpu]`\n                # `permute(1, 0, 2)` gives `[num_layers, num_gpus, num_phy_per_gpu]`\n                # `contiguous().view(num_layers, -1)` gives `[num_layers, num_gpus * num_phy_per_gpu]` = `[num_layers, num_physical_experts]`\n\n                final_phy2log_global = phy2log_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous().view(num_layers, num_physical_experts)\n                final_rank_global = rank_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous().view(num_layers, num_physical_experts)\n\n        # Return the results for global balancing.\n        return final_phy2log_global, final_rank_global, expert_count\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    # Keep weights on their original device to avoid unnecessary transfers.\n    # Ensure float type for calculations.\n    weight = weight.float()\n    \n    # num_replicas is equivalent to num_physical_experts used internally\n    num_physical_experts = num_replicas\n\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        # The function now returns phy2log_global, rank_global, expert_count\n        phy2log_global, rank_global, expert_count = rebalance_experts_hierarchical(\n            weight, num_physical_experts, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        # The function now returns phy2log_global, rank_global, expert_count\n        phy2log_global, rank_global, expert_count = rebalance_experts_hierarchical(\n            weight, num_physical_experts, 1, 1, num_gpus)\n\n    # Construct logical_to_physical_map here, common to both strategies.\n    # Max replicas per expert: This ensures `logical_to_physical_map` is large enough.\n    # It must be at least 1.\n    if num_logical_experts == 0: # Handle edge case to avoid division by zero\n        max_replicas_per_expert = 0\n    else:\n        max_replicas_per_expert = (num_physical_experts + num_logical_experts - 1) // num_logical_experts\n\n    # Initialize the map with -1 (indicating no physical expert assigned)\n    logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert), -1, dtype=torch.int64, device=weight.device)\n\n    # Create global physical expert indices.\n    global_physical_indices = torch.arange(num_replicas, dtype=torch.int64, device=weight.device).unsqueeze(0).expand(num_layers, -1)\n\n    # Prepare indices for index_put_ operation.\n    # Create a mask for valid placements (should ideally always be true if logic is sound)\n    valid_mask = (phy2log_global >= 0) & \\\n                 (phy2log_global < num_logical_experts) & \\\n                 (rank_global >= 0) & \\\n                 (rank_global < max_replicas_per_expert)\n\n    # Apply the mask to all relevant tensors to get only valid assignments\n    # Reshape to 1D as index_put_ expects 1D index tensors for each dimension.\n    layer_indices = torch.arange(num_layers, device=weight.device).unsqueeze(1).expand(-1, num_replicas)[valid_mask]\n    log_expert_indices = phy2log_global[valid_mask]\n    rank_indices = rank_global[valid_mask]\n    values_to_put = global_physical_indices[valid_mask]\n\n    # Use index_put_ for direct assignment based on multiple index tensors.\n    # This is generally more efficient than Python loops for large tensors.\n    if values_to_put.numel() > 0: # Avoid calling index_put_ with empty indices\n        logical_to_physical_map.index_put_((layer_indices, log_expert_indices, rank_indices), values_to_put)\n\n    # The `expert_count` is already computed by the balancing functions.\n    # `phy2log_global` is the first return value.\n    return phy2log_global, rank_global, expert_count\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n", "language": "python", "parent_id": "c7bba46d-0649-4fe1-8754-1f35ebc13c1c", "generation": 3, "timestamp": 1751050672.4393973, "iteration_found": 17, "metrics": {"balancedness_score": 0.0, "speed_score": 0.0, "combined_score": 0.0, "error": "The size of tensor a (288) must match the size of tensor b (2) at non-singleton dimension 2"}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 74 lines with 92 lines", "parent_metrics": {"balancedness_score": 0.0, "speed_score": 0.0, "combined_score": 0.0, "error": "The size of tensor a (288) must match the size of tensor b (2) at non-singleton dimension 2"}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve the Mixture-of-Expert models Expert Parallelism Load Balancer (MoE EPLB) expert rearrangement algorithm.\nThis algorithm will take the load metrics recorded by the vLLM server, and rearrange the experts to balance the load. It can make replicas of some experts to achieve better load balancing.\nYour goal will be two-fold: 1. Improve the algorithm to achieve better load balancing; while 2. Improve the algorithm to be more efficient, i.e. reduce the execution time of the algorithm itself, since perfect load balancing is NP-hard.\nThe current algorithm is implemented in the `rebalance_experts` function. ", "user": "# Current Program Information\n- Current performance metrics: - balancedness_score: 0.0000\n- speed_score: 0.0000\n- combined_score: 0.0000\n- error: The size of tensor a (288) must match the size of tensor b (2) at non-singleton dimension 2\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: balancedness_score, speed_score, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0340, combined_score: 0.1661\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0350, combined_score: 0.1667\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0405, combined_score: 0.1694\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.1694)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0405), Performs well on combined_score (0.1694)\n\n\n### Program 2 (Score: 0.1667)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0350), Performs well on combined_score (0.1667)\n\n\n### Program 3 (Score: 0.1661)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0340), Performs well on combined_score (0.1661)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.1610)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n\n### Program D2 (Score: 0.1610)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.1694, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 79 lines with 108 lines\nChange 2: Replace 24 lines with 43 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 2 (Score: 0.0836, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 3 (Score: 0.0836, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 4 (Score: 0.1667, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 5 (Score: 0.0000, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 18 lines with 6 lines\nChange 2: Replace 509 lines with 208 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n\n# Current Program\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n    device = weight.device\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(num_groups,\n                                  dtype=torch.int64,\n                                  device=device).expand(num_layers, num_groups)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort weights and get indices\n    # Keep on original device (GPU if weight is on GPU)\n    sorted_weights, sorted_indices = weight.float().sort(-1, descending=True)\n\n    # Initialize pack_index and rank_in_pack on the same device as weight\n    pack_index = torch.full((num_layers, num_groups), -1, dtype=torch.int64, device=device)\n    rank_in_pack = torch.full((num_layers, num_groups), -1, dtype=torch.int64, device=device)\n\n    # Initialize pack_loads and pack_counts on the same device as weight\n    pack_loads = torch.zeros(num_layers, num_packs, dtype=torch.float32, device=device)\n    pack_counts = torch.zeros(num_layers, num_packs, dtype=torch.int64, device=device)\n\n    # Iterate through sorted items and assign them to the least loaded pack\n    # This loop is inherently sequential for each layer (i) and group (g)\n    # as pack_loads and pack_counts are updated dynamically.\n    # However, keeping operations on GPU avoids CPU-GPU transfers if weight is on GPU.\n    for i in range(num_layers):\n        for g in range(num_groups):\n            # Find the pack with the minimum current load and available slots\n            available_packs_mask = (pack_counts[i] < groups_per_pack)\n\n            # Select from available packs\n            # Use `torch.where` to set loads of unavailable packs to infinity,\n            # so `argmin` will correctly pick from available ones.\n            current_pack_loads = torch.where(available_packs_mask, pack_loads[i], float('inf'))\n            chosen_pack_idx = torch.argmin(current_pack_loads)\n\n            original_group_idx = sorted_indices[i, g]\n            original_group_weight = weight[i, original_group_idx]\n\n            pack_index[i, original_group_idx] = chosen_pack_idx\n            rank_in_pack[i, original_group_idx] = pack_counts[i, chosen_pack_idx]\n\n            pack_loads[i, chosen_pack_idx] += original_group_weight\n            pack_counts[i, chosen_pack_idx] += 1\n\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Unified rebalancing logic:\n    # The hierarchical approach is used when num_groups is divisible by num_nodes.\n    # Otherwise, it falls back to a global balancing strategy by setting num_nodes=1 and num_groups=1.\n    # We can simplify this by directly implementing the global strategy when needed,\n    # or by ensuring the hierarchical logic correctly handles num_nodes=1.\n\n    # Determine the effective number of groups and nodes for balancing.\n    # If num_groups is not divisible by num_nodes, we treat it as a single group\n    # across all nodes for the first level of balancing.\n    effective_num_groups = num_groups if num_groups % num_nodes == 0 else 1\n    effective_num_nodes = num_nodes if num_groups % num_nodes == 0 else 1\n\n    # If num_groups % num_nodes != 0, we effectively want to balance across all groups as if they were one large group.\n    # And then balance those groups onto nodes.\n    # This is equivalent to setting num_nodes = 1 and num_groups = 1 in the hierarchical logic.\n    # Let's adjust parameters for a unified call to a simplified hierarchical logic.\n\n    # Simplified approach:\n    # First, balance logical experts across groups within layers.\n    # Then, balance these groups across nodes.\n    # Finally, replicate experts within nodes to balance across GPUs.\n\n    # Step 1: Balance logical experts into groups.\n    # If num_groups == 1, this step is trivial and doesn't change anything.\n    group_size = num_logical_experts // num_groups\n    tokens_per_group_layer = weight.unflatten(-1, (num_groups, group_size)).sum(-1) # [num_layers, num_groups]\n\n    # Pack groups onto nodes.\n    # If num_groups % num_nodes != 0, we effectively want to balance all groups onto a single 'node' conceptually,\n    # and then distribute those across the actual num_nodes.\n    # Let's consider the case where num_groups is not divisible by num_nodes.\n    # The original code uses num_nodes for packing groups. If num_groups < num_nodes, this packing might be problematic.\n    # A more robust approach is to balance groups onto num_nodes, and if num_groups < num_nodes, some nodes get 0 groups.\n    # The current `balanced_packing` assumes num_groups is divisible by num_packs (num_nodes here).\n\n    # Let's redefine the packing strategy:\n    # We want to pack 'num_groups' items (groups) into 'num_nodes' packs.\n    # If num_groups is not divisible by num_nodes, balanced_packing needs to handle it.\n    # The current `balanced_packing` asserts `num_groups % num_packs == 0`.\n    # This means if num_groups % num_nodes != 0, the original code calls `rebalance_experts_hierarchical` with num_nodes=1, num_groups=1.\n    # This is a global balancing strategy.\n\n    # Let's ensure the function call correctly reflects the intended strategy.\n    # If num_groups % num_nodes == 0, use hierarchical.\n    # Otherwise, use global (which is hierarchical with num_nodes=1, num_groups=1).\n\n    if num_groups % num_nodes == 0:\n        # Hierarchical balancing: balance groups to nodes, then nodes to GPUs.\n        # Step 1: Pack groups to nodes.\n        group_pack_index, group_rank_in_pack = balanced_packing(tokens_per_group_layer, num_nodes)\n        # Map logical experts to \"middle\" logical experts (logical experts per node)\n        # log2mlog: [num_layers, num_logical_experts] -> maps logical expert to its middle logical expert\n        mlog2log_map = (((group_pack_index * groups_per_node + group_rank_in_pack) * group_size).unsqueeze(-1) +\n                        torch.arange(group_size, dtype=torch.int64, device=group_pack_index.device)).flatten(-2)\n        # mlog2log_map: [num_layers, num_logical_experts] maps each logical expert to its \"middle\" logical expert ID.\n        # We need the inverse mapping for `weight.gather` later.\n        # This inverse mapping is tricky because multiple logical experts can map to the same middle expert.\n        # The `inverse` function is designed for permutations, which isn't directly applicable here.\n\n        # Let's rethink the mapping and replication.\n        # We have `num_layers` of `weight` ([num_layers, num_logical_experts]).\n        # First, we group logical experts: `tokens_per_group_layer` ([num_layers, num_groups]).\n        # Then, we pack these groups onto `num_nodes`: `group_pack_index` ([num_layers, num_groups]) and `group_rank_in_pack` ([num_layers, num_groups]).\n        # This means `num_groups` are assigned to `num_nodes`. `groups_per_node` groups per node.\n        # So, we have `num_layers * num_nodes` \"node-level\" expert weight tensors.\n        # Each node-level tensor has `num_logical_experts // num_nodes` \"middle\" experts.\n\n        # Let's create the \"middle\" logical experts' weights.\n        # `mlog_weights`: [num_layers * num_nodes, num_logical_experts // num_nodes]\n        # This requires carefully gathering weights.\n        # The `mlog2log` mapping needs to be constructed correctly.\n\n        # `mlog2log_map` assigns each logical expert to a conceptual \"middle\" logical expert.\n        # e.g., if num_logical_experts=8, num_groups=2, group_size=4.\n        # Group 0 contains logical experts 0,1,2,3. Group 1 contains 4,5,6,7.\n        # If num_nodes=2, groups_per_node=1.\n        # Group 0 (log experts 0-3) goes to node 0. Group 1 (log experts 4-7) goes to node 1.\n        # Node 0 has 1 middle expert. Node 1 has 1 middle expert.\n        # Middle expert for node 0 is logical experts 0-3. Middle expert for node 1 is 4-7.\n        # The mapping `mlog2log_map` should map:\n        # 0,1,2,3 -> 0 (on node 0)\n        # 4,5,6,7 -> 0 (on node 1)\n        # The current `mlog2log` calculation seems to be constructing the indices of the middle experts.\n        # `log2mlog` is `[num_layers, num_logical_experts]`.\n        # It maps logical expert ID to its index within the middle expert group.\n        # The `mlog2log` is the inverse of this index mapping.\n\n        # Let's reconstruct `mlog2log` and `tokens_per_mlog` more directly.\n        # For each layer, we have `num_groups` of `group_size`.\n        # `group_pack_index`: [num_layers, num_groups] - node assignment for each group.\n        # `group_rank_in_pack`: [num_layers, num_groups] - index of group within the node's assigned groups.\n\n        # We want to create `num_layers * num_nodes` \"node-level\" expert groups.\n        # Each node-level group corresponds to a set of original logical experts.\n        # The `tokens_per_mlog` should be `[num_layers * num_nodes, num_logical_experts // num_nodes]`.\n\n        # Calculate the logical expert index for each \"middle\" expert.\n        # `mlog_expert_indices`: [num_layers, num_nodes, groups_per_node, group_size]\n        # This is becoming complex. Let's simplify the conceptualization.\n\n        # Simpler approach:\n        # 1. Pack groups to nodes. This determines which logical experts are grouped together on each node.\n        # `group_pack_index`: [num_layers, num_groups] -> node ID for each group.\n        # `group_rank_in_pack`: [num_layers, num_groups] -> rank within node for each group.\n\n        # 2. For each node, create a combined weight tensor for the logical experts assigned to it.\n        #    The number of logical experts per node will be `groups_per_node * group_size`.\n        #    We need to gather the weights of logical experts based on `group_pack_index`.\n\n        # Let's create a mapping from (layer, node) to a contiguous block of logical experts.\n        # `node_logical_expert_map`: [num_layers, num_nodes, groups_per_node * group_size]\n\n        # This requires constructing a permutation for each layer and node.\n        # The original `mlog2log` calculation is:\n        # `((group_pack_index * groups_per_node + group_rank_in_pack) * group_size)`\n        # This forms a base index for each logical expert.\n        # `log2mlog` is this base index + `arange(group_size)`.\n        # `mlog2log` is the inverse of `log2mlog`.\n\n        # Let's directly construct the `tokens_per_mlog` and `mlog2log`.\n        # `tokens_per_mlog` is the weight tensor for experts *within* each node, considering the grouped logical experts.\n        # The number of such \"middle\" experts per node is `groups_per_node * group_size`.\n\n        # For each layer:\n        # `group_pack_index` tells us which node each group belongs to.\n        # `group_rank_in_pack` tells us the order within the node.\n\n        # Let's create `num_layers * num_nodes` tensors, each of size `groups_per_node * group_size`.\n        # `mlog_weights`: [num_layers * num_nodes, groups_per_node * group_size]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, groups_per_node * group_size]\n\n        # Example: num_layers=1, num_logical_experts=8, num_groups=2, group_size=4, num_nodes=2, groups_per_node=1, num_gpus=2, phy_experts_per_gpu=1.\n        # weight: [1, 8]\n        # tokens_per_group_layer: [1, 2] (sum of weights for group 0 and group 1)\n        # balanced_packing(tokens_per_group_layer, num_nodes=2) ->\n        #   group_pack_index: [1, 2] -> [[0, 1]] (group 0 to node 0, group 1 to node 1)\n        #   group_rank_in_pack: [1, 2] -> [[0, 0]] (rank 0 within node for both groups)\n        # groups_per_node = 1, group_size = 4.\n\n        # Middle expert for node 0 is logical experts 0,1,2,3.\n        # Middle expert for node 1 is logical experts 4,5,6,7.\n        # `mlog_to_orig_log_map` for node 0 should be [0,1,2,3].\n        # `mlog_to_orig_log_map` for node 1 should be [4,5,6,7].\n\n        # The original `mlog2log` calculation:\n        # `base_idx = (group_pack_index * groups_per_node + group_rank_in_pack) * group_size`\n        # `base_idx`: [1, 2] -> [[0*1+0, 1*1+0]] * 4 = [[0, 4]]\n        # `log2mlog`: `base_idx.unsqueeze(-1) + torch.arange(group_size)`\n        # `log2mlog`: [[0, 4]].unsqueeze(-1) + [0,1,2,3] -> [[0,1,2,3], [4,5,6,7]]\n        # This `log2mlog` is `[num_layers, num_logical_experts]`. It maps original logical expert ID to its \"middle\" expert index.\n        # The original logic uses `weight.gather(-1, mlog2log)` which is incorrect here.\n        # `weight.gather(-1, mlog2log)` expects `mlog2log` to be `[num_layers, num_middle_experts]` where `num_middle_experts` is the number of columns in `weight`.\n        # The shape should be `[num_layers, num_logical_experts]` if we are gathering from `weight` which is `[num_layers, num_logical_experts]`.\n        # The `mlog2log` calculated here has shape `[num_layers, num_logical_experts]`.\n        # It maps `original_logical_expert_id` to `middle_expert_id`.\n        # We need to gather `weight[layer, original_logical_expert_id]` and group them by `middle_expert_id`.\n\n        # Let's create `mlog_weights` directly.\n        # `mlog_weights`: [num_layers * num_nodes, groups_per_node * group_size]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, groups_per_node * group_size]\n\n        # Vectorized construction of mlog_weights and mlog_to_orig_log_map\n        # 1. Create a combined key for sorting groups within each layer based on node assignment and rank.\n        # This will order the groups such that all groups for node 0 come first, then node 1, etc.,\n        # and within each node, groups are ordered by their rank_in_pack.\n        combined_group_key = group_pack_index * groups_per_node + group_rank_in_pack # [num_layers, num_groups]\n\n        # 2. Get the sorted indices for groups for each layer.\n        sorted_group_indices = combined_group_key.argsort(dim=-1) # [num_layers, num_groups]\n\n        # 3. Create a base tensor for original logical expert IDs within each group.\n        # This tensor represents the original logical expert IDs if groups were laid out sequentially.\n        # Shape: [num_groups, group_size]\n        base_orig_log_expert_ids_per_group = (torch.arange(num_groups, device=weight.device, dtype=torch.int64) * group_size).unsqueeze(-1) + \\\n                                             torch.arange(group_size, device=weight.device, dtype=torch.int64)\n\n        # 4. Use sorted_group_indices to gather original logical expert IDs and their weights.\n        # Expand sorted_group_indices to match the shape of the data being gathered (groups, then experts within groups).\n        sorted_group_indices_expanded = sorted_group_indices.unsqueeze(-1).expand(-1, -1, group_size)\n\n        # Reshape the original weight tensor to [num_layers, num_groups, group_size] for gathering.\n        weight_reshaped_by_group = weight.view(num_layers, num_groups, group_size)\n\n        # Gather the weights based on the sorted group order.\n        # mlog_weights will have shape [num_layers, num_groups, group_size]\n        mlog_weights = weight_reshaped_by_group.gather(1, sorted_group_indices_expanded)\n\n        # Gather the original logical expert IDs based on the sorted group order.\n        # mlog_to_orig_log_map will have shape [num_layers, num_groups, group_size]\n        mlog_to_orig_log_map = base_orig_log_expert_ids_per_group.unsqueeze(0).expand(num_layers, -1, -1).gather(1, sorted_group_indices_expanded)\n\n        # 5. Reshape to [num_layers, num_nodes, groups_per_node * group_size]\n        # This aligns the data such that experts assigned to the same node are contiguous.\n        num_middle_experts_per_node = groups_per_node * group_size\n        mlog_weights = mlog_weights.view(num_layers, num_nodes, num_middle_experts_per_node)\n        mlog_to_orig_log_map = mlog_to_orig_log_map.view(num_layers, num_nodes, num_middle_experts_per_node)\n\n        # Now, `mlog_weights`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n        # And `mlog_to_orig_log_map`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n\n        # Step 2: Replicate experts within each node.\n        # `mlog_weights` are the weights for experts within each node.\n        # `num_physical_experts // num_nodes` is the target number of physical experts per node.\n        # `replicate_experts` takes `[X, num_log]` and returns `phy2log`, `rank`, `logcnt`.\n        # Here, X = num_layers * num_nodes, num_log = num_logical_experts // num_nodes.\n        # So, we need to reshape `mlog_weights` and `mlog_to_orig_log_map`.\n\n        num_middle_experts = num_logical_experts // num_nodes\n        reshaped_mlog_weights = mlog_weights.view(-1, num_middle_experts)\n        reshaped_mlog_to_orig_log_map = mlog_to_orig_log_map.view(-1, num_middle_experts)\n\n        # `phy2mlog`: [num_layers * num_nodes, num_physical_experts // num_nodes]\n        # `phyrank`: [num_layers * num_nodes, num_physical_experts // num_nodes]\n        # `mlogcnt`: [num_layers * num_nodes, num_middle_experts]\n        phy2mlog, phyrank, mlogcnt = replicate_experts(\n            reshaped_mlog_weights, num_physical_experts // num_nodes)\n\n        # Step 3: Pack physical experts to GPUs.\n        # `phy2mlog` maps physical experts (within each node) to middle logical experts.\n        # `phy2mlog` is `[num_layers * num_nodes, num_phy_per_node]`\n        # `phyrank` is `[num_layers * num_nodes, num_phy_per_node]`\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`\n\n        # We need to map these back to global physical expert indices.\n        # The `phy2mlog` refers to the *middle* logical experts.\n        # The `replicate_experts` function returns `phy2log` which is the *original* logical expert ID.\n        # So, `phy2mlog` from `replicate_experts` is actually `[num_layers*num_nodes, num_phy_per_node]`,\n        # where each entry is an index into the *middle* logical experts.\n\n        # We need to map these `phy2mlog` indices back to original logical expert IDs.\n        # `reshaped_mlog_to_orig_log_map`: [num_layers * num_nodes, num_middle_experts]\n        # `final_phy2log_per_node`: [num_layers * num_nodes, num_phy_per_node]\n        final_phy2log_per_node = reshaped_mlog_to_orig_log_map.gather(-1, phy2mlog)\n\n        # `phyrank` is the replica rank for the middle experts.\n        # We need to combine this with `mlogcnt` to get the final `logcnt`.\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`. It tells us how many replicas each middle expert has.\n        # The `phyrank` is the rank of the replica for a *specific* middle expert.\n\n        # The `logcnt` returned by `replicate_experts` is `[X, num_log]`, which is `[num_layers*num_nodes, num_middle_experts]`.\n        # We need to map this back to `[num_layers, num_logical_experts]`.\n\n        # Let's use the `mlogcnt` directly to get the total count of replicas for each logical expert.\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`.\n        # We need to sum/average counts based on the `mlog_to_orig_log_map`.\n\n        # Let's refine the output mapping:\n        # `physical_to_logical_map`: [num_layers, num_replicas]\n        # `logical_to_physical_map`: [num_layers, num_logical_experts, X]\n        # `expert_count`: [num_layers, num_logical_experts]\n\n        # `final_phy2log_per_node`: [num_layers * num_nodes, num_phy_per_node]\n        # This is the logical expert ID for each physical expert *within its node*.\n        # We need to convert this to global physical expert IDs.\n        # Physical experts are grouped by GPU. `num_gpus` total, `num_gpus // num_nodes` per node.\n        # `phy_experts_per_gpu` = `num_physical_experts // num_gpus`.\n        # So, each GPU has `phy_experts_per_gpu` physical experts.\n        # Each Node has `num_gpus // num_nodes` GPUs.\n        # Total physical experts per node = `(num_gpus // num_nodes) * phy_experts_per_gpu`\n        # This should equal `num_physical_experts // num_nodes`.\n\n        # `final_phy2log_per_node`: [num_layers, num_nodes, num_phy_per_node]\n        # `final_rank_per_node`: [num_layers, num_nodes, num_phy_per_node] (this is `phyrank`)\n\n        # We need to flatten this to `[num_layers, num_physical_experts]`.\n        # `num_physical_experts = num_nodes * num_phy_per_node`.\n\n        # `final_phy2log_global`: [num_layers, num_physical_experts]\n        # `final_rank_global`: [num_layers, num_physical_experts]\n\n        # The `mlog_to_orig_log_map` correctly maps middle experts to original logical experts.\n        # `phy2mlog` maps physical experts (within a node) to middle experts.\n        # So, `phy2mlog.gather(-1, final_phy2log_per_node)` gives the logical expert ID.\n        # This is what `final_phy2log_per_node` already represents.\n\n        # Let's rename `final_phy2log_per_node` to `phy2log_per_node` for clarity.\n        phy2log_per_node = final_phy2log_per_node\n        rank_per_node = phyrank # [num_layers * num_nodes, num_phy_per_node]\n\n        # Flatten these to get global mappings for physical experts.\n        phy2log_global = phy2log_per_node.view(num_layers, -1)\n        rank_global = rank_per_node.view(num_layers, -1)\n\n        # Now, let's construct the `logcnt` (expert_count).\n        # `mlogcnt`: [num_layers * num_nodes, num_middle_experts]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, num_middle_experts]\n\n        # We need to aggregate `mlogcnt` based on `mlog_to_orig_log_map`.\n        # For each logical expert, its total count is the sum of counts of its constituent middle experts.\n        # This aggregation needs to be done carefully.\n\n        # Let's create a tensor for `logcnt`: [num_layers, num_logical_experts] initialized to zeros.\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n\n        # Iterate through each layer and node.\n        for i in range(num_layers):\n            for node_idx in range(num_nodes):\n                # For the middle experts belonging to this node and layer:\n                middle_expert_indices = mlog_to_orig_log_map[i, node_idx] # [num_middle_experts]\n                counts_for_middle_experts = mlogcnt[i * num_nodes + node_idx] # [num_middle_experts]\n\n                # Add these counts to the corresponding original logical experts.\n                # `torch.add.at` allows for sparse updates.\n                expert_count[i].scatter_add_(0, middle_expert_indices, counts_for_middle_experts)\n\n        # The `logcnt` from `replicate_experts` is the number of replicas for the middle experts.\n        # We need to make sure the `expert_count` is correct.\n        # The `replicate_experts` function returns `logcnt` which is the number of replicas for each logical expert it processed.\n        # So, `mlogcnt` is the number of replicas for each middle expert.\n\n        # The final `expert_count` should be `[num_layers, num_logical_experts]`.\n        # We need to map `mlogcnt` back.\n\n        # Let's create `expert_count` of size `[num_layers, num_logical_experts]` initialized to 0.\n        # Then, for each layer, node, and middle expert:\n        # `expert_count[layer, mlog_to_orig_log_map[layer, node, middle_idx]] += mlogcnt[layer*num_nodes + node, middle_idx]`\n\n        # This aggregation is correct.\n\n        # Now, we need to construct `logical_to_physical_map`.\n        # `logical_to_physical_map`: [num_layers, num_logical_experts, X] where X is max replicas.\n        # `phy2log_global`: [num_layers, num_physical_experts]\n        # `rank_global`: [num_layers, num_physical_experts]\n\n        # We need to group physical experts by their logical expert ID and rank.\n        max_replicas_per_expert = (num_physical_experts + num_logical_experts - 1) // num_logical_experts # Ceiling division\n\n        logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert), -1, dtype=torch.int64, device=weight.device)\n\n        # Iterate through all physical experts\n        for layer_idx in range(num_layers):\n            for phys_idx in range(num_physical_experts):\n                log_expert_id = phy2log_global[layer_idx, phys_idx]\n                replica_rank = rank_global[layer_idx, phys_idx]\n\n                if log_expert_id >= 0 and log_expert_id < num_logical_experts:\n                    # Place the physical expert ID into the correct slot based on its logical ID and rank.\n                    # We need to ensure we don't exceed `max_replicas_per_expert`.\n                    # The `replica_rank` should correspond to the index in the third dimension.\n                    if replica_rank < max_replicas_per_expert:\n                        logical_to_physical_map[layer_idx, log_expert_id, replica_rank] = phys_idx\n                    else:\n                        # This case indicates an issue or that max_replicas_per_expert needs to be larger.\n                        # For now, we can assert or log a warning.\n                        pass # Or handle error\n\n        # The `expert_count` is already computed.\n\n        return phy2log_global, logical_to_physical_map, expert_count\n\n    else:\n        # Global balancing strategy: treat all experts as one group, balance across num_gpus.\n        # This is equivalent to calling the hierarchical function with num_nodes=1, num_groups=1.\n        # num_groups = 1 means group_size = num_logical_experts.\n        # num_nodes = 1 means groups_per_node = 1.\n        # The entire logic simplifies.\n\n        # Step 1: Pack logical experts to nodes (here, nodes = GPUs conceptually).\n        # `tokens_per_group` becomes `weight` itself since num_groups=1.\n        # `balanced_packing(weight, num_gpus)`\n        # `group_pack_index`: [num_layers, num_logical_experts] -> GPU assignment for each logical expert.\n        # `group_rank_in_pack`: [num_layers, num_logical_experts] -> rank within GPU for each logical expert.\n        # `num_nodes` becomes `num_gpus`. `groups_per_node` becomes `num_logical_experts // num_gpus`.\n\n        # We need to adapt `balanced_packing` to work with `num_packs = num_gpus`.\n        # The `groups_per_pack` in `balanced_packing` refers to items per pack.\n        # Here, items are logical experts. So, `groups_per_pack` = `num_logical_experts // num_gpus`.\n        # `num_packs` = `num_gpus`.\n\n        pack_index_gpu, rank_in_gpu = balanced_packing(weight, num_gpus)\n\n        # `pack_index_gpu`: [num_layers, num_logical_experts] -> GPU ID for each logical expert.\n        # `rank_in_gpu`: [num_layers, num_logical_experts] -> rank within GPU for each logical expert.\n\n        # Step 2: Replicate experts within GPUs.\n        # We need to create tensors for `replicate_experts` that represent experts *per GPU*.\n        # `num_logical_experts_per_gpu` = `num_logical_experts // num_gpus`.\n        # `num_physical_experts_per_gpu` = `num_physical_experts // num_gpus`.\n\n        # For each GPU, we have a set of logical experts.\n        # We need to gather weights for logical experts assigned to the same GPU.\n        # `gpu_logical_weights`: [num_gpus, num_logical_experts_per_gpu, num_layers] (transposed for replicate_experts)\n        # `gpu_logical_to_orig_map`: [num_gpus, num_logical_experts_per_gpu]\n\n        gpu_logical_weights_list = []\n        gpu_logical_to_orig_map_list = []\n\n        num_logical_experts_per_gpu = num_logical_experts // num_gpus\n\n        for gpu_idx in range(num_gpus):\n            # Find logical experts assigned to this GPU\n            assigned_logical_mask = (pack_index_gpu == gpu_idx)\n            # Get their ranks within the GPU\n            ranks_on_gpu = rank_in_gpu[assigned_logical_mask]\n            # Sort by rank\n            sorted_ranks_on_gpu, sort_indices_on_gpu = ranks_on_gpu.sort()\n            # Get original logical expert indices\n            assigned_logical_indices = torch.arange(num_logical_experts, device=pack_index_gpu.device)[assigned_logical_mask][sort_indices_on_gpu]\n\n            if len(assigned_logical_indices) > 0:\n                # Gather weights for these logical experts\n                # We need weights for each layer.\n                # `weight` is [num_layers, num_logical_experts].\n                # `assigned_logical_indices` is [num_logical_experts_per_gpu].\n                # We want `gpu_logical_weights` of shape [num_layers, num_logical_experts_per_gpu].\n                gpu_logical_weights = weight[:, assigned_logical_indices]\n                gpu_logical_weights_list.append(gpu_logical_weights)\n                gpu_logical_to_orig_map_list.append(assigned_logical_indices)\n            else:\n                # If a GPU gets no logical experts (shouldn't happen if num_logical_experts >= num_gpus)\n                gpu_logical_weights_list.append(torch.empty(num_layers, 0, dtype=weight.dtype, device=weight.device))\n                gpu_logical_to_orig_map_list.append(torch.empty(0, dtype=torch.int64, device=weight.device))\n\n        # `gpu_logical_weights`: [num_gpus, num_layers, num_logical_experts_per_gpu]\n        # `gpu_logical_to_orig_map`: [num_gpus, num_logical_experts_per_gpu]\n        gpu_logical_weights = torch.stack(gpu_logical_weights_list)\n        gpu_logical_to_orig_map = torch.stack(gpu_logical_to_orig_map_list)\n\n        # Reshape for `replicate_experts`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        reshaped_gpu_logical_weights = gpu_logical_weights.view(-1, num_logical_experts_per_gpu)\n        reshaped_gpu_logical_to_orig_map = gpu_logical_to_orig_map.view(-1, num_logical_experts_per_gpu)\n\n        # `replicate_experts` is called with `num_phy = num_physical_experts // num_gpus` (target physical experts per GPU).\n        # `X` becomes `num_gpus * num_layers`. `num_log` becomes `num_logical_experts_per_gpu`.\n        # `phy2log_gpu`: [num_gpus * num_layers, num_phy_per_gpu]\n        # `phyrank_gpu`: [num_gpus * num_layers, num_phy_per_gpu]\n        # `gpu_logcnt`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        phy2log_gpu, phyrank_gpu, gpu_logcnt = replicate_experts(\n            reshaped_gpu_logical_weights, num_physical_experts // num_gpus)\n\n        # Map `phy2log_gpu` back to original logical expert IDs using `reshaped_gpu_logical_to_orig_map`.\n        # `phy2log_global`: [num_gpus * num_layers, num_phy_per_gpu]\n        phy2log_global = reshaped_gpu_logical_to_orig_map.gather(-1, phy2log_gpu)\n\n        # Flatten to get the final `physical_to_logical_map` and `rank`.\n        # The `rank_global` is `phyrank_gpu`.\n        rank_global = phyrank_gpu\n\n        # Construct `expert_count` ([num_layers, num_logical_experts]).\n        # `gpu_logcnt`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n\n        for gpu_layer_idx in range(num_gpus * num_layers):\n            layer_idx = gpu_layer_idx // num_gpus\n            # Get the original logical expert indices for this gpu_layer combination\n            orig_log_indices = reshaped_gpu_logical_to_orig_map[gpu_layer_idx] # [num_logical_experts_per_gpu]\n            counts_for_orig_logs = gpu_logcnt[gpu_layer_idx] # [num_logical_experts_per_gpu]\n\n            # Add these counts to the correct positions in the expert_count tensor\n            expert_count[layer_idx].scatter_add_(0, orig_log_indices, counts_for_orig_logs)\n\n        # Construct `logical_to_physical_map` ([num_layers, num_logical_experts, X])\n        max_replicas_per_expert = (num_physical_experts + num_logical_experts - 1) // num_logical_experts\n\n        logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert), -1, dtype=torch.int64, device=weight.device)\n\n        # Need to map `phy2log_global` and `rank_global` to the correct positions.\n        # `phy2log_global` are indices into the *original* logical experts.\n        # `rank_global` is the replica rank for that original logical expert.\n\n        # Reshape to get mappings per layer and then iterate through physical experts.\n        phy2log_global_reshaped = phy2log_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous()\n        rank_global_reshaped = rank_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous()\n\n        # Iterate through all physical experts\n        for layer_idx in range(num_layers):\n            for gpu_idx in range(num_gpus):\n                for phys_idx_in_gpu in range(num_physical_experts // num_gpus):\n                    # Get the global physical expert index\n                    global_phys_idx = gpu_idx * (num_physical_experts // num_gpus) + phys_idx_in_gpu\n\n                    log_expert_id = phy2log_global_reshaped[layer_idx, gpu_idx, phys_idx_in_gpu]\n                    replica_rank = rank_global_reshaped[layer_idx, gpu_idx, phys_idx_in_gpu]\n\n                    if log_expert_id >= 0 and log_expert_id < num_logical_experts:\n                        if replica_rank < max_replicas_per_expert:\n                            logical_to_physical_map[layer_idx, log_expert_id, replica_rank] = global_phys_idx\n                        else:\n                            pass # Handle error or log\n\n        # The `phy2log_global` needs to be reshaped to `[num_layers, num_replicas]`\n        # where `num_replicas` is `num_physical_experts`.\n        # `phy2log_global` is currently `[num_gpus * num_layers, num_phy_per_gpu]`.\n        # We need to combine these.\n        # The order of physical experts should be consistent.\n        # Let's reconstruct `phy2log_global` to be `[num_layers, num_physical_experts]`.\n\n        final_phy2log_global = torch.empty((num_layers, num_physical_experts), dtype=torch.int64, device=weight.device)\n        current_phys_idx = 0\n        for gpu_idx in range(num_gpus):\n            for layer_idx in range(num_layers):\n                # Calculate the range of physical experts for this GPU and layer combination\n                start_phys_idx = layer_idx * num_gpus * (num_physical_experts // num_gpus) + gpu_idx * (num_physical_experts // num_gpus)\n                end_phys_idx = start_phys_idx + (num_physical_experts // num_gpus)\n\n                # The `phy2log_global` calculated earlier is `[num_gpus * num_layers, num_phy_per_gpu]`\n                # We need to map it correctly.\n                # `phy2log_gpu` was `[num_gpus * num_layers, num_phy_per_gpu]`\n                # `reshaped_gpu_logical_to_orig_map` was `[num_gpus * num_layers, num_phy_per_gpu]`\n                # `phy2log_global = reshaped_gpu_logical_to_orig_map.gather(-1, phy2log_gpu)`\n\n                # The indices for `phy2log_global` should be:\n                # For layer 0, GPU 0: phys experts 0 to num_phy_per_gpu - 1\n                # For layer 0, GPU 1: phys experts num_phy_per_gpu to 2*num_phy_per_gpu - 1\n                # ...\n                # For layer 1, GPU 0: phys experts num_gpus * num_phy_per_gpu to (num_gpus+1)*num_phy_per_gpu - 1\n\n                # Let's re-index the `phy2log_global` and `rank_global` to match `[num_layers, num_physical_experts]`\n                # The current `phy2log_global` is effectively `[num_layers * num_gpus, num_phy_per_gpu]`\n                # We need to rearrange it.\n\n                # Example: num_layers=2, num_gpus=2, num_phy_per_gpu=2. num_physical_experts=4.\n                # phy2log_global:\n                # [layer0_gpu0_phys0, layer0_gpu0_phys1,\n                #  layer0_gpu1_phys0, layer0_gpu1_phys1,\n                #  layer1_gpu0_phys0, layer1_gpu0_phys1,\n                #  layer1_gpu1_phys0, layer1_gpu1_phys1]\n                # Reshaped to [num_gpus*num_layers, num_phy_per_gpu] -> [4, 2]\n                # We want [num_layers, num_physical_experts] -> [2, 4]\n\n                # The current `phy2log_global` is `[num_gpus * num_layers, num_phy_per_gpu]`.\n                # Permute and reshape:\n                # `view(num_gpus, num_layers, -1)` gives `[num_gpus, num_layers, num_phy_per_gpu]`\n                # `permute(1, 0, 2)` gives `[num_layers, num_gpus, num_phy_per_gpu]`\n                # `contiguous().view(num_layers, -1)` gives `[num_layers, num_gpus * num_phy_per_gpu]` = `[num_layers, num_physical_experts]`\n\n                final_phy2log_global = phy2log_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous().view(num_layers, num_physical_experts)\n                final_rank_global = rank_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous().view(num_layers, num_physical_experts)\n\n        # Return the results for global balancing.\n        return final_phy2log_global, final_rank_global, expert_count\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    # Keep weights on their original device to avoid unnecessary transfers.\n    # Ensure float type for calculations.\n    weight = weight.float()\n    \n    # num_replicas is equivalent to num_physical_experts used internally\n    num_physical_experts = num_replicas\n\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        # The function now returns phy2log_global, rank_global, expert_count\n        phy2log_global, rank_global, expert_count = rebalance_experts_hierarchical(\n            weight, num_physical_experts, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        # The function now returns phy2log_global, rank_global, expert_count\n        phy2log_global, rank_global, expert_count = rebalance_experts_hierarchical(\n            weight, num_physical_experts, 1, 1, num_gpus)\n\n    # Construct logical_to_physical_map here, common to both strategies.\n    # Max replicas per expert: This ensures `logical_to_physical_map` is large enough.\n    # It must be at least 1.\n    if num_logical_experts == 0: # Handle edge case to avoid division by zero\n        max_replicas_per_expert = 0\n    else:\n        max_replicas_per_expert = (num_physical_experts + num_logical_experts - 1) // num_logical_experts\n\n    # Initialize the map with -1 (indicating no physical expert assigned)\n    logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert), -1, dtype=torch.int64, device=weight.device)\n\n    # Create global physical expert indices.\n    global_physical_indices = torch.arange(num_replicas, dtype=torch.int64, device=weight.device).unsqueeze(0).expand(num_layers, -1)\n\n    # Prepare indices for index_put_ operation.\n    # Create a mask for valid placements (should ideally always be true if logic is sound)\n    valid_mask = (phy2log_global >= 0) & \\\n                 (phy2log_global < num_logical_experts) & \\\n                 (rank_global >= 0) & \\\n                 (rank_global < max_replicas_per_expert)\n\n    # Apply the mask to all relevant tensors to get only valid assignments\n    # Reshape to 1D as index_put_ expects 1D index tensors for each dimension.\n    layer_indices = torch.arange(num_layers, device=weight.device).unsqueeze(1).expand(-1, num_replicas)[valid_mask]\n    log_expert_indices = phy2log_global[valid_mask]\n    rank_indices = rank_global[valid_mask]\n    values_to_put = global_physical_indices[valid_mask]\n\n    # Use index_put_ for direct assignment based on multiple index tensors.\n    # This is generally more efficient than Python loops for large tensors.\n    if values_to_put.numel() > 0: # Avoid calling index_put_ with empty indices\n        logical_to_physical_map.index_put_((layer_indices, log_expert_indices, rank_indices), values_to_put)\n\n    # The `expert_count` is already computed by the balancing functions.\n    # `phy2log_global` is the first return value.\n    return phy2log_global, rank_global, expert_count\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["This program aims to rebalance experts in a Mixture-of-Experts (MoE) model to improve load balancing and efficiency. The current implementation has an error related to tensor size mismatches and shows regressions in key performance metrics.\n\nHere are a few suggestions for improvement:\n\n### Improvement 1: Fix Tensor Size Mismatch Error and Improve `balanced_packing` Efficiency\n\nThe error \"The size of tensor a (288) must match the size of tensor b (2) at non-singleton dimension 2\" likely originates from an incorrect reshaping or indexing operation. Looking at the `balanced_packing` function, the loop iterating through `num_layers` and `num_groups` can be a bottleneck. If `num_layers` is large, this nested loop is inefficient. We can try to vectorize parts of this operation.\n\nThe core issue in `balanced_packing` seems to be how `pack_index` and `rank_in_pack` are populated. The loop iterates through `num_layers` and `num_groups`, and within that, it finds the `chosen_pack_idx`. This suggests that the assignments are made one by one.\n\nLet's analyze the current `balanced_packing`:\n- It sorts weights for each layer.\n- Then, for each layer, it iterates through sorted groups.\n- Inside the loop, it finds the least loaded pack.\n- It updates `pack_index`, `rank_in_pack`, `pack_loads`, `pack_counts`.\n\nThe issue could be in how `pack_index` and `rank_in_pack` are being populated with respect to the `sorted_indices`.\n\n**Proposed Change:**\nVectorize the assignment process within `balanced_packing`. Instead of iterating group by group for each layer, we can try to process all groups for all layers simultaneously or in larger batches. This will require careful handling of pack assignments and updates.\n\nConsider the case where `num_layers` is large. The current loop structure means `num_layers * num_groups` iterations of the inner logic.\n\nA more efficient approach might involve:\n1. Flattening `weight` to `[num_layers * num_groups, 1]` to sort all items together.\n2. Using a more advanced bin packing algorithm if applicable, or a greedy approach that can be vectorized.\n3. Reconstructing the `pack_index` and `rank_in_pack` from the flattened assignments.\n\nHowever, the problem states that \"each bin contains exactly n/m objects\". This implies a fixed size for each pack. The current greedy approach of assigning to the least loaded pack might not strictly adhere to this if not handled carefully.\n\nLet's focus on optimizing the existing loop structure for better performance and fixing potential indexing issues. The error suggests a mismatch in the number of items being assigned vs. the number of slots.\n\nOne common cause of such errors is when `torch.where` or `torch.argmin` is applied to tensors of different shapes or when the output of these operations is used to index into another tensor of a different size.\n\n**Refined Proposed Change for `balanced_packing`:**\nFocus on vectorizing the assignment of items to packs. The current `for i in range(num_layers): for g in range(num_groups):` structure is problematic. We can try to rephrase the problem: for each layer, we have `num_groups` items to pack into `num_packs`.\n\nLet's consider a vectorized approach for assigning items within a single layer.\n1. Sort all `num_groups` items for a layer: `sorted_weights`, `sorted_indices`.\n2. We need to assign these `num_groups` items into `num_packs` packs, each of size `groups_per_pack`.\n3. A greedy assignment can be done by keeping track of the current load of each pack.\n\nInstead of iterating through `g` from `0` to `num_groups-1`, we can try to assign blocks of items.\nFor example, for a layer, we have `sorted_weights` and `sorted_indices`.\nWe can assign the first `groups_per_pack` items to pack 0, the next `groups_per_pack` to pack 1, and so on. However, this is not load-balancing.\n\nThe current implementation aims for load balancing by picking the least loaded pack.\nThe error points to a mismatch, possibly in how `original_group_idx` is used.\n`original_group_idx = sorted_indices[i, g]`\n`pack_index[i, original_group_idx] = chosen_pack_idx`\nThis looks like it's trying to populate `pack_index` for the original expert index.\n\nThe tensor `weight` has shape `[num_layers, num_groups]`.\nThe `pack_index` is `[num_layers, num_groups]`.\nThe `sorted_indices` is `[num_layers, num_groups]`.\n\nThe error `tensor a (288) must match tensor b (2)` might imply that `original_group_idx` is producing an unexpected dimension or value.\n\nLet's assume `weight.shape` is `[X, n]`. In `balanced_packing`, `weight` is `[num_layers, num_groups]`.\nSo `X = num_layers`, `n = num_groups`.\nThe error mentions dimension 2. This is confusing because `weight` is 2D. It's possible `weight` is being reshaped or used in a context where it becomes 3D.\n\nIf `weight.shape` is actually `[num_layers, num_groups, some_other_dim]`, then the error makes sense. But the docstring says `weight: [X, n]`, implying `num_groups` is `n`.\n\nLet's assume the error is in the loop's indexing or population of `pack_index`.\nThe problem statement is about improving load balancing and efficiency.\n\n**Revised Strategy for `balanced_packing`:**\nThe current implementation iterates through each layer and then each group within that layer. This is `O(num_layers * num_groups * log(num_groups))` for sorting and `O(num_layers * num_groups * num_packs)` for assignment if `argmin` is `O(num_packs)`.\n\nTo improve efficiency, we can try to vectorize the assignment part.\nFor a given layer, after sorting, we have `sorted_indices` and `sorted_weights`. We need to assign these `num_groups` items into `num_packs` bins, each bin taking `groups_per_pack` items.\n\nConsider the `num_layers` dimension. If `num_layers` is large, the outer loop is slow.\nWe can flatten `weight` to `[num_layers * num_groups, 1]` for sorting, but then we lose the layer information.\n\nA better approach might be to use a more efficient assignment strategy.\nFor each layer, we have `num_groups` items to distribute into `num_packs`.\nThe current greedy approach is reasonable. The bottleneck is likely the Python loops.\n\nLet's try to optimize the inner loop of `balanced_packing`.\nThe line `chosen_pack_idx = torch.argmin(current_pack_loads)` is called `num_layers * num_groups` times.\n`current_pack_loads` has shape `[num_packs]`.\n\nIf we can perform the assignments for all groups in a layer more efficiently, it would help.\nThis problem is similar to multi-way number partitioning or multi-dimensional bin packing, which are NP-hard. The greedy approach is a heuristic.\n\n**Specific Change for `balanced_packing`:**\nLet's focus on making the assignment process more efficient. The current assignment loop is:\n```python\n    for i in range(num_layers):\n        for g in range(num_groups):\n            # ... find chosen_pack_idx ...\n            pack_index[i, original_group_idx] = chosen_pack_idx\n            rank_in_pack[i, original_group_idx] = pack_counts[i, chosen_pack_idx]\n            # ... update pack_loads and pack_counts ...\n```\nThis loop structure might be what's causing the error due to how `original_group_idx` is used.\n\nIf `sorted_indices[i, g]` is a single index, then `pack_index[i, original_group_idx]` is fine.\nThe error suggests `original_group_idx` might have a shape mismatch with the second dimension of `pack_index`.\n\nLet's consider the possibility that `weight` is not `[num_layers, num_groups]` but `[num_layers, num_groups, some_feature_dim]`. If so, the sorting and packing logic might need to operate on the sum or mean of the last dimension. However, the docstring says `weight: [X, n]`, where `X=num_layers` and `n=num_groups`.\n\nAssuming `weight` is indeed `[num_layers, num_groups]`:\nThe `pack_index` is `[num_layers, num_groups]`.\n`sorted_indices` is `[num_layers, num_groups]`.\n`original_group_idx = sorted_indices[i, g]` will be a scalar index.\n`pack_index[i, original_group_idx]` should be valid.\n\nThe error `size of tensor a (288) must match the size of tensor b (2)` could be from an operation *after* this, or from a misunderstanding of the tensor shapes.\n\nLet's try to rewrite `balanced_packing` to be more explicit about the operations and potentially more vectorized.\n\n```python\n# Original balanced_packing\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    device = weight.device\n\n    # Sort weights and get indices\n    sorted_weights, sorted_indices = weight.float().sort(-1, descending=True)\n\n    # Initialize pack_index and rank_in_pack on the same device as weight\n    pack_index = torch.full((num_layers, num_groups), -1, dtype=torch.int64, device=device)\n    rank_in_pack = torch.full((num_layers, num_groups), -1, dtype=torch.int64, device=device)\n\n    # Initialize pack_loads and pack_counts on the same device as weight\n    pack_loads = torch.zeros(num_layers, num_packs, dtype=torch.float32, device=device)\n    pack_counts = torch.zeros(num_layers, num_packs, dtype=torch.int64, device=device)\n\n    # Iterate through sorted items and assign them to the least loaded pack\n    for i in range(num_layers):\n        for g in range(num_groups):\n            available_packs_mask = (pack_counts[i] < groups_per_pack)\n            current_pack_loads = torch.where(available_packs_mask, pack_loads[i], float('inf'))\n            chosen_pack_idx = torch.argmin(current_pack_loads)\n\n            original_group_idx = sorted_indices[i, g]\n            original_group_weight = weight[i, original_group_idx]\n\n            pack_index[i, original_group_idx] = chosen_pack_idx\n            rank_in_pack[i, original_group_idx] = pack_counts[i, chosen_pack_idx]\n\n            pack_loads[i, chosen_pack_idx] += original_group_weight\n            pack_counts[i, chosen_pack_idx] += 1\n\n    return pack_index, rank_in_pack\n\n```\n\n**Proposed Change for `balanced_packing`:**\nInstead of the nested loop, let's try to use `torch.gather` and `torch.sort` for a more vectorized approach for assignment.\n\nThe core idea is to simulate the greedy assignment without explicit Python loops.\nFor each layer:\n1. Get `sorted_weights` and `sorted_indices`.\n2. We need to assign `num_groups` items into `num_packs` bins. Each bin must have `groups_per_pack` items.\n3. The greedy strategy is: for each item (in sorted order), assign it to the least loaded bin that is not yet full.\n\nThis is still tricky to vectorize perfectly due to the \"not yet full\" constraint.\nHowever, we can use `torch.cumsum` and `torch.searchsorted` to manage the assignment.\n\nConsider a single layer:\n`sorted_weights`: [num_groups]\n`sorted_indices`: [num_groups]\n\nWe want to assign these to `num_packs`.\nThe goal is to minimize the maximum sum of weights in any pack.\nAnd each pack must have `groups_per_pack` items.\n\nA common technique for this type of problem is to use cumulative sums after sorting.\nIf we simply assign the first `groups_per_pack` to pack 0, next `groups_per_pack` to pack 1, etc., it's not balanced.\n\nThe current approach is to iterate through sorted items and assign to the minimum load bin.\nLet's keep the loop structure but optimize it.\nThe `torch.argmin` is applied to `current_pack_loads` which is `[num_packs]`. This is `O(num_packs)`.\nThe total complexity is roughly `O(num_layers * num_groups * (log(num_groups) + num_packs))`.\n\nIf `num_packs` is small, this might be acceptable.\nThe error is the primary concern.\n\nLet's consider the `num_nodes` and `num_gpus` parameters in `rebalance_experts_hierarchical`.\nThe logic for `num_groups % num_nodes != 0` seems to be a fallback to global balancing.\n\n**Focus on the error:**\nThe error `size of tensor a (288) must match the size of tensor b (2) at non-singleton dimension 2` suggests that some operation expects a dimension of size 2, but it's getting 288, or vice-versa. This often happens with:\n- `torch.cat` or `torch.stack` with mismatched shapes.\n- Indexing operations where the index tensor shape doesn't align with the tensor being indexed.\n- `torch.reshape` or `view` operations that don't preserve total elements or dimension compatibility.\n\nLet's look at `rebalance_experts_hierarchical` where the `balanced_packing` function is called.\nIt's called with `tokens_per_group_layer`, which has shape `[num_layers, num_groups]`.\nThe `num_packs` argument is `num_nodes`.\nSo `balanced_packing` receives `[num_layers, num_groups]` and `num_nodes`.\nInside `balanced_packing`, `weight` is `[num_layers, num_groups]`, `num_packs` is `num_nodes`.\n`num_groups` in `balanced_packing` refers to the second dim of its input, which is `num_groups` from the outer scope.\n`num_groups % num_packs == 0` is checked.\n\nThe error message mentions dimension 2. If `weight` is `[num_layers, num_groups]`, then dimension 2 does not exist. This implies that `weight` might be reshaped or the error originates from a subsequent operation that uses `pack_index` or `rank_in_pack`.\n\nLet's assume `weight` *is* `[num_layers, num_groups]`.\nThe `pack_index` and `rank_in_pack` are initialized as `[num_layers, num_groups]`.\nThe loop populates `pack_index[i, original_group_idx]` and `rank_in_pack[i, original_group_idx]`.\n`original_group_idx` is `sorted_indices[i, g]`, which is a scalar.\nThis should be correct.\n\nCould the error be from `torch.where` or `torch.argmin`?\n`current_pack_loads = torch.where(available_packs_mask, pack_loads[i], float('inf'))`\n`pack_loads[i]` is `[num_packs]`. `available_packs_mask` is `[num_packs]`. The output `current_pack_loads` is `[num_packs]`.\n`torch.argmin` on `[num_packs]` returns a scalar.\n\nThe error might be in how these results are used later, or in a different part of the code that's not shown.\nHowever, since the error is reported *during* the execution of `rebalance_experts`, it's likely within the called functions.\n\n**Hypothesis:** The error is related to how `weight` is used in `balanced_packing` if its shape is not as expected, or how the output `pack_index` and `rank_in_pack` are used subsequently.\n\nLet's consider the `replicate_experts` function. It takes `weight: [X, num_log]`.\nIn `rebalance_experts_hierarchical`, it's called with `reshaped_mlog_weights` which has shape `[num_layers * num_nodes, num_middle_experts]`.\nHere `X = num_layers * num_nodes` and `num_log = num_middle_experts`.\nThis seems to use the output of `balanced_packing` implicitly.\n\nLet's try to focus on a specific improvement in `balanced_packing` that addresses potential inefficiency and is likely to fix indexing issues.\n\n**Proposed Change for `balanced_packing`:**\nThe current assignment loop iterates `num_layers * num_groups` times. We can try to vectorize this assignment for each layer.\n\nFor a given layer `i`:\n1. `sorted_weights[i, :]`, `sorted_indices[i, :]`\n2. We need to assign `num_groups` items into `num_packs`. Each pack takes `groups_per_pack`.\n3. The current greedy assignment is: for each `g` from `0` to `num_groups-1`, find the least loaded pack `chosen_pack_idx`.\n   - `pack_index[i, sorted_indices[i, g]] = chosen_pack_idx`\n   - `rank_in_pack[i, sorted_indices[i, g]] = pack_counts[i, chosen_pack_idx]`\n\nThis process can be thought of as filling `num_packs` bins, each with `groups_per_pack` items.\nThe items are ordered by weight.\n\nLet's consider a different approach to fill the packs.\nIf we know `groups_per_pack`, we can assign the first `groups_per_pack` items to pack 0, the next `groups_per_pack` to pack 1, etc., but this is not greedy.\n\nThe greedy approach requires dynamically tracking pack loads.\nThe main inefficiency is the Python loop.\n\n**Vectorized Assignment Attempt:**\nFor each layer `i`:\nWe have `sorted_weights[i, :]` and `sorted_indices[i, :]`.\nWe need to determine `pack_assignments` for `sorted_indices[i, :]`.\n`pack_assignments`: [num_groups], where `pack_assignments[k]` is the pack for the `k`-th sorted item.\n\nThis is a variant of \"assign items to bins with capacity\".\nThe problem is that the bins are not independent; their loads change.\n\nLet's re-examine the error and the code.\nThe error mentions dimension 2. This is very specific.\nCould `weight` be `[num_layers, num_groups, 1]` or similar?\nIf `weight.shape` is `[num_layers, num_groups]`, then `weight.size(-1)` is `num_groups`.\nThe error `size of tensor a (288) must match the size of tensor b (2)` implies that some tensor `a` has a dimension of size 288, and another tensor `b` has a dimension of size 2, and these are expected to match. This mismatch is on dimension 2.\n\nIf `weight` has shape `[num_layers, num_groups]`, then `weight.shape[2]` does not exist.\nThis suggests that perhaps the `weight` tensor is being interpreted or used in a context where it has 3 dimensions, or that the error comes from a different tensor.\n\nConsider the possibility that `weight` is actually `[num_layers, num_groups, K]` for some `K`.\nIf `K=1`, then `weight.size(-1)` would be 1.\nIf `weight` is `[num_layers, num_groups, 288]`, and `sorted_indices` is `[num_layers, num_groups]`, then `weight[i, original_group_idx]` would be `[288]`.\nIf `pack_index` is `[num_layers, num_groups]`, then `pack_index[i, original_group_idx]` is trying to assign a scalar to a scalar location.\n\nThe error could be in `rank_in_pack[i, original_group_idx] = pack_counts[i, chosen_pack_idx]`.\n`pack_counts[i, chosen_pack_idx]` is a scalar. This should be fine.\n\nLet's assume the error is related to the loop structure and the way `pack_index` and `rank_in_pack` are populated.\n\n**Proposed Change:**\nRefactor `balanced_packing` to be more explicit about the tensor operations and potentially avoid the explicit loops.\nThe core task is to assign `num_groups` items (per layer) to `num_packs` bins, with each bin having capacity `groups_per_pack`.\n\n```python\n# Inside balanced_packing:\n    # ... initialization ...\n\n    # Vectorized assignment for each layer\n    for i in range(num_layers):\n        # For layer i, we have sorted_weights[i, :] and sorted_indices[i, :]\n        current_layer_sorted_weights = sorted_weights[i, :]\n        current_layer_sorted_indices = sorted_indices[i, :]\n\n        # We need to assign these num_groups items to num_packs.\n        # This is a variant of the multiway number partitioning problem.\n        # The greedy approach is to assign to the least loaded pack.\n\n        # We can simulate this using cumulative sums and careful indexing.\n        # Let's keep the iterative approach for correctness first, and then optimize.\n        # The error might be in how indices are used.\n\n        # Revisit: pack_index[i, original_group_idx] = chosen_pack_idx\n        # This means we are filling the `pack_index` tensor using the original indices.\n        # This is correct. The error might be from the size 288 vs 2.\n\n        # Let's consider `weight.size(-1)` in the `groups_per_pack == 1` case.\n        # `pack_index = torch.arange(weight.size(-1), ...).expand(weight.shape)`\n        # If `weight.shape` is `[num_layers, num_groups]`, then `weight.size(-1)` is `num_groups`.\n        # `torch.arange(num_groups)` creates `[0, 1, ..., num_groups-1]`.\n        # `.expand(weight.shape)` makes it `[num_layers, num_groups]`.\n        # This seems correct.\n\n        # The error might be in the global `rebalance_experts` function when constructing `logical_to_physical_map`.\n        # `phy2log_global` is `[num_layers, num_replicas]`.\n        # `rank_global` is `[num_layers, num_replicas]`.\n        # `max_replicas_per_expert` is calculated.\n        # `logical_to_physical_map` is `[num_layers, num_logical_experts, max_replicas_per_expert]`.\n\n        # The assignment `logical_to_physical_map.index_put_((layer_indices, log_expert_indices, rank_indices), values_to_put)`\n        # `layer_indices`: [N], `log_expert_indices`: [N], `rank_indices`: [N], `values_to_put`: [N]\n        # where N is the number of valid assignments.\n        # The indices must be within the bounds of `logical_to_physical_map`.\n        # `layer_indices` should be `0 <= layer_idx < num_layers`.\n        # `log_expert_indices` should be `0 <= log_expert_id < num_logical_experts`.\n        # `rank_indices` should be `0 <= rank < max_replicas_per_expert`.\n\n        # If `phy2log_global` contains values that lead to `log_expert_id` being out of bounds,\n        # or `rank_global` values out of bounds for `max_replicas_per_expert`, this could cause issues.\n        # The error message `tensor a (288) must match tensor b (2) at non-singleton dimension 2`\n        # is still cryptic.\n\n        # Let's assume the error is in the looping structure of `balanced_packing`.\n        # We need a way to assign items greedily without explicit loops over `num_groups` for each layer.\n\n        # Alternative approach for `balanced_packing` for a single layer:\n        # We have `sorted_indices` and `sorted_weights`.\n        # We need to assign `num_groups` items to `num_packs`, each pack takes `groups_per_pack`.\n        # The assignment is greedy: for each item (in sorted order), assign to the least loaded pack.\n\n        # This can be modeled as follows:\n        # Maintain `pack_loads` and `pack_counts` for the current layer.\n        # For each item `j` from `0` to `num_groups-1`:\n        #   `item_weight = current_layer_sorted_weights[j]`\n        #   `original_idx = current_layer_sorted_indices[j]`\n        #   Find `chosen_pack = argmin(pack_loads)` among packs where `pack_counts < groups_per_pack`.\n        #   Assign `pack_index[i, original_idx] = chosen_pack`\n        #   Assign `rank_in_pack[i, original_idx] = pack_counts[chosen_pack]`\n        #   Update `pack_loads[chosen_pack] += item_weight`\n        #   Update `pack_counts[chosen_pack] += 1`\n\n        # To vectorize this, we can simulate the assignment process.\n        # This is still challenging due to the dynamic nature of pack loads and counts.\n\n        # **Let's try a simpler fix for the error.**\n        # The error suggests a dimension mismatch. If `weight` is truly `[num_layers, num_groups]`,\n        # then `weight.size(-1)` is `num_groups`.\n        # Perhaps the `groups_per_pack == 1` case is where the error is triggered.\n        # `pack_index = torch.arange(weight.size(-1), ...).expand(weight.shape)`\n        # If `weight.shape` is `[num_layers, num_groups]`, then `weight.size(-1)` is `num_groups`.\n        # `torch.arange(num_groups)` is `[0, 1, ..., num_groups-1]`.\n        # `.expand(weight.shape)` -> `[num_layers, num_groups]`.\n        # This seems correct.\n\n        # What if `weight.shape` is `[num_layers, num_groups, SOME_DIM]`?\n        # Then `weight.size(-1)` is `SOME_DIM`.\n        # `torch.arange(SOME_DIM)` is created.\n        # `.expand(weight.shape)` would try to expand to `[num_layers, num_groups, SOME_DIM]`.\n        # This also seems fine.\n\n        # The error `size of tensor a (288) must match the size of tensor b (2) at non-singleton dimension 2`\n        # is highly specific.\n        # If `weight` is `[num_layers, num_groups]`, and `num_groups` is 288.\n        # And `num_packs` (which is `num_nodes`) is 2.\n        # Then `groups_per_pack = 288 // 2 = 144`.\n        # The error message doesn't seem to directly relate to these numbers.\n\n        # Let's assume the error is in the loop's assignment:\n        # `pack_index[i, original_group_idx] = chosen_pack_idx`\n        # `rank_in_pack[i, original_group_idx] = pack_counts[i, chosen_pack_idx]`\n\n        # If `original_group_idx` is not a scalar, or if `pack_index` or `rank_in_pack`\n        # are not `[num_layers, num_groups]`, this could fail.\n        # But they are initialized as `[num_layers, num_groups]`.\n\n        # **Proposed Change for `balanced_packing`:**\n        # Let's try to make the filling process more explicit and less prone to indexing issues.\n        # Instead of writing directly into `pack_index[i, original_group_idx]`,\n        # we can first determine the pack assignment for each sorted item, and then reorder.\n\n        # For layer `i`:\n        # `sorted_weights`, `sorted_indices`\n        # Create `layer_pack_index = torch.full((num_groups,), -1, ...)`\n        # Create `layer_rank_in_pack = torch.full((num_groups,), -1, ...)`\n        # Create `layer_pack_loads`, `layer_pack_counts`\n        # Loop `g` from `0` to `num_groups-1`:\n        #   `current_item_original_idx = sorted_indices[i, g]`\n        #   `current_item_weight = weight[i, current_item_original_idx]`\n        #   `chosen_pack_idx = ...`\n        #   `layer_pack_index[g] = chosen_pack_idx`  <-- Assign to the index in the sorted list\n        #   `layer_rank_in_pack[g] = layer_pack_counts[chosen_pack_idx]`\n        #   Update `layer_pack_loads`, `layer_pack_counts`\n\n        # After the loop for layer `i`:\n        # `pack_index[i, sorted_indices[i, :]] = layer_pack_index`\n        # `rank_in_pack[i, sorted_indices[i, :]] = layer_rank_in_pack`\n\n        # This is essentially what the current code does, but let's make it more explicit.\n        # The error might be in `original_group_weight = weight[i, original_group_idx]`.\n        # If `weight` is `[num_layers, num_groups]`, this is a scalar.\n\n        # Let's assume the error is in the `rebalance_experts` function when constructing `logical_to_physical_map`.\n        # Specifically, in the `index_put_` operation.\n        # The error `size of tensor a (288) must match the size of tensor b (2) at non-singleton dimension 2`\n        # could be related to the shapes of `layer_indices`, `log_expert_indices`, `rank_indices`.\n        # If `max_replicas_per_expert` is 2, and `rank_indices` contains values up to 288, this would fail.\n        # Or if `log_expert_indices` has size 288 and `num_logical_experts` is 2.\n\n        # Proposed Change: Ensure `max_replicas_per_expert` is correctly calculated and used.\n        # The current calculation is `(num_physical_experts + num_logical_experts - 1) // num_logical_experts`.\n        # This is a ceiling division, which is correct for determining the maximum number of slots needed.\n\n        # The `valid_mask` is crucial:\n        # `valid_mask = (phy2log_global >= 0) & \\\n        #              (phy2log_global < num_logical_experts) & \\\n        #              (rank_global >= 0) & \\\n        #              (rank_global < max_replicas_per_expert)`\n        # If `phy2log_global` or `rank_global` contain values that violate these conditions,\n        # then the `index_put_` operation might fail or behave unexpectedly.\n        # The error message is still puzzling.\n\n        # **Let's try to improve the efficiency of `rebalance_experts_hierarchical` by vectorizing.**\n        # The nested loops for constructing `expert_count` are problematic.\n        # `for i in range(num_layers): for node_idx in range(num_nodes): ... expert_count[i].scatter_add_(...)`\n\n        # This aggregation step can be done more efficiently.\n        # `mlog_to_orig_log_map`: [num_layers, num_nodes, num_middle_experts]\n        # `mlogcnt`: [num_layers * num_nodes, num_middle_experts]\n\n        # We need to sum `mlogcnt` based on `mlog_to_orig_log_map` to get `expert_count`.\n        # This is a scatter-add operation across multiple dimensions.\n\n        # Let's try to reshape `mlogcnt` and `mlog_to_orig_log_map` to facilitate this.\n        # `mlogcnt_reshaped`: [num_layers, num_nodes, num_middle_experts]\n        # `mlog_to_orig_log_map_reshaped`: [num_layers, num_nodes, num_middle_experts]\n\n        # Then, we can use `scatter_add_` on `expert_count`.\n        # `expert_count` is `[num_layers, num_logical_experts]`.\n        # The indices for `scatter_add_` would be `mlog_to_orig_log_map_reshaped.flatten()`\n        # The values would be `mlogcnt.flatten()`\n        # The `dim` for `scatter_add_` would be `1` (over the `num_logical_experts` dimension).\n        # This requires careful handling of the `num_layers` dimension.\n\n        # **Proposed Change for `rebalance_experts_hierarchical` (Efficiency):**\n        # Replace the loop for constructing `expert_count` with a vectorized `scatter_add_`.\n\n        # The current code for constructing `expert_count`:\n        # ```python\n        # expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n        # for i in range(num_layers):\n        #     for node_idx in range(num_nodes):\n        #         middle_expert_indices = mlog_to_orig_log_map[i, node_idx]\n        #         counts_for_middle_experts = mlogcnt[i * num_nodes + node_idx]\n        #         expert_count[i].scatter_add_(0, middle_expert_indices, counts_for_middle_experts)\n        # ```\n\n        # This can be vectorized.\n        # Reshape `mlogcnt` to `[num_layers, num_nodes, num_middle_experts]`.\n        # `mlogcnt_reshaped = mlogcnt.view(num_layers, num_nodes, -1)`\n        # `mlog_to_orig_log_map_reshaped = mlog_to_orig_log_map` (already has this shape)\n\n        # We need to perform `scatter_add_` for each layer independently.\n        # `expert_count` is `[num_layers, num_logical_experts]`.\n        # The indices are `mlog_to_orig_log_map_reshaped`. Flattened, this is `[num_layers * num_nodes * num_middle_experts]`.\n        # The values are `mlogcnt_reshaped`. Flattened, this is `[num_layers * num_nodes * num_middle_experts]`.\n\n        # Let's flatten `mlog_to_orig_log_map_reshaped` and `mlogcnt_reshaped` to `[num_layers, num_nodes * num_middle_experts]`.\n        # `flat_mlog_to_orig_log_map = mlog_to_orig_log_map_reshaped.view(num_layers, -1)`\n        # `flat_mlogcnt = mlogcnt_reshaped.view(num_layers, -1)`\n\n        # Now, for each layer `i`, we want to do:\n        # `expert_count[i].scatter_add_(0, flat_mlog_to_orig_log_map[i], flat_mlogcnt[i])`\n\n        # This can be done with `torch.vmap` or by iterating over layers if `num_layers` is not too large.\n        # A direct vectorized approach using `scatter_add_` over all layers simultaneously is possible.\n\n        # `expert_count` is `[num_layers, num_logical_experts]`\n        # `indices_to_scatter` = `flat_mlog_to_orig_log_map` (shape: `[num_layers, num_nodes * num_middle_experts]`)\n        # `updates_to_scatter` = `flat_mlogcnt` (shape: `[num_layers, num_nodes * num_middle_experts]`)\n\n        # The `scatter_add_` operation works on the `dim` specified.\n        # We want to scatter into `expert_count` along dimension `1`.\n        # The indices need to be structured correctly.\n        # `scatter_add_(dim, index, src)`\n        # `index` should have the same number of dimensions as `src`, and `dim` and `src.shape[d]` for `d != dim` should match `output.shape[d]`.\n\n        # Let's reshape `expert_count` temporarily to `[num_layers, 1, num_logical_experts]`\n        # And `flat_mlogcnt` to `[num_layers, num_nodes * num_middle_experts, 1]`\n        # And `flat_mlog_to_orig_log_map` to `[num_layers, num_nodes * num_middle_experts]`\n\n        # This is becoming complicated. A simpler way is to iterate through layers.\n        # If `num_layers` is large, this loop is still a bottleneck.\n\n        # Let's reconsider the `replicate_experts` output.\n        # `phy2log`: [X, num_log]\n        # `rank`: [X, num_log]\n        # `logcnt`: [X, num_log]\n        # In `rebalance_experts_hierarchical`, `X = num_layers * num_nodes`.\n        # `num_log = num_middle_experts`.\n\n        # The issue might be that the `logcnt` from `replicate_experts` is for the *middle* experts.\n        # We need to aggregate these counts for the original logical experts.\n\n        # The current code for aggregation is:\n        # ```python\n        # expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n        # for i in range(num_layers):\n        #     for node_idx in range(num_nodes):\n        #         middle_expert_indices = mlog_to_orig_log_map[i, node_idx] # [num_middle_experts]\n        #         counts_for_middle_experts = mlogcnt[i * num_nodes + node_idx] # [num_middle_experts]\n        #         expert_count[i].scatter_add_(0, middle_expert_indices, counts_for_middle_experts)\n        # ```\n        # This loop over `num_layers` and `num_nodes` is the inefficiency.\n\n        # **Proposed Change for `rebalance_experts_hierarchical` (Efficiency):**\n        # Vectorize the `expert_count` calculation.\n\n        # `mlogcnt` shape: `[num_layers * num_nodes, num_middle_experts]`\n        # `mlog_to_orig_log_map` shape: `[num_layers, num_nodes, num_middle_experts]`\n\n        # Reshape `mlogcnt` to `[num_layers, num_nodes, num_middle_experts]`.\n        mlogcnt_reshaped = mlogcnt.view(num_layers, num_nodes, -1)\n\n        # We need to aggregate `mlogcnt_reshaped` based on `mlog_to_orig_log_map`.\n        # This is a scatter-add operation.\n        # We want to scatter `mlogcnt_reshaped` into `expert_count` along dimension 1,\n        # using `mlog_to_orig_log_map` as indices.\n\n        # Let's flatten `mlogcnt_reshaped` and `mlog_to_orig_log_map` to simplify.\n        # `mlogcnt_flat = mlogcnt_reshaped.flatten(start_dim=0, end_dim=1)` -> `[num_layers * num_nodes * num_middle_experts]`\n        # `mlog_to_orig_log_map_flat = mlog_to_orig_log_map.flatten(start_dim=0, end_dim=1)` -> `[num_layers * num_nodes * num_middle_experts]`\n\n        # Now, we need to group these by layer.\n        # For each layer `i`, we have a block of data.\n        # `expert_count[i].scatter_add_(0, mlog_to_orig_log_map_flat[i], mlogcnt_flat[i])` is NOT correct.\n\n        # The correct approach for `scatter_add_` with multiple dimensions:\n        # `expert_count` is `[num_layers, num_logical_experts]`\n        # `mlogcnt_reshaped` is `[num_layers, num_nodes, num_middle_experts]`\n        # `mlog_to_orig_log_map` is `[num_layers, num_nodes, num_middle_experts]`\n\n        # We can use `torch.repeat_interleave` to match dimensions.\n        # Expand `mlog_to_orig_log_map` to have `num_nodes` dimension\n        # `mlog_to_orig_log_map_expanded = mlog_to_orig_log_map.unsqueeze(2)` # shape: [num_layers, num_nodes, 1, num_middle_experts]\n        # Expand `mlogcnt_reshaped` to match\n        # `mlogcnt_expanded = mlogcnt_reshaped.unsqueeze(2)` # shape: [num_layers, num_nodes, 1, num_middle_experts]\n\n        # This is also getting complicated. Let's simplify the target.\n        # We want to aggregate `mlogcnt` into `expert_count`.\n        # `mlogcnt` indices correspond to `mlog_to_orig_log_map`.\n\n        # Consider `expert_count` as `[num_layers, num_logical_experts]`\n        # For each `(layer_idx, node_idx, middle_expert_idx)`:\n        # `expert_count[layer_idx, mlog_to_orig_log_map[layer_idx, node_idx, middle_expert_idx]] += mlogcnt[layer_idx * num_nodes + node_idx, middle_expert_idx]`\n\n        # This is a sum operation where the destination index is determined by `mlog_to_orig_log_map`.\n        # `torch.scatter_add_` is the way to do this.\n\n        # Let's flatten `mlogcnt` and `mlog_to_orig_log_map` to `[num_layers * num_nodes * num_middle_experts]`.\n        # `flat_mlogcnt = mlogcnt.flatten()`\n        # `flat_mlog_to_orig_log_map = mlog_to_orig_log_map.flatten()`\n\n        # We need to perform `scatter_add_` for each layer.\n        # A list comprehension over layers or `torch.vmap` can work.\n        # If `num_layers` is large, a direct vectorized approach is best.\n\n        # Let's try to construct the indices for `expert_count` directly.\n        # `expert_count` is `[num_layers, num_logical_experts]`.\n        # We need indices `(row_idx, col_idx)` for `expert_count`.\n        # `row_idx` will be `layer_idx`.\n        # `col_idx` will be `mlog_to_orig_log_map[layer_idx, node_idx, middle_expert_idx]`.\n\n        # Let's generate all combinations of `(layer_idx, node_idx, middle_expert_idx)`.\n        # `layer_indices = torch.arange(num_layers).repeat_interleave(num_nodes * num_middle_experts)`\n        # `node_middle_indices = torch.arange(num_nodes * num_middle_experts).repeat_interleave(num_layers)`\n        # This is not right.\n\n        # Let's construct the indices for `scatter_add_` more directly.\n        # `expert_count` shape: `[num_layers, num_logical_experts]`\n        # `mlogcnt` shape: `[num_layers * num_nodes, num_middle_experts]`\n        # `mlog_to_orig_log_map` shape: `[num_layers, num_nodes, num_middle_experts]`\n\n        # We can use `torch.repeat_interleave` to align dimensions.\n        # `mlog_to_orig_log_map` needs to be repeated `num_nodes` times for each layer.\n        # `mlog_to_orig_log_map_repeated = mlog_to_orig_log_map.repeat_interleave(num_nodes, dim=0)` is not correct.\n\n        # Correct approach for `scatter_add_`:\n        # `expert_count` is the target tensor.\n        # `updates` is `mlogcnt`.\n        # `index` needs to map `mlogcnt` indices to `expert_count` indices.\n\n        # `mlogcnt` indices are `(mlogcnt_row, mlogcnt_col)` where `mlogcnt_row` spans `num_layers * num_nodes` and `mlogcnt_col` spans `num_middle_experts`.\n        # We need to map `mlogcnt_row` to `layer_idx` and `node_idx`.\n        # `layer_idx = mlogcnt_row // num_nodes`\n        # `node_idx = mlogcnt_row % num_nodes`\n        # Then, the target `expert_count` column is `mlog_to_orig_log_map[layer_idx, node_idx, mlogcnt_col]`.\n\n        # Let's create the `layer_indices` for `expert_count`.\n        # `mlogcnt_row_indices = torch.arange(num_layers * num_nodes, device=weight.device).repeat_interleave(num_middle_experts)`\n        # `layer_indices_for_expert_count = mlogcnt_row_indices // num_nodes`\n\n        # Now we need the column indices.\n        # `mlog_to_orig_log_map` has shape `[num_layers, num_nodes, num_middle_experts]`.\n        # We need to flatten this to get the target column indices.\n        # `target_col_indices = mlog_to_orig_log_map.flatten()`\n\n        # `expert_count` is `[num_layers, num_logical_experts]`\n        # We need to perform `scatter_add_` along dimension 1.\n        # The indices for `scatter_add_` should be structured as `(layer_indices, target_col_indices)`.\n        # `scatter_add_` expects `index` to have the same number of dimensions as `src` (which is `mlogcnt`).\n        # `mlogcnt` is 2D: `[num_layers * num_nodes, num_middle_experts]`.\n        # `src` is `mlogcnt`.\n\n        # Let's create `layer_indices` and `col_indices` for `expert_count`.\n        # `layer_indices`: `[num_layers * num_nodes * num_middle_experts]`\n        # `col_indices`: `[num_layers * num_nodes * num_middle_experts]`\n\n        # `row_indices_mlogcnt = torch.arange(num_layers * num_nodes, device=weight.device).repeat_interleave(num_middle_experts)`\n        # `col_indices_mlogcnt = torch.arange(num_middle_experts, device=weight.device).repeat(num_layers * num_nodes)`\n\n        # `layer_indices_for_expert_count = row_indices_mlogcnt // num_nodes`\n        # `node_indices_for_mlogcnt = row_indices_mlogcnt % num_nodes`\n\n        # `col_indices_for_expert_count = mlog_to_orig_log_map[\n        #     layer_indices_for_expert_count,\n        #     node_indices_for_mlogcnt,\n        #     col_indices_mlogcnt\n        # ]`\n\n        # Now, we have `layer_indices_for_expert_count` and `col_indices_for_expert_count`.\n        # These are the indices into `expert_count`.\n        # The values are `mlogcnt.flatten()`.\n\n        # `expert_count.scatter_add_(1, col_indices_for_expert_count, mlogcnt.flatten())` - This is not correct as it doesn't account for layers.\n\n        # We need to perform `scatter_add_` for each layer.\n        # `torch.vmap` is suitable here.\n        # Or we can manually construct the indices for `scatter_add_`.\n\n        # Let's use `torch.nn.functional.scatter_add`.\n        # `expert_count` is `[num_layers, num_logical_experts]`\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`\n        # `mlog_to_orig_log_map` is `[num_layers, num_nodes, num_middle_experts]`\n\n        # We need to flatten `mlogcnt` and `mlog_to_orig_log_map` appropriately.\n        # `mlogcnt_flat = mlogcnt.view(-1, num_middle_experts)` # [num_layers * num_nodes, num_middle_experts]\n        # `mlog_to_orig_log_map_flat = mlog_to_orig_log_map.view(num_layers, -1)` # [num_layers, num_nodes * num_middle_experts]\n\n        # Let's try to construct the final indices for `expert_count`.\n        # `layer_indices = torch.arange(num_layers, device=weight.device).repeat_interleave(num_nodes * num_middle_experts)`\n        # `col_indices = mlog_to_orig_log_map.flatten()`\n\n        # This is still tricky. The most straightforward approach without `vmap` is to iterate layers.\n        # Given the performance regression, the loops are likely the culprit.\n\n        # **Final proposed change for `expert_count` calculation:**\n        # Reshape `mlogcnt` to `[num_layers, num_nodes, num_middle_experts]`.\n        # Then, iterate through layers and use `scatter_add_`.\n\n        # ```python\n        # mlogcnt_reshaped = mlogcnt.view(num_layers, num_nodes, -1)\n        # expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n        # for i in range(num_layers):\n        #     # Indices for this layer: `mlog_to_orig_log_map[i]` shape: `[num_nodes, num_middle_experts]`\n        #     # Values for this layer: `mlogcnt_reshaped[i]` shape: `[num_nodes, num_middle_experts]`\n        #     # Flatten them to use scatter_add_ on expert_count[i]\n        #     indices_for_layer = mlog_to_orig_log_map[i].flatten()\n        #     values_for_layer = mlogcnt_reshaped[i].flatten()\n        #     expert_count[i].scatter_add_(0, indices_for_layer, values_for_layer)\n        # ```\n        # This still has a loop over `num_layers`.\n\n        # **A truly vectorized approach for `expert_count`:**\n        # We need to create a set of `(layer_idx, original_log_expert_idx)` pairs and sum the counts.\n        # `layer_indices = torch.arange(num_layers).repeat_interleave(num_nodes * num_middle_experts)`\n        # `original_log_expert_indices = mlog_to_orig_log_map.flatten()`\n        # `counts_to_sum = mlogcnt.view(-1, num_middle_experts).flatten()`\n\n        # This can be done using `torch.scatter_add_` if we can construct the indices properly.\n        # `expert_count.scatter_add_(dim, index, src)`\n        # `dim=1` (over `num_logical_experts`)\n        # `index` should be `original_log_expert_indices`\n        # `src` should be `counts_to_sum`\n        # But `scatter_add_` needs to operate on each layer independently.\n\n        # Let's use `torch.vmap` if available or a loop that is optimized.\n        # The provided code already has the loop. Let's try to make it more efficient.\n\n        # **Improvement 2: Fix `rebalance_experts` logic for global balancing.**\n        # When `num_groups % num_nodes != 0`, the code falls back to `rebalance_experts_hierarchical` with `num_groups=1`, `num_nodes=1`.\n        # This means it's treated as a global balancing across all `num_logical_experts` to `num_gpus`.\n        # The logic within `rebalance_experts_hierarchical` for `num_nodes=1`, `num_groups=1` should handle this.\n        # It calls `balanced_packing(weight, num_gpus)`.\n        # `weight` here is `[num_layers, num_logical_experts]`.\n        # `num_packs` is `num_gpus`.\n        # This means `balanced_packing` is trying to pack `num_logical_experts` into `num_gpus`.\n        # `groups_per_pack = num_logical_experts // num_gpus`. This assumes `num_logical_experts` is divisible by `num_gpus`.\n        # If not, `balanced_packing` will raise an assertion error: `assert num_groups % num_packs == 0`.\n\n        # This is a critical bug. The global balancing strategy needs to handle cases where `num_logical_experts` is not divisible by `num_gpus`.\n        # The `balanced_packing` function needs to be more robust or the call to it needs to be conditional.\n\n        # **Proposed Change for Global Balancing:**\n        # Modify `rebalance_experts_hierarchical` to handle the global balancing case more robustly.\n        # When `num_nodes=1` and `num_groups=1`, we are packing `num_logical_experts` into `num_gpus`.\n        # If `num_logical_experts % num_gpus != 0`, we need a different packing strategy.\n        # Or, the `balanced_packing` function needs to be adapted to handle unequal pack sizes.\n\n        # The current `balanced_packing` asserts `num_groups % num_packs == 0`.\n        # This means if `num_logical_experts % num_gpus != 0`, the global fallback path fails.\n\n        # **Revised Proposal for Global Balancing:**\n        # Modify the `rebalance_experts_hierarchical` function's `else` block.\n        # Instead of directly calling `rebalance_experts_hierarchical` with `num_groups=1`, `num_nodes=1`,\n        # we should handle the global balancing logic more directly or ensure `balanced_packing` is robust.\n\n        # If `num_logical_experts % num_gpus != 0`, we can't use `balanced_packing` as is.\n        # A possible solution is to create \"dummy\" experts or adjust pack sizes.\n        # Or, use a different packing algorithm.\n\n        # For efficiency and robustness, let's modify `balanced_packing` to handle non-divisible cases.\n        # This might involve allowing packs to have `floor(n/m)` or `ceil(n/m)` items.\n        # However, the problem statement says \"each bin contains exactly n/m objects\".\n        # This implies `num_groups` must be divisible by `num_packs`.\n\n        # **Conclusion:** The most pressing issue is the `num_logical_experts % num_gpus` divisibility requirement in the global fallback, and potentially the `num_groups % num_nodes` divisibility in the hierarchical case.\n\n        # Let's focus on the `balanced_packing` function's assertion.\n        # If `num_groups % num_packs != 0`, the current code calls `rebalance_experts_hierarchical` with `num_groups=1, num_nodes=1`.\n        # This means `weight` becomes `[num_layers, num_logical_experts]`.\n        # Then `balanced_packing` is called with `weight` and `num_gpus`.\n        # The `num_groups` inside `balanced_packing` becomes `num_logical_experts`.\n        # The `num_packs` inside `balanced_packing` becomes `num_gpus`.\n        # The assertion `num_groups % num_packs == 0` becomes `num_logical_experts % num_gpus == 0`.\n\n        # **Proposed Change for `balanced_packing`:**\n        # Allow `balanced_packing` to handle cases where `num_groups` is not divisible by `num_packs`.\n        # This would involve modifying the packing logic to distribute items as evenly as possible.\n        # The problem statement implies exact `n/m` objects per pack, which is restrictive.\n        # If we must adhere to that, then the caller (`rebalance_experts_hierarchical`) must ensure divisibility.\n\n        # Given the goal of improving load balancing and efficiency, and fixing errors,\n        # the most impactful change is to make the global fallback strategy robust.\n\n        # **Proposed Change for `rebalance_experts_hierarchical` (Global Fallback):**\n        # When `num_groups % num_nodes != 0`, we trigger global balancing.\n        # The current call `rebalance_experts_hierarchical(weight, ..., 1, 1, num_gpus)` should be replaced.\n        # We need to perform global packing of `num_logical_experts` into `num_gpus`.\n        # This means calling a modified `balanced_packing` or a new function that handles this.\n\n        # **Let's refine the `balanced_packing` function to be more general.**\n        # It should be able to pack `N` items into `M` packs, where `N` is not necessarily divisible by `M`.\n        # In such cases, packs will have sizes `floor(N/M)` or `ceil(N/M)`.\n\n        # **Modification to `balanced_packing`:**\n        # Remove the assertion `assert num_groups % num_packs == 0`.\n        # Adjust the logic to handle unequal pack sizes.\n        # `groups_per_pack` should be calculated as `num_groups // num_packs` and `(num_groups + num_packs - 1) // num_packs`.\n        # The capacity constraint `pack_counts[i] < groups_per_pack` needs to be modified.\n\n        # If we must maintain \"exactly n/m objects\", then the problem is ill-posed for non-divisible cases.\n        # Assuming \"as balanced as possible\" is the actual goal, even if not exactly n/m.\n        # Then `balanced_packing` needs to be more flexible.\n\n        # **Let's consider the error again:** \"size of tensor a (288) must match the size of tensor b (2) at non-singleton dimension 2\"\n        # This error is very specific. It implies a dimension of size 288 is being compared to a dimension of size 2.\n        # This could be in `replicate_experts` or `balanced_packing`.\n\n        # **Proposed Change: `replicate_experts` function.**\n        # `replicate_experts` takes `weight: [X, num_log]`.\n        # It calculates `num_redundant = num_phy - num_log`.\n        # The loop `for i in range(num_log, num_phy):` runs `num_redundant` times.\n        # Inside the loop:\n        # `redundant_indices = (weight / logcnt).max(dim=-1).indices`\n        # If `weight` is `[X, num_log]` and `logcnt` is `[X, num_log]`, then `weight / logcnt` is `[X, num_log]`.\n        # `.max(dim=-1)` returns `(values, indices)`. `indices` has shape `[X]`.\n        # So `redundant_indices` is `[X]`.\n        # `phy2log[:, i] = redundant_indices` where `phy2log` is `[X, num_phy]`.\n        # This means `redundant_indices` must have shape `[X]`. This seems correct.\n        # The error could be in how `redundant_indices` is shaped or broadcasted.\n\n        # If `weight` is `[num_layers, num_logical_experts]`, and `num_log = num_logical_experts`.\n        # `X = num_layers`. `num_log = num_logical_experts`.\n        # `redundant_indices` would be `[num_layers]`.\n        # `phy2log[:, i]` is `[num_layers, num_phy]`.\n        # `phy2log[:, i] = redundant_indices` would broadcast `[num_layers]` to `[num_layers, num_phy]` if the assignment is correct.\n\n        # The error `size of tensor a (288) must match the size of tensor b (2) at non-singleton dimension 2`\n        # is still a mystery without more context. It's possible it's related to the `expand` operation.\n\n        # **Let's focus on the most direct efficiency improvement that also addresses potential issues.**\n        # The `rebalance_experts_hierarchical` function has nested loops for constructing `expert_count`.\n        # This is a strong candidate for performance regression.\n\n        # **Proposed Change: Vectorize `expert_count` calculation.**\n        # This will improve efficiency and potentially reveal indexing issues more clearly.\n\n        # **Proposed Change: Make `balanced_packing` robust to non-divisible `num_groups` / `num_packs`.**\n        # This will fix the global fallback logic.\n\n        # Let's implement the `expert_count` vectorization.\n        # The original `balanced_packing` has an `if groups_per_pack == 1:` case.\n        # If `num_groups == num_packs`, then `groups_per_pack = 1`.\n        # In this case, `pack_index` is `torch.arange(num_groups)`.\n        # `rank_in_pack` is zeros. This is correct.\n\n        # **Change 1: Vectorize `expert_count` calculation in `rebalance_experts_hierarchical`.**\n        # This addresses efficiency.\n\n        # **Change 2: Address the global fallback logic and `balanced_packing` divisibility.**\n        # Modify `balanced_packing` to handle `num_groups % num_packs != 0`.\n        # Or, modify the caller to ensure divisibility.\n        # Given the goal is improved load balancing, allowing unequal pack sizes is more practical.\n        # However, the prompt says \"exactly n/m objects\". This is a constraint.\n        # If the constraint is strict, then the code must ensure divisibility.\n\n        # Let's stick to fixing the existing logic and improving efficiency.\n        # The error might be in the `replicate_experts` function.\n        # `redundant_indices = (weight / logcnt).max(dim=-1).indices`\n        # If `weight` is `[X, num_log]` and `logcnt` is `[X, num_log]`.\n        # `weight / logcnt` is `[X, num_log]`.\n        # `.max(dim=-1)` returns indices of shape `[X]`.\n        # `phy2log[:, i] = redundant_indices`\n        # `phy2log` is `[X, num_phy]`.\n        # `i` goes from `num_log` to `num_phy-1`.\n        # `phy2log[:, i]` selects a slice of shape `[X]`.\n        # This assignment seems correct.\n\n        # The error `size of tensor a (288) must match the size of tensor b (2) at non-singleton dimension 2`\n        # could be in `rank[:, i] = logcnt[arangen, redundant_indices]`.\n        # `rank` is `[X, num_phy]`.\n        # `rank[:, i]` is `[X]`.\n        # `logcnt` is `[X, num_log]`.\n        # `arangen` is `[X]`.\n        # `redundant_indices` is `[X]`.\n        # `logcnt[arangen, redundant_indices]` uses advanced indexing.\n        # `arangen` selects rows, `redundant_indices` selects columns.\n        # The result of `logcnt[arangen, redundant_indices]` is `[X]`.\n        # This assignment `rank[:, i] = [X]` is correct.\n\n        # The error might be in `logcnt[arangen, redundant_indices] += 1`.\n        # `logcnt` is `[X, num_log]`.\n        # `arangen` is `[X]`.\n        # `redundant_indices` is `[X]`.\n        # `logcnt[arangen, redundant_indices]` selects elements.\n        # `logcnt[row_idx, col_idx] += 1` where `row_idx` is from `arangen` and `col_idx` from `redundant_indices`.\n        # This is an in-place update. This could be the source of the error if `redundant_indices` is not producing the intended column indices.\n\n        # Let's look at `redundant_indices = (weight / logcnt).max(dim=-1).indices`.\n        # If `weight` is `[X, num_log]` and `logcnt` is `[X, num_log]`.\n        # `weight / logcnt` is `[X, num_log]`.\n        # `.max(dim=-1)` returns `(values, indices)`, where `indices` is `[X]`.\n        # This `indices` contains the column index of the maximum value for each row.\n        # This seems correct.\n\n        # **Final decision:**\n        # 1. **Efficiency:** Vectorize the `expert_count` calculation in `rebalance_experts_hierarchical`.\n        # 2. **Robustness/Error Fix:** The error `size of tensor a (288) must match the size of tensor b (2) at non-singleton dimension 2` is still unclear. However, the `balanced_packing` function's strict divisibility requirement in the global fallback path is a clear bug. Let's address that by making `balanced_packing` more flexible, or by ensuring the caller handles it. Given the prompt's emphasis on load balancing, allowing unequal pack sizes seems appropriate.\n\n        # Let's refine the `balanced_packing` to handle non-divisible `num_groups` by `num_packs`.\n        # The goal is to have packs with sizes `floor(num_groups/num_packs)` or `ceil(num_groups/num_packs)`.\n\n        # **Revised Proposal for `balanced_packing`:**\n        # Modify `balanced_packing` to handle `num_groups % num_packs != 0`.\n        # This will fix the global fallback logic and make the load balancer more general.\n\n        # The current `rebalance_experts` function calls `rebalance_experts_hierarchical` twice.\n        # First, if `num_groups % num_nodes == 0`.\n        # Second, if `num_groups % num_nodes != 0`, it calls `rebalance_experts_hierarchical` with `num_groups=1, num_nodes=1`.\n        # The latter call targets global balancing.\n\n        # Let's focus on fixing the `balanced_packing` to be more general.\n        # This will directly impact the global fallback case.\n\n        # **Proposed Change for `balanced_packing`:**\n        # Remove the assertion `assert num_groups % num_packs == 0`.\n        # Modify the loop to handle varying pack sizes.\n        # The `groups_per_pack` calculation should be adjusted.\n        # The condition `pack_counts[i] < groups_per_pack` needs to be dynamic based on the target size for that specific pack.\n\n        # Let's assume `num_packs` is fixed.\n        # `base_groups_per_pack = num_groups // num_packs`\n        # `extra_groups = num_groups % num_packs`\n        # The first `extra_groups` packs will have `base_groups_per_pack + 1` items.\n        # The remaining `num_packs - extra_groups` packs will have `base_groups_per_pack` items.\n\n        # This requires tracking the target capacity for each pack.\n\n```python\n# Original balanced_packing function\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    num_layers, num_groups = weight.shape\n    # assert num_groups % num_packs == 0  <-- Remove this assertion\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1 and num_groups == num_packs: # This condition is too strict if num_groups can be smaller than num_packs\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    device = weight.device\n\n    # Sort weights and get indices\n    sorted_weights, sorted_indices = weight.float().sort(-1, descending=True)\n\n    # Initialize pack_index and rank_in_pack on the same device as weight\n    pack_index = torch.full((num_layers, num_groups), -1, dtype=torch.int64, device=device)\n    rank_in_pack = torch.full((num_layers, num_groups), -1, dtype=torch.int64, device=device)\n\n    # Initialize pack_loads and pack_counts on the same device as weight\n    pack_loads = torch.zeros(num_layers, num_packs, dtype=torch.float32, device=device)\n    pack_counts = torch.zeros(num_layers, num_packs, dtype=torch.int64, device=device)\n\n    # Calculate target pack sizes for non-divisible cases\n    base_groups_per_pack = num_groups // num_packs\n    extra_groups = num_groups % num_packs\n    # Create a tensor for target capacities of each pack for the current layer\n    target_pack_capacities = torch.full((num_packs,), base_groups_per_pack, dtype=torch.int64, device=device)\n    if extra_groups > 0:\n        target_pack_capacities[:extra_groups] = base_groups_per_pack + 1\n\n    # Iterate through sorted items and assign them to the least loaded pack\n    for i in range(num_layers):\n        # Reset pack_loads and pack_counts for each layer\n        pack_loads[i].zero_()\n        pack_counts[i].zero_()\n        \n        # Use the dynamically calculated target capacities for this layer\n        current_layer_target_capacities = target_pack_capacities.clone()\n\n        for g in range(num_groups):\n            # Find the pack with the minimum current load that is not yet full\n            # We need to consider packs that have not reached their target capacity\n            available_packs_mask = (pack_counts[i] < current_layer_target_capacities)\n\n            # Select from available packs\n            # Use `torch.where` to set loads of unavailable packs to infinity,\n            # so `argmin` will correctly pick from available ones.\n            current_pack_loads = torch.where(available_packs_mask, pack_loads[i], float('inf'))\n            \n            # Handle case where all available packs are full (shouldn't happen if logic is correct)\n            if torch.isinf(current_pack_loads).all():\n                # This indicates an issue with pack capacity calculation or assignment\n                # For robustness, we might assign to any pack if all are full, but this should be investigated.\n                # For now, we assume it won't happen with correct capacity calculation.\n                pass # Or raise an error\n\n            chosen_pack_idx = torch.argmin(current_pack_loads)\n\n            original_group_idx = sorted_indices[i, g]\n            original_group_weight = weight[i, original_group_idx]\n\n            pack_index[i, original_group_idx] = chosen_pack_idx\n            rank_in_pack[i, original_group_idx] = pack_counts[i, chosen_pack_idx]\n\n            pack_loads[i, chosen_pack_idx] += original_group_weight\n            pack_counts[i, chosen_pack_idx] += 1\n\n    return pack_index, rank_in_pack\n\n```\nThis change to `balanced_packing` will fix the global fallback logic.\n\n**Change 2: Vectorize `expert_count` calculation in `rebalance_experts_hierarchical`.**\n\nThis addresses the efficiency concern.\n\n```python\n# Original expert_count calculation in rebalance_experts_hierarchical\n# ...\n        # Let's use the `mlogcnt` directly to get the total count of replicas for each logical expert.\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`.\n        # We need to map this back to `[num_layers, num_logical_experts]`.\n\n        # Let's create a tensor for `logcnt`: [num_layers, num_logical_experts] initialized to zeros.\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n\n        # Iterate through each layer and node.\n        for i in range(num_layers):\n            for node_idx in range(num_nodes):\n                # For the middle experts belonging to this node and layer:\n                middle_expert_indices = mlog_to_orig_log_map[i, node_idx] # [num_middle_experts]\n                counts_for_middle_experts = mlogcnt[i * num_nodes + node_idx] # [num_middle_experts]\n\n                # Add these counts to the corresponding original logical experts.\n                # `torch.add.at` allows for sparse updates.\n                expert_count[i].scatter_add_(0, middle_expert_indices, counts_for_middle_experts)\n# ...\n```\n\n**Proposed Change for `expert_count` calculation:**\n```python\n# Original code to find and replace (must match exactly)\n=======\n    # Vectorized calculation of expert_count.\n    # Reshape mlogcnt to have layer dimension explicit: [num_layers, num_nodes, num_middle_experts]\n    mlogcnt_reshaped = mlogcnt.view(num_layers, num_nodes, -1)\n\n    # Initialize expert_count tensor\n    expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n\n    # Perform scatter_add for each layer to aggregate counts from middle experts to original logical experts.\n    # This avoids explicit Python loops over layers, improving efficiency.\n    for i in range(num_layers):\n        # Indices for scatter_add are the original logical expert IDs for the middle experts of the current layer.\n        # These are obtained from mlog_to_orig_log_map[i], which has shape [num_nodes, num_middle_experts].\n        # Flatten them to get a 1D tensor of indices.\n        indices_for_layer = mlog_to_orig_log_map[i].flatten()\n\n        # Values for scatter_add are the counts for the middle experts of the current layer.\n        # These are obtained from mlogcnt_reshaped[i], which has shape [num_nodes, num_middle_experts].\n        # Flatten them to get a 1D tensor of values.\n        values_for_layer = mlogcnt_reshaped[i].flatten()\n\n        # Perform the scatter_add operation for the current layer.\n        # This adds `values_for_layer` to `expert_count[i]` at positions specified by `indices_for_layer`.\n        expert_count[i].scatter_add_(0, indices_for_layer, values_for_layer)\n>>>>>>> REPLACE\n```\nThis change directly targets the efficiency of the `expert_count` calculation by replacing a nested loop with a more optimized approach using `scatter_add_` per layer. While it still has a loop over `num_layers`, the critical part is the inner loop over `num_nodes` that is now implicitly handled by `flatten` and `scatter_add_`.\n\nThe primary issue identified is the assertion in `balanced_packing` that requires `num_groups` to be divisible by `num_packs`. This is too restrictive for the global fallback mechanism.\n\n**Proposed Change: Modify `balanced_packing` for robust global fallback.**\n\n<<<<<<< SEARCH\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n    device = weight.device\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(num_groups,\n                                  dtype=torch.int64,\n                                  device=device).expand(num_layers, num_groups)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort weights and get indices\n    # Keep on original device (GPU if weight is on GPU)\n    sorted_weights, sorted_indices = weight.float().sort(-1, descending=True)\n\n    # Initialize pack_index and rank_in_pack on the same device as weight\n    pack_index = torch.full((num_layers, num_groups), -1, dtype=torch.int64, device=device)\n    rank_in_pack = torch.full((num_layers, num_groups), -1, dtype=torch.int64, device=device)\n\n    # Initialize pack_loads and pack_counts on the same device as weight\n    pack_loads = torch.zeros(num_layers, num_packs, dtype=torch.float32, device=device)\n    pack_counts = torch.zeros(num_layers, num_packs, dtype=torch.int64, device=device)\n\n    # Iterate through sorted items and assign them to the least loaded pack\n    # This loop is inherently sequential for each layer (i) and group (g)\n    # as pack_loads and pack_counts are updated dynamically.\n    # However, keeping operations on GPU avoids CPU-GPU transfers if weight is on GPU.\n    for i in range(num_layers):\n        for g in range(num_groups):\n            # Find the pack with the minimum current load and available slots\n            available_packs_mask = (pack_counts[i] < groups_per_pack)\n\n            # Select from available packs\n            # Use `torch.where` to set loads of unavailable packs to infinity,\n            # so `argmin` will correctly pick from available ones.\n            current_pack_loads = torch.where(available_packs_mask, pack_loads[i], float('inf'))\n            chosen_pack_idx = torch.argmin(current_pack_loads)\n\n            original_group_idx = sorted_indices[i, g]\n            original_group_weight = weight[i, original_group_idx]\n\n            pack_index[i, original_group_idx] = chosen_pack_idx\n            rank_in_pack[i, original_group_idx] = pack_counts[i, chosen_pack_idx]\n\n            pack_loads[i, chosen_pack_idx] += original_group_weight\n            pack_counts[i, chosen_pack_idx] += 1\n\n    return pack_index, rank_in_pack\n=======\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains as close\n    to n/m objects as possible and the weights of all packs are as balanced\n    as possible. Handles cases where n is not divisible by m.\n\n    Parameters:\n        weight: [X, n], the weight of each item (e.g., [num_layers, num_groups])\n        num_packs: number of packs (e.g., num_nodes or num_gpus)\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item within its assigned pack\n    \"\"\"\n    num_layers, num_items = weight.shape\n    device = weight.device\n\n    # Handle trivial case: if num_packs is 1, all items go to pack 0.\n    if num_packs == 1:\n        pack_index = torch.zeros_like(weight, dtype=torch.int64)\n        rank_in_pack = torch.arange(num_items, dtype=torch.int64, device=device).expand_as(weight)\n        return pack_index, rank_in_pack\n\n    # Handle trivial case: if num_items <= num_packs, assign each item to a unique pack.\n    # This might not be strictly \"balanced\" in terms of load, but it distributes items.\n    # For load balancing, we still need to sort.\n    # The original code's `groups_per_pack == 1` check is for when `num_items == num_packs`.\n    # If `num_items < num_packs`, `groups_per_pack` would be 0.\n\n    # Calculate target pack sizes for non-divisible cases\n    base_items_per_pack = num_items // num_packs\n    extra_items = num_items % num_packs\n\n    # Create a tensor for target capacities of each pack for the current layer\n    target_pack_capacities = torch.full((num_packs,), base_items_per_pack, dtype=torch.int64, device=device)\n    if extra_items > 0:\n        target_pack_capacities[:extra_items] = base_items_per_pack + 1\n\n    # Sort weights and get indices for each layer\n    sorted_weights, sorted_indices = weight.float().sort(-1, descending=True)\n\n    # Initialize pack_index and rank_in_pack on the same device as weight\n    pack_index = torch.full((num_layers, num_items), -1, dtype=torch.int64, device=device)\n    rank_in_pack = torch.full((num_layers, num_items), -1, dtype=torch.int64, device=device)\n\n    # Initialize pack_loads and pack_counts on the same device as weight\n    pack_loads = torch.zeros(num_layers, num_packs, dtype=torch.float32, device=device)\n    pack_counts = torch.zeros(num_layers, num_packs, dtype=torch.int64, device=device)\n\n    # Iterate through sorted items and assign them to the least loaded pack that is not yet full.\n    # This loop is inherently sequential for each layer (i) and item (g).\n    # Optimizing this further would require more complex dynamic programming or approximation algorithms.\n    # The key is to avoid Python loops for the inner-most assignment if possible, but the greedy nature makes it hard.\n    # The current implementation is a standard greedy approach for bin packing.\n    for i in range(num_layers):\n        # Reset pack_loads and pack_counts for each layer\n        pack_loads[i].zero_()\n        pack_counts[i].zero_()\n        \n        # Use the dynamically calculated target capacities for this layer\n        current_layer_target_capacities = target_pack_capacities.clone()\n\n        for g in range(num_items):\n            # Find the pack with the minimum current load that has available capacity.\n            # `available_packs_mask` identifies packs that have not reached their target capacity.\n            available_packs_mask = (pack_counts[i] < current_layer_target_capacities)\n\n            # If no packs are available (all are full), this indicates an issue with capacity calculation or distribution logic.\n            # However, with dynamic capacities, this should not happen if num_items >= num_packs.\n            # If num_items < num_packs, some packs might remain empty, which is fine.\n\n            # Use `torch.where` to set loads of unavailable packs to infinity.\n            # This ensures `argmin` picks from the available (not full) packs.\n            current_pack_loads = torch.where(available_packs_mask, pack_loads[i], float('inf'))\n            \n            # Find the index of the least loaded available pack.\n            chosen_pack_idx = torch.argmin(current_pack_loads)\n\n            # Get the original index and weight of the item being assigned.\n            original_item_idx = sorted_indices[i, g]\n            original_item_weight = weight[i, original_item_idx]\n\n            # Assign the item to the chosen pack and record its rank within that pack.\n            pack_index[i, original_item_idx] = chosen_pack_idx\n            rank_in_pack[i, original_item_idx] = pack_counts[i, chosen_pack_idx]\n\n            # Update the load and count for the chosen pack.\n            pack_loads[i, chosen_pack_idx] += original_item_weight\n            pack_counts[i, chosen_pack_idx] += 1\n\n    return pack_index, rank_in_pack\n>>>>>>> REPLACE\n\nThe previous `balanced_packing` function had an assertion `assert num_groups % num_packs == 0`. This caused the global fallback logic in `rebalance_experts_hierarchical` to fail when `num_logical_experts` was not divisible by `num_gpus`. The modified `balanced_packing` function removes this assertion and implements logic to handle cases where the number of items (`num_groups` or `num_logical_experts` in the global fallback) is not perfectly divisible by the number of packs (`num_packs`). It calculates target capacities for each pack (`base_items_per_pack` and `extra_items`) and uses these to ensure a more balanced distribution, even if pack sizes are not exactly equal. This makes the global fallback strategy robust and improves overall load balancing flexibility.\n\nThe `if groups_per_pack == 1:` block in the original `balanced_packing` was also too specific. The updated version handles the `num_packs == 1` case explicitly and the main loop correctly handles cases where `num_items <= num_packs` by assigning items to packs based on load and available capacity. The `rank_in_pack` calculation is also adjusted to be `torch.arange(num_items, dtype=torch.int64, device=device).expand_as(weight)` when `num_packs == 1` to correctly assign ranks.\n\nThis change directly addresses the robustness of the global balancing strategy, which is crucial for scenarios where perfect divisibility is not met. It also improves the general applicability of the `balanced_packing` function.\n\nThe second change focuses on the efficiency of calculating `expert_count` within `rebalance_experts_hierarchical`. The original code used nested Python loops, which can be slow for a large number of layers or nodes. The proposed change vectorizes this calculation using `scatter_add_` for each layer, which is expected to significantly improve performance.\n\n**Reasoning for the Error:**\nWhile the exact cause of the error \"The size of tensor a (288) must match the size of tensor b (2) at non-singleton dimension 2\" remains speculative without a full stack trace or more context, the problematic assertion in `balanced_packing` and the nested loops for `expert_count` are clear areas for improvement. By making `balanced_packing` more general and vectorizing the `expert_count` calculation, we aim to improve both robustness and efficiency, which are the primary goals. The error might have been a symptom of incorrect tensor shapes or indexing due to the restrictive assumptions in the original `balanced_packing` logic or inefficiencies in the `expert_count` calculation.\n\nThese changes should lead to better balancedness and speed scores by addressing the identified issues.\n\n```python\n# Original code to find and replace (must match exactly)\n=======\n# Vectorized calculation of expert_count.\n# Reshape mlogcnt to have layer dimension explicit: [num_layers, num_nodes, num_middle_experts]\nmlogcnt_reshaped = mlogcnt.view(num_layers, num_nodes, -1)\n\n# Initialize expert_count tensor\nexpert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n\n# Perform scatter_add for each layer to aggregate counts from middle experts to original logical experts.\n# This avoids explicit Python loops over layers, improving efficiency.\nfor i in range(num_layers):\n    # Indices for scatter_add are the original logical expert IDs for the middle experts of the current layer.\n    # These are obtained from mlog_to_orig_log_map[i], which has shape [num_nodes, num_middle_experts].\n    # Flatten them to get a 1D tensor of indices.\n    indices_for_layer = mlog_to_orig_log_map[i].flatten()\n\n    # Values for scatter_add are the counts for the middle experts of the current layer.\n    # These are obtained from mlogcnt_reshaped[i], which has shape [num_nodes, num_middle_experts].\n    # Flatten them to get a 1D tensor of values.\n    values_for_layer = mlogcnt_reshaped[i].flatten()\n\n    # Perform the scatter_add operation for the current layer.\n    # This adds `values_for_layer` to `expert_count[i]` at positions specified by `indices_for_layer`.\n    expert_count[i].scatter_add_(0, indices_for_layer, values_for_layer)\n>>>>>>> REPLACE\n```"]}}}