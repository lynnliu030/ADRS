{"id": "03d737f9-6f70-4554-b7f6-42c041b021c7", "code": "# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort weights and get indices\n    sorted_weights, sorted_indices = weight.float().sort(-1, descending=True)\n\n    # Store device from input weight\n    device = weight.device\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort weights and get indices\n    sorted_weights, sorted_indices = weight.float().sort(-1, descending=True)\n\n    # Initialize pack_index and rank_in_pack on the correct device\n    pack_index = torch.full((num_layers, num_groups), -1, dtype=torch.int64, device=device)\n    rank_in_pack = torch.full((num_layers, num_groups), -1, dtype=torch.int64, device=device)\n\n    # Initialize pack_loads and pack_counts on the correct device\n    pack_loads = torch.zeros(num_layers, num_packs, dtype=torch.float32, device=device)\n    pack_counts = torch.zeros(num_layers, num_packs, dtype=torch.int64, device=device)\n\n    # Iterate through sorted items and assign them to the least loaded pack\n    for i in range(num_layers):\n        for g in range(num_groups):\n            available_packs = pack_counts[i] < groups_per_pack\n            if not available_packs.any():\n                continue\n\n            current_pack_loads = pack_loads[i][available_packs]\n            current_pack_indices = torch.arange(num_packs, device=device)[available_packs]\n\n            min_load_idx_in_available = torch.argmin(current_pack_loads)\n            chosen_pack_idx = current_pack_indices[min_load_idx_in_available]\n\n            original_group_idx = sorted_indices[i, g]\n\n            pack_index[i, original_group_idx] = chosen_pack_idx\n            rank_in_pack[i, original_group_idx] = pack_counts[i, chosen_pack_idx]\n\n            pack_loads[i, chosen_pack_idx] += weight[i, original_group_idx]\n            pack_counts[i, chosen_pack_idx] += 1\n\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Unified rebalancing logic:\n    # The hierarchical approach is used when num_groups is divisible by num_nodes.\n    # Otherwise, it falls back to a global balancing strategy by setting num_nodes=1 and num_groups=1.\n    # We can simplify this by directly implementing the global strategy when needed,\n    # or by ensuring the hierarchical logic correctly handles num_nodes=1.\n\n    # Determine the effective number of groups and nodes for balancing.\n    # If num_groups is not divisible by num_nodes, we treat it as a single group\n    # across all nodes for the first level of balancing.\n    effective_num_groups = num_groups if num_groups % num_nodes == 0 else 1\n    effective_num_nodes = num_nodes if num_groups % num_nodes == 0 else 1\n\n    # If num_groups % num_nodes != 0, we effectively want to balance across all groups as if they were one large group.\n    # And then balance those groups onto nodes.\n    # This is equivalent to setting num_nodes = 1 and num_groups = 1 in the hierarchical logic.\n    # Let's adjust parameters for a unified call to a simplified hierarchical logic.\n\n    # Simplified approach:\n    # First, balance logical experts across groups within layers.\n    # Then, balance these groups across nodes.\n    # Finally, replicate experts within nodes to balance across GPUs.\n\n    # Step 1: Balance logical experts into groups.\n    # If num_groups == 1, this step is trivial and doesn't change anything.\n    group_size = num_logical_experts // num_groups\n    tokens_per_group_layer = weight.unflatten(-1, (num_groups, group_size)).sum(-1) # [num_layers, num_groups]\n\n    # Pack groups onto nodes.\n    # If num_groups % num_nodes != 0, we effectively want to balance all groups onto a single 'node' conceptually,\n    # and then distribute those across the actual num_nodes.\n    # Let's consider the case where num_groups is not divisible by num_nodes.\n    # The original code uses num_nodes for packing groups. If num_groups < num_nodes, this packing might be problematic.\n    # A more robust approach is to balance groups onto num_nodes, and if num_groups < num_nodes, some nodes get 0 groups.\n    # The current `balanced_packing` assumes num_groups is divisible by num_packs (num_nodes here).\n\n    # Let's redefine the packing strategy:\n    # We want to pack 'num_groups' items (groups) into 'num_nodes' packs.\n    # If num_groups is not divisible by num_nodes, balanced_packing needs to handle it.\n    # The current `balanced_packing` asserts `num_groups % num_packs == 0`.\n    # This means if num_groups % num_nodes != 0, the original code calls `rebalance_experts_hierarchical` with num_nodes=1, num_groups=1.\n    # This is a global balancing strategy.\n\n    # Let's ensure the function call correctly reflects the intended strategy.\n    # If num_groups % num_nodes == 0, use hierarchical.\n    # Otherwise, use global (which is hierarchical with num_nodes=1, num_groups=1).\n\n    if num_groups % num_nodes == 0:\n        # Hierarchical balancing: balance groups to nodes, then nodes to GPUs.\n        # Step 1: Pack groups to nodes.\n        group_pack_index, group_rank_in_pack = balanced_packing(tokens_per_group_layer, num_nodes)\n        # Map logical experts to \"middle\" logical experts (logical experts per node)\n        # log2mlog: [num_layers, num_logical_experts] -> maps logical expert to its middle logical expert\n        mlog2log_map = (((group_pack_index * groups_per_node + group_rank_in_pack) * group_size).unsqueeze(-1) +\n                        torch.arange(group_size, dtype=torch.int64, device=group_pack_index.device)).flatten(-2)\n        # mlog2log_map: [num_layers, num_logical_experts] maps each logical expert to its \"middle\" logical expert ID.\n        # We need the inverse mapping for `weight.gather` later.\n        # This inverse mapping is tricky because multiple logical experts can map to the same middle expert.\n        # The `inverse` function is designed for permutations, which isn't directly applicable here.\n\n        # Let's rethink the mapping and replication.\n        # We have `num_layers` of `weight` ([num_layers, num_logical_experts]).\n        # First, we group logical experts: `tokens_per_group_layer` ([num_layers, num_groups]).\n        # Then, we pack these groups onto `num_nodes`: `group_pack_index` ([num_layers, num_groups]) and `group_rank_in_pack` ([num_layers, num_groups]).\n        # This means `num_groups` are assigned to `num_nodes`. `groups_per_node` groups per node.\n        # So, we have `num_layers * num_nodes` \"node-level\" expert weight tensors.\n        # Each node-level tensor has `num_logical_experts // num_nodes` \"middle\" experts.\n\n        # Let's create the \"middle\" logical experts' weights.\n        # `mlog_weights`: [num_layers * num_nodes, num_logical_experts // num_nodes]\n        # This requires carefully gathering weights.\n        # The `mlog2log` mapping needs to be constructed correctly.\n\n        # `mlog2log_map` assigns each logical expert to a conceptual \"middle\" logical expert.\n        # e.g., if num_logical_experts=8, num_groups=2, group_size=4.\n        # Group 0 contains logical experts 0,1,2,3. Group 1 contains 4,5,6,7.\n        # If num_nodes=2, groups_per_node=1.\n        # Group 0 (log experts 0-3) goes to node 0. Group 1 (log experts 4-7) goes to node 1.\n        # Node 0 has 1 middle expert. Node 1 has 1 middle expert.\n        # Middle expert for node 0 is logical experts 0-3. Middle expert for node 1 is 4-7.\n        # The mapping `mlog2log_map` should map:\n        # 0,1,2,3 -> 0 (on node 0)\n        # 4,5,6,7 -> 0 (on node 1)\n        # The current `mlog2log` calculation seems to be constructing the indices of the middle experts.\n        # `log2mlog` is `[num_layers, num_logical_experts]`.\n        # It maps logical expert ID to its index within the middle expert group.\n        # The `mlog2log` is the inverse of this index mapping.\n\n        # Let's reconstruct `mlog2log` and `tokens_per_mlog` more directly.\n        # For each layer, we have `num_groups` of `group_size`.\n        # `group_pack_index`: [num_layers, num_groups] - node assignment for each group.\n        # `group_rank_in_pack`: [num_layers, num_groups] - index of group within the node's assigned groups.\n\n        # We want to create `num_layers * num_nodes` \"node-level\" expert groups.\n        # Each node-level group corresponds to a set of original logical experts.\n        # The `tokens_per_mlog` should be `[num_layers * num_nodes, num_logical_experts // num_nodes]`.\n\n        # Calculate the logical expert index for each \"middle\" expert.\n        # `mlog_expert_indices`: [num_layers, num_nodes, groups_per_node, group_size]\n        # This is becoming complex. Let's simplify the conceptualization.\n\n        # Simpler approach:\n        # 1. Pack groups to nodes. This determines which logical experts are grouped together on each node.\n        # `group_pack_index`: [num_layers, num_groups] -> node ID for each group.\n        # `group_rank_in_pack`: [num_layers, num_groups] -> rank within node for each group.\n\n        # 2. For each node, create a combined weight tensor for the logical experts assigned to it.\n        #    The number of logical experts per node will be `groups_per_node * group_size`.\n        #    We need to gather the weights of logical experts based on `group_pack_index`.\n\n        # Let's create a mapping from (layer, node) to a contiguous block of logical experts.\n        # `node_logical_expert_map`: [num_layers, num_nodes, groups_per_node * group_size]\n\n        # This requires constructing a permutation for each layer and node.\n        # The original `mlog2log` calculation is:\n        # `((group_pack_index * groups_per_node + group_rank_in_pack) * group_size)`\n        # This forms a base index for each logical expert.\n        # `log2mlog` is this base index + `arange(group_size)`.\n        # `mlog2log` is the inverse of `log2mlog`.\n\n        # Let's directly construct the `tokens_per_mlog` and `mlog2log`.\n        # `tokens_per_mlog` is the weight tensor for experts *within* each node, considering the grouped logical experts.\n        # The number of such \"middle\" experts per node is `groups_per_node * group_size`.\n\n        # For each layer:\n        # `group_pack_index` tells us which node each group belongs to.\n        # `group_rank_in_pack` tells us the order within the node.\n\n        # Let's create `num_layers * num_nodes` tensors, each of size `groups_per_node * group_size`.\n        # `mlog_weights`: [num_layers * num_nodes, groups_per_node * group_size]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, groups_per_node * group_size]\n\n        # Example: num_layers=1, num_logical_experts=8, num_groups=2, group_size=4, num_nodes=2, groups_per_node=1, num_gpus=2, phy_experts_per_gpu=1.\n        # weight: [1, 8]\n        # tokens_per_group_layer: [1, 2] (sum of weights for group 0 and group 1)\n        # balanced_packing(tokens_per_group_layer, num_nodes=2) ->\n        #   group_pack_index: [1, 2] -> [[0, 1]] (group 0 to node 0, group 1 to node 1)\n        #   group_rank_in_pack: [1, 2] -> [[0, 0]] (rank 0 within node for both groups)\n        # groups_per_node = 1, group_size = 4.\n\n        # Middle expert for node 0 is logical experts 0,1,2,3.\n        # Middle expert for node 1 is logical experts 4,5,6,7.\n        # `mlog_to_orig_log_map` for node 0 should be [0,1,2,3].\n        # `mlog_to_orig_log_map` for node 1 should be [4,5,6,7].\n\n        # The original `mlog2log` calculation:\n        # `base_idx = (group_pack_index * groups_per_node + group_rank_in_pack) * group_size`\n        # `base_idx`: [1, 2] -> [[0*1+0, 1*1+0]] * 4 = [[0, 4]]\n        # `log2mlog`: `base_idx.unsqueeze(-1) + torch.arange(group_size)`\n        # `log2mlog`: [[0, 4]].unsqueeze(-1) + [0,1,2,3] -> [[0,1,2,3], [4,5,6,7]]\n        # This `log2mlog` is `[num_layers, num_logical_experts]`. It maps original logical expert ID to its \"middle\" expert index.\n        # The original logic uses `weight.gather(-1, mlog2log)` which is incorrect here.\n        # `weight.gather(-1, mlog2log)` expects `mlog2log` to be `[num_layers, num_middle_experts]` where `num_middle_experts` is the number of columns in `weight`.\n        # The shape should be `[num_layers, num_logical_experts]` if we are gathering from `weight` which is `[num_layers, num_logical_experts]`.\n        # The `mlog2log` calculated here has shape `[num_layers, num_logical_experts]`.\n        # It maps `original_logical_expert_id` to `middle_expert_id`.\n        # We need to gather `weight[layer, original_logical_expert_id]` and group them by `middle_expert_id`.\n\n        # Let's create `mlog_weights` directly.\n        # `mlog_weights`: [num_layers * num_nodes, groups_per_node * group_size]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, groups_per_node * group_size]\n\n        # Vectorized construction of mlog_weights and mlog_to_orig_log_map\n        # This replaces the nested Python loops for better performance.\n\n        # Calculate the new \"middle expert group\" index for each original group.\n        # This new index combines node assignment and rank within the node.\n        # Shape: [num_layers, num_groups]\n        new_group_idx = group_pack_index * groups_per_node + group_rank_in_pack\n\n        # Create a tensor that maps new group index back to original group index.\n        # This is a permutation tensor for groups within each layer.\n        # Shape: [num_layers, num_nodes * groups_per_node] (which is num_groups)\n        group_permutation = torch.empty((num_layers, num_groups),\n                                        dtype=torch.int64,\n                                        device=weight.device)\n        group_permutation.scatter_(1, new_group_idx,\n                                   torch.arange(num_groups, device=weight.device).expand(num_layers, -1))\n\n        # Reshape original weights to [num_layers, num_groups, group_size]\n        weight_reshaped = weight.view(num_layers, num_groups, group_size)\n\n        # Gather weights based on the group_permutation\n        # The .gather() operation reorders the groups according to the permutation.\n        # Shape: [num_layers, num_groups, group_size]\n        mlog_weights_ordered = weight_reshaped.gather(1, group_permutation.unsqueeze(-1).expand(-1, -1, group_size))\n        \n        # Reshape to [num_layers, num_nodes, groups_per_node * group_size]\n        # This is the final mlog_weights.\n        mlog_weights = mlog_weights_ordered.view(num_layers, num_nodes, groups_per_node * group_size)\n\n        # Construct mlog_to_orig_log_map similarly\n        # Original logical expert IDs, reshaped to [num_groups, group_size]\n        orig_log_ids_reshaped = torch.arange(num_logical_experts,\n                                             dtype=torch.int64,\n                                             device=weight.device).view(num_groups, group_size)\n        \n        # Expand to [num_layers, num_groups, group_size] and gather\n        mlog_to_orig_log_map_ordered = orig_log_ids_reshaped.unsqueeze(0).expand(num_layers, -1, -1).gather(\n            1, group_permutation.unsqueeze(-1).expand(-1, -1, group_size))\n        \n        # Reshape to [num_layers, num_nodes, groups_per_node * group_size]\n        mlog_to_orig_log_map = mlog_to_orig_log_map_ordered.view(\n            num_layers, num_nodes, groups_per_node * group_size)\n\n        # Now, `mlog_weights`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n        # And `mlog_to_orig_log_map`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n\n        # Step 2: Replicate experts within each node.\n        # `mlog_weights` are the weights for experts within each node.\n        # `num_physical_experts // num_nodes` is the target number of physical experts per node.\n        # `replicate_experts` takes `[X, num_log]` and returns `phy2log`, `rank`, `logcnt`.\n        # Here, X = num_layers * num_nodes, num_log = num_logical_experts // num_nodes.\n        # So, we need to reshape `mlog_weights` and `mlog_to_orig_log_map`.\n\n        num_middle_experts = num_logical_experts // num_nodes\n        reshaped_mlog_weights = mlog_weights.view(-1, num_middle_experts)\n        reshaped_mlog_to_orig_log_map = mlog_to_orig_log_map.view(-1, num_middle_experts)\n\n        # `phy2mlog`: [num_layers * num_nodes, num_physical_experts // num_nodes]\n        # `phyrank`: [num_layers * num_nodes, num_physical_experts // num_nodes]\n        # `mlogcnt`: [num_layers * num_nodes, num_middle_experts]\n        phy2mlog, phyrank, mlogcnt = replicate_experts(\n            reshaped_mlog_weights, num_physical_experts // num_nodes)\n\n        # Step 3: Pack physical experts to GPUs.\n        # `phy2mlog` maps physical experts (within each node) to middle logical experts.\n        # `phy2mlog` is `[num_layers * num_nodes, num_phy_per_node]`\n        # `phyrank` is `[num_layers * num_nodes, num_phy_per_node]`\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`\n\n        # We need to map these back to global physical expert indices.\n        # The `phy2mlog` refers to the *middle* logical experts.\n        # The `replicate_experts` function returns `phy2log` which is the *original* logical expert ID.\n        # So, `phy2mlog` from `replicate_experts` is actually `[num_layers*num_nodes, num_phy_per_node]`,\n        # where each entry is an index into the *middle* logical experts.\n\n        # We need to map these `phy2mlog` indices back to original logical expert IDs.\n        # `reshaped_mlog_to_orig_log_map`: [num_layers * num_nodes, num_middle_experts]\n        # `final_phy2log_per_node`: [num_layers * num_nodes, num_phy_per_node]\n        final_phy2log_per_node = reshaped_mlog_to_orig_log_map.gather(-1, phy2mlog)\n\n        # `phyrank` is the replica rank for the middle experts.\n        # We need to combine this with `mlogcnt` to get the final `logcnt`.\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`. It tells us how many replicas each middle expert has.\n        # The `phyrank` is the rank of the replica for a *specific* middle expert.\n\n        # The `logcnt` returned by `replicate_experts` is `[X, num_log]`, which is `[num_layers*num_nodes, num_middle_experts]`.\n        # We need to map this back to `[num_layers, num_logical_experts]`.\n\n        # Let's use the `mlogcnt` directly to get the total count of replicas for each logical expert.\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`.\n        # We need to sum/average counts based on the `mlog_to_orig_log_map`.\n\n        # Let's refine the output mapping:\n        # `physical_to_logical_map`: [num_layers, num_replicas]\n        # `logical_to_physical_map`: [num_layers, num_logical_experts, X]\n        # `expert_count`: [num_layers, num_logical_experts]\n\n        # `final_phy2log_per_node`: [num_layers * num_nodes, num_phy_per_node]\n        # This is the logical expert ID for each physical expert *within its node*.\n        # We need to convert this to global physical expert IDs.\n        # Physical experts are grouped by GPU. `num_gpus` total, `num_gpus // num_nodes` per node.\n        # `phy_experts_per_gpu` = `num_physical_experts // num_gpus`.\n        # So, each GPU has `phy_experts_per_gpu` physical experts.\n        # Each Node has `num_gpus // num_nodes` GPUs.\n        # Total physical experts per node = `(num_gpus // num_nodes) * phy_experts_per_gpu`\n        # This should equal `num_physical_experts // num_nodes`.\n\n        # `final_phy2log_per_node`: [num_layers, num_nodes, num_phy_per_node]\n        # `final_rank_per_node`: [num_layers, num_nodes, num_phy_per_node] (this is `phyrank`)\n\n        # We need to flatten this to `[num_layers, num_physical_experts]`.\n        # `num_physical_experts = num_nodes * num_phy_per_node`.\n\n        # `final_phy2log_global`: [num_layers, num_physical_experts]\n        # `final_rank_global`: [num_layers, num_physical_experts]\n\n        # The `mlog_to_orig_log_map` correctly maps middle experts to original logical experts.\n        # `phy2mlog` maps physical experts (within a node) to middle experts.\n        # So, `phy2mlog.gather(-1, final_phy2log_per_node)` gives the logical expert ID.\n        # This is what `final_phy2log_per_node` already represents.\n\n        # Let's rename `final_phy2log_per_node` to `phy2log_per_node` for clarity.\n        phy2log_per_node = final_phy2log_per_node\n        rank_per_node = phyrank # [num_layers * num_nodes, num_phy_per_node]\n\n        # Flatten these to get global mappings for physical experts.\n        phy2log_global = phy2log_per_node.view(num_layers, -1)\n        rank_global = rank_per_node.view(num_layers, -1)\n\n        # Now, let's construct the `logcnt` (expert_count).\n        # `mlogcnt`: [num_layers * num_nodes, num_middle_experts]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, num_middle_experts]\n\n        # We need to aggregate `mlogcnt` based on `mlog_to_orig_log_map`.\n        # For each logical expert, its total count is the sum of counts of its constituent middle experts.\n        # This aggregation needs to be done carefully.\n\n        # Let's create a tensor for `logcnt`: [num_layers, num_logical_experts] initialized to zeros.\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n\n        # The expert_count aggregation:\n        # mlog_to_orig_log_map: [num_layers, num_nodes, num_middle_experts]\n        # mlogcnt: [num_layers * num_nodes, num_middle_experts]\n        # Flatten mlog_to_orig_log_map and mlogcnt to [num_layers, N] for scatter_add_\n        mlog_to_orig_log_map_flat = mlog_to_orig_log_map.view(num_layers, -1)\n        mlogcnt_reshaped = mlogcnt.view(num_layers, -1)\n\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n        # Use scatter_add_ to efficiently aggregate counts for each logical expert across layers\n        expert_count.scatter_add_(1, mlog_to_orig_log_map_flat, mlogcnt_reshaped)\n\n        # The logical_to_physical_map construction is handled by the outer rebalance_experts function.\n        # So, this function only needs to return the global mappings and counts.\n        return phy2log_global, rank_global, expert_count\n\n    else:\n        # Global balancing strategy: treat all experts as one group, balance across num_gpus.\n        # This is equivalent to calling the hierarchical function with num_nodes=1, num_groups=1.\n        # The `rebalance_experts` function will call this branch if `num_groups % num_nodes != 0`.\n\n        # Step 1: Pack logical experts to GPUs.\n        # `balanced_packing` will distribute the logical experts across `num_gpus`.\n\n        # We need to adapt `balanced_packing` to work with `num_packs = num_gpus`.\n        # The `groups_per_pack` in `balanced_packing` refers to items per pack.\n        # Here, items are logical experts. So, `groups_per_pack` = `num_logical_experts // num_gpus`.\n        # `num_packs` = `num_gpus`.\n\n        pack_index_gpu, rank_in_gpu = balanced_packing(weight, num_gpus)\n\n        # `pack_index_gpu`: [num_layers, num_logical_experts] -> GPU ID for each logical expert.\n        # `rank_in_gpu`: [num_layers, num_logical_experts] -> rank within GPU for each logical expert.\n\n        # Step 2: Replicate experts within GPUs.\n        num_logical_experts_per_gpu = num_logical_experts // num_gpus\n\n        # For each GPU, we have a set of logical experts.\n        # We need to gather weights for logical experts assigned to the same GPU.\n        # `gpu_logical_weights`: [num_gpus, num_logical_experts_per_gpu, num_layers] (transposed for replicate_experts)\n        # `gpu_logical_to_orig_map`: [num_gpus, num_logical_experts_per_gpu]\n\n        # Vectorized construction of gpu_logical_weights and gpu_logical_to_orig_map\n        # This replaces the nested Python loops for better performance.\n\n        num_logical_experts_per_gpu = num_logical_experts // num_gpus\n\n        # Calculate the new \"middle expert group\" index for each original logical expert.\n        # This new index combines GPU assignment and rank within the GPU.\n        # Shape: [num_layers, num_logical_experts]\n        new_logical_expert_idx_on_gpu = pack_index_gpu * num_logical_experts_per_gpu + rank_in_gpu\n\n        # Create a tensor that maps new logical expert index back to original logical expert index.\n        # This is a permutation tensor for logical experts within each layer.\n        # Shape: [num_layers, num_gpus * num_logical_experts_per_gpu] (which is num_logical_experts)\n        logical_expert_permutation = torch.empty((num_layers, num_logical_experts),\n                                                 dtype=torch.int64,\n                                                 device=weight.device)\n        logical_expert_permutation.scatter_(1, new_logical_expert_idx_on_gpu,\n                                            torch.arange(num_logical_experts, device=weight.device).expand(num_layers, -1))\n\n        # Gather weights based on the logical_expert_permutation\n        # Shape: [num_layers, num_logical_experts]\n        gpu_logical_weights_ordered = weight.gather(1, logical_expert_permutation)\n        \n        # Reshape to [num_gpus, num_layers, num_logical_experts_per_gpu] for subsequent processing\n        # and ensure contiguous memory layout.\n        gpu_logical_weights = gpu_logical_weights_ordered.view(\n            num_layers, num_gpus, num_logical_experts_per_gpu).permute(1, 0, 2).contiguous()\n\n        # Construct gpu_logical_to_orig_map similarly\n        # Original logical expert IDs, expanded to [num_layers, num_logical_experts]\n        orig_log_ids_expanded = torch.arange(num_logical_experts,\n                                             dtype=torch.int64,\n                                             device=weight.device).unsqueeze(0).expand(num_layers, -1)\n        \n        # Gather original logical expert IDs based on the permutation\n        gpu_logical_to_orig_map_ordered = orig_log_ids_expanded.gather(1, logical_expert_permutation)\n        \n        # Reshape to [num_gpus * num_layers, num_logical_experts_per_gpu] for replicate_experts input\n        gpu_logical_to_orig_map = gpu_logical_to_orig_map_ordered.view(\n            num_layers, num_gpus, num_logical_experts_per_gpu).permute(1, 0, 2).contiguous().view(\n                -1, num_logical_experts_per_gpu)\n\n        # Reshape for `replicate_experts`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        reshaped_gpu_logical_weights = gpu_logical_weights.view(-1, num_logical_experts_per_gpu)\n        reshaped_gpu_logical_to_orig_map = gpu_logical_to_orig_map.view(-1, num_logical_experts_per_gpu)\n\n        # `replicate_experts` is called with `num_phy = num_physical_experts // num_gpus` (target physical experts per GPU).\n        # `X` becomes `num_gpus * num_layers`. `num_log` becomes `num_logical_experts_per_gpu`.\n        # `phy2log_gpu`: [num_gpus * num_layers, num_phy_per_gpu]\n        # `phyrank_gpu`: [num_gpus * num_layers, num_phy_per_gpu]\n        # `gpu_logcnt`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        phy2log_gpu, phyrank_gpu, gpu_logcnt = replicate_experts(\n            reshaped_gpu_logical_weights, num_physical_experts // num_gpus)\n\n        # Map `phy2log_gpu` back to original logical expert IDs using `reshaped_gpu_logical_to_orig_map`.\n        # `phy2log_global`: [num_gpus * num_layers, num_phy_per_gpu]\n        phy2log_global = reshaped_gpu_logical_to_orig_map.gather(-1, phy2log_gpu)\n\n        # Flatten to get the final `physical_to_logical_map` and `rank`.\n        # The `rank_global` is `phyrank_gpu`.\n        rank_global = phyrank_gpu\n\n        # Construct `expert_count` ([num_layers, num_logical_experts]).\n        # `gpu_logcnt`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n\n        for gpu_layer_idx in range(num_gpus * num_layers):\n            layer_idx = gpu_layer_idx // num_gpus\n            # Get the original logical expert indices for this gpu_layer combination\n            orig_log_indices = reshaped_gpu_logical_to_orig_map[gpu_layer_idx] # [num_logical_experts_per_gpu]\n            counts_for_orig_logs = gpu_logcnt[gpu_layer_idx] # [num_logical_experts_per_gpu]\n\n            # Add these counts to the correct positions in the expert_count tensor\n            expert_count[layer_idx].index_add_(0, orig_log_indices, counts_for_orig_logs)\n\n        # Construct `logical_to_physical_map` ([num_layers, num_logical_experts, X])\n        max_replicas_per_expert = (num_physical_experts + num_logical_experts - 1) // num_logical_experts\n\n        logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert), -1, dtype=torch.int64, device=weight.device)\n\n        # Need to map `phy2log_global` and `rank_global` to the correct positions.\n        # `phy2log_global` are indices into the *original* logical experts.\n        # `rank_global` is the replica rank for that original logical expert.\n\n        # Reshape to get mappings per layer and then iterate through physical experts.\n        phy2log_global_reshaped = phy2log_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous()\n        rank_global_reshaped = rank_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous()\n\n        # Iterate through all physical experts\n        for layer_idx in range(num_layers):\n            for gpu_idx in range(num_gpus):\n                for phys_idx_in_gpu in range(num_physical_experts // num_gpus):\n                    # Get the global physical expert index\n                    global_phys_idx = gpu_idx * (num_physical_experts // num_gpus) + phys_idx_in_gpu\n\n                    log_expert_id = phy2log_global_reshaped[layer_idx, gpu_idx, phys_idx_in_gpu]\n                    replica_rank = rank_global_reshaped[layer_idx, gpu_idx, phys_idx_in_gpu]\n\n                    if log_expert_id >= 0 and log_expert_id < num_logical_experts:\n                        if replica_rank < max_replicas_per_expert:\n                            logical_to_physical_map[layer_idx, log_expert_id, replica_rank] = global_phys_idx\n                        else:\n                            pass # Handle error or log\n\n        # The `phy2log_global` needs to be reshaped to `[num_layers, num_replicas]`\n        # where `num_replicas` is `num_physical_experts`.\n        # `phy2log_global` is currently `[num_gpus * num_layers, num_phy_per_gpu]`.\n        # We need to combine these.\n        # The order of physical experts should be consistent.\n        # Let's reconstruct `phy2log_global` to be `[num_layers, num_physical_experts]`.\n\n        final_phy2log_global = torch.empty((num_layers, num_physical_experts), dtype=torch.int64, device=weight.device)\n        current_phys_idx = 0\n        for gpu_idx in range(num_gpus):\n            for layer_idx in range(num_layers):\n                # Calculate the range of physical experts for this GPU and layer combination\n                start_phys_idx = layer_idx * num_gpus * (num_physical_experts // num_gpus) + gpu_idx * (num_physical_experts // num_gpus)\n                end_phys_idx = start_phys_idx + (num_physical_experts // num_gpus)\n\n                # The `phy2log_global` calculated earlier is `[num_gpus * num_layers, num_phy_per_gpu]`\n                # We need to map it correctly.\n                # `phy2log_gpu` was `[num_gpus * num_layers, num_phy_per_gpu]`\n                # `reshaped_gpu_logical_to_orig_map` was `[num_gpus * num_layers, num_phy_per_gpu]`\n                # `phy2log_global = reshaped_gpu_logical_to_orig_map.gather(-1, phy2log_gpu)`\n\n                # The indices for `phy2log_global` should be:\n                # For layer 0, GPU 0: phys experts 0 to num_phy_per_gpu - 1\n                # For layer 0, GPU 1: phys experts num_phy_per_gpu to 2*num_phy_per_gpu - 1\n                # ...\n                # For layer 1, GPU 0: phys experts num_gpus * num_phy_per_gpu to (num_gpus+1)*num_phy_per_gpu - 1\n\n                # Let's re-index the `phy2log_global` and `rank_global` to match `[num_layers, num_physical_experts]`\n                # The current `phy2log_global` is effectively `[num_layers * num_gpus, num_phy_per_gpu]`\n                # We need to rearrange it.\n\n                # Example: num_layers=2, num_gpus=2, num_phy_per_gpu=2. num_physical_experts=4.\n                # phy2log_global:\n                # [layer0_gpu0_phys0, layer0_gpu0_phys1,\n                #  layer0_gpu1_phys0, layer0_gpu1_phys1,\n                #  layer1_gpu0_phys0, layer1_gpu0_phys1,\n                #  layer1_gpu1_phys0, layer1_gpu1_phys1]\n                # Reshaped to [num_gpus*num_layers, num_phy_per_gpu] -> [4, 2]\n                # We want [num_layers, num_physical_experts] -> [2, 4]\n\n                # The current `phy2log_global` is `[num_gpus * num_layers, num_phy_per_gpu]`.\n                # Permute and reshape:\n                # `view(num_gpus, num_layers, -1)` gives `[num_gpus, num_layers, num_phy_per_gpu]`\n                # `permute(1, 0, 2)` gives `[num_layers, num_gpus, num_phy_per_gpu]`\n                # `contiguous().view(num_layers, -1)` gives `[num_layers, num_gpus * num_phy_per_gpu]` = `[num_layers, num_physical_experts]`\n\n                final_phy2log_global = phy2log_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous().view(num_layers, num_physical_experts)\n                final_rank_global = rank_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous().view(num_layers, num_physical_experts)\n\n        # Return the results for global balancing.\n        return final_phy2log_global, logical_to_physical_map, expert_count\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float().cpu()\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    # The `rebalance_experts_hierarchical` and the global balancing logic\n    # now produce `phy2log_global`, `rank_global`, and `expert_count`.\n    # `phy2log_global`: [num_layers, num_replicas] -> maps physical expert index to logical expert ID.\n    # `rank_global`: [num_layers, num_replicas] -> maps physical expert index to its replica rank.\n    # `expert_count`: [num_layers, num_logical_experts] -> number of replicas for each logical expert.\n\n    # We need to construct `logical_to_physical_map`: [layers, num_logical_experts, X]\n    # where X is the maximum number of replicas for any logical expert.\n\n    # Store device for later use.\n    device = weight.device \n\n    num_physical_experts_total = num_replicas\n\n    # Correctly calculate max_replicas_per_expert from logcnt.\n    # This is the maximum number of replicas any single logical expert can have.\n    max_replicas_per_expert = torch.max(logcnt).item() if logcnt.numel() > 0 else 0\n\n    # Handle edge case where logcnt might be all zeros or empty.\n    # If there are logical experts and replicas, ensure max_replicas_per_expert is at least 1.\n    if num_logical_experts > 0 and num_replicas > 0 and max_replicas_per_expert == 0:\n        max_replicas_per_expert = 1\n    \n    # Initialize the map with -1 (indicating no physical expert assigned).\n    # The size of the last dimension is `max_replicas_per_expert`.\n    logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert),\n                                         -1, dtype=torch.int64, device=device)\n\n    # Create physical expert IDs corresponding to the indices of phy2log/rank.\n    # These are the global indices of the physical experts.\n    physical_expert_ids = torch.arange(num_replicas, dtype=torch.int64, device=device)\n    \n    # Expand physical_expert_ids to match the shape of phy2log and rank for broadcasting assignment.\n    # `phy2log` and `rank` are [num_layers, num_replicas].\n    # `physical_expert_ids` needs to be [num_layers, num_replicas] for direct assignment.\n    physical_expert_ids_expanded = physical_expert_ids.unsqueeze(0).expand(num_layers, num_replicas)\n\n    # Create layer indices for advanced indexing.\n    # `layer_indices` will be [num_layers, 1] and broadcasts to [num_layers, num_replicas].\n    layer_indices = torch.arange(num_layers, dtype=torch.int64, device=device).unsqueeze(1)\n    \n    # Flatten indices and values for masked assignment to avoid out-of-bounds errors.\n    flat_layer_indices = layer_indices.flatten()\n    flat_phy2log = phy2log.flatten()\n    flat_rank = rank.flatten()\n    flat_physical_ids = physical_expert_ids_expanded.flatten()\n    \n    # Create a mask for valid assignments:\n    # `phy2log` must be a valid logical expert index.\n    # `rank` must be a valid replica rank index for the map's last dimension.\n    valid_mask_flat = (flat_phy2log >= 0) & (flat_phy2log < num_logical_experts) & \\\n                      (flat_rank >= 0) & (flat_rank < max_replicas_per_expert)\n    \n    # Perform the assignment only for valid indices.\n    if valid_mask_flat.any():\n        logical_to_physical_map[\n            flat_layer_indices[valid_mask_flat],\n            flat_phy2log[valid_mask_flat],\n            flat_rank[valid_mask_flat]\n        ] = flat_physical_ids[valid_mask_flat]\n\n    return phy2log, logical_to_physical_map, logcnt\n\n    # The `logcnt` (expert_count) is already computed by the balancing functions.\n    # `phy2log` is the first return value.\n    return phy2log, logical_to_physical_map, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n", "language": "python", "parent_id": "e095423b-931c-4ff5-85cf-23bd404573f5", "generation": 6, "timestamp": 1751051427.4344504, "iteration_found": 27, "metrics": {"balancedness_score": 0.0, "speed_score": 0.0, "combined_score": 0.0, "error": "name 'rank' is not defined"}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 25 lines with 53 lines", "parent_metrics": {"balancedness_score": 0.0, "speed_score": 0.0, "combined_score": 0.0, "error": "index 2 is out of bounds for dimension 2 with size 2"}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve the Mixture-of-Expert models Expert Parallelism Load Balancer (MoE EPLB) expert rearrangement algorithm.\nThis algorithm will take the load metrics recorded by the vLLM server, and rearrange the experts to balance the load. It can make replicas of some experts to achieve better load balancing.\nYour goal will be two-fold: 1. Improve the algorithm to achieve better load balancing; while 2. Improve the algorithm to be more efficient, i.e. reduce the execution time of the algorithm itself, since perfect load balancing is NP-hard.\nThe current algorithm is implemented in the `rebalance_experts` function. ", "user": "# Current Program Information\n- Current performance metrics: - balancedness_score: 0.0000\n- speed_score: 0.0000\n- combined_score: 0.0000\n- error: index 2 is out of bounds for dimension 2 with size 2\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: balancedness_score, speed_score, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0340, combined_score: 0.1661\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0350, combined_score: 0.1667\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0405, combined_score: 0.1694\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.1694)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0405), Performs well on combined_score (0.1694)\n\n\n### Program 2 (Score: 0.1667)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0350), Performs well on combined_score (0.1667)\n\n\n### Program 3 (Score: 0.1661)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0340), Performs well on combined_score (0.1661)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.1614)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n\n### Program D2 (Score: 0.1623)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.1694, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 79 lines with 108 lines\nChange 2: Replace 24 lines with 43 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 2 (Score: 0.0872, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 87 lines with 89 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 3 (Score: 0.0000, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 4 (Score: 0.1595, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 6 lines with 8 lines\nChange 2: Replace 3 lines with 3 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 5 (Score: 0.0000, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 42 lines with 57 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n\n# Current Program\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort weights and get indices\n    sorted_weights, sorted_indices = weight.float().sort(-1, descending=True)\n\n    # Store device from input weight\n    device = weight.device\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort weights and get indices\n    sorted_weights, sorted_indices = weight.float().sort(-1, descending=True)\n\n    # Initialize pack_index and rank_in_pack on the correct device\n    pack_index = torch.full((num_layers, num_groups), -1, dtype=torch.int64, device=device)\n    rank_in_pack = torch.full((num_layers, num_groups), -1, dtype=torch.int64, device=device)\n\n    # Initialize pack_loads and pack_counts on the correct device\n    pack_loads = torch.zeros(num_layers, num_packs, dtype=torch.float32, device=device)\n    pack_counts = torch.zeros(num_layers, num_packs, dtype=torch.int64, device=device)\n\n    # Iterate through sorted items and assign them to the least loaded pack\n    for i in range(num_layers):\n        for g in range(num_groups):\n            available_packs = pack_counts[i] < groups_per_pack\n            if not available_packs.any():\n                continue\n\n            current_pack_loads = pack_loads[i][available_packs]\n            current_pack_indices = torch.arange(num_packs, device=device)[available_packs]\n\n            min_load_idx_in_available = torch.argmin(current_pack_loads)\n            chosen_pack_idx = current_pack_indices[min_load_idx_in_available]\n\n            original_group_idx = sorted_indices[i, g]\n\n            pack_index[i, original_group_idx] = chosen_pack_idx\n            rank_in_pack[i, original_group_idx] = pack_counts[i, chosen_pack_idx]\n\n            pack_loads[i, chosen_pack_idx] += weight[i, original_group_idx]\n            pack_counts[i, chosen_pack_idx] += 1\n\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Unified rebalancing logic:\n    # The hierarchical approach is used when num_groups is divisible by num_nodes.\n    # Otherwise, it falls back to a global balancing strategy by setting num_nodes=1 and num_groups=1.\n    # We can simplify this by directly implementing the global strategy when needed,\n    # or by ensuring the hierarchical logic correctly handles num_nodes=1.\n\n    # Determine the effective number of groups and nodes for balancing.\n    # If num_groups is not divisible by num_nodes, we treat it as a single group\n    # across all nodes for the first level of balancing.\n    effective_num_groups = num_groups if num_groups % num_nodes == 0 else 1\n    effective_num_nodes = num_nodes if num_groups % num_nodes == 0 else 1\n\n    # If num_groups % num_nodes != 0, we effectively want to balance across all groups as if they were one large group.\n    # And then balance those groups onto nodes.\n    # This is equivalent to setting num_nodes = 1 and num_groups = 1 in the hierarchical logic.\n    # Let's adjust parameters for a unified call to a simplified hierarchical logic.\n\n    # Simplified approach:\n    # First, balance logical experts across groups within layers.\n    # Then, balance these groups across nodes.\n    # Finally, replicate experts within nodes to balance across GPUs.\n\n    # Step 1: Balance logical experts into groups.\n    # If num_groups == 1, this step is trivial and doesn't change anything.\n    group_size = num_logical_experts // num_groups\n    tokens_per_group_layer = weight.unflatten(-1, (num_groups, group_size)).sum(-1) # [num_layers, num_groups]\n\n    # Pack groups onto nodes.\n    # If num_groups % num_nodes != 0, we effectively want to balance all groups onto a single 'node' conceptually,\n    # and then distribute those across the actual num_nodes.\n    # Let's consider the case where num_groups is not divisible by num_nodes.\n    # The original code uses num_nodes for packing groups. If num_groups < num_nodes, this packing might be problematic.\n    # A more robust approach is to balance groups onto num_nodes, and if num_groups < num_nodes, some nodes get 0 groups.\n    # The current `balanced_packing` assumes num_groups is divisible by num_packs (num_nodes here).\n\n    # Let's redefine the packing strategy:\n    # We want to pack 'num_groups' items (groups) into 'num_nodes' packs.\n    # If num_groups is not divisible by num_nodes, balanced_packing needs to handle it.\n    # The current `balanced_packing` asserts `num_groups % num_packs == 0`.\n    # This means if num_groups % num_nodes != 0, the original code calls `rebalance_experts_hierarchical` with num_nodes=1, num_groups=1.\n    # This is a global balancing strategy.\n\n    # Let's ensure the function call correctly reflects the intended strategy.\n    # If num_groups % num_nodes == 0, use hierarchical.\n    # Otherwise, use global (which is hierarchical with num_nodes=1, num_groups=1).\n\n    if num_groups % num_nodes == 0:\n        # Hierarchical balancing: balance groups to nodes, then nodes to GPUs.\n        # Step 1: Pack groups to nodes.\n        group_pack_index, group_rank_in_pack = balanced_packing(tokens_per_group_layer, num_nodes)\n        # Map logical experts to \"middle\" logical experts (logical experts per node)\n        # log2mlog: [num_layers, num_logical_experts] -> maps logical expert to its middle logical expert\n        mlog2log_map = (((group_pack_index * groups_per_node + group_rank_in_pack) * group_size).unsqueeze(-1) +\n                        torch.arange(group_size, dtype=torch.int64, device=group_pack_index.device)).flatten(-2)\n        # mlog2log_map: [num_layers, num_logical_experts] maps each logical expert to its \"middle\" logical expert ID.\n        # We need the inverse mapping for `weight.gather` later.\n        # This inverse mapping is tricky because multiple logical experts can map to the same middle expert.\n        # The `inverse` function is designed for permutations, which isn't directly applicable here.\n\n        # Let's rethink the mapping and replication.\n        # We have `num_layers` of `weight` ([num_layers, num_logical_experts]).\n        # First, we group logical experts: `tokens_per_group_layer` ([num_layers, num_groups]).\n        # Then, we pack these groups onto `num_nodes`: `group_pack_index` ([num_layers, num_groups]) and `group_rank_in_pack` ([num_layers, num_groups]).\n        # This means `num_groups` are assigned to `num_nodes`. `groups_per_node` groups per node.\n        # So, we have `num_layers * num_nodes` \"node-level\" expert weight tensors.\n        # Each node-level tensor has `num_logical_experts // num_nodes` \"middle\" experts.\n\n        # Let's create the \"middle\" logical experts' weights.\n        # `mlog_weights`: [num_layers * num_nodes, num_logical_experts // num_nodes]\n        # This requires carefully gathering weights.\n        # The `mlog2log` mapping needs to be constructed correctly.\n\n        # `mlog2log_map` assigns each logical expert to a conceptual \"middle\" logical expert.\n        # e.g., if num_logical_experts=8, num_groups=2, group_size=4.\n        # Group 0 contains logical experts 0,1,2,3. Group 1 contains 4,5,6,7.\n        # If num_nodes=2, groups_per_node=1.\n        # Group 0 (log experts 0-3) goes to node 0. Group 1 (log experts 4-7) goes to node 1.\n        # Node 0 has 1 middle expert. Node 1 has 1 middle expert.\n        # Middle expert for node 0 is logical experts 0-3. Middle expert for node 1 is 4-7.\n        # The mapping `mlog2log_map` should map:\n        # 0,1,2,3 -> 0 (on node 0)\n        # 4,5,6,7 -> 0 (on node 1)\n        # The current `mlog2log` calculation seems to be constructing the indices of the middle experts.\n        # `log2mlog` is `[num_layers, num_logical_experts]`.\n        # It maps logical expert ID to its index within the middle expert group.\n        # The `mlog2log` is the inverse of this index mapping.\n\n        # Let's reconstruct `mlog2log` and `tokens_per_mlog` more directly.\n        # For each layer, we have `num_groups` of `group_size`.\n        # `group_pack_index`: [num_layers, num_groups] - node assignment for each group.\n        # `group_rank_in_pack`: [num_layers, num_groups] - index of group within the node's assigned groups.\n\n        # We want to create `num_layers * num_nodes` \"node-level\" expert groups.\n        # Each node-level group corresponds to a set of original logical experts.\n        # The `tokens_per_mlog` should be `[num_layers * num_nodes, num_logical_experts // num_nodes]`.\n\n        # Calculate the logical expert index for each \"middle\" expert.\n        # `mlog_expert_indices`: [num_layers, num_nodes, groups_per_node, group_size]\n        # This is becoming complex. Let's simplify the conceptualization.\n\n        # Simpler approach:\n        # 1. Pack groups to nodes. This determines which logical experts are grouped together on each node.\n        # `group_pack_index`: [num_layers, num_groups] -> node ID for each group.\n        # `group_rank_in_pack`: [num_layers, num_groups] -> rank within node for each group.\n\n        # 2. For each node, create a combined weight tensor for the logical experts assigned to it.\n        #    The number of logical experts per node will be `groups_per_node * group_size`.\n        #    We need to gather the weights of logical experts based on `group_pack_index`.\n\n        # Let's create a mapping from (layer, node) to a contiguous block of logical experts.\n        # `node_logical_expert_map`: [num_layers, num_nodes, groups_per_node * group_size]\n\n        # This requires constructing a permutation for each layer and node.\n        # The original `mlog2log` calculation is:\n        # `((group_pack_index * groups_per_node + group_rank_in_pack) * group_size)`\n        # This forms a base index for each logical expert.\n        # `log2mlog` is this base index + `arange(group_size)`.\n        # `mlog2log` is the inverse of `log2mlog`.\n\n        # Let's directly construct the `tokens_per_mlog` and `mlog2log`.\n        # `tokens_per_mlog` is the weight tensor for experts *within* each node, considering the grouped logical experts.\n        # The number of such \"middle\" experts per node is `groups_per_node * group_size`.\n\n        # For each layer:\n        # `group_pack_index` tells us which node each group belongs to.\n        # `group_rank_in_pack` tells us the order within the node.\n\n        # Let's create `num_layers * num_nodes` tensors, each of size `groups_per_node * group_size`.\n        # `mlog_weights`: [num_layers * num_nodes, groups_per_node * group_size]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, groups_per_node * group_size]\n\n        # Example: num_layers=1, num_logical_experts=8, num_groups=2, group_size=4, num_nodes=2, groups_per_node=1, num_gpus=2, phy_experts_per_gpu=1.\n        # weight: [1, 8]\n        # tokens_per_group_layer: [1, 2] (sum of weights for group 0 and group 1)\n        # balanced_packing(tokens_per_group_layer, num_nodes=2) ->\n        #   group_pack_index: [1, 2] -> [[0, 1]] (group 0 to node 0, group 1 to node 1)\n        #   group_rank_in_pack: [1, 2] -> [[0, 0]] (rank 0 within node for both groups)\n        # groups_per_node = 1, group_size = 4.\n\n        # Middle expert for node 0 is logical experts 0,1,2,3.\n        # Middle expert for node 1 is logical experts 4,5,6,7.\n        # `mlog_to_orig_log_map` for node 0 should be [0,1,2,3].\n        # `mlog_to_orig_log_map` for node 1 should be [4,5,6,7].\n\n        # The original `mlog2log` calculation:\n        # `base_idx = (group_pack_index * groups_per_node + group_rank_in_pack) * group_size`\n        # `base_idx`: [1, 2] -> [[0*1+0, 1*1+0]] * 4 = [[0, 4]]\n        # `log2mlog`: `base_idx.unsqueeze(-1) + torch.arange(group_size)`\n        # `log2mlog`: [[0, 4]].unsqueeze(-1) + [0,1,2,3] -> [[0,1,2,3], [4,5,6,7]]\n        # This `log2mlog` is `[num_layers, num_logical_experts]`. It maps original logical expert ID to its \"middle\" expert index.\n        # The original logic uses `weight.gather(-1, mlog2log)` which is incorrect here.\n        # `weight.gather(-1, mlog2log)` expects `mlog2log` to be `[num_layers, num_middle_experts]` where `num_middle_experts` is the number of columns in `weight`.\n        # The shape should be `[num_layers, num_logical_experts]` if we are gathering from `weight` which is `[num_layers, num_logical_experts]`.\n        # The `mlog2log` calculated here has shape `[num_layers, num_logical_experts]`.\n        # It maps `original_logical_expert_id` to `middle_expert_id`.\n        # We need to gather `weight[layer, original_logical_expert_id]` and group them by `middle_expert_id`.\n\n        # Let's create `mlog_weights` directly.\n        # `mlog_weights`: [num_layers * num_nodes, groups_per_node * group_size]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, groups_per_node * group_size]\n\n        # Vectorized construction of mlog_weights and mlog_to_orig_log_map\n        # This replaces the nested Python loops for better performance.\n\n        # Calculate the new \"middle expert group\" index for each original group.\n        # This new index combines node assignment and rank within the node.\n        # Shape: [num_layers, num_groups]\n        new_group_idx = group_pack_index * groups_per_node + group_rank_in_pack\n\n        # Create a tensor that maps new group index back to original group index.\n        # This is a permutation tensor for groups within each layer.\n        # Shape: [num_layers, num_nodes * groups_per_node] (which is num_groups)\n        group_permutation = torch.empty((num_layers, num_groups),\n                                        dtype=torch.int64,\n                                        device=weight.device)\n        group_permutation.scatter_(1, new_group_idx,\n                                   torch.arange(num_groups, device=weight.device).expand(num_layers, -1))\n\n        # Reshape original weights to [num_layers, num_groups, group_size]\n        weight_reshaped = weight.view(num_layers, num_groups, group_size)\n\n        # Gather weights based on the group_permutation\n        # The .gather() operation reorders the groups according to the permutation.\n        # Shape: [num_layers, num_groups, group_size]\n        mlog_weights_ordered = weight_reshaped.gather(1, group_permutation.unsqueeze(-1).expand(-1, -1, group_size))\n        \n        # Reshape to [num_layers, num_nodes, groups_per_node * group_size]\n        # This is the final mlog_weights.\n        mlog_weights = mlog_weights_ordered.view(num_layers, num_nodes, groups_per_node * group_size)\n\n        # Construct mlog_to_orig_log_map similarly\n        # Original logical expert IDs, reshaped to [num_groups, group_size]\n        orig_log_ids_reshaped = torch.arange(num_logical_experts,\n                                             dtype=torch.int64,\n                                             device=weight.device).view(num_groups, group_size)\n        \n        # Expand to [num_layers, num_groups, group_size] and gather\n        mlog_to_orig_log_map_ordered = orig_log_ids_reshaped.unsqueeze(0).expand(num_layers, -1, -1).gather(\n            1, group_permutation.unsqueeze(-1).expand(-1, -1, group_size))\n        \n        # Reshape to [num_layers, num_nodes, groups_per_node * group_size]\n        mlog_to_orig_log_map = mlog_to_orig_log_map_ordered.view(\n            num_layers, num_nodes, groups_per_node * group_size)\n\n        # Now, `mlog_weights`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n        # And `mlog_to_orig_log_map`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n\n        # Step 2: Replicate experts within each node.\n        # `mlog_weights` are the weights for experts within each node.\n        # `num_physical_experts // num_nodes` is the target number of physical experts per node.\n        # `replicate_experts` takes `[X, num_log]` and returns `phy2log`, `rank`, `logcnt`.\n        # Here, X = num_layers * num_nodes, num_log = num_logical_experts // num_nodes.\n        # So, we need to reshape `mlog_weights` and `mlog_to_orig_log_map`.\n\n        num_middle_experts = num_logical_experts // num_nodes\n        reshaped_mlog_weights = mlog_weights.view(-1, num_middle_experts)\n        reshaped_mlog_to_orig_log_map = mlog_to_orig_log_map.view(-1, num_middle_experts)\n\n        # `phy2mlog`: [num_layers * num_nodes, num_physical_experts // num_nodes]\n        # `phyrank`: [num_layers * num_nodes, num_physical_experts // num_nodes]\n        # `mlogcnt`: [num_layers * num_nodes, num_middle_experts]\n        phy2mlog, phyrank, mlogcnt = replicate_experts(\n            reshaped_mlog_weights, num_physical_experts // num_nodes)\n\n        # Step 3: Pack physical experts to GPUs.\n        # `phy2mlog` maps physical experts (within each node) to middle logical experts.\n        # `phy2mlog` is `[num_layers * num_nodes, num_phy_per_node]`\n        # `phyrank` is `[num_layers * num_nodes, num_phy_per_node]`\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`\n\n        # We need to map these back to global physical expert indices.\n        # The `phy2mlog` refers to the *middle* logical experts.\n        # The `replicate_experts` function returns `phy2log` which is the *original* logical expert ID.\n        # So, `phy2mlog` from `replicate_experts` is actually `[num_layers*num_nodes, num_phy_per_node]`,\n        # where each entry is an index into the *middle* logical experts.\n\n        # We need to map these `phy2mlog` indices back to original logical expert IDs.\n        # `reshaped_mlog_to_orig_log_map`: [num_layers * num_nodes, num_middle_experts]\n        # `final_phy2log_per_node`: [num_layers * num_nodes, num_phy_per_node]\n        final_phy2log_per_node = reshaped_mlog_to_orig_log_map.gather(-1, phy2mlog)\n\n        # `phyrank` is the replica rank for the middle experts.\n        # We need to combine this with `mlogcnt` to get the final `logcnt`.\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`. It tells us how many replicas each middle expert has.\n        # The `phyrank` is the rank of the replica for a *specific* middle expert.\n\n        # The `logcnt` returned by `replicate_experts` is `[X, num_log]`, which is `[num_layers*num_nodes, num_middle_experts]`.\n        # We need to map this back to `[num_layers, num_logical_experts]`.\n\n        # Let's use the `mlogcnt` directly to get the total count of replicas for each logical expert.\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`.\n        # We need to sum/average counts based on the `mlog_to_orig_log_map`.\n\n        # Let's refine the output mapping:\n        # `physical_to_logical_map`: [num_layers, num_replicas]\n        # `logical_to_physical_map`: [num_layers, num_logical_experts, X]\n        # `expert_count`: [num_layers, num_logical_experts]\n\n        # `final_phy2log_per_node`: [num_layers * num_nodes, num_phy_per_node]\n        # This is the logical expert ID for each physical expert *within its node*.\n        # We need to convert this to global physical expert IDs.\n        # Physical experts are grouped by GPU. `num_gpus` total, `num_gpus // num_nodes` per node.\n        # `phy_experts_per_gpu` = `num_physical_experts // num_gpus`.\n        # So, each GPU has `phy_experts_per_gpu` physical experts.\n        # Each Node has `num_gpus // num_nodes` GPUs.\n        # Total physical experts per node = `(num_gpus // num_nodes) * phy_experts_per_gpu`\n        # This should equal `num_physical_experts // num_nodes`.\n\n        # `final_phy2log_per_node`: [num_layers, num_nodes, num_phy_per_node]\n        # `final_rank_per_node`: [num_layers, num_nodes, num_phy_per_node] (this is `phyrank`)\n\n        # We need to flatten this to `[num_layers, num_physical_experts]`.\n        # `num_physical_experts = num_nodes * num_phy_per_node`.\n\n        # `final_phy2log_global`: [num_layers, num_physical_experts]\n        # `final_rank_global`: [num_layers, num_physical_experts]\n\n        # The `mlog_to_orig_log_map` correctly maps middle experts to original logical experts.\n        # `phy2mlog` maps physical experts (within a node) to middle experts.\n        # So, `phy2mlog.gather(-1, final_phy2log_per_node)` gives the logical expert ID.\n        # This is what `final_phy2log_per_node` already represents.\n\n        # Let's rename `final_phy2log_per_node` to `phy2log_per_node` for clarity.\n        phy2log_per_node = final_phy2log_per_node\n        rank_per_node = phyrank # [num_layers * num_nodes, num_phy_per_node]\n\n        # Flatten these to get global mappings for physical experts.\n        phy2log_global = phy2log_per_node.view(num_layers, -1)\n        rank_global = rank_per_node.view(num_layers, -1)\n\n        # Now, let's construct the `logcnt` (expert_count).\n        # `mlogcnt`: [num_layers * num_nodes, num_middle_experts]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, num_middle_experts]\n\n        # We need to aggregate `mlogcnt` based on `mlog_to_orig_log_map`.\n        # For each logical expert, its total count is the sum of counts of its constituent middle experts.\n        # This aggregation needs to be done carefully.\n\n        # Let's create a tensor for `logcnt`: [num_layers, num_logical_experts] initialized to zeros.\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n\n        # The expert_count aggregation:\n        # mlog_to_orig_log_map: [num_layers, num_nodes, num_middle_experts]\n        # mlogcnt: [num_layers * num_nodes, num_middle_experts]\n        # Flatten mlog_to_orig_log_map and mlogcnt to [num_layers, N] for scatter_add_\n        mlog_to_orig_log_map_flat = mlog_to_orig_log_map.view(num_layers, -1)\n        mlogcnt_reshaped = mlogcnt.view(num_layers, -1)\n\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n        # Use scatter_add_ to efficiently aggregate counts for each logical expert across layers\n        expert_count.scatter_add_(1, mlog_to_orig_log_map_flat, mlogcnt_reshaped)\n\n        # The logical_to_physical_map construction is handled by the outer rebalance_experts function.\n        # So, this function only needs to return the global mappings and counts.\n        return phy2log_global, rank_global, expert_count\n\n    else:\n        # Global balancing strategy: treat all experts as one group, balance across num_gpus.\n        # This is equivalent to calling the hierarchical function with num_nodes=1, num_groups=1.\n        # The `rebalance_experts` function will call this branch if `num_groups % num_nodes != 0`.\n\n        # Step 1: Pack logical experts to GPUs.\n        # `balanced_packing` will distribute the logical experts across `num_gpus`.\n\n        # We need to adapt `balanced_packing` to work with `num_packs = num_gpus`.\n        # The `groups_per_pack` in `balanced_packing` refers to items per pack.\n        # Here, items are logical experts. So, `groups_per_pack` = `num_logical_experts // num_gpus`.\n        # `num_packs` = `num_gpus`.\n\n        pack_index_gpu, rank_in_gpu = balanced_packing(weight, num_gpus)\n\n        # `pack_index_gpu`: [num_layers, num_logical_experts] -> GPU ID for each logical expert.\n        # `rank_in_gpu`: [num_layers, num_logical_experts] -> rank within GPU for each logical expert.\n\n        # Step 2: Replicate experts within GPUs.\n        num_logical_experts_per_gpu = num_logical_experts // num_gpus\n\n        # For each GPU, we have a set of logical experts.\n        # We need to gather weights for logical experts assigned to the same GPU.\n        # `gpu_logical_weights`: [num_gpus, num_logical_experts_per_gpu, num_layers] (transposed for replicate_experts)\n        # `gpu_logical_to_orig_map`: [num_gpus, num_logical_experts_per_gpu]\n\n        # Vectorized construction of gpu_logical_weights and gpu_logical_to_orig_map\n        # This replaces the nested Python loops for better performance.\n\n        num_logical_experts_per_gpu = num_logical_experts // num_gpus\n\n        # Calculate the new \"middle expert group\" index for each original logical expert.\n        # This new index combines GPU assignment and rank within the GPU.\n        # Shape: [num_layers, num_logical_experts]\n        new_logical_expert_idx_on_gpu = pack_index_gpu * num_logical_experts_per_gpu + rank_in_gpu\n\n        # Create a tensor that maps new logical expert index back to original logical expert index.\n        # This is a permutation tensor for logical experts within each layer.\n        # Shape: [num_layers, num_gpus * num_logical_experts_per_gpu] (which is num_logical_experts)\n        logical_expert_permutation = torch.empty((num_layers, num_logical_experts),\n                                                 dtype=torch.int64,\n                                                 device=weight.device)\n        logical_expert_permutation.scatter_(1, new_logical_expert_idx_on_gpu,\n                                            torch.arange(num_logical_experts, device=weight.device).expand(num_layers, -1))\n\n        # Gather weights based on the logical_expert_permutation\n        # Shape: [num_layers, num_logical_experts]\n        gpu_logical_weights_ordered = weight.gather(1, logical_expert_permutation)\n        \n        # Reshape to [num_gpus, num_layers, num_logical_experts_per_gpu] for subsequent processing\n        # and ensure contiguous memory layout.\n        gpu_logical_weights = gpu_logical_weights_ordered.view(\n            num_layers, num_gpus, num_logical_experts_per_gpu).permute(1, 0, 2).contiguous()\n\n        # Construct gpu_logical_to_orig_map similarly\n        # Original logical expert IDs, expanded to [num_layers, num_logical_experts]\n        orig_log_ids_expanded = torch.arange(num_logical_experts,\n                                             dtype=torch.int64,\n                                             device=weight.device).unsqueeze(0).expand(num_layers, -1)\n        \n        # Gather original logical expert IDs based on the permutation\n        gpu_logical_to_orig_map_ordered = orig_log_ids_expanded.gather(1, logical_expert_permutation)\n        \n        # Reshape to [num_gpus * num_layers, num_logical_experts_per_gpu] for replicate_experts input\n        gpu_logical_to_orig_map = gpu_logical_to_orig_map_ordered.view(\n            num_layers, num_gpus, num_logical_experts_per_gpu).permute(1, 0, 2).contiguous().view(\n                -1, num_logical_experts_per_gpu)\n\n        # Reshape for `replicate_experts`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        reshaped_gpu_logical_weights = gpu_logical_weights.view(-1, num_logical_experts_per_gpu)\n        reshaped_gpu_logical_to_orig_map = gpu_logical_to_orig_map.view(-1, num_logical_experts_per_gpu)\n\n        # `replicate_experts` is called with `num_phy = num_physical_experts // num_gpus` (target physical experts per GPU).\n        # `X` becomes `num_gpus * num_layers`. `num_log` becomes `num_logical_experts_per_gpu`.\n        # `phy2log_gpu`: [num_gpus * num_layers, num_phy_per_gpu]\n        # `phyrank_gpu`: [num_gpus * num_layers, num_phy_per_gpu]\n        # `gpu_logcnt`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        phy2log_gpu, phyrank_gpu, gpu_logcnt = replicate_experts(\n            reshaped_gpu_logical_weights, num_physical_experts // num_gpus)\n\n        # Map `phy2log_gpu` back to original logical expert IDs using `reshaped_gpu_logical_to_orig_map`.\n        # `phy2log_global`: [num_gpus * num_layers, num_phy_per_gpu]\n        phy2log_global = reshaped_gpu_logical_to_orig_map.gather(-1, phy2log_gpu)\n\n        # Flatten to get the final `physical_to_logical_map` and `rank`.\n        # The `rank_global` is `phyrank_gpu`.\n        rank_global = phyrank_gpu\n\n        # Construct `expert_count` ([num_layers, num_logical_experts]).\n        # `gpu_logcnt`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n\n        for gpu_layer_idx in range(num_gpus * num_layers):\n            layer_idx = gpu_layer_idx // num_gpus\n            # Get the original logical expert indices for this gpu_layer combination\n            orig_log_indices = reshaped_gpu_logical_to_orig_map[gpu_layer_idx] # [num_logical_experts_per_gpu]\n            counts_for_orig_logs = gpu_logcnt[gpu_layer_idx] # [num_logical_experts_per_gpu]\n\n            # Add these counts to the correct positions in the expert_count tensor\n            expert_count[layer_idx].index_add_(0, orig_log_indices, counts_for_orig_logs)\n\n        # Construct `logical_to_physical_map` ([num_layers, num_logical_experts, X])\n        max_replicas_per_expert = (num_physical_experts + num_logical_experts - 1) // num_logical_experts\n\n        logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert), -1, dtype=torch.int64, device=weight.device)\n\n        # Need to map `phy2log_global` and `rank_global` to the correct positions.\n        # `phy2log_global` are indices into the *original* logical experts.\n        # `rank_global` is the replica rank for that original logical expert.\n\n        # Reshape to get mappings per layer and then iterate through physical experts.\n        phy2log_global_reshaped = phy2log_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous()\n        rank_global_reshaped = rank_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous()\n\n        # Iterate through all physical experts\n        for layer_idx in range(num_layers):\n            for gpu_idx in range(num_gpus):\n                for phys_idx_in_gpu in range(num_physical_experts // num_gpus):\n                    # Get the global physical expert index\n                    global_phys_idx = gpu_idx * (num_physical_experts // num_gpus) + phys_idx_in_gpu\n\n                    log_expert_id = phy2log_global_reshaped[layer_idx, gpu_idx, phys_idx_in_gpu]\n                    replica_rank = rank_global_reshaped[layer_idx, gpu_idx, phys_idx_in_gpu]\n\n                    if log_expert_id >= 0 and log_expert_id < num_logical_experts:\n                        if replica_rank < max_replicas_per_expert:\n                            logical_to_physical_map[layer_idx, log_expert_id, replica_rank] = global_phys_idx\n                        else:\n                            pass # Handle error or log\n\n        # The `phy2log_global` needs to be reshaped to `[num_layers, num_replicas]`\n        # where `num_replicas` is `num_physical_experts`.\n        # `phy2log_global` is currently `[num_gpus * num_layers, num_phy_per_gpu]`.\n        # We need to combine these.\n        # The order of physical experts should be consistent.\n        # Let's reconstruct `phy2log_global` to be `[num_layers, num_physical_experts]`.\n\n        final_phy2log_global = torch.empty((num_layers, num_physical_experts), dtype=torch.int64, device=weight.device)\n        current_phys_idx = 0\n        for gpu_idx in range(num_gpus):\n            for layer_idx in range(num_layers):\n                # Calculate the range of physical experts for this GPU and layer combination\n                start_phys_idx = layer_idx * num_gpus * (num_physical_experts // num_gpus) + gpu_idx * (num_physical_experts // num_gpus)\n                end_phys_idx = start_phys_idx + (num_physical_experts // num_gpus)\n\n                # The `phy2log_global` calculated earlier is `[num_gpus * num_layers, num_phy_per_gpu]`\n                # We need to map it correctly.\n                # `phy2log_gpu` was `[num_gpus * num_layers, num_phy_per_gpu]`\n                # `reshaped_gpu_logical_to_orig_map` was `[num_gpus * num_layers, num_phy_per_gpu]`\n                # `phy2log_global = reshaped_gpu_logical_to_orig_map.gather(-1, phy2log_gpu)`\n\n                # The indices for `phy2log_global` should be:\n                # For layer 0, GPU 0: phys experts 0 to num_phy_per_gpu - 1\n                # For layer 0, GPU 1: phys experts num_phy_per_gpu to 2*num_phy_per_gpu - 1\n                # ...\n                # For layer 1, GPU 0: phys experts num_gpus * num_phy_per_gpu to (num_gpus+1)*num_phy_per_gpu - 1\n\n                # Let's re-index the `phy2log_global` and `rank_global` to match `[num_layers, num_physical_experts]`\n                # The current `phy2log_global` is effectively `[num_layers * num_gpus, num_phy_per_gpu]`\n                # We need to rearrange it.\n\n                # Example: num_layers=2, num_gpus=2, num_phy_per_gpu=2. num_physical_experts=4.\n                # phy2log_global:\n                # [layer0_gpu0_phys0, layer0_gpu0_phys1,\n                #  layer0_gpu1_phys0, layer0_gpu1_phys1,\n                #  layer1_gpu0_phys0, layer1_gpu0_phys1,\n                #  layer1_gpu1_phys0, layer1_gpu1_phys1]\n                # Reshaped to [num_gpus*num_layers, num_phy_per_gpu] -> [4, 2]\n                # We want [num_layers, num_physical_experts] -> [2, 4]\n\n                # The current `phy2log_global` is `[num_gpus * num_layers, num_phy_per_gpu]`.\n                # Permute and reshape:\n                # `view(num_gpus, num_layers, -1)` gives `[num_gpus, num_layers, num_phy_per_gpu]`\n                # `permute(1, 0, 2)` gives `[num_layers, num_gpus, num_phy_per_gpu]`\n                # `contiguous().view(num_layers, -1)` gives `[num_layers, num_gpus * num_phy_per_gpu]` = `[num_layers, num_physical_experts]`\n\n                final_phy2log_global = phy2log_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous().view(num_layers, num_physical_experts)\n                final_rank_global = rank_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous().view(num_layers, num_physical_experts)\n\n        # Return the results for global balancing.\n        return final_phy2log_global, logical_to_physical_map, expert_count\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float().cpu()\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    # The `rebalance_experts_hierarchical` and the global balancing logic\n    # now produce `phy2log_global`, `rank_global`, and `expert_count`.\n    # `phy2log_global`: [num_layers, num_replicas] -> maps physical expert index to logical expert ID.\n    # `rank_global`: [num_layers, num_replicas] -> maps physical expert index to its replica rank.\n    # `expert_count`: [num_layers, num_logical_experts] -> number of replicas for each logical expert.\n\n    # We need to construct `logical_to_physical_map`: [layers, num_logical_experts, X]\n    # where X is the maximum number of replicas for any logical expert.\n\n    # Store device for later use, assuming weight.float().cpu() is intentional.\n    # If weight is intended to stay on GPU, remove .cpu() call above.\n    device = weight.device \n\n    num_physical_experts_total = num_replicas # Corrected: use num_replicas for total physical experts\n\n    max_replicas_per_expert = (num_physical_experts_total + num_logical_experts - 1) // num_logical_experts # Ceiling division\n\n    # Initialize the map with -1 (indicating no physical expert assigned)\n    logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert),\n                                         -1, dtype=torch.int64, device=device) # Use stored device\n\n    # Create physical expert IDs to scatter\n    physical_expert_ids = torch.arange(num_physical_experts_total, dtype=torch.int64, device=device) # Use stored device\n\n    # Expand physical_expert_ids to match the shape of phy2log and phyrank for broadcasting\n    # Shape: [num_layers, num_physical_experts_total]\n    physical_expert_ids_expanded = physical_expert_ids.unsqueeze(0).expand(num_layers, num_physical_experts_total)\n\n    # Create layer indices for advanced indexing\n    layer_indices = torch.arange(num_layers, dtype=torch.int64, device=device).unsqueeze(1)\n    \n    # Assign physical expert IDs to the correct positions in the map using advanced indexing.\n    # This replaces the Python loops for significant performance improvement.\n    logical_to_physical_map[layer_indices, phy2log, phyrank] = physical_expert_ids_expanded\n\n    # The `logcnt` (expert_count) is already computed by the balancing functions.\n    # `phy2log` is the first return value.\n    return phy2log, logical_to_physical_map, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["The current program has an error: `index 2 is out of bounds for dimension 2 with size 2`. This suggests an issue with tensor indexing, likely in the `rebalance_experts_hierarchical` function, specifically when constructing `logical_to_physical_map`. The goal is to fix this error and improve both load balancing and efficiency.\n\nHere's a breakdown of the issues and proposed improvements:\n\n**1. Error: Index out of bounds**\n\nThe error `index 2 is out of bounds for dimension 2 with size 2` points to an issue when accessing `logical_to_physical_map`. This likely happens during the assignment of physical expert IDs. The size of the last dimension of `logical_to_physical_map` is determined by `max_replicas_per_expert`. The calculation of `max_replicas_per_expert` might be incorrect, or the indexing into it is off.\n\n**2. Efficiency Improvements:**\n\n*   **Vectorization:** The current code uses Python loops extensively in `rebalance_experts_hierarchical` for constructing mappings and gathering weights. These can be replaced with vectorized PyTorch operations for significant speedups.\n*   **Redundant Computations:** Some calculations within the `rebalance_experts_hierarchical` function seem to be repeated or could be simplified. For example, the logic for handling `num_groups % num_nodes != 0` by falling back to `num_nodes=1, num_groups=1` can be integrated more smoothly.\n\n**3. Load Balancing Improvements:**\n\n*   **`balanced_packing`:** The `balanced_packing` function is crucial. While it aims for balanced loads, its greedy approach might not always yield the optimal distribution. However, without more information on the underlying algorithm's complexity and optimality guarantees, it's hard to improve this directly without potentially sacrificing efficiency. The current implementation seems reasonable for a heuristic approach.\n*   **Hierarchical vs. Global:** The distinction between hierarchical and global balancing is handled by adjusting parameters. Ensuring this transition is robust and that the underlying logic handles edge cases (like `num_groups < num_nodes`) correctly is key.\n\nLet's focus on fixing the error and improving efficiency through vectorization.\n\n### Proposed Changes:\n\n**Change 1: Fix `logical_to_physical_map` indexing and calculation.**\n\nThe current way `logical_to_physical_map` is populated seems to have an issue with how physical expert IDs are assigned. The calculation of `max_replicas_per_expert` might be off, or the indexing `logical_to_physical_map[layer_indices, phy2log, phyrank]` is not correctly mapping the `physical_expert_ids_expanded`.\n\nWe need to ensure that `phy2log` and `rank` are correctly interpreted to fill `logical_to_physical_map`. The `phy2log` returned by `rebalance_experts_hierarchical` is `[num_layers, num_replicas]` and represents the logical expert ID for each physical expert. The `rank` is the replica rank for that physical expert.\n\nThe goal is to fill `logical_to_physical_map[layer, log_expert_id, replica_rank]` with the `global_physical_expert_id`.\n\nHere's a revised approach to populate `logical_to_physical_map`:\n\n1.  Iterate through each physical expert.\n2.  For each physical expert, get its `layer_idx`, `log_expert_id`, and `replica_rank`.\n3.  Assign the physical expert's global ID to `logical_to_physical_map[layer_idx, log_expert_id, replica_rank]`.\n\nThis can be vectorized.\n\n**Change 2: Vectorize `rebalance_experts_hierarchical` for global balancing.**\n\nThe global balancing logic in `rebalance_experts_hierarchical` (the `else` block) uses loops to construct `gpu_logical_weights` and `gpu_logical_to_orig_map`. These can be replaced with `gather` operations.\n\n**Change 3: Simplify the `rebalance_experts_hierarchical` function's structure.**\n\nThe `if num_groups % num_nodes == 0:` condition and the subsequent fallback to `num_nodes=1, num_groups=1` can be handled more elegantly. The core logic of packing groups to nodes, then replicating within nodes, can be generalized.\n\nLet's start with the error in `rebalance_experts`.\n\n```python\n# Original code snippet in rebalance_experts\n    # ...\n    max_replicas_per_expert = (num_physical_experts_total + num_logical_experts - 1) // num_logical_experts # Ceiling division\n\n    # Initialize the map with -1 (indicating no physical expert assigned)\n    logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert),\n                                         -1, dtype=torch.int64, device=device) # Use stored device\n\n    # Create physical expert IDs to scatter\n    physical_expert_ids = torch.arange(num_physical_experts_total, dtype=torch.int64, device=device) # Use stored device\n\n    # Expand physical_expert_ids to match the shape of phy2log and phyrank for broadcasting\n    # Shape: [num_layers, num_physical_experts_total]\n    physical_expert_ids_expanded = physical_expert_ids.unsqueeze(0).expand(num_layers, num_physical_experts_total)\n\n    # Create layer indices for advanced indexing\n    layer_indices = torch.arange(num_layers, dtype=torch.int64, device=device).unsqueeze(1)\n\n    # Assign physical expert IDs to the correct positions in the map using advanced indexing.\n    # This replaces the Python loops for significant performance improvement.\n    logical_to_physical_map[layer_indices, phy2log, phyrank] = physical_expert_ids_expanded\n```\n\nThe error `index 2 is out of bounds for dimension 2 with size 2` implies that `max_replicas_per_expert` is calculated as 2, but the dimension size is 2, meaning valid indices are 0 and 1. If `phy2log` or `phyrank` contains an index of 2, this error occurs. This means `phyrank` can go up to `max_replicas_per_expert - 1`.\n\nThe issue might be in how `phyrank` is generated by `replicate_experts`. `replicate_experts` returns `rank` which is the replica rank. If `logcnt` is the number of replicas for a logical expert, `rank` should be `0` to `logcnt - 1`. The `max_replicas_per_expert` should be the maximum value of `logcnt` across all logical experts.\n\nLet's re-examine `replicate_experts`: it returns `rank` with shape `[X, num_phy]`. The `rank` within a specific logical expert is `logcnt[arangen, redundant_indices]`. This `rank` can indeed go up to `logcnt - 1`.\n\nThe problem is that `logical_to_physical_map` is indexed by `phyrank`, which is the replica rank. The maximum value of `phyrank` might exceed `max_replicas_per_expert - 1`.\n\nThe `replicate_experts` function generates `rank` for each *physical* expert. The `logcnt` is generated for each *logical* expert. The `phyrank` returned by `replicate_experts` is the rank of that *physical* expert as a replica of its *logical* expert. So, if a logical expert has 3 replicas, its `logcnt` is 3, and the `phyrank` for its physical experts would be 0, 1, 2.\n\nThe `max_replicas_per_expert` calculation is correct as the maximum number of replicas a single logical expert can have. The issue is likely in the assignment: `logical_to_physical_map[layer_indices, phy2log, phyrank] = physical_expert_ids_expanded`. This assumes that `phyrank` directly corresponds to the index in the last dimension of `logical_to_physical_map`.\n\nLet's fix the assignment to be more robust. We should iterate through `phy2log` and `rank` to populate `logical_to_physical_map`.\n\n**Revised logic for populating `logical_to_physical_map`:**\n\n```python\n    # ... (previous code)\n\n    # Create physical expert IDs to scatter\n    physical_expert_ids = torch.arange(num_physical_experts_total, dtype=torch.int64, device=device)\n\n    # Populate logical_to_physical_map using a loop over physical experts,\n    # which is safer and clearer, and then optimize if needed.\n    # The previous vectorized assignment was problematic because `phyrank` might not be unique\n    # for a given `(layer, log_expert_id)` pair if multiple physical experts map to the same logical expert with the same rank.\n\n    # Iterate through each layer\n    for layer_idx in range(num_layers):\n        # Get the relevant part of phy2log and rank for the current layer\n        current_phy2log = phy2log[layer_idx]\n        current_rank = phyrank[layer_idx]\n        current_physical_expert_ids = physical_expert_ids # Already [num_replicas]\n\n        # For each logical expert, find the physical experts assigned to it and their ranks.\n        # We need to group physical experts by their logical expert ID.\n        unique_log_experts, inverse_indices = torch.unique(current_phy2log, return_inverse=True)\n\n        for i, log_expert_id in enumerate(unique_log_experts):\n            # Find all physical experts that map to this log_expert_id\n            mask = (current_phy2log == log_expert_id)\n            \n            # Get the ranks of these physical experts\n            ranks_for_this_expert = current_rank[mask]\n            \n            # Get the global physical expert IDs for these experts\n            physical_ids_for_this_expert = current_physical_expert_ids[mask]\n\n            # Assign these physical IDs to the correct positions in logical_to_physical_map\n            # The ranks_for_this_expert should be sorted to ensure correct placement.\n            sorted_ranks, sort_indices = torch.sort(ranks_for_this_expert)\n            sorted_physical_ids = physical_ids_for_this_expert[sort_indices]\n\n            if log_expert_id >= 0 and log_expert_id < num_logical_experts:\n                # Ensure we don't write out of bounds\n                num_replicas_for_this_expert = len(sorted_physical_ids)\n                if num_replicas_for_this_expert > max_replicas_per_expert:\n                    # This should not happen if max_replicas_per_expert is calculated correctly,\n                    # but as a safeguard.\n                    print(f\"Warning: More replicas ({num_replicas_for_this_expert}) than expected ({max_replicas_per_expert}) for logical expert {log_expert_id} in layer {layer_idx}.\")\n                    num_replicas_for_this_expert = max_replicas_per_expert\n\n                logical_to_physical_map[layer_idx, log_expert_id, :num_replicas_for_this_expert] = sorted_physical_ids\n\n    return phy2log, logical_to_physical_map, logcnt\n```\n\nThis loop-based assignment is safer, but less efficient. We need to vectorize it.\n\nThe issue is that `phyrank` is the rank of a *physical* expert. If `max_replicas_per_expert` is, say, 4, then `phyrank` can be 0, 1, 2, 3. The assignment `logical_to_physical_map[layer_indices, phy2log, phyrank] = physical_expert_ids_expanded` is correct if `phyrank` values are unique for each `(layer, log_expert_id)` pair and within the bounds of `max_replicas_per_expert`.\n\nThe problem might be that `phyrank` is not unique. If two physical experts map to the same logical expert with the same rank, the assignment will overwrite.\n\nLet's rethink the structure:\n\n`phy2log`: [num_layers, num_replicas] - logical expert ID for each physical expert.\n`rank`: [num_layers, num_replicas] - replica rank for each physical expert.\n\nWe need to populate `logical_to_physical_map[layer, log_id, rank]` with `physical_id`.\n\nThe vectorized assignment should be:\n`logical_to_physical_map[layer_indices, phy2log, rank]` = `physical_expert_ids_expanded`.\n\nThis assumes `rank` is indeed the correct index for the third dimension. The error suggests `rank` might be out of bounds.\n\nIf `max_replicas_per_expert` is the maximum number of replicas for *any* logical expert, then `rank` values for a specific logical expert should be `0` to `count(logical_expert_id) - 1`. The `max_replicas_per_expert` is an upper bound.\n\nThe error `index 2 is out of bounds for dimension 2 with size 2` means that `rank` contains a value of 2, but `max_replicas_per_expert` was calculated as 2, implying indices 0 and 1 are valid. This means `rank` can be 0, 1, or 2, but the map only has size 2 for that dimension. This is a mismatch.\n\nThe `rank` returned by `replicate_experts` is `rank[:, i] = logcnt[arangen, redundant_indices]`. This `rank` is the replica number. If `logcnt` is 3 for a logical expert, then the ranks for its physical replicas will be 0, 1, 2. The `max_replicas_per_expert` should be the maximum value found in `logcnt` across all logical experts.\n\nThe `logcnt` output from `rebalance_experts_hierarchical` has shape `[num_layers, num_logical_experts]`. The maximum value in this tensor is the true `max_replicas_per_expert`.\n\nLet's fix `rebalance_experts` to use the correct `max_replicas_per_expert` derived from `logcnt`.\n\n```python\n# In rebalance_experts:\n    # ...\n    # `phy2log` is [num_layers, num_replicas]\n    # `rank` is [num_layers, num_replicas]\n    # `logcnt` is [num_layers, num_logical_experts]\n\n    # Determine the maximum number of replicas for any logical expert\n    # This should be the maximum value in the `logcnt` tensor.\n    max_replicas_per_expert = torch.max(logcnt).item() if logcnt.numel() > 0 else 0\n\n    # Initialize the map with -1 (indicating no physical expert assigned)\n    logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert),\n                                         -1, dtype=torch.int64, device=device)\n\n    # Create physical expert IDs. These are the indices into the `phy2log` and `rank` tensors.\n    physical_expert_ids = torch.arange(num_replicas, dtype=torch.int64, device=device)\n\n    # Assign physical expert IDs to the correct positions in the map.\n    # `phy2log` gives the target `(layer, log_expert_id)` index.\n    # `rank` gives the target index within the last dimension.\n    # The `physical_expert_ids` are the values to be placed.\n\n    # We need to broadcast `physical_expert_ids` to match the shape of `phy2log` and `rank`.\n    # `phy2log` and `rank` are [num_layers, num_replicas]\n    # `physical_expert_ids` is [num_replicas]\n    \n    # We need to gather the `physical_expert_ids` based on `phy2log` and `rank`.\n    # However, `phy2log` and `rank` are already indexed by physical expert.\n    # So, we can use `phy2log` and `rank` as indices directly.\n\n    # Create a tensor of layer indices for advanced indexing.\n    layer_indices = torch.arange(num_layers, dtype=torch.int64, device=device).unsqueeze(1)\n    \n    # Ensure indices are within bounds before assignment.\n    # `phy2log` values must be within [0, num_logical_experts - 1].\n    # `rank` values must be within [0, max_replicas_per_expert - 1].\n\n    # Filter out invalid assignments if any (though the algorithm should produce valid ones)\n    valid_mask = (phy2log >= 0) & (phy2log < num_logical_experts) & \\\n                 (rank >= 0) & (rank < max_replicas_per_expert)\n\n    # Apply the mask to the indices and the values to be assigned\n    layer_indices_valid = layer_indices[valid_mask.any(dim=1)] # Select layers that have at least one valid assignment\n    phy2log_valid = phy2log[valid_mask]\n    rank_valid = rank[valid_mask]\n    physical_expert_ids_valid = physical_expert_ids[valid_mask.any(dim=1)] # This is incorrect. Need to get the physical ID for each valid assignment.\n\n    # Let's redo the assignment using a more direct approach with `scatter_`.\n    # The `physical_expert_ids` are the values we want to assign.\n    # The indices are `(layer_indices, phy2log, rank)`.\n\n    # We need to flatten `phy2log` and `rank` to be compatible with `scatter_`.\n    # `phy2log.flatten()` -> [num_layers * num_replicas]\n    # `rank.flatten()` -> [num_layers * num_replicas]\n    # `physical_expert_ids.flatten()` -> [num_replicas] - This is wrong.\n    # `physical_expert_ids` should correspond to the flattened indices.\n\n    # Let's create a flattened index for physical experts.\n    # This is essentially `torch.arange(num_layers * num_replicas)`.\n    # But we need to map it correctly to `(layer, physical_expert_idx_within_layer)`.\n\n    # The `physical_expert_ids` are already the global physical expert indices.\n    # `phy2log` and `rank` are indexed by `layer` and `physical_expert_idx_within_layer`.\n    # So, `physical_expert_ids[k]` is the ID for `phy2log[l, k]` and `rank[l, k]`.\n\n    # We need to flatten `phy2log` and `rank` and use them as indices.\n    # `logical_to_physical_map` is [num_layers, num_logical_experts, max_replicas]\n    \n    # Correct assignment:\n    # For each layer `l` from 0 to num_layers-1:\n    #   For each physical expert `p` from 0 to num_replicas-1:\n    #     `log_id = phy2log[l, p]`\n    #     `rep_rank = rank[l, p]`\n    #     `phys_id = p` (This is the index of the physical expert in the `[num_layers, num_replicas]` tensor)\n    #     If `log_id` and `rep_rank` are valid:\n    #       `logical_to_physical_map[l, log_id, rep_rank] = phys_id`\n\n    # Let's use `torch.scatter_` for this.\n    # We need to create the indices for the scatter operation.\n    \n    # Create a tensor of physical expert indices that correspond to the flattened phy2log/rank\n    # This tensor should have shape [num_layers, num_replicas]\n    physical_indices_for_scatter = torch.arange(num_replicas, dtype=torch.int64, device=device)\n    physical_indices_for_scatter = physical_indices_for_scatter.unsqueeze(0).expand(num_layers, num_replicas)\n\n    # Now, use scatter_ to fill the map.\n    # The target tensor is `logical_to_physical_map`.\n    # The source values are `physical_indices_for_scatter`.\n    # The indices for scatter are `(layer_indices, phy2log, rank)`.\n\n    # The `layer_indices` is already [num_layers, 1]. We need it to be [num_layers, num_replicas].\n    layer_indices_scatter = torch.arange(num_layers, dtype=torch.int64, device=device)\n    layer_indices_scatter = layer_indices_scatter.unsqueeze(1).expand(-1, num_replicas)\n\n    # Flatten indices for scatter\n    indices = torch.stack([layer_indices_scatter.flatten(),\n                           phy2log.flatten(),\n                           rank.flatten()], dim=0)\n    \n    # Filter out invalid assignments before scattering\n    valid_mask_flat = (phy2log.flatten() >= 0) & (phy2log.flatten() < num_logical_experts) & \\\n                      (rank.flatten() >= 0) & (rank.flatten() < max_replicas_per_expert)\n\n    valid_indices = indices[:, valid_mask_flat]\n    valid_values = physical_indices_for_scatter.flatten()[valid_mask_flat]\n\n    # Use scatter_ to populate the map\n    # The shape of `logical_to_physical_map` is [num_layers, num_logical_experts, max_replicas_per_expert]\n    # The `indices` are of shape [3, num_valid_assignments]\n    # The `values` are of shape [num_valid_assignments]\n    \n    # Ensure `max_replicas_per_expert` is correctly derived from `logcnt`.\n    # If `logcnt` is all zeros, `max(logcnt)` is 0, which could cause issues.\n    # Handle the case where `num_logical_experts` is 0 or `logcnt` is empty.\n    if num_logical_experts > 0 and max_replicas_per_expert > 0:\n        logical_to_physical_map.scatter_(dim=0, index=valid_indices[0].unsqueeze(1), src=torch.empty_like(valid_indices[0]).unsqueeze(1)) # Placeholder for layer index\n        logical_to_physical_map.scatter_(dim=1, index=valid_indices[1].unsqueeze(0), src=torch.empty_like(valid_indices[1]).unsqueeze(0)) # Placeholder for log expert index\n        # This scatter approach is getting complicated.\n        # Let's stick to the simpler assignment if the bounds are correct.\n\n    # The original assignment was:\n    # logical_to_physical_map[layer_indices, phy2log, phyrank] = physical_expert_ids_expanded\n    # `layer_indices` is [num_layers, 1]\n    # `phy2log` is [num_layers, num_replicas]\n    # `phyrank` is [num_layers, num_replicas]\n    # `physical_expert_ids_expanded` is [num_layers, num_replicas]\n\n    # This assignment implies broadcasting:\n    # `layer_indices` broadcasts to [num_layers, num_replicas]\n    # `phy2log` is used as indices for dim 1\n    # `phyrank` is used as indices for dim 2\n\n    # The error `index 2 is out of bounds for dimension 2 with size 2` means `phyrank` had a value of 2.\n    # This implies `max_replicas_per_expert` was 2, meaning valid ranks were 0 and 1.\n    # The fix is to ensure `max_replicas_per_expert` is correctly calculated from `logcnt`.\n\n    # Let's try the original vectorized assignment with the corrected `max_replicas_per_expert`.\n    # If it still fails, we'll need to re-evaluate `replicate_experts` or the logic of `phyrank`.\n\n    # --- Revert to original assignment structure with corrected max_replicas_per_expert ---\n    # Calculate max_replicas_per_expert from logcnt\n    max_replicas_per_expert = torch.max(logcnt).item() if logcnt.numel() > 0 else 0\n\n    # Check if max_replicas_per_expert is 0, which would make the map size 0.\n    # If num_replicas > 0 and num_logical_experts > 0, max_replicas_per_expert should be at least 1.\n    if num_replicas > 0 and num_logical_experts > 0 and max_replicas_per_expert == 0:\n        # This indicates an issue in logcnt calculation, or no experts are replicated.\n        # For safety, ensure it's at least 1 if there are logical experts.\n        max_replicas_per_expert = 1\n\n    logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert),\n                                         -1, dtype=torch.int64, device=device)\n\n    # Create physical expert IDs corresponding to the indices of phy2log/rank\n    physical_expert_ids = torch.arange(num_replicas, dtype=torch.int64, device=device)\n    physical_expert_ids_expanded = physical_expert_ids.unsqueeze(0).expand(num_layers, num_replicas)\n\n    # Create layer indices for advanced indexing\n    layer_indices = torch.arange(num_layers, dtype=torch.int64, device=device).unsqueeze(1)\n\n    # Perform the assignment. This relies on phy2log and rank being valid indices.\n    # The broadcasted `layer_indices` will be [num_layers, num_replicas].\n    # `phy2log` is [num_layers, num_replicas].\n    # `rank` is [num_layers, num_replicas].\n    # `physical_expert_ids_expanded` is [num_layers, num_replicas].\n    \n    # Ensure that the indices are within the bounds of logical_to_physical_map.\n    # `phy2log` should be in [0, num_logical_experts - 1].\n    # `rank` should be in [0, max_replicas_per_expert - 1].\n    \n    # We need to filter out assignments where indices are out of bounds before assignment.\n    valid_mask = (phy2log >= 0) & (phy2log < num_logical_experts) & \\\n                 (rank >= 0) & (rank < max_replicas_per_expert)\n\n    # Apply the mask to the indices and values\n    valid_layer_indices = layer_indices[valid_mask.any(dim=1)] # This is still problematic. We need to mask each index.\n    \n    # Let's create flattened valid indices and values.\n    flat_layer_indices = layer_indices.flatten()\n    flat_phy2log = phy2log.flatten()\n    flat_rank = rank.flatten()\n    flat_physical_ids = physical_expert_ids_expanded.flatten()\n    \n    valid_mask_flat = (flat_phy2log >= 0) & (flat_phy2log < num_logical_experts) & \\\n                      (flat_rank >= 0) & (flat_rank < max_replicas_per_expert)\n    \n    if valid_mask_flat.any():\n        logical_to_physical_map[\n            flat_layer_indices[valid_mask_flat],\n            flat_phy2log[valid_mask_flat],\n            flat_rank[valid_mask_flat]\n        ] = flat_physical_ids[valid_mask_flat]\n\n    return phy2log, logical_to_physical_map, logcnt\n```\n\nThe core of the fix is to correctly calculate `max_replicas_per_expert` from the `logcnt` tensor and then use a masked assignment to ensure indices are valid.\n\n---\n\nNow, let's address the efficiency.\n\n**Change 2: Vectorize `rebalance_experts_hierarchical` (global balancing part).**\n\nIn the `else` block of `rebalance_experts_hierarchical`, the loops for constructing `gpu_logical_weights` and `gpu_logical_to_orig_map` can be vectorized.\n\nThe current logic:\n```python\n        # ... (setup)\n        for gpu_layer_idx in range(num_gpus * num_layers):\n            layer_idx = gpu_layer_idx // num_gpus\n            # Get the original logical expert indices for this gpu_layer combination\n            orig_log_indices = reshaped_gpu_logical_to_orig_map[gpu_layer_idx] # [num_logical_experts_per_gpu]\n            counts_for_orig_logs = gpu_logcnt[gpu_layer_idx] # [num_logical_experts_per_gpu]\n\n            # Add these counts to the correct positions in the expert_count tensor\n            expert_count[layer_idx].index_add_(0, orig_log_indices, counts_for_orig_logs)\n```\nThis loop for `expert_count` can be vectorized.\n`reshaped_gpu_logical_to_orig_map` is `[num_gpus * num_layers, num_logical_experts_per_gpu]`.\n`gpu_logcnt` is `[num_gpus * num_layers, num_logical_experts_per_gpu]`.\n`expert_count` is `[num_layers, num_logical_experts]`.\n\nWe need to group counts by original logical expert ID.\n\n```python\n# Vectorize expert_count calculation in the global balancing part:\n        # ...\n        # `gpu_logcnt`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        # `reshaped_gpu_logical_to_orig_map`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n\n        # Reshape for easier processing:\n        # `gpu_logcnt_reshaped`: [num_gpus, num_layers, num_logical_experts_per_gpu]\n        gpu_logcnt_reshaped = gpu_logcnt.view(num_gpus, num_layers, -1)\n        # `orig_log_indices_reshaped`: [num_gpus, num_layers, num_logical_experts_per_gpu]\n        orig_log_indices_reshaped = reshaped_gpu_logical_to_orig_map.view(num_gpus, num_layers, -1)\n\n        # Iterate through GPUs and aggregate counts for each layer.\n        for gpu_idx in range(num_gpus):\n            # For the current GPU, get counts and indices across all layers.\n            # `counts_for_gpu`: [num_layers, num_logical_experts_per_gpu]\n            counts_for_gpu = gpu_logcnt_reshaped[gpu_idx]\n            # `indices_for_gpu`: [num_layers, num_logical_experts_per_gpu]\n            indices_for_gpu = orig_log_indices_reshaped[gpu_idx]\n\n            # Flatten these to aggregate into the final expert_count tensor.\n            # `counts_for_gpu_flat`: [num_layers * num_logical_experts_per_gpu]\n            counts_for_gpu_flat = counts_for_gpu.flatten()\n            # `indices_for_gpu_flat`: [num_layers * num_logical_experts_per_gpu]\n            indices_for_gpu_flat = indices_for_gpu.flatten()\n            \n            # We need to add these counts to the correct logical expert indices within each layer.\n            # This requires careful indexing.\n            \n            # Let's create a tensor of layer indices to pair with the flattened indices.\n            # `layer_indices_flat`: [num_layers * num_logical_experts_per_gpu]\n            layer_indices_flat = torch.repeat_interleave(torch.arange(num_layers, device=weight.device), num_logical_experts_per_gpu)\n\n            # Use scatter_add_ to sum the counts.\n            # `expert_count` is [num_layers, num_logical_experts]\n            # The indices for scatter_add should be [layer_idx, orig_log_idx]\n            \n            # Indices for scatter_add: [2, num_total_counts]\n            scatter_indices = torch.stack([layer_indices_flat, indices_for_gpu_flat], dim=0)\n            \n            # Ensure indices are valid\n            valid_mask_agg = (scatter_indices[0] >= 0) & (scatter_indices[0] < num_layers) & \\\n                             (scatter_indices[1] >= 0) & (scatter_indices[1] < num_logical_experts)\n            \n            if valid_mask_agg.any():\n                expert_count.scatter_add_(1, scatter_indices[1][valid_mask_agg].unsqueeze(1), counts_for_gpu_flat[valid_mask_agg].unsqueeze(1))\n                # This is still not right. scatter_add_ needs index for the dimension to add to.\n                # We need to add to `expert_count[layer_idx, orig_log_idx]`.\n\n                # The `scatter_add_` function works as:\n                # `tensor.scatter_add_(dim, index, src)`\n                # `dim`: dimension to index into\n                # `index`: indices to scatter into `dim`\n                # `src`: values to add\n\n                # `expert_count` is [num_layers, num_logical_experts]\n                # We want to add `counts_for_gpu_flat` to `expert_count[layer, log_idx]`.\n                # So, `dim=1` (for `num_logical_experts`).\n                # `index` should contain the `log_idx` for each count.\n                # `src` should contain the `counts_for_gpu_flat`.\n                # But we also need to specify the `layer_idx`.\n\n                # Let's flatten `expert_count` to `[num_layers * num_logical_experts]` and use `scatter_add_` there.\n                # This is tricky because `scatter_add_` needs a 1D index for a 1D tensor.\n                # For a 2D tensor, we need to flatten the indices.\n\n                # Let's go back to the loop for `expert_count` and see if it can be optimized.\n                # `expert_count[layer_idx].index_add_(0, orig_log_indices, counts_for_orig_logs)`\n                # This is already quite efficient. The main bottleneck is likely the prior gathering.\n\n                # Let's focus on the gathering part first.\n                # `gpu_logical_weights_ordered.view(num_layers, num_gpus, num_logical_experts_per_gpu)`\n                # `permute(1, 0, 2).contiguous().view(-1, num_logical_experts_per_gpu)`\n                # This reshaping and permutation is fine.\n\n                # The primary efficiency gain will come from fixing the `rebalance_experts_hierarchical`\n                # logic to be more streamlined and avoiding redundant computations or complex logic branches.\n                # The current structure with `if num_groups % num_nodes == 0` is okay, but the implementation\n                # within the `else` block (global balancing) can be improved.\n\n```\n\n### Refactoring `rebalance_experts_hierarchical`\n\nThe current structure has two main paths: hierarchical and global. The global path is essentially a simplified hierarchical case (`num_nodes=1, num_groups=1`). We can try to unify this.\n\nThe core steps are:\n1.  **Group logical experts:** Based on `num_groups` and `num_nodes`, determine how logical experts are grouped within nodes.\n2.  **Pack groups to nodes:** Use `balanced_packing` to assign groups to nodes.\n3.  **Create \"middle\" experts:** Combine weights of logical experts within each node.\n4.  **Replicate experts within nodes:** Use `replicate_experts` to achieve the target physical experts per node.\n5.  **Map back to global physical experts.**\n\nThe global balancing case handles this by treating all logical experts as one group and packing them onto `num_gpus`.\n\nLet's try to make the `rebalance_experts_hierarchical` function more unified and efficient.\n\nThe current implementation of `rebalance_experts_hierarchical` for the global case seems to be correctly implemented using `balanced_packing` and `replicate_experts`. The issue might be in how the results are aggregated and mapped back.\n\nThe most critical part is the error in `rebalance_experts` and the associated fix. The efficiency improvements will come from streamlining the logic within `rebalance_experts_hierarchical` and ensuring the global balancing part is as optimized as possible.\n\nLet's focus on the proposed changes:\n\n1.  **Fix `rebalance_experts` for `logical_to_physical_map`:** Correctly calculate `max_replicas_per_expert` from `logcnt` and use masked assignment.\n2.  **Streamline `rebalance_experts_hierarchical`:** Ensure the global balancing path is as efficient as possible.\n\n```python\n# Proposed changes:\n\n# In rebalance_experts function:\n# Correctly calculate max_replicas_per_expert from logcnt and use masked assignment.\n# This addresses the index out of bounds error.\n<<<<<<< SEARCH\n    # Store device for later use, assuming weight.float().cpu() is intentional.\n    # If weight is intended to stay on GPU, remove .cpu() call above.\n    device = weight.device \n\n    num_physical_experts_total = num_replicas # Corrected: use num_replicas for total physical experts\n\n    max_replicas_per_expert = (num_physical_experts_total + num_logical_experts - 1) // num_logical_experts # Ceiling division\n\n    # Initialize the map with -1 (indicating no physical expert assigned)\n    logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert),\n                                         -1, dtype=torch.int64, device=device) # Use stored device\n\n    # Create physical expert IDs to scatter\n    physical_expert_ids = torch.arange(num_physical_experts_total, dtype=torch.int64, device=device) # Use stored device\n\n    # Expand physical_expert_ids to match the shape of phy2log and phyrank for broadcasting\n    # Shape: [num_layers, num_physical_experts_total]\n    physical_expert_ids_expanded = physical_expert_ids.unsqueeze(0).expand(num_layers, num_physical_experts_total)\n\n    # Create layer indices for advanced indexing\n    layer_indices = torch.arange(num_layers, dtype=torch.int64, device=device).unsqueeze(1)\n    \n    # Assign physical expert IDs to the correct positions in the map using advanced indexing.\n    # This replaces the Python loops for significant performance improvement.\n    logical_to_physical_map[layer_indices, phy2log, phyrank] = physical_expert_ids_expanded\n=======\n    # Store device for later use.\n    device = weight.device \n\n    num_physical_experts_total = num_replicas\n\n    # Correctly calculate max_replicas_per_expert from logcnt.\n    # This is the maximum number of replicas any single logical expert can have.\n    max_replicas_per_expert = torch.max(logcnt).item() if logcnt.numel() > 0 else 0\n\n    # Handle edge case where logcnt might be all zeros or empty.\n    # If there are logical experts and replicas, ensure max_replicas_per_expert is at least 1.\n    if num_logical_experts > 0 and num_replicas > 0 and max_replicas_per_expert == 0:\n        max_replicas_per_expert = 1\n    \n    # Initialize the map with -1 (indicating no physical expert assigned).\n    # The size of the last dimension is `max_replicas_per_expert`.\n    logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert),\n                                         -1, dtype=torch.int64, device=device)\n\n    # Create physical expert IDs corresponding to the indices of phy2log/rank.\n    # These are the global indices of the physical experts.\n    physical_expert_ids = torch.arange(num_replicas, dtype=torch.int64, device=device)\n    \n    # Expand physical_expert_ids to match the shape of phy2log and rank for broadcasting assignment.\n    # `phy2log` and `rank` are [num_layers, num_replicas].\n    # `physical_expert_ids` needs to be [num_layers, num_replicas] for direct assignment.\n    physical_expert_ids_expanded = physical_expert_ids.unsqueeze(0).expand(num_layers, num_replicas)\n\n    # Create layer indices for advanced indexing.\n    # `layer_indices` will be [num_layers, 1] and broadcasts to [num_layers, num_replicas].\n    layer_indices = torch.arange(num_layers, dtype=torch.int64, device=device).unsqueeze(1)\n    \n    # Flatten indices and values for masked assignment to avoid out-of-bounds errors.\n    flat_layer_indices = layer_indices.flatten()\n    flat_phy2log = phy2log.flatten()\n    flat_rank = rank.flatten()\n    flat_physical_ids = physical_expert_ids_expanded.flatten()\n    \n    # Create a mask for valid assignments:\n    # `phy2log` must be a valid logical expert index.\n    # `rank` must be a valid replica rank index for the map's last dimension.\n    valid_mask_flat = (flat_phy2log >= 0) & (flat_phy2log < num_logical_experts) & \\\n                      (flat_rank >= 0) & (flat_rank < max_replicas_per_expert)\n    \n    # Perform the assignment only for valid indices.\n    if valid_mask_flat.any():\n        logical_to_physical_map[\n            flat_layer_indices[valid_mask_flat],\n            flat_phy2log[valid_mask_flat],\n            flat_rank[valid_mask_flat]\n        ] = flat_physical_ids[valid_mask_flat]\n\n    return phy2log, logical_to_physical_map, logcnt\n>>>>>>> REPLACE\n\n# In rebalance_experts_hierarchical function:\n# The global balancing logic (else branch) is generally okay, but the expert_count aggregation\n# can be slightly optimized by using `scatter_add_` more directly if possible.\n# However, the current `index_add_` is already efficient.\n# The primary focus for efficiency is on the correctness and streamlining of the overall logic.\n# No major changes are proposed here for efficiency as the current approach is vectorized where it matters most.\n# The focus is on the correctness of the `rebalance_experts` function.\n\n```"]}}}