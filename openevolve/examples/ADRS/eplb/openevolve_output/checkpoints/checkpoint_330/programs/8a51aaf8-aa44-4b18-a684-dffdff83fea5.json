{"id": "8a51aaf8-aa44-4b18-a684-dffdff83fea5", "code": "# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort groups by weight in descending order.\n    # 'weight' is already float and on CPU from rebalance_experts,\n    # so no need for .float().cpu() here.\n    indices = weight.sort(-1, descending=True).indices\n\n    # Initialize pack_index and rank_in_pack.\n    # These will be fully populated, so `empty_like` is sufficient.\n    # They will be on the same device as 'weight' (i.e., CPU).\n    pack_index = torch.empty_like(weight, dtype=torch.int64)\n    rank_in_pack = torch.empty_like(pack_index)\n    \n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These tensors are on the same device as 'weight' (which is CPU).\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id in indices[i]:\n            # Get current state for this layer's packs.\n            current_pack_weights = pack_weights_per_layer[i]\n            current_pack_items = pack_items_per_layer[i]\n\n            # Find the pack with the minimum current weight among available ones.\n            # Set weights of full packs to infinity to exclude them from argmin.\n            full_mask = (current_pack_items == groups_per_pack)\n            \n            # Directly create a masked tensor for argmin.\n            # This avoids creating a separate temporary tensor.\n            masked_weights = current_pack_weights.masked_fill(full_mask, float('inf'))\n            pack = torch.argmin(masked_weights).item()\n\n            # Assert that a valid pack was found and it has space.\n            assert current_pack_items[pack] < groups_per_pack\n\n            # Assign the group to the chosen pack.\n            # `group_id` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            # `weight[i, group_id]` is the weight of the current group.\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    # Pre-calculate the maximum number of replicas for each logical expert.\n    # This helps in determining the size of `log2phy` later and can inform\n    # the replication strategy.\n    max_replicas_per_log = torch.ceil(torch.tensor(num_phy / num_log, dtype=torch.float32)).to(torch.int64)\n    \n    # Initialize logcnt with 1 for all logical experts.\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n\n    # Iterate to assign replicas to physical experts.\n    for i in range(num_log, num_phy):\n        # Calculate the load per replica for each logical expert.\n        # Use `torch.clamp_max` to avoid division by zero if logcnt[..., redundant_indices] is 0.\n        # Although logcnt is initialized to 1, this adds robustness.\n        load_per_replica = weight / torch.clamp(logcnt, min=1)\n        \n        # Select the logical expert that has the highest load per replica.\n        # This greedy approach aims to balance the load across physical experts.\n        _, redundant_indices = load_per_replica.max(dim=-1) # Use _ to discard max value, keep indices\n\n        # Assign the selected logical expert to the current physical expert.\n        phy2log[:, i] = redundant_indices\n        \n        # Record the rank of this replica within its logical expert.\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        \n        # Increment the replica count for the chosen logical expert.\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float().cpu()\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float().cpu()\n\n    if num_groups > 1 or num_nodes > 1:\n        # Use hierarchical balancing if topology information is relevant\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # Fallback to global balancing if only one group and one node\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n\n    num_redundant_experts = num_replicas - num_logical_experts\n    # Calculate maxlogcnt safely, ensuring it's at least 1\n    maxlogcnt = max(1, num_redundant_experts + 1) \n\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    \n    # Ensure scatter operation is safe when maxlogcnt is 1\n    scatter_dim_size = maxlogcnt\n    if scatter_dim_size == 0: # Should not happen with max(1, ...) but for safety\n        return phy2log, log2phy, logcnt\n\n    # Flatten for scatter, then reshape to handle potential broadcasting issues\n    flat_phy2log_rank = phy2log * scatter_dim_size + phyrank\n    \n    # Create the range of replica indices\n    replica_indices = torch.arange(num_replicas, dtype=torch.int64, device=log2phy.device).expand(num_layers, -1)\n\n    # Perform scatter operation\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        flat_phy2log_rank.flatten().unsqueeze(0), # Ensure correct shape for scatter\n        replica_indices.flatten().unsqueeze(0), # Ensure correct shape for scatter\n    )\n    \n    # Reshape back to the original log2phy shape if needed, or adjust indices if scatter was done differently\n    # The current scatter logic should place values correctly if flat_phy2log_rank is flattened correctly.\n    # For safety, let's ensure the output shape is as expected.\n    log2phy = log2phy.view(num_layers, num_logical_experts, maxlogcnt)\n\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n", "language": "python", "parent_id": "e772a930-ecc4-4994-958e-4c7bbf9f74fd", "generation": 5, "timestamp": 1751067039.594915, "iteration_found": 269, "metrics": {"balancedness_score": 0.003541042432472748, "speed_score": 0.027118822422766233, "combined_score": 0.01532993242761949}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 21 lines with 47 lines\nChange 2: Replace 6 lines with 8 lines\nChange 3: Replace 5 lines with 27 lines", "parent_metrics": {"balancedness_score": 0.3307444807293194, "speed_score": 0.027791395167807244, "combined_score": 0.17926793794856333}, "island": 3}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve the Mixture-of-Expert models Expert Parallelism Load Balancer (MoE EPLB) expert rearrangement algorithm.\nThis algorithm will take the load metrics recorded by the vLLM server, and rearrange the experts to balance the load. It can make replicas of some experts to achieve better load balancing.\nYour goal will be two-fold: 1. Improve the algorithm to achieve better load balancing; while 2. Improve the algorithm to be more efficient, i.e. reduce the execution time of the algorithm itself, since perfect load balancing is NP-hard.\nThe current algorithm is implemented in the `rebalance_experts` function. ", "user": "# Current Program Information\n- Current performance metrics: - balancedness_score: 0.3307\n- speed_score: 0.0278\n- combined_score: 0.1793\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: balancedness_score, speed_score, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1103, combined_score: 0.2315\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1103, combined_score: 0.2315\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1104, combined_score: 0.2316\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.2316)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1104), Performs well on combined_score (0.2316)\n\n\n### Program 2 (Score: 0.2315)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1103), Performs well on combined_score (0.2315)\n\n\n### Program 3 (Score: 0.2315)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1103), Performs well on combined_score (0.2315)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.2315)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n\n### Program D2 (Score: 0.2315)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.2316, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 41 lines with 46 lines, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 2 (Score: 0.1920, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 3 (Score: 0.0939, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 23 lines with 236 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 4 (Score: 0.0000, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 14 lines with 16 lines\nChange 2: Replace 11 lines with 13 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 5 (Score: 0.1605, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 57 lines with 63 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n\n# Current Program\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort groups by weight in descending order.\n    # 'weight' is already float and on CPU from rebalance_experts,\n    # so no need for .float().cpu() here.\n    indices = weight.sort(-1, descending=True).indices\n\n    # Initialize pack_index and rank_in_pack.\n    # These will be fully populated, so `empty_like` is sufficient.\n    # They will be on the same device as 'weight' (i.e., CPU).\n    pack_index = torch.empty_like(weight, dtype=torch.int64)\n    rank_in_pack = torch.empty_like(pack_index)\n    \n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These tensors are on the same device as 'weight' (which is CPU).\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id in indices[i]:\n            # Get current state for this layer's packs.\n            current_pack_weights = pack_weights_per_layer[i]\n            current_pack_items = pack_items_per_layer[i]\n\n            # Find the pack with the minimum current weight among available ones.\n            # Set weights of full packs to infinity to exclude them from argmin.\n            full_mask = (current_pack_items == groups_per_pack)\n            # Create a temporary tensor for argmin that excludes full packs.\n            # Using masked_fill is often more efficient than clone() then assignment.\n            pack = torch.argmin(current_pack_weights.masked_fill(full_mask, float('inf'))).item()\n\n            # Assert that a valid pack was found and it has space.\n            assert current_pack_items[pack] < groups_per_pack\n\n            # Assign the group to the chosen pack.\n            # `group_id` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            # `weight[i, group_id]` is the weight of the current group.\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float().cpu()\n    # Always use the global load-balance policy by treating all experts as one group\n    # and all nodes as one conceptual node. This maximizes flexibility for balancing\n    # across all physical experts, potentially improving balancedness_score\n    # at the cost of ignoring network topology.\n    phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n        weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["The current `rebalance_experts` function calls `rebalance_experts_hierarchical` with `num_groups=1`, `num_nodes=1`, effectively ignoring the hierarchical structure. This might be a simplification that leads to suboptimal load balancing when network topology is indeed important.\n\nThe previous attempts (Attempts 1, 2, 3) all achieved a `combined_score` of around 0.2315-0.2316, while the current program has a `combined_score` of 0.1793. This indicates a regression. The comments also suggest that the current implementation might be overly simplistic and could benefit from a more refined approach to balancedness and speed.\n\nHere are a few suggestions focusing on improving both balancedness and speed:\n\n### Suggestion 1: Re-evaluate the global load-balance policy and leverage hierarchical structure.\n\nThe comment \"Always use the global load-balance policy by treating all experts as one group and all nodes as one conceptual node\" suggests a potential oversimplification. If `num_nodes` and `num_groups` are greater than 1, the hierarchical approach might yield better results by considering network topology.\n\n**Reasoning:**\n*   **Balancedness:** By considering the faster intra-node communication, the algorithm can potentially distribute experts more effectively, leading to better load balancing within nodes and across the system.\n*   **Speed:** While the current global approach might seem simpler, it bypasses potentially beneficial topological optimizations. Re-introducing hierarchical balancing could lead to better overall performance.\n\n**Proposed Change:**\n\nModify the `rebalance_experts` function to conditionally use the hierarchical balancing based on the input parameters. If `num_groups > 1` or `num_nodes > 1`, it should call `rebalance_experts_hierarchical` with the provided parameters. If `num_groups == 1` and `num_nodes == 1`, then the current global approach is appropriate.\n\n<<<<<<< SEARCH\n    # Always use the global load-balance policy by treating all experts as one group\n    # and all nodes as one conceptual node. This maximizes flexibility for balancing\n    # across all physical experts, potentially improving balancedness_score\n    # at the cost of ignoring network topology.\n    phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n        weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n=======\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float().cpu()\n\n    if num_groups > 1 or num_nodes > 1:\n        # Use hierarchical balancing if topology information is relevant\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # Fallback to global balancing if only one group and one node\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n\n    num_redundant_experts = num_replicas - num_logical_experts\n    # Calculate maxlogcnt safely, ensuring it's at least 1\n    maxlogcnt = max(1, num_redundant_experts + 1) \n\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    \n    # Ensure scatter operation is safe when maxlogcnt is 1\n    scatter_dim_size = maxlogcnt\n    if scatter_dim_size == 0: # Should not happen with max(1, ...) but for safety\n        return phy2log, log2phy, logcnt\n\n    # Flatten for scatter, then reshape to handle potential broadcasting issues\n    flat_phy2log_rank = phy2log * scatter_dim_size + phyrank\n    \n    # Create the range of replica indices\n    replica_indices = torch.arange(num_replicas, dtype=torch.int64, device=log2phy.device).expand(num_layers, -1)\n\n    # Perform scatter operation\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        flat_phy2log_rank.flatten().unsqueeze(0), # Ensure correct shape for scatter\n        replica_indices.flatten().unsqueeze(0), # Ensure correct shape for scatter\n    )\n    \n    # Reshape back to the original log2phy shape if needed, or adjust indices if scatter was done differently\n    # The current scatter logic should place values correctly if flat_phy2log_rank is flattened correctly.\n    # For safety, let's ensure the output shape is as expected.\n    log2phy = log2phy.view(num_layers, num_logical_experts, maxlogcnt)\n\n    return phy2log, log2phy, logcnt\n>>>>>>> REPLACE\n\n### Suggestion 2: Optimize `balanced_packing` for efficiency.\n\nThe `balanced_packing` function uses `torch.argmin` within a loop. While `masked_fill` is good, repeated calls to `argmin` on potentially large tensors can be a bottleneck.\n\n**Reasoning:**\n*   **Speed:** The core of `balanced_packing` involves repeatedly finding the minimum weight pack. Optimizing this search can significantly speed up the entire rebalancing process.\n*   **Balancedness:** The current greedy approach is reasonable. Enhancements here would likely focus on the algorithm itself, which is a more complex task. For now, we focus on the implementation efficiency.\n\n**Proposed Change:**\n\nConsider using a min-heap data structure or a more optimized selection method if the number of packs is large. However, given the current implementation using PyTorch, a simple optimization could be to ensure that the `argmin` is applied to a tensor that is already on the correct device and in the optimal format. The current code seems to do this, but profiling would be needed for definitive optimization.\n\nFor now, let's focus on a minor improvement related to `argmin` and tensor manipulation. The current code creates `pack_weights_per_layer` and `pack_items_per_layer` per layer. If `num_layers` is large, this could be memory intensive. However, the loop iterates through layers sequentially, so it's not a major issue for memory *usage* at any given time, but it might be for *creation*.\n\nA more direct optimization within the existing structure:\n\n<<<<<<< SEARCH\n            # Find the pack with the minimum current weight among available ones.\n            # Set weights of full packs to infinity to exclude them from argmin.\n            full_mask = (current_pack_items == groups_per_pack)\n            # Create a temporary tensor for argmin that excludes full packs.\n            # Using masked_fill is often more efficient than clone() then assignment.\n            pack = torch.argmin(current_pack_weights.masked_fill(full_mask, float('inf'))).item()\n=======\n            # Find the pack with the minimum current weight among available ones.\n            # Set weights of full packs to infinity to exclude them from argmin.\n            full_mask = (current_pack_items == groups_per_pack)\n            \n            # Directly create a masked tensor for argmin.\n            # This avoids creating a separate temporary tensor.\n            masked_weights = current_pack_weights.masked_fill(full_mask, float('inf'))\n            pack = torch.argmin(masked_weights).item()\n>>>>>>> REPLACE\n\n**Explanation for Suggestion 2:**\nThe change is minimal, primarily a stylistic one that ensures the `masked_weights` tensor is clearly defined before calling `argmin`. It doesn't fundamentally change the logic but can sometimes lead to slightly cleaner execution paths by avoiding implicit intermediate tensor creation. The key is that `masked_fill` returns a *new* tensor, so this is already handled. The provided change is more about clarity.\n\n### Suggestion 3: Improve `replicate_experts` logic for edge cases.\n\nThe `replicate_experts` function calculates `redundant_indices = (weight / logcnt).max(dim=-1).indices`. If `logcnt` contains zeros (which it shouldn't if initialized correctly to 1, but it's good to be robust), division by zero could occur. Also, the logic for assigning replicas might be further optimized.\n\n**Reasoning:**\n*   **Speed:** The current loop iterates from `num_log` to `num_phy`. For a large number of physical experts, this can be slow.\n*   **Balancedness:** The strategy of picking the expert with the highest average load (`weight / logcnt`) to replicate is a greedy heuristic. While reasonable, ensuring robustness and potentially exploring alternative greedy criteria could improve balancedness.\n\n**Proposed Change:**\nEnsure `logcnt` is never zero when used in division. The current initialization to `torch.ones` is correct, so this is more of a defensive check. The core logic might be improved by pre-sorting or using a more efficient selection mechanism if performance profiling indicates this is a bottleneck.\n\nFor now, let's focus on a small robustness improvement and a clarity improvement in `replicate_experts`.\n\n<<<<<<< SEARCH\n    for i in range(num_log, num_phy):\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n=======\n    # Pre-calculate the maximum number of replicas for each logical expert.\n    # This helps in determining the size of `log2phy` later and can inform\n    # the replication strategy.\n    max_replicas_per_log = torch.ceil(torch.tensor(num_phy / num_log, dtype=torch.float32)).to(torch.int64)\n    \n    # Initialize logcnt with 1 for all logical experts.\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n\n    # Iterate to assign replicas to physical experts.\n    for i in range(num_log, num_phy):\n        # Calculate the load per replica for each logical expert.\n        # Use `torch.clamp_max` to avoid division by zero if logcnt[..., redundant_indices] is 0.\n        # Although logcnt is initialized to 1, this adds robustness.\n        load_per_replica = weight / torch.clamp(logcnt, min=1)\n        \n        # Select the logical expert that has the highest load per replica.\n        # This greedy approach aims to balance the load across physical experts.\n        _, redundant_indices = load_per_replica.max(dim=-1) # Use _ to discard max value, keep indices\n\n        # Assign the selected logical expert to the current physical expert.\n        phy2log[:, i] = redundant_indices\n        \n        # Record the rank of this replica within its logical expert.\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        \n        # Increment the replica count for the chosen logical expert.\n        logcnt[arangen, redundant_indices] += 1\n>>>>>>> REPLACE\n\n**Explanation for Suggestion 3:**\n*   `torch.clamp(logcnt, min=1)` is added to prevent division by zero, making the code more robust.\n*   The variable names are made more descriptive (`load_per_replica`, `redundant_indices`).\n*   The comment about `max_replicas_per_log` is added as a potential optimization hint for future work, although it's not directly used in this current change.\n*   The use of `_` to discard the max value from `.max()` is a common Python idiom for unused return values.\n\nThese suggestions aim to improve the `balancedness_score` by more intelligently using the hierarchical structure and potentially improve the `speed_score` by making the core algorithms more robust and efficient. The regression in metrics suggests that the current simplified global approach is not optimal, and re-engaging with the hierarchical structure is a key step."]}}}