{"id": "f6bf3f7b-6945-4b2f-b12e-580b1cf98cd0", "code": "# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\nimport torch.jit as jit\n\n# Define the inner loop logic as a TorchScript function\n@jit.script\ndef _jit_assign_group_and_update(\n    group_id_tensor: torch.Tensor, # Group ID for the current item being placed (0-dim tensor)\n    pack_weights_layer_i: torch.Tensor, # 1D tensor representing weights of packs for current layer\n    pack_items_layer_i: torch.Tensor,   # 1D tensor representing item counts of packs for current layer\n    groups_per_pack: int, # Max items per pack\n    inf_val: torch.Tensor, # Pre-created infinity tensor\n    group_weight: torch.Tensor # Weight of the current group (0-dim tensor)\n) -> tuple[torch.Tensor, torch.Tensor]: # Returns chosen pack index and rank within that pack (0-dim tensors)\n    \"\"\"\n    JIT-compiled function for the core greedy assignment logic within balanced_packing.\n    This function modifies pack_weights_layer_i and pack_items_layer_i in-place.\n    \"\"\"\n    available_mask = (pack_items_layer_i < groups_per_pack)\n    candidate_weights = torch.where(available_mask, pack_weights_layer_i, inf_val)\n    \n    # Find the pack with the minimum current weight among available ones.\n    # Keep it as a 0-dim tensor for indexing in JIT-compiled code.\n    pack_idx_tensor = torch.argmin(candidate_weights)\n    \n    # Get rank before update. Keep as 0-dim tensor.\n    rank_in_pack_tensor = pack_items_layer_i[pack_idx_tensor]\n\n    # Update pack weights and item counts for the chosen pack in-place\n    pack_weights_layer_i[pack_idx_tensor] += group_weight\n    pack_items_layer_i[pack_idx_tensor] += 1\n    \n    return pack_idx_tensor, rank_in_pack_tensor\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # `weight` is already float from `rebalance_experts`\n    indices = weight.sort(-1, descending=True).indices  # Keep on original device\n    pack_index = torch.full_like(weight,\n                                 fill_value=-1,\n                                 dtype=torch.int64,\n                                 device=weight.device)\n    rank_in_pack = torch.full_like(pack_index,\n                                   fill_value=-1,\n                                   device=weight.device)\n\n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These are on the same device as the input 'weight'.\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Pre-create a tensor for infinity value to avoid repeated creation in loop\n    inf_tensor = torch.tensor(float('inf'), device=weight.device, dtype=weight.dtype)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # Get 1D slices for the current layer's pack states.\n        # These slices will be passed to the JIT-compiled function and modified in-place.\n        current_pack_weights_slice = pack_weights_per_layer[i]\n        current_pack_items_slice = pack_items_per_layer[i]\n\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id_tensor in indices[i]:\n            # Call the JIT-compiled function for the assignment logic\n            # Pass group_id_tensor directly as a 0-dim tensor\n            chosen_pack_idx_tensor, chosen_rank_in_pack_tensor = _jit_assign_group_and_update(\n                group_id_tensor,\n                current_pack_weights_slice,\n                current_pack_items_slice,\n                groups_per_pack,\n                inf_tensor,\n                weight[i, group_id_tensor] # Pass the weight of the current group as a 0-dim tensor\n            )\n\n            # Assign the group to the chosen pack based on results from JIT function.\n            # `group_id_tensor` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id_tensor] = chosen_pack_idx_tensor\n            rank_in_pack[i, group_id_tensor] = chosen_rank_in_pack_tensor\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    # Pre-calculate initial average loads for all logical experts\n    # Use float for division and average load calculations\n    avg_loads = weight.float() / logcnt.float() # Optimization: Pre-calculate average loads\n\n    for i in range(num_log, num_phy):\n        # Find the logical expert with the maximum current average load.\n        # This determines which logical expert gets the next replica.\n        _, redundant_indices = torch.max(avg_loads, dim=-1) # Optimization: Use pre-calculated avg_loads\n\n        # Assign the new physical expert to this logical expert\n        phy2log[:, i] = redundant_indices\n        \n        # Determine the replica rank for this new physical expert\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        \n        # Increment the replica count for the chosen logical expert\n        logcnt[arangen, redundant_indices] += 1\n        \n        # Update the average load for the chosen logical expert.\n        # This is crucial for the next iteration's decision.\n        # The average load is the total weight of the logical expert divided by its current replica count.\n        avg_loads[arangen, redundant_indices] = weight[arangen, redundant_indices].float() / logcnt[arangen, redundant_indices].float() # Optimization: Update only the relevant average load\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float()  # Ensure float, but keep on original device (e.g., GPU)\n    num_layers, num_logical_experts = weight.shape\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n\n    if num_groups % num_nodes == 0 and num_nodes > 0: # Added num_nodes > 0 check for robustness\n        # use hierarchical load-balance policy\n        # The hierarchical function is designed for balanced_packing and replication across groups/nodes/gpus.\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy: replicate experts greedily on the global scale.\n        # Directly use the replicate_experts function for a simpler global strategy.\n        # replicate_experts returns: phy2log [n, num_phy], rank [n, num_phy], logcnt [n, num_log]\n        # These directly map to the expected outputs for the global case.\n        phy2log, phyrank, logcnt = replicate_experts(weight, num_replicas)\n\n    # Ensure log2phy is created on the same device as other tensors\n    # The construction of log2phy from phy2log, phyrank, and logcnt remains the same.\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=weight.device,  # Use weight's device\n    )\n    # The scatter_ operation and arange should also use the correct device\n    # The indices for scatter_ need to be calculated correctly.\n    # `phy2log` is [num_layers, num_replicas]\n    # `phyrank` is [num_layers, num_replicas]\n    # `maxlogcnt` is the number of slots for replicas for a logical expert.\n    # The target index in `log2phy` for physical expert `p` (which maps to logical expert `l_id` with rank `r`) is `l_id * maxlogcnt + r`.\n    # `phy2log` gives `l_id` for each physical expert `p`.\n    # `phyrank` gives `r` for each physical expert `p`.\n    # So, `target_indices = phy2log * maxlogcnt + phyrank` (after appropriate reshaping/broadcasting).\n\n    # The original code had `phy2log * maxlogcnt + phyrank` which assumes `phy2log` and `phyrank` are aligned.\n    # Let's ensure this alignment is handled correctly by reshaping if necessary.\n    # `phy2log` is [num_layers, num_replicas]\n    # `phyrank` is [num_layers, num_replicas]\n    # `maxlogcnt` is a scalar.\n    # The target index calculation should be:\n    # `target_indices = phy2log * maxlogcnt + phyrank`\n    # This result needs to be reshaped to match `log2phy.view(num_layers, -1)`.\n    # The `log2phy.view(num_layers, -1)` has shape `[num_layers, num_logical_experts * maxlogcnt]`.\n    # The `phy2log` and `phyrank` are already shaped correctly for this.\n\n    # Ensure the scatter operation uses the correct device for the arange tensor.\n    # The `arange(num_replicas)` should be on `weight.device`.\n    replica_indices = torch.arange(num_replicas, dtype=torch.int64, device=weight.device).expand(num_layers, -1)\n\n    # The indices for scatter_ are calculated by mapping each physical expert to its slot in log2phy.\n    # For physical expert `p` (column in `phy2log` and `phyrank`), its logical ID is `phy2log[l, p]` and its rank is `phyrank[l, p]`.\n    # The slot in `log2phy[l, :, :]` for this physical expert is `log2phy[l, phy2log[l, p], phyrank[l, p]]`.\n    # The `scatter_` operation expects a flattened index for the last dimension.\n    # So, `indices_to_scatter` should be `phy2log * maxlogcnt + phyrank`.\n    # This results in a tensor of shape `[num_layers, num_replicas]`.\n    # The `scatter_` operation then flattens `log2phy` along the last two dimensions, effectively treating it as `[num_layers, num_logical_experts * maxlogcnt]`.\n    # The `indices_to_scatter` needs to be mapped to this flattened view.\n    # The current logic `phy2log * maxlogcnt + phyrank` correctly computes the index within the `[num_logical_experts, maxlogcnt]` sub-tensors.\n    # When `log2phy.view(num_layers, -1)` is used, this maps correctly.\n\n    # The original calculation `phy2log * maxlogcnt + phyrank` is correct for creating the indices.\n    # The `expand(num_layers, -1)` for `arange` is also correct.\n    # No change needed here, just ensuring consistency.\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        replica_indices,\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n", "language": "python", "parent_id": "0b7dad60-e1fd-456e-b9b8-f43c17cd20c0", "generation": 9, "timestamp": 1751069547.3711848, "iteration_found": 309, "metrics": {"balancedness_score": 0.29830835153138974, "speed_score": 0.031279671964856544, "combined_score": 0.16479401174812314}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 25 lines with 69 lines", "parent_metrics": {"balancedness_score": 0.29830835153138974, "speed_score": 0.031281843494993126, "combined_score": 0.16479509751319144}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve the Mixture-of-Expert models Expert Parallelism Load Balancer (MoE EPLB) expert rearrangement algorithm.\nThis algorithm will take the load metrics recorded by the vLLM server, and rearrange the experts to balance the load. It can make replicas of some experts to achieve better load balancing.\nYour goal will be two-fold: 1. Improve the algorithm to achieve better load balancing; while 2. Improve the algorithm to be more efficient, i.e. reduce the execution time of the algorithm itself, since perfect load balancing is NP-hard.\nThe current algorithm is implemented in the `rebalance_experts` function. ", "user": "# Current Program Information\n- Current performance metrics: - balancedness_score: 0.2983\n- speed_score: 0.0313\n- combined_score: 0.1648\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: balancedness_score, speed_score, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1125, combined_score: 0.2326\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1126, combined_score: 0.2327\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1143, combined_score: 0.2335\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.2335)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1143), Performs well on combined_score (0.2335)\n\n\n### Program 2 (Score: 0.2327)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1126), Performs well on combined_score (0.2327)\n\n\n### Program 3 (Score: 0.2326)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1125), Performs well on combined_score (0.2326)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.2325)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n\n### Program D2 (Score: 0.2322)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.2335, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 32 lines with 17 lines, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 2 (Score: 0.0939, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 23 lines with 236 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 3 (Score: 0.0939, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 23 lines with 236 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 4 (Score: 0.1908, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 35 lines with 48 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 5 (Score: 0.1494, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 11 lines with 7 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n\n# Current Program\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\nimport torch.jit as jit\n\n# Define the inner loop logic as a TorchScript function\n@jit.script\ndef _jit_assign_group_and_update(\n    group_id_tensor: torch.Tensor, # Group ID for the current item being placed (0-dim tensor)\n    pack_weights_layer_i: torch.Tensor, # 1D tensor representing weights of packs for current layer\n    pack_items_layer_i: torch.Tensor,   # 1D tensor representing item counts of packs for current layer\n    groups_per_pack: int, # Max items per pack\n    inf_val: torch.Tensor, # Pre-created infinity tensor\n    group_weight: torch.Tensor # Weight of the current group (0-dim tensor)\n) -> tuple[torch.Tensor, torch.Tensor]: # Returns chosen pack index and rank within that pack (0-dim tensors)\n    \"\"\"\n    JIT-compiled function for the core greedy assignment logic within balanced_packing.\n    This function modifies pack_weights_layer_i and pack_items_layer_i in-place.\n    \"\"\"\n    available_mask = (pack_items_layer_i < groups_per_pack)\n    candidate_weights = torch.where(available_mask, pack_weights_layer_i, inf_val)\n    \n    # Find the pack with the minimum current weight among available ones.\n    # Keep it as a 0-dim tensor for indexing in JIT-compiled code.\n    pack_idx_tensor = torch.argmin(candidate_weights)\n    \n    # Get rank before update. Keep as 0-dim tensor.\n    rank_in_pack_tensor = pack_items_layer_i[pack_idx_tensor]\n\n    # Update pack weights and item counts for the chosen pack in-place\n    pack_weights_layer_i[pack_idx_tensor] += group_weight\n    pack_items_layer_i[pack_idx_tensor] += 1\n    \n    return pack_idx_tensor, rank_in_pack_tensor\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # `weight` is already float from `rebalance_experts`\n    indices = weight.sort(-1, descending=True).indices  # Keep on original device\n    pack_index = torch.full_like(weight,\n                                 fill_value=-1,\n                                 dtype=torch.int64,\n                                 device=weight.device)\n    rank_in_pack = torch.full_like(pack_index,\n                                   fill_value=-1,\n                                   device=weight.device)\n\n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These are on the same device as the input 'weight'.\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Pre-create a tensor for infinity value to avoid repeated creation in loop\n    inf_tensor = torch.tensor(float('inf'), device=weight.device, dtype=weight.dtype)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # Get 1D slices for the current layer's pack states.\n        # These slices will be passed to the JIT-compiled function and modified in-place.\n        current_pack_weights_slice = pack_weights_per_layer[i]\n        current_pack_items_slice = pack_items_per_layer[i]\n\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id_tensor in indices[i]:\n            # Call the JIT-compiled function for the assignment logic\n            # Pass group_id_tensor directly as a 0-dim tensor\n            chosen_pack_idx_tensor, chosen_rank_in_pack_tensor = _jit_assign_group_and_update(\n                group_id_tensor,\n                current_pack_weights_slice,\n                current_pack_items_slice,\n                groups_per_pack,\n                inf_tensor,\n                weight[i, group_id_tensor] # Pass the weight of the current group as a 0-dim tensor\n            )\n\n            # Assign the group to the chosen pack based on results from JIT function.\n            # `group_id_tensor` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id_tensor] = chosen_pack_idx_tensor\n            rank_in_pack[i, group_id_tensor] = chosen_rank_in_pack_tensor\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    # Pre-calculate initial average loads for all logical experts\n    # Use float for division and average load calculations\n    avg_loads = weight.float() / logcnt.float() # Optimization: Pre-calculate average loads\n\n    for i in range(num_log, num_phy):\n        # Find the logical expert with the maximum current average load.\n        # This determines which logical expert gets the next replica.\n        _, redundant_indices = torch.max(avg_loads, dim=-1) # Optimization: Use pre-calculated avg_loads\n\n        # Assign the new physical expert to this logical expert\n        phy2log[:, i] = redundant_indices\n        \n        # Determine the replica rank for this new physical expert\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        \n        # Increment the replica count for the chosen logical expert\n        logcnt[arangen, redundant_indices] += 1\n        \n        # Update the average load for the chosen logical expert.\n        # This is crucial for the next iteration's decision.\n        # The average load is the total weight of the logical expert divided by its current replica count.\n        avg_loads[arangen, redundant_indices] = weight[arangen, redundant_indices].float() / logcnt[arangen, redundant_indices].float() # Optimization: Update only the relevant average load\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float()  # Ensure float, but keep on original device (e.g., GPU)\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    # Ensure log2phy is created on the same device as other tensors\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=weight.device,  # Use weight's device\n    )\n    # The scatter_ operation and arange should also use the correct device\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=weight.device).expand(num_layers, -1),  # Use weight's device\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["## Analysis of Current Program and Improvement Strategy\n\nThe current program implements a hierarchical load balancing strategy for Mixture-of-Experts (MoE) models. It aims to balance the load across experts by replicating them. The core functions are `balanced_packing` and `replicate_experts`, which are then orchestrated by `rebalance_experts_hierarchical` and `rebalance_experts`.\n\nThe provided performance metrics show a significant drop from previous attempts (balancedness_score: 0.2983, speed_score: 0.0313, combined_score: 0.1648 vs. previous best of ~0.23). This suggests that recent changes might have introduced inefficiencies or suboptimal logic. The identified areas for improvement are: simplifying code for readability and maintainability, and addressing the performance regressions.\n\nThe goal is to improve both balancedness and speed. The problem of perfect load balancing is NP-hard, so we aim for a good approximation that is also computationally efficient.\n\n**Key areas for potential improvement:**\n\n1.  **`replicate_experts` function:** The current greedy approach of replicating the expert with the maximum average load might not be optimal. We can explore more sophisticated methods to decide which logical expert to replicate.\n2.  **`balanced_packing` function:** While JIT-compiled, the greedy approach of assigning items to the least loaded pack could be a bottleneck or not achieve the best packing.\n3.  **Overall efficiency:** Identify any redundant computations or opportunities for vectorization.\n4.  **Code Readability:** Simplify complex tensor manipulations and variable naming.\n\nLet's focus on improving the `replicate_experts` function first, as it directly addresses the replication strategy, which is core to load balancing. The previous best scores were achieved by \"Attempt 3\", \"Attempt 2\", and \"Attempt 1\" (all around 0.23), and \"Program 1\", \"Program 2\", \"Program 3\" (also around 0.23). Inspiration 1 also achieved 0.2335 with a modification that replaced 32 lines with 17, suggesting a simplification or more efficient logic was introduced there.\n\n### Strategy for `replicate_experts`\n\nThe current `replicate_experts` function iteratively selects the logical expert with the *maximum* current average load and assigns a replica to it. This is a greedy approach. A potential issue is that this might lead to a situation where many experts have similar loads, and the greedy choice doesn't significantly improve overall balance.\n\nA more robust approach could be to consider the *variance* or *standard deviation* of loads for logical experts, or to use a method that aims to equalize the maximum load across all experts more directly. However, given the goal of improving *speed* as well, a complex optimization might be too slow.\n\nLet's consider a slight modification to `replicate_experts` to potentially improve balancedness without drastically increasing complexity. Instead of just picking the *single* expert with the max average load, we could consider a small set of top experts. However, this adds complexity.\n\nA simpler approach for speed and potentially better balancedness could be to re-evaluate the *initial* assignment of replicas. The current code pre-calculates `avg_loads` and then iteratively updates it.\n\nLet's look at the \"Inspiration 1\" program, which achieved a good score and involved replacing 32 lines with 17. This suggests a significant simplification or algorithmic change. Without the specific code of Inspiration 1, it's hard to know the exact change. However, a common way to simplify and potentially speed up such operations is to leverage more direct tensor operations or a different greedy strategy.\n\nOne potential improvement for `replicate_experts` related to speed and balancedness could be to use `torch.topk` to find the indices of the `k` largest average loads and then pick among them. However, for simplicity and speed, sticking to a single greedy choice but optimizing its calculation might be better.\n\nLet's re-examine the `replicate_experts` function:\n```python\n    for i in range(num_log, num_phy):\n        # Find the logical expert with the maximum current average load.\n        # This determines which logical expert gets the next replica.\n        _, redundant_indices = torch.max(avg_loads, dim=-1) # Optimization: Use pre-calculated avg_loads\n\n        # Assign the new physical expert to this logical expert\n        phy2log[:, i] = redundant_indices\n\n        # Determine the replica rank for this new physical expert\n        rank[:, i] = logcnt[arangen, redundant_indices]\n\n        # Increment the replica count for the chosen logical expert\n        logcnt[arangen, redundant_indices] += 1\n\n        # Update the average load for the chosen logical expert.\n        # This is crucial for the next iteration's decision.\n        # The average load is the total weight of the logical expert divided by its current replica count.\n        avg_loads[arangen, redundant_indices] = weight[arangen, redundant_indices].float() / logcnt[arangen, redundant_indices].float() # Optimization: Update only the relevant average load\n```\nThe update of `avg_loads` is done efficiently by only updating the selected expert. The `torch.max` operation is also efficient. The current implementation seems reasonable for a greedy approach.\n\nHowever, the *initial* calculation of `avg_loads` is `weight.float() / logcnt.float()`. `logcnt` is initialized to ones. This is correct.\n\nLet's consider the \"Diverse Programs\" section. \"Program D1\" and \"Program D2\" mention \"Alternative approach to balancedness_score\" and \"Alternative approach to speed_score\". This suggests that different strategies for calculating these scores or for the balancing itself were explored.\n\nGiven the significant performance regression, it's possible that a recent change introduced a subtle bug or an inefficient computation. The prompt mentions that \"Metrics showing regression: balancedness_score, speed_score, combined_score. Consider reverting or revising recent changes in these areas.\"\n\nThe most significant change in the provided code compared to a hypothetical earlier version is the introduction of JIT compilation for `_jit_assign_group_and_update`. While JIT is intended to improve performance, it can sometimes introduce overhead or not be as effective as anticipated if the function is too simple or called in a way that negates its benefits.\n\nLet's consider the `rebalance_experts_hierarchical` function. It calls `balanced_packing` and `replicate_experts`. The main logic for replication is in `replicate_experts`.\n\nOne potential area for improvement in `replicate_experts` could be how `avg_loads` is calculated and updated. The current approach is:\n`avg_loads = weight.float() / logcnt.float()`\nand then in the loop:\n`avg_loads[arangen, redundant_indices] = weight[arangen, redundant_indices].float() / logcnt[arangen, redundant_indices].float()`\n\nThis looks correct. Let's consider the possibility that the *way* `torch.max` is used is inefficient or that the overall loop structure for replication can be optimized.\n\nA common strategy for load balancing that often yields good results and can be efficient is to use a \"longest processing time first\" (LPT) heuristic, which is what the current `replicate_experts` seems to be implementing by picking the max average load.\n\nLet's consider if there's a way to make the `replicate_experts` function more efficient or potentially better balanced by changing how it selects the next expert to replicate. Instead of just picking the single max, we could consider a more global view. However, this can increase complexity.\n\nGiven the prompt's emphasis on *simplifying* code and addressing *regressions*, let's focus on potential inefficiencies in the current implementation of `replicate_experts` or `balanced_packing`.\n\nThe `replicate_experts` function iterates `num_redundant` times. Inside the loop, it performs `torch.max`, indexing, and division. These are generally efficient operations.\n\nPerhaps the issue lies in the interaction between `balanced_packing` and `replicate_experts` within `rebalance_experts_hierarchical`.\n\nLet's look at the `rebalance_experts_hierarchical` function:\n1.  `tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)`: This flattens and sums, which is standard.\n2.  `group_pack_index, group_rank_in_pack = balanced_packing(tokens_per_group, num_nodes)`: This uses the packing logic.\n3.  `log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) * group_size).unsqueeze(-1) + torch.arange(group_size, dtype=torch.int64, device=group_pack_index.device)).flatten(-2)`: This is a complex index calculation.\n4.  `mlog2log = inverse(log2mlog)`: This calculates the inverse permutation.\n5.  `tokens_per_mlog = weight.gather(-1, mlog2log).view(-1, num_logical_experts // num_nodes)`: Gathers weights based on the new logical expert ordering.\n6.  `phy2mlog, phyrank, mlogcnt = replicate_experts(tokens_per_mlog, num_physical_experts // num_nodes)`: This is the core replication step on a subset of experts.\n7.  `tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)`: Calculates average load for physical experts.\n8.  `pack_index, rank_in_pack = balanced_packing(tokens_per_phy, num_gpus // num_nodes)`: Packs physical experts to GPUs.\n9.  `phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack`: Calculates final physical expert mapping.\n10. `pphy2phy = inverse(phy2pphy)`: Inverse mapping.\n11. `pphy2mlog = phy2mlog.gather(-1, pphy2phy)`: Gathers logical expert IDs for packed physical experts.\n12. `pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(0, num_logical_experts, num_logical_experts // num_nodes, device=group_pack_index.device).view(1, -1, 1)).flatten(-2)`: Reshapes and adjusts indices.\n13. `pphy2log = mlog2log.gather(-1, pphy2mlog)`: Final mapping.\n14. `pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)`: Replica ranks.\n15. `logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)`: Counts.\n\nThe most complex parts seem to be the index manipulations (steps 3, 4, 11, 12, 13) and the two calls to `balanced_packing`.\n\nLet's focus on `replicate_experts` and `balanced_packing`.\n\n### Improving `replicate_experts`\n\nThe core of `replicate_experts` is the greedy selection of the logical expert with the maximum average load. To improve balancedness, we could consider a strategy that aims to minimize the *maximum* load across all experts. This is related to the multiway number partitioning problem, which is NP-hard.\n\nA common heuristic for this is the \"Largest Differencing Method\" or variations thereof. However, implementing these can be complex and might not be faster.\n\nGiven the regression and the desire for simplification, let's re-examine the `replicate_experts` function for potential inefficiencies or simpler alternatives.\n\nOne observation: `avg_loads` is calculated as `weight.float() / logcnt.float()`. The `weight` tensor itself represents the load statistics. `logcnt` is the number of replicas for each logical expert.\n\nConsider the case where `num_redundant` is large. The loop runs many times.\n\nA potential optimization that might also improve balancedness is to use `torch.scatter_add` or similar operations if we can express the replication process more directly.\n\nHowever, the current loop structure for `replicate_experts` is quite standard for greedy assignment. The performance regression might stem from other parts or an interaction.\n\nLet's revisit the \"Inspiration 1\" program which simplified 32 lines to 17. This suggests a more direct computation.\n\nCould the issue be in how `avg_loads` is updated?\n`avg_loads[arangen, redundant_indices] = weight[arangen, redundant_indices].float() / logcnt[arangen, redundant_indices].float()`\n\nThis line updates `avg_loads` for the chosen expert. The `weight` tensor is the original load. `logcnt` is the current number of replicas. The division gives the average load.\n\nLet's consider an alternative way to calculate `avg_loads` and update it. If we have `N` logical experts and want `P` physical experts, we need to add `P-N` replicas.\n\nThe current approach is:\n1. Initialize `logcnt` to 1 for all logical experts.\n2. For each additional replica (from `N+1` to `P`):\n    a. Calculate current average loads for all logical experts: `avg_load_i = weight_i / logcnt_i`.\n    b. Find logical expert `j` with max `avg_load_j`.\n    c. Assign the next replica to logical expert `j`.\n    d. Increment `logcnt_j`.\n\nThis is a standard greedy algorithm. If it's regressing, it might be due to:\n*   **Overhead:** The `torch.max` operation repeatedly on a large tensor.\n*   **Suboptimality:** The greedy choice might not lead to the best overall balance, especially if there are many experts with very similar loads.\n\nLet's consider a different approach to `replicate_experts` that might be more efficient or balanced.\nInstead of iteratively picking the max, we could try to determine the target number of replicas for each logical expert more directly.\n\nHowever, the prompt asks for *improvements* to the *current* program, implying we should modify the existing logic rather than replace it entirely.\n\nLet's focus on the `replicate_experts` function and see if we can optimize the loop or the selection process.\n\nThe `torch.max(avg_loads, dim=-1)` operation finds the maximum value and its index along the last dimension. This is efficient.\n\nThe update of `avg_loads` is also done efficiently.\n\nPerhaps the issue is not in `replicate_experts` itself, but in how `tokens_per_mlog` is computed or used.\n\n`tokens_per_mlog = weight.gather(-1, mlog2log).view(-1, num_logical_experts // num_nodes)`\nThis line takes the original weights, reorders them according to `mlog2log`, and then reshapes. `mlog2log` is derived from `balanced_packing` of groups to nodes.\n\nLet's consider the `balanced_packing` function. It uses JIT. The JIT function `_jit_assign_group_and_update` seems to implement a greedy assignment: find the available pack with the minimum current weight, assign the item, and update. This is a standard greedy bin-packing heuristic.\n\nThe performance regression is a key signal. The previous best scores were significantly higher. This suggests a recent change might have made the algorithm less efficient or less effective at balancing.\n\nOne possibility is that the complexity of the hierarchical approach itself is leading to overhead that wasn't present in earlier, simpler versions. The prompt also mentions simplifying the code.\n\nLet's look at the `rebalance_experts` function's conditional logic:\n```python\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n```\nThe \"global load-balance policy\" simply calls the hierarchical function with `num_groups=1`, `num_nodes=1`. This means the hierarchical logic is always used. This might be a source of unnecessary complexity if the hierarchical structure isn't beneficial in all cases.\n\nIf `num_nodes` is 1, the `rebalance_experts_hierarchical` function effectively becomes a simpler process. Let's trace it:\n- `num_groups=1`, `group_size = num_logical_experts`.\n- `tokens_per_group = weight.unflatten(-1, (1, num_logical_experts)).sum(-1)` which is just `weight`.\n- `group_pack_index, group_rank_in_pack = balanced_packing(weight, 1)`: This packs all logical experts into a single \"group\" (node). `group_pack_index` will be all zeros, `group_rank_in_pack` will be `0, 1, 2, ...`.\n- `log2mlog = (((0 * 1 + group_rank_in_pack) * num_logical_experts).unsqueeze(-1) + torch.arange(num_logical_experts, dtype=torch.int64, device=group_pack_index.device)).flatten(-2)`: This seems overly complex for the `num_nodes=1` case.\n    - `group_rank_in_pack` is `[0, 1, ..., num_logical_experts-1]`.\n    - `log2mlog` becomes `[0, 1, ..., num_logical_experts-1]` repeated `num_logical_experts` times. This is effectively `torch.arange(num_logical_experts).repeat(num_logical_experts, 1)`.\n- `mlog2log = inverse(log2mlog)`: This would be the identity permutation if `log2mlog` is `arange(N).repeat(N, 1)`. This seems wrong. The inverse of `arange(N).repeat(N, 1)` should be `arange(N).view(-1, 1).repeat(1, N)`.\n    - The `inverse` function: `inv.scatter_(1, perm, torch.arange(perm.size(1), ...))`.\n    - If `perm` is `[0,1,2,0,1,2]` (N=3, K=2), `perm.size(1)` is 2. `arange(2)` is `[0,1]`.\n    - `inv.scatter_(1, [0,1,2,0,1,2], [0,1,0,1,0,1])`\n    - `inv` starts as `[[0,0],[0,0],[0,0]]`.\n    - `inv[:, 0]` gets `[0,1,2,0,1,2]`. `inv[:, 1]` gets `[0,1,0,1,0,1]`. This is not an inverse.\n\nThe `inverse` function seems to be designed for a different permutation structure. It's likely intended for mapping `[num_layers, num_physical_experts]` to `[num_layers, num_logical_experts]`.\n\nLet's re-examine the `inverse` function's intended purpose. It's used to create `mlog2log` from `log2mlog` and `pphy2phy` from `phy2pphy`.\n`log2mlog` is `[num_layers * num_nodes, num_logical_experts // num_nodes]` after flattening.\n`mlog2log` is its inverse.\n\nThe issue might be in the `inverse` function or its usage when `num_nodes` is 1.\nIf `num_nodes = 1`, `num_groups = 1`, `group_size = num_logical_experts`.\n`tokens_per_group` is `weight` (shape `[num_layers, num_logical_experts]`).\n`balanced_packing(weight, 1)` returns `group_pack_index` (all zeros) and `group_rank_in_pack` (`[0, 1, ..., num_logical_experts-1]`).\n\nThen:\n`log2mlog = (((0 * num_logical_experts + torch.arange(num_logical_experts)).unsqueeze(-1) + torch.arange(num_logical_experts, device=weight.device)).flatten(-2))`\nThis simplifies to:\n`log2mlog = (torch.arange(num_logical_experts).unsqueeze(-1) + torch.arange(num_logical_experts, device=weight.device)).flatten(-2)`\nThis is `[[0,1,2],[1,2,3],[2,3,4]]` for `num_logical_experts=3`. This is not an index mapping.\nIt should be `log2mlog[i, j]` is the index of the `j`-th logical expert within the `i`-th physical expert.\n\nThe calculation of `log2mlog` and `mlog2log` seems to be where the logic breaks down for the \"global\" policy.\n\n**Proposal 1: Fix the global load balancing logic.**\nThe `else` branch in `rebalance_experts` should implement a simple global replication, not a flawed hierarchical one.\n\nA simple global replication would be:\n1.  Calculate average load for each logical expert: `avg_load = weight.float() / logcnt.float()`.\n2.  Iteratively assign replicas to the logical expert with the maximum average load until `num_replicas` is reached.\n\nThis is essentially what `replicate_experts` does. So, if the global policy is intended to be simpler, it should just call `replicate_experts` directly on the original `weight` tensor.\n\nLet's assume the `rebalance_experts_hierarchical` function is correct for the hierarchical case. The issue is likely in the `else` branch.\n\n**Proposed Change:** Replace the `else` branch with a direct call to `replicate_experts` on the original `weight` tensor.\n\n```python\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy: replicate experts greedily on the global scale.\n        # The hierarchical function with num_groups=1, num_nodes=1 is intended for this,\n        # but the internal index calculations seem problematic for these parameters.\n        # A direct call to replicate_experts is a simpler and more robust global approach.\n        # We need to ensure the output shapes match the hierarchical function.\n        # `replicate_experts` returns `phy2log`, `rank`, `logcnt`.\n        # The hierarchical function returns `phy2log`, `pphyrank`, `logcnt`.\n        # `pphyrank` is the rank within the final physical expert.\n        # Let's adapt `replicate_experts` to produce the correct output format for the global case.\n\n        # For global replication, we directly apply replicate_experts to the original weight.\n        # The number of physical experts is `num_replicas`.\n        # The number of logical experts is `num_logical_experts`.\n        # `replicate_experts` takes `weight` [n, num_log] and `num_phy`.\n        # It returns `phy2log` [n, num_phy], `rank` [n, num_phy], `logcnt` [n, num_log].\n        # This matches the expected output structure for `phy2log`, `rank`, `logcnt`.\n        # However, the final mapping `log2phy` is constructed later.\n        # The `rebalance_experts_hierarchical` function returns `pphy2log`, `pphyrank`, `logcnt`.\n        # These need to be mapped to the final `phy2log`, `log2phy`, `expert_count`.\n\n        # Let's re-examine the structure of the returned values and how they are used.\n        # `rebalance_experts` returns `phy2log`, `log2phy`, `expert_count`.\n        # `phy2log` is [layers, num_replicas]\n        # `log2phy` is [layers, num_logical_experts, maxlogcnt]\n        # `expert_count` is [layers, num_logical_experts]\n\n        # If we use `replicate_experts` directly:\n        # `phy2log_global, rank_global, logcnt_global = replicate_experts(weight, num_replicas)`\n        # `phy2log_global` is [num_layers, num_replicas]\n        # `rank_global` is [num_layers, num_replicas]\n        # `logcnt_global` is [num_layers, num_logical_experts]\n\n        # Now we need to construct `log2phy` from `phy2log_global` and `rank_global`.\n        # `log2phy` is `[num_layers, num_logical_experts, maxlogcnt]`\n        # `maxlogcnt` = `num_replicas - num_logical_experts + 1`\n\n        # We need to map physical experts to logical experts.\n        # `phy2log_global` gives this mapping.\n        # For each layer `l`:\n        #   For each physical expert `p` from `0` to `num_replicas-1`:\n        #     logical_expert_id = phy2log_global[l, p]\n        #     replica_rank = rank_global[l, p]\n        #     `log2phy[l, logical_expert_id, replica_rank] = p`\n\n        # This construction is what the end of `rebalance_experts` does:\n        # `log2phy.view(num_layers, -1).scatter_(...)`\n\n        # So, the `else` branch should simply call `replicate_experts` and then\n        # use its output to construct the final `phy2log`, `log2phy`, `logcnt`.\n        # The `replicate_experts` function itself is designed to produce the\n        # `logcnt` and a preliminary `phy2log` and `rank`.\n\n        # Let's make `replicate_experts` produce the same output signature as the\n        # hierarchical function's intermediate results for consistency.\n        # The hierarchical function returns `phy2log`, `pphyrank`, `logcnt`.\n        # `replicate_experts` returns `phy2log`, `rank`, `logcnt`.\n        # So, we can directly use `replicate_experts` if we rename `rank` to `pphyrank`.\n\n        phy2log_global, rank_global, logcnt_global = replicate_experts(weight, num_replicas)\n\n        # The rest of the logic in rebalance_experts constructs log2phy from phy2log and rank.\n        # This logic is already present at the end of the function.\n        # So, we just need to assign the results from `replicate_experts` to the\n        # variables that the final part of `rebalance_experts` expects.\n\n        phy2log = phy2log_global\n        # `rank` from `replicate_experts` is the rank of the replica within its logical expert.\n        # This is what `pphyrank` should be.\n        # The current implementation uses `phyrank` for this.\n        phyrank = rank_global\n        logcnt = logcnt_global\n\n```\nThis change addresses the identified issue with the \"global\" policy and simplifies the `else` branch by directly using the `replicate_experts` function, which is likely more robust for the global case. This also aligns with the goal of simplifying the code.\n\n**Proposed Change 1:**\n\n<<<<<<< SEARCH\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    # Ensure log2phy is created on the same device as other tensors\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=weight.device,  # Use weight's device\n    )\n    # The scatter_ operation and arange should also use the correct device\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=weight.device).expand(num_layers, -1),  # Use weight's device\n    )\n    return phy2log, log2phy, logcnt\n=======\n    num_layers, num_logical_experts = weight.shape\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n\n    if num_groups % num_nodes == 0 and num_nodes > 0: # Added num_nodes > 0 check for robustness\n        # use hierarchical load-balance policy\n        # The hierarchical function is designed for balanced_packing and replication across groups/nodes/gpus.\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy: replicate experts greedily on the global scale.\n        # Directly use the replicate_experts function for a simpler global strategy.\n        # replicate_experts returns: phy2log [n, num_phy], rank [n, num_phy], logcnt [n, num_log]\n        # These directly map to the expected outputs for the global case.\n        phy2log, phyrank, logcnt = replicate_experts(weight, num_replicas)\n\n    # Ensure log2phy is created on the same device as other tensors\n    # The construction of log2phy from phy2log, phyrank, and logcnt remains the same.\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=weight.device,  # Use weight's device\n    )\n    # The scatter_ operation and arange should also use the correct device\n    # The indices for scatter_ need to be calculated correctly.\n    # `phy2log` is [num_layers, num_replicas]\n    # `phyrank` is [num_layers, num_replicas]\n    # `maxlogcnt` is the number of slots for replicas for a logical expert.\n    # The target index in `log2phy` for physical expert `p` (which maps to logical expert `l_id` with rank `r`) is `l_id * maxlogcnt + r`.\n    # `phy2log` gives `l_id` for each physical expert `p`.\n    # `phyrank` gives `r` for each physical expert `p`.\n    # So, `target_indices = phy2log * maxlogcnt + phyrank` (after appropriate reshaping/broadcasting).\n\n    # The original code had `phy2log * maxlogcnt + phyrank` which assumes `phy2log` and `phyrank` are aligned.\n    # Let's ensure this alignment is handled correctly by reshaping if necessary.\n    # `phy2log` is [num_layers, num_replicas]\n    # `phyrank` is [num_layers, num_replicas]\n    # `maxlogcnt` is a scalar.\n    # The target index calculation should be:\n    # `target_indices = phy2log * maxlogcnt + phyrank`\n    # This result needs to be reshaped to match `log2phy.view(num_layers, -1)`.\n    # The `log2phy.view(num_layers, -1)` has shape `[num_layers, num_logical_experts * maxlogcnt]`.\n    # The `phy2log` and `phyrank` are already shaped correctly for this.\n\n    # Ensure the scatter operation uses the correct device for the arange tensor.\n    # The `arange(num_replicas)` should be on `weight.device`.\n    replica_indices = torch.arange(num_replicas, dtype=torch.int64, device=weight.device).expand(num_layers, -1)\n\n    # The indices for scatter_ are calculated by mapping each physical expert to its slot in log2phy.\n    # For physical expert `p` (column in `phy2log` and `phyrank`), its logical ID is `phy2log[l, p]` and its rank is `phyrank[l, p]`.\n    # The slot in `log2phy[l, :, :]` for this physical expert is `log2phy[l, phy2log[l, p], phyrank[l, p]]`.\n    # The `scatter_` operation expects a flattened index for the last dimension.\n    # So, `indices_to_scatter` should be `phy2log * maxlogcnt + phyrank`.\n    # This results in a tensor of shape `[num_layers, num_replicas]`.\n    # The `scatter_` operation then flattens `log2phy` along the last two dimensions, effectively treating it as `[num_layers, num_logical_experts * maxlogcnt]`.\n    # The `indices_to_scatter` needs to be mapped to this flattened view.\n    # The current logic `phy2log * maxlogcnt + phyrank` correctly computes the index within the `[num_logical_experts, maxlogcnt]` sub-tensors.\n    # When `log2phy.view(num_layers, -1)` is used, this maps correctly.\n\n    # The original calculation `phy2log * maxlogcnt + phyrank` is correct for creating the indices.\n    # The `expand(num_layers, -1)` for `arange` is also correct.\n    # No change needed here, just ensuring consistency.\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        replica_indices,\n    )\n    return phy2log, log2phy, logcnt\n>>>>>>> REPLACE\n\n**Reasoning for Proposed Change 1:**\n\nThe original code had a conditional that was meant to switch between a hierarchical and a global load balancing policy. However, the `else` branch simply called `rebalance_experts_hierarchical` with `num_groups=1` and `num_nodes=1`. Analysis revealed that the internal index calculations within `rebalance_experts_hierarchical` (specifically `log2mlog` and `mlog2log` generation) are likely incorrect or inefficient when `num_nodes=1`. This could explain the performance regression.\n\nThe proposed change replaces the problematic `else` branch with a direct call to `replicate_experts`. The `replicate_experts` function implements a standard greedy replication strategy that is suitable for a global load balancing scenario. This simplification addresses the potential bug in the hierarchical logic when applied globally and adheres to the goal of simplifying the code. The output of `replicate_experts` (`phy2log`, `rank`, `logcnt`) directly maps to the expected intermediate variables (`phy2log`, `phyrank`, `logcnt`) used in the subsequent steps of `rebalance_experts` to construct the final `log2phy` mapping. I've also added a check `num_nodes > 0` for robustness, although `num_nodes` is usually expected to be at least 1.\n\nThis change is expected to:\n1.  **Improve Balancedness:** By correctly implementing a global greedy replication strategy when hierarchical is not applicable or beneficial, it should lead to better load balancing in those cases.\n2.  **Improve Speed:** By avoiding the complex and potentially incorrect hierarchical index calculations for the global case, this change should be more computationally efficient.\n3.  **Improve Readability:** The code becomes clearer by using the appropriate function for the global strategy.\n\nLet's consider if any other changes are warranted. The `balanced_packing` function uses JIT, which is generally good for performance. Its greedy strategy is a reasonable heuristic. The `replicate_experts` function's greedy approach is also standard. The main issue seems to be the handling of the global policy.\n\nIf the performance regression is severe, it might indicate that the *hierarchical* logic itself is also inefficient or has subtle bugs. However, without specific details from \"Inspiration 1\" or other high-scoring programs, it's hard to pinpoint further optimizations. The current proposal focuses on a clear identified issue.\n\nOne final thought: the prompt mentions \"Consider reverting or revising recent changes in these areas.\" The introduction of JIT for `_jit_assign_group_and_update` is a significant recent change. While JIT is intended to speed up, it can sometimes add overhead if the function is too simple or the overhead of JIT compilation/tracing outweighs the execution speedup. However, the current performance drop is too significant to be solely attributed to JIT overhead unless there's a major bug. The change to the global policy handling is a more likely culprit for the regression.\n\nThe proposed change aims to fix the global policy, which is a significant identified issue. If further improvements are needed, we could investigate the `balanced_packing` and `replicate_experts` functions more deeply, but for now, addressing the flawed global policy is the most direct path based on the provided information."]}}}