{"id": "68c169b2-2b33-44ba-bf78-380fb98d0148", "code": "# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    This implementation vectorizes the greedy assignment across layers for improved efficiency.\n\n    Parameters:\n        weight: [X, n], the weight of each item (X is num_layers or num_layers * num_nodes)\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        # If each group is a pack, assignment is straightforward.\n        pack_index = torch.arange(num_groups,\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(num_layers, -1)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort groups by weight in descending order for all layers.\n    # 'weight' is already float and on CPU from rebalance_experts.\n    indices = weight.sort(-1, descending=True).indices  # [num_layers, num_groups]\n\n    # Initialize output tensors.\n    pack_index = torch.empty_like(weight, dtype=torch.int64)\n    rank_in_pack = torch.empty_like(pack_index)\n    \n    # Initialize tensors to hold current pack weights and item counts for all layers.\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Precompute layer indices for vectorized operations.\n    layer_indices = torch.arange(num_layers, device=weight.device)\n\n    # Iterate over sorted group ranks (k) for all layers simultaneously.\n    # In each iteration, we assign the k-th heaviest group (for each layer)\n    # to the least loaded pack (for that layer).\n    for k in range(num_groups):\n        # group_ids_to_assign: [num_layers] - the actual group_id for each layer at rank k\n        group_ids_to_assign = indices[:, k] \n        \n        # weights_to_assign: [num_layers] - the weight of these groups\n        weights_to_assign = weight[layer_indices, group_ids_to_assign]\n\n        # Find the pack with the minimum current weight for each layer.\n        # Mask full packs by setting their weights to infinity.\n        current_pack_weights_masked = torch.where(\n            pack_items_per_layer == groups_per_pack,\n            float('inf'),  # Set full packs to infinity\n            pack_weights_per_layer\n        )\n        \n        # chosen_packs: [num_layers] - the chosen pack index for each layer\n        chosen_packs = torch.argmin(current_pack_weights_masked, dim=1)\n\n        # Assign the group to the chosen pack for each layer.\n        pack_index[layer_indices, group_ids_to_assign] = chosen_packs\n        rank_in_pack[layer_indices, group_ids_to_assign] = \\\n            pack_items_per_layer[layer_indices, chosen_packs]\n\n        # Update pack weights and item counts for the chosen packs across layers.\n        pack_weights_per_layer[layer_indices, chosen_packs] += weights_to_assign\n        pack_items_per_layer[layer_indices, chosen_packs] += 1\n    \n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\n# Helper function for global rebalancing\ndef rebalance_experts_global(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate experts to minimize maximum load across all GPUs globally.\n\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: total number of physical experts after replication\n        num_gpus: total number of GPUs\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        rank: [num_moe_layers, num_physical_experts]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_physical_experts >= num_logical_experts\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    # Step 1: Use replicate_experts to create num_physical_experts across all layers.\n    # This function balances the load distribution of logical experts into physical slots.\n    phy2log_all, rank_all, logcnt_all = replicate_experts(weight, num_physical_experts)\n    # phy2log_all: [num_layers, num_physical_experts] - logical expert ID for each physical expert.\n    # rank_all: [num_layers, num_physical_experts] - replica rank of each physical expert.\n    # logcnt_all: [num_layers, num_logical_experts] - number of replicas for each logical expert.\n\n    # Step 2: Pack these physical experts into num_gpus.\n    # We need a weight tensor for balanced_packing that represents the load of each physical expert.\n    # The load of a physical expert is related to the load of its logical expert.\n    # `replicate_experts` already balanced the load across `num_physical_experts`.\n    # Calculate the effective load for each physical expert.\n    # A physical expert's load is its logical expert's original load divided by the number of replicas\n    # that logical expert has.\n    # 1. Get the replica count for each logical expert, mapped to its corresponding physical expert.\n    logical_expert_replica_counts_for_physical_experts = torch.gather(logcnt_all, 1, phy2log_all)\n\n    # 2. Get the original weight of the logical expert for each physical expert.\n    original_logical_expert_weights_for_physical_experts = torch.gather(weight, 1, phy2log_all)\n\n    # 3. Calculate the effective load per physical expert.\n    # Use torch.max(..., 1) to prevent division by zero, though logcnt should always be >= 1 here.\n    physical_expert_load = original_logical_expert_weights_for_physical_experts / \\\n                           torch.max(logical_expert_replica_counts_for_physical_experts, torch.tensor(1, device=weight.device))\n\n    # Use balanced_packing to assign these physical experts (with their effective loads) to num_gpus.\n    # `balanced_packing` will pack `num_physical_experts` items (per layer) into `num_gpus` packs.\n    pack_index_gpu, rank_in_gpu_pack = balanced_packing(\n        physical_expert_load, num_gpus\n    ) # pack_index_gpu: [num_layers, num_physical_experts]\n\n    # Step 3: Reorder the physical experts based on the GPU assignment to create the final mapping.\n    # Create a combined sort key to order physical experts first by GPU, then by rank within GPU.\n    combined_sort_key = pack_index_gpu * phy_experts_per_gpu + rank_in_gpu_pack\n    sorted_indices = torch.argsort(combined_sort_key, dim=1)\n\n    # Reorder phy2log_all and rank_all using the sorted indices.\n    final_phy2log = torch.gather(phy2log_all, 1, sorted_indices)\n    final_rank = torch.gather(rank_all, 1, sorted_indices)\n\n    # The logical_count (logcnt_all) is already determined by replicate_experts and is correct.\n    return final_phy2log, final_rank, logcnt_all\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    # Ensure weight is float. Keep on original device (CPU or GPU).\n    # Moving data to CPU can be a significant bottleneck in GPU environments.\n    if weight.dtype != torch.float:\n        weight = weight.float()\n    \n    # Use hierarchical load-balance policy if num_groups > 1 and is divisible by num_nodes.\n    # Otherwise, use the dedicated global load-balance policy.\n    if num_groups > 1 and num_groups % num_nodes == 0:\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # Use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_global(\n            weight, num_replicas, num_gpus)\n            \n    num_redundant_experts = num_replicas - num_logical_experts\n    # Ensure maxlogcnt is at least 1, even if num_redundant_experts is negative (should not happen).\n    maxlogcnt = max(1, num_redundant_experts + 1)\n    \n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    \n    # The scatter operation maps physical experts to their logical expert and rank slots.\n    # The indices into the flattened log2phy are derived from phy2log and phyrank.\n    # The values being scattered are the physical expert indices (0 to num_replicas-1).\n    scatter_indices = phy2log * maxlogcnt + phyrank\n    \n    # Ensure scatter_indices are within bounds.\n    # max_idx = num_logical_experts * maxlogcnt\n    # scatter_indices = torch.remainder(scatter_indices, max_idx) # This might not be correct.\n    # The logic relies on the structure of phy2log and phyrank correctly mapping.\n    # Let's re-check the original scatter logic.\n\n    # The original scatter:\n    # log2phy.view(num_layers, -1).scatter_(-1, phy2log * maxlogcnt + phyrank, torch.arange(num_replicas, dtype=torch.int64, device=log2phy.device).expand(num_layers, -1))\n    # This scatter targets indices based on `phy2log * maxlogcnt + phyrank`.\n    # The number of elements in `phy2log` is `num_layers * num_replicas`.\n    # The number of elements in `log2phy.view(num_layers, -1)` is `num_layers * num_logical_experts * maxlogcnt`.\n    # The shape of `phy2log * maxlogcnt + phyrank` is `[num_layers, num_replicas]`.\n    # The shape of `torch.arange(num_replicas).expand(num_layers, -1)` is `[num_layers, num_replicas]`.\n\n    # The scatter operation is `scatter_(dim, index, src)`.\n    # `scatter_(dim=-1, index=[num_layers, num_replicas], src=[num_layers, num_replicas])`\n    # This means for each `l` from `0` to `num_layers-1`:\n    # For each `p` from `0` to `num_replicas-1`:\n    #   `target_flat_idx = (phy2log[l, p] * maxlogcnt + phyrank[l, p])`\n    #   `value_to_scatter = p`\n    #   `log2phy.view(num_layers, -1)[l, target_flat_idx] = value_to_scatter`\n    # This correctly places the physical expert index `p` into the slot corresponding to its logical expert `phy2log[l, p]` and rank `phyrank[l, p]`.\n\n    # The scatter logic remains the same, only the inputs `phy2log` and `phyrank` might change\n    # based on the chosen rebalancing strategy.\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n", "language": "python", "parent_id": "d49056ac-cdd4-46bd-947e-9a37f2119d51", "generation": 9, "timestamp": 1751067850.9824715, "iteration_found": 282, "metrics": {"balancedness_score": 0.29830835153138974, "speed_score": 0.08314452438743546, "combined_score": 0.1907264379594126}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 5 lines with 15 lines", "parent_metrics": {"balancedness_score": 0.29830835153138974, "speed_score": 0.08301414105783737, "combined_score": 0.19066124629461356}, "island": 4}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve the Mixture-of-Expert models Expert Parallelism Load Balancer (MoE EPLB) expert rearrangement algorithm.\nThis algorithm will take the load metrics recorded by the vLLM server, and rearrange the experts to balance the load. It can make replicas of some experts to achieve better load balancing.\nYour goal will be two-fold: 1. Improve the algorithm to achieve better load balancing; while 2. Improve the algorithm to be more efficient, i.e. reduce the execution time of the algorithm itself, since perfect load balancing is NP-hard.\nThe current algorithm is implemented in the `rebalance_experts` function. ", "user": "# Current Program Information\n- Current performance metrics: - balancedness_score: 0.2983\n- speed_score: 0.0830\n- combined_score: 0.1907\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: balancedness_score, speed_score, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1125, combined_score: 0.2326\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1126, combined_score: 0.2327\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1143, combined_score: 0.2335\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.2335)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1143), Performs well on combined_score (0.2335)\n\n\n### Program 2 (Score: 0.2327)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1126), Performs well on combined_score (0.2327)\n\n\n### Program 3 (Score: 0.2326)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1125), Performs well on combined_score (0.2326)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.2322)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n\n### Program D2 (Score: 0.2325)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.2335, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 32 lines with 17 lines, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 2 (Score: 0.1920, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 3 (Score: 0.0939, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 23 lines with 236 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 4 (Score: 0.0000, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 3 lines with 6 lines\nChange 2: Replace 32 lines with 35 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 5 (Score: 0.1898, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n\n# Current Program\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    This implementation vectorizes the greedy assignment across layers for improved efficiency.\n\n    Parameters:\n        weight: [X, n], the weight of each item (X is num_layers or num_layers * num_nodes)\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        # If each group is a pack, assignment is straightforward.\n        pack_index = torch.arange(num_groups,\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(num_layers, -1)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort groups by weight in descending order for all layers.\n    # 'weight' is already float and on CPU from rebalance_experts.\n    indices = weight.sort(-1, descending=True).indices  # [num_layers, num_groups]\n\n    # Initialize output tensors.\n    pack_index = torch.empty_like(weight, dtype=torch.int64)\n    rank_in_pack = torch.empty_like(pack_index)\n    \n    # Initialize tensors to hold current pack weights and item counts for all layers.\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Precompute layer indices for vectorized operations.\n    layer_indices = torch.arange(num_layers, device=weight.device)\n\n    # Iterate over sorted group ranks (k) for all layers simultaneously.\n    # In each iteration, we assign the k-th heaviest group (for each layer)\n    # to the least loaded pack (for that layer).\n    for k in range(num_groups):\n        # group_ids_to_assign: [num_layers] - the actual group_id for each layer at rank k\n        group_ids_to_assign = indices[:, k] \n        \n        # weights_to_assign: [num_layers] - the weight of these groups\n        weights_to_assign = weight[layer_indices, group_ids_to_assign]\n\n        # Find the pack with the minimum current weight for each layer.\n        # Mask full packs by setting their weights to infinity.\n        current_pack_weights_masked = torch.where(\n            pack_items_per_layer == groups_per_pack,\n            float('inf'),  # Set full packs to infinity\n            pack_weights_per_layer\n        )\n        \n        # chosen_packs: [num_layers] - the chosen pack index for each layer\n        chosen_packs = torch.argmin(current_pack_weights_masked, dim=1)\n\n        # Assign the group to the chosen pack for each layer.\n        pack_index[layer_indices, group_ids_to_assign] = chosen_packs\n        rank_in_pack[layer_indices, group_ids_to_assign] = \\\n            pack_items_per_layer[layer_indices, chosen_packs]\n\n        # Update pack weights and item counts for the chosen packs across layers.\n        pack_weights_per_layer[layer_indices, chosen_packs] += weights_to_assign\n        pack_items_per_layer[layer_indices, chosen_packs] += 1\n    \n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\n# Helper function for global rebalancing\ndef rebalance_experts_global(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate experts to minimize maximum load across all GPUs globally.\n\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: total number of physical experts after replication\n        num_gpus: total number of GPUs\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        rank: [num_moe_layers, num_physical_experts]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_physical_experts >= num_logical_experts\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    # Step 1: Use replicate_experts to create num_physical_experts across all layers.\n    # This function balances the load distribution of logical experts into physical slots.\n    phy2log_all, rank_all, logcnt_all = replicate_experts(weight, num_physical_experts)\n    # phy2log_all: [num_layers, num_physical_experts] - logical expert ID for each physical expert.\n    # rank_all: [num_layers, num_physical_experts] - replica rank of each physical expert.\n    # logcnt_all: [num_layers, num_logical_experts] - number of replicas for each logical expert.\n\n    # Step 2: Pack these physical experts into num_gpus.\n    # We need a weight tensor for balanced_packing that represents the load of each physical expert.\n    # The load of a physical expert is related to the load of its logical expert.\n    # `replicate_experts` already balanced the load across `num_physical_experts`.\n    # The `weight` passed to `balanced_packing` should reflect the load of the logical expert\n    # that each physical expert represents.\n    physical_expert_load = torch.gather(weight, 1, phy2log_all) # [num_layers, num_physical_experts]\n\n    # Use balanced_packing to assign these physical experts to num_gpus.\n    # `balanced_packing` will pack `num_physical_experts` items (per layer) into `num_gpus` packs.\n    pack_index_gpu, rank_in_gpu_pack = balanced_packing(\n        physical_expert_load, num_gpus\n    ) # pack_index_gpu: [num_layers, num_physical_experts]\n\n    # Step 3: Reorder the physical experts based on the GPU assignment to create the final mapping.\n    # Create a combined sort key to order physical experts first by GPU, then by rank within GPU.\n    combined_sort_key = pack_index_gpu * phy_experts_per_gpu + rank_in_gpu_pack\n    sorted_indices = torch.argsort(combined_sort_key, dim=1)\n\n    # Reorder phy2log_all and rank_all using the sorted indices.\n    final_phy2log = torch.gather(phy2log_all, 1, sorted_indices)\n    final_rank = torch.gather(rank_all, 1, sorted_indices)\n\n    # The logical_count (logcnt_all) is already determined by replicate_experts and is correct.\n    return final_phy2log, final_rank, logcnt_all\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    # Ensure weight is float. Keep on original device (CPU or GPU).\n    # Moving data to CPU can be a significant bottleneck in GPU environments.\n    if weight.dtype != torch.float:\n        weight = weight.float()\n    \n    # Use hierarchical load-balance policy if num_groups > 1 and is divisible by num_nodes.\n    # Otherwise, use the dedicated global load-balance policy.\n    if num_groups > 1 and num_groups % num_nodes == 0:\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # Use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_global(\n            weight, num_replicas, num_gpus)\n            \n    num_redundant_experts = num_replicas - num_logical_experts\n    # Ensure maxlogcnt is at least 1, even if num_redundant_experts is negative (should not happen).\n    maxlogcnt = max(1, num_redundant_experts + 1)\n    \n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    \n    # The scatter operation maps physical experts to their logical expert and rank slots.\n    # The indices into the flattened log2phy are derived from phy2log and phyrank.\n    # The values being scattered are the physical expert indices (0 to num_replicas-1).\n    scatter_indices = phy2log * maxlogcnt + phyrank\n    \n    # Ensure scatter_indices are within bounds.\n    # max_idx = num_logical_experts * maxlogcnt\n    # scatter_indices = torch.remainder(scatter_indices, max_idx) # This might not be correct.\n    # The logic relies on the structure of phy2log and phyrank correctly mapping.\n    # Let's re-check the original scatter logic.\n\n    # The original scatter:\n    # log2phy.view(num_layers, -1).scatter_(-1, phy2log * maxlogcnt + phyrank, torch.arange(num_replicas, dtype=torch.int64, device=log2phy.device).expand(num_layers, -1))\n    # This scatter targets indices based on `phy2log * maxlogcnt + phyrank`.\n    # The number of elements in `phy2log` is `num_layers * num_replicas`.\n    # The number of elements in `log2phy.view(num_layers, -1)` is `num_layers * num_logical_experts * maxlogcnt`.\n    # The shape of `phy2log * maxlogcnt + phyrank` is `[num_layers, num_replicas]`.\n    # The shape of `torch.arange(num_replicas).expand(num_layers, -1)` is `[num_layers, num_replicas]`.\n\n    # The scatter operation is `scatter_(dim, index, src)`.\n    # `scatter_(dim=-1, index=[num_layers, num_replicas], src=[num_layers, num_replicas])`\n    # This means for each `l` from `0` to `num_layers-1`:\n    # For each `p` from `0` to `num_replicas-1`:\n    #   `target_flat_idx = (phy2log[l, p] * maxlogcnt + phyrank[l, p])`\n    #   `value_to_scatter = p`\n    #   `log2phy.view(num_layers, -1)[l, target_flat_idx] = value_to_scatter`\n    # This correctly places the physical expert index `p` into the slot corresponding to its logical expert `phy2log[l, p]` and rank `phyrank[l, p]`.\n\n    # The scatter logic remains the same, only the inputs `phy2log` and `phyrank` might change\n    # based on the chosen rebalancing strategy.\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["The current performance metrics show a regression in `balancedness_score`, `speed_score`, and `combined_score` compared to previous attempts. The primary goal is to improve these metrics.\n\nUpon reviewing the code, especially comparing the current implementation with the description of top-performing programs, it appears that the current version introduces a new `rebalance_experts_global` helper function and a conditional logic in `rebalance_experts` to choose between `rebalance_experts_hierarchical` and `rebalance_experts_global`.\n\nThe `balanced_packing` and `replicate_experts` functions appear to be identical to those in the top-performing programs, suggesting they are not the source of the regression. Therefore, the issue likely lies in the overall strategy or implementation of the new `rebalance_experts_global` path.\n\nThe `rebalance_experts_hierarchical` approach is designed to leverage faster intra-node communication, which typically leads to better load balancing in distributed systems. The `rebalance_experts_global` approach acts as a fallback when the `num_groups` parameter does not allow for clean hierarchical packing (i.e., `num_groups` is not a multiple of `num_nodes`, or `num_groups` is 1).\n\nA critical flaw has been identified in how `rebalance_experts_global` calculates the `physical_expert_load` before passing it to `balanced_packing`.\nCurrently, `physical_expert_load = torch.gather(weight, 1, phy2log_all)` uses the *total original load* of the logical expert. However, when a logical expert is replicated, its total load is distributed among its replicas. Therefore, the effective load handled by each *physical* expert replica is its logical expert's original load divided by the number of replicas it has.\n\n**Problem:** If logical expert E1 has a load of 100 and is replicated twice (P1, P2), the current code assigns a load of 100 to both P1 and P2 for the purpose of packing. This is incorrect. The actual load handled by P1 (and P2) is 100 / 2 = 50. This miscalculation can lead to sub-optimal load balancing across GPUs, directly impacting the `balancedness_score`.\n\n**Proposed Improvement:**\nModify `rebalance_experts_global` to correctly calculate the `physical_expert_load` by dividing the logical expert's original load by its replica count (`logcnt_all`). This will ensure that `balanced_packing` operates on the true effective load of each physical expert.\n\nThis correction addresses a fundamental misrepresentation of load within the `rebalance_experts_global` algorithm, which is expected to significantly improve the `balancedness_score`. While this change primarily targets `balancedness_score`, it might indirectly improve `speed_score` if better load distribution leads to fewer retries or more efficient processing downstream, though the computational overhead of the change itself is minimal.\n\n**Detailed Change:**\n\n<<<<<<< SEARCH\n    # The `weight` passed to `balanced_packing` should reflect the load of the logical expert\n    # that each physical expert represents.\n    physical_expert_load = torch.gather(weight, 1, phy2log_all) # [num_layers, num_physical_experts]\n\n    # Use balanced_packing to assign these physical experts to num_gpus.\n=======\n    # Calculate the effective load for each physical expert.\n    # A physical expert's load is its logical expert's original load divided by the number of replicas\n    # that logical expert has.\n    # 1. Get the replica count for each logical expert, mapped to its corresponding physical expert.\n    logical_expert_replica_counts_for_physical_experts = torch.gather(logcnt_all, 1, phy2log_all)\n\n    # 2. Get the original weight of the logical expert for each physical expert.\n    original_logical_expert_weights_for_physical_experts = torch.gather(weight, 1, phy2log_all)\n\n    # 3. Calculate the effective load per physical expert.\n    # Use torch.max(..., 1) to prevent division by zero, though logcnt should always be >= 1 here.\n    physical_expert_load = original_logical_expert_weights_for_physical_experts / \\\n                           torch.max(logical_expert_replica_counts_for_physical_experts, torch.tensor(1, device=weight.device))\n\n    # Use balanced_packing to assign these physical experts (with their effective loads) to num_gpus.\n>>>>>>> REPLACE"]}}}