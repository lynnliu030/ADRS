{"id": "67f4933c-ce4c-4812-a017-9dcfac7cc9d7", "code": "# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort weights and get indices\n    sorted_weights, sorted_indices = weight.float().sort(-1, descending=True)\n\n    # Store device from input weight\n    device = weight.device\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort weights and get indices\n    sorted_weights, sorted_indices = weight.float().sort(-1, descending=True)\n\n    # Initialize pack_index and rank_in_pack on the correct device\n    pack_index = torch.full((num_layers, num_groups), -1, dtype=torch.int64, device=device)\n    rank_in_pack = torch.full((num_layers, num_groups), -1, dtype=torch.int64, device=device)\n\n    # Initialize pack_loads and pack_counts on the correct device\n    pack_loads = torch.zeros(num_layers, num_packs, dtype=torch.float32, device=device)\n    pack_counts = torch.zeros(num_layers, num_packs, dtype=torch.int64, device=device)\n\n    # Iterate through sorted items and assign them to the least loaded pack\n    for i in range(num_layers):\n        for g in range(num_groups):\n            available_packs = pack_counts[i] < groups_per_pack\n            if not available_packs.any():\n                continue\n\n            current_pack_loads = pack_loads[i][available_packs]\n            current_pack_indices = torch.arange(num_packs, device=device)[available_packs]\n\n            min_load_idx_in_available = torch.argmin(current_pack_loads)\n            chosen_pack_idx = current_pack_indices[min_load_idx_in_available]\n\n            original_group_idx = sorted_indices[i, g]\n\n            pack_index[i, original_group_idx] = chosen_pack_idx\n            rank_in_pack[i, original_group_idx] = pack_counts[i, chosen_pack_idx]\n\n            pack_loads[i, chosen_pack_idx] += weight[i, original_group_idx]\n            pack_counts[i, chosen_pack_idx] += 1\n\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Unified rebalancing logic:\n    # The hierarchical approach is used when num_groups is divisible by num_nodes.\n    # Otherwise, it falls back to a global balancing strategy by setting num_nodes=1 and num_groups=1.\n    # We can simplify this by directly implementing the global strategy when needed,\n    # or by ensuring the hierarchical logic correctly handles num_nodes=1.\n\n    # Determine the effective number of groups and nodes for balancing.\n    # If num_groups is not divisible by num_nodes, we treat it as a single group\n    # across all nodes for the first level of balancing.\n    effective_num_groups = num_groups if num_groups % num_nodes == 0 else 1\n    effective_num_nodes = num_nodes if num_groups % num_nodes == 0 else 1\n\n    # If num_groups % num_nodes != 0, we effectively want to balance across all groups as if they were one large group.\n    # And then balance those groups onto nodes.\n    # This is equivalent to setting num_nodes = 1 and num_groups = 1 in the hierarchical logic.\n    # Let's adjust parameters for a unified call to a simplified hierarchical logic.\n\n    # Simplified approach:\n    # First, balance logical experts across groups within layers.\n    # Then, balance these groups across nodes.\n    # Finally, replicate experts within nodes to balance across GPUs.\n\n    # Step 1: Balance logical experts into groups.\n    # If num_groups == 1, this step is trivial and doesn't change anything.\n    group_size = num_logical_experts // num_groups\n    tokens_per_group_layer = weight.unflatten(-1, (num_groups, group_size)).sum(-1) # [num_layers, num_groups]\n\n    # Pack groups onto nodes.\n    # If num_groups % num_nodes != 0, we effectively want to balance all groups onto a single 'node' conceptually,\n    # and then distribute those across the actual num_nodes.\n    # Let's consider the case where num_groups is not divisible by num_nodes.\n    # The original code uses num_nodes for packing groups. If num_groups < num_nodes, this packing might be problematic.\n    # A more robust approach is to balance groups onto num_nodes, and if num_groups < num_nodes, some nodes get 0 groups.\n    # The current `balanced_packing` assumes num_groups is divisible by num_packs (num_nodes here).\n\n    # Let's redefine the packing strategy:\n    # We want to pack 'num_groups' items (groups) into 'num_nodes' packs.\n    # If num_groups is not divisible by num_nodes, balanced_packing needs to handle it.\n    # The current `balanced_packing` asserts `num_groups % num_packs == 0`.\n    # This means if num_groups % num_nodes != 0, the original code calls `rebalance_experts_hierarchical` with num_nodes=1, num_groups=1.\n    # This is a global balancing strategy.\n\n    # Let's ensure the function call correctly reflects the intended strategy.\n    # If num_groups % num_nodes == 0, use hierarchical.\n    # Otherwise, use global (which is hierarchical with num_nodes=1, num_groups=1).\n\n    if num_groups % num_nodes == 0:\n        # Hierarchical balancing: balance groups to nodes, then nodes to GPUs.\n        # Step 1: Pack groups to nodes.\n        group_pack_index, group_rank_in_pack = balanced_packing(tokens_per_group_layer, num_nodes)\n        # Map logical experts to \"middle\" logical experts (logical experts per node)\n        # log2mlog: [num_layers, num_logical_experts] -> maps logical expert to its middle logical expert\n        mlog2log_map = (((group_pack_index * groups_per_node + group_rank_in_pack) * group_size).unsqueeze(-1) +\n                        torch.arange(group_size, dtype=torch.int64, device=group_pack_index.device)).flatten(-2)\n        # mlog2log_map: [num_layers, num_logical_experts] maps each logical expert to its \"middle\" logical expert ID.\n        # We need the inverse mapping for `weight.gather` later.\n        # This inverse mapping is tricky because multiple logical experts can map to the same middle expert.\n        # The `inverse` function is designed for permutations, which isn't directly applicable here.\n\n        # Let's rethink the mapping and replication.\n        # We have `num_layers` of `weight` ([num_layers, num_logical_experts]).\n        # First, we group logical experts: `tokens_per_group_layer` ([num_layers, num_groups]).\n        # Then, we pack these groups onto `num_nodes`: `group_pack_index` ([num_layers, num_groups]) and `group_rank_in_pack` ([num_layers, num_groups]).\n        # This means `num_groups` are assigned to `num_nodes`. `groups_per_node` groups per node.\n        # So, we have `num_layers * num_nodes` \"node-level\" expert weight tensors.\n        # Each node-level tensor has `num_logical_experts // num_nodes` \"middle\" experts.\n\n        # Let's create the \"middle\" logical experts' weights.\n        # `mlog_weights`: [num_layers * num_nodes, num_logical_experts // num_nodes]\n        # This requires carefully gathering weights.\n        # The `mlog2log` mapping needs to be constructed correctly.\n\n        # `mlog2log_map` assigns each logical expert to a conceptual \"middle\" logical expert.\n        # e.g., if num_logical_experts=8, num_groups=2, group_size=4.\n        # Group 0 contains logical experts 0,1,2,3. Group 1 contains 4,5,6,7.\n        # If num_nodes=2, groups_per_node=1.\n        # Group 0 (log experts 0-3) goes to node 0. Group 1 (log experts 4-7) goes to node 1.\n        # Node 0 has 1 middle expert. Node 1 has 1 middle expert.\n        # Middle expert for node 0 is logical experts 0-3. Middle expert for node 1 is 4-7.\n        # The mapping `mlog2log_map` should map:\n        # 0,1,2,3 -> 0 (on node 0)\n        # 4,5,6,7 -> 0 (on node 1)\n        # The current `mlog2log` calculation seems to be constructing the indices of the middle experts.\n        # `log2mlog` is `[num_layers, num_logical_experts]`.\n        # It maps logical expert ID to its index within the middle expert group.\n        # The `mlog2log` is the inverse of this index mapping.\n\n        # Let's reconstruct `mlog2log` and `tokens_per_mlog` more directly.\n        # For each layer, we have `num_groups` of `group_size`.\n        # `group_pack_index`: [num_layers, num_groups] - node assignment for each group.\n        # `group_rank_in_pack`: [num_layers, num_groups] - index of group within the node's assigned groups.\n\n        # We want to create `num_layers * num_nodes` \"node-level\" expert groups.\n        # Each node-level group corresponds to a set of original logical experts.\n        # The `tokens_per_mlog` should be `[num_layers * num_nodes, num_logical_experts // num_nodes]`.\n\n        # Calculate the logical expert index for each \"middle\" expert.\n        # `mlog_expert_indices`: [num_layers, num_nodes, groups_per_node, group_size]\n        # This is becoming complex. Let's simplify the conceptualization.\n\n        # Simpler approach:\n        # 1. Pack groups to nodes. This determines which logical experts are grouped together on each node.\n        # `group_pack_index`: [num_layers, num_groups] -> node ID for each group.\n        # `group_rank_in_pack`: [num_layers, num_groups] -> rank within node for each group.\n\n        # 2. For each node, create a combined weight tensor for the logical experts assigned to it.\n        #    The number of logical experts per node will be `groups_per_node * group_size`.\n        #    We need to gather the weights of logical experts based on `group_pack_index`.\n\n        # Let's create a mapping from (layer, node) to a contiguous block of logical experts.\n        # `node_logical_expert_map`: [num_layers, num_nodes, groups_per_node * group_size]\n\n        # This requires constructing a permutation for each layer and node.\n        # The original `mlog2log` calculation is:\n        # `((group_pack_index * groups_per_node + group_rank_in_pack) * group_size)`\n        # This forms a base index for each logical expert.\n        # `log2mlog` is this base index + `arange(group_size)`.\n        # `mlog2log` is the inverse of `log2mlog`.\n\n        # Let's directly construct the `tokens_per_mlog` and `mlog2log`.\n        # `tokens_per_mlog` is the weight tensor for experts *within* each node, considering the grouped logical experts.\n        # The number of such \"middle\" experts per node is `groups_per_node * group_size`.\n\n        # For each layer:\n        # `group_pack_index` tells us which node each group belongs to.\n        # `group_rank_in_pack` tells us the order within the node.\n\n        # Let's create `num_layers * num_nodes` tensors, each of size `groups_per_node * group_size`.\n        # `mlog_weights`: [num_layers * num_nodes, groups_per_node * group_size]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, groups_per_node * group_size]\n\n        # Example: num_layers=1, num_logical_experts=8, num_groups=2, group_size=4, num_nodes=2, groups_per_node=1, num_gpus=2, phy_experts_per_gpu=1.\n        # weight: [1, 8]\n        # tokens_per_group_layer: [1, 2] (sum of weights for group 0 and group 1)\n        # balanced_packing(tokens_per_group_layer, num_nodes=2) ->\n        #   group_pack_index: [1, 2] -> [[0, 1]] (group 0 to node 0, group 1 to node 1)\n        #   group_rank_in_pack: [1, 2] -> [[0, 0]] (rank 0 within node for both groups)\n        # groups_per_node = 1, group_size = 4.\n\n        # Middle expert for node 0 is logical experts 0,1,2,3.\n        # Middle expert for node 1 is logical experts 4,5,6,7.\n        # `mlog_to_orig_log_map` for node 0 should be [0,1,2,3].\n        # `mlog_to_orig_log_map` for node 1 should be [4,5,6,7].\n\n        # The original `mlog2log` calculation:\n        # `base_idx = (group_pack_index * groups_per_node + group_rank_in_pack) * group_size`\n        # `base_idx`: [1, 2] -> [[0*1+0, 1*1+0]] * 4 = [[0, 4]]\n        # `log2mlog`: `base_idx.unsqueeze(-1) + torch.arange(group_size)`\n        # `log2mlog`: [[0, 4]].unsqueeze(-1) + [0,1,2,3] -> [[0,1,2,3], [4,5,6,7]]\n        # This `log2mlog` is `[num_layers, num_logical_experts]`. It maps original logical expert ID to its \"middle\" expert index.\n        # The original logic uses `weight.gather(-1, mlog2log)` which is incorrect here.\n        # `weight.gather(-1, mlog2log)` expects `mlog2log` to be `[num_layers, num_middle_experts]` where `num_middle_experts` is the number of columns in `weight`.\n        # The shape should be `[num_layers, num_logical_experts]` if we are gathering from `weight` which is `[num_layers, num_logical_experts]`.\n        # The `mlog2log` calculated here has shape `[num_layers, num_logical_experts]`.\n        # It maps `original_logical_expert_id` to `middle_expert_id`.\n        # We need to gather `weight[layer, original_logical_expert_id]` and group them by `middle_expert_id`.\n\n        # Let's create `mlog_weights` directly.\n        # `mlog_weights`: [num_layers * num_nodes, groups_per_node * group_size]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, groups_per_node * group_size]\n\n        # Vectorized construction of mlog_weights and mlog_to_orig_log_map\n        # This replaces the nested Python loops for better performance.\n\n        # Calculate the new \"middle expert group\" index for each original group.\n        # This new index combines node assignment and rank within the node.\n        # Shape: [num_layers, num_groups]\n        new_group_idx = group_pack_index * groups_per_node + group_rank_in_pack\n\n        # Create a tensor that maps new group index back to original group index.\n        # This is a permutation tensor for groups within each layer.\n        # Shape: [num_layers, num_nodes * groups_per_node] (which is num_groups)\n        group_permutation = torch.empty((num_layers, num_groups),\n                                        dtype=torch.int64,\n                                        device=weight.device)\n        group_permutation.scatter_(1, new_group_idx,\n                                   torch.arange(num_groups, device=weight.device).expand(num_layers, -1))\n\n        # Reshape original weights to [num_layers, num_groups, group_size]\n        weight_reshaped = weight.view(num_layers, num_groups, group_size)\n\n        # Gather weights based on the group_permutation\n        # The .gather() operation reorders the groups according to the permutation.\n        # Shape: [num_layers, num_groups, group_size]\n        mlog_weights_ordered = weight_reshaped.gather(1, group_permutation.unsqueeze(-1).expand(-1, -1, group_size))\n        \n        # Reshape to [num_layers, num_nodes, groups_per_node * group_size]\n        # This is the final mlog_weights.\n        mlog_weights = mlog_weights_ordered.view(num_layers, num_nodes, groups_per_node * group_size)\n\n        # Construct mlog_to_orig_log_map similarly\n        # Original logical expert IDs, reshaped to [num_groups, group_size]\n        orig_log_ids_reshaped = torch.arange(num_logical_experts,\n                                             dtype=torch.int64,\n                                             device=weight.device).view(num_groups, group_size)\n        \n        # Expand to [num_layers, num_groups, group_size] and gather\n        mlog_to_orig_log_map_ordered = orig_log_ids_reshaped.unsqueeze(0).expand(num_layers, -1, -1).gather(\n            1, group_permutation.unsqueeze(-1).expand(-1, -1, group_size))\n        \n        # Reshape to [num_layers, num_nodes, groups_per_node * group_size]\n        mlog_to_orig_log_map = mlog_to_orig_log_map_ordered.view(\n            num_layers, num_nodes, groups_per_node * group_size)\n\n        # Now, `mlog_weights`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n        # And `mlog_to_orig_log_map`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n\n        # Step 2: Replicate experts within each node.\n        # `mlog_weights` are the weights for experts within each node.\n        # `num_physical_experts // num_nodes` is the target number of physical experts per node.\n        # `replicate_experts` takes `[X, num_log]` and returns `phy2log`, `rank`, `logcnt`.\n        # Here, X = num_layers * num_nodes, num_log = num_logical_experts // num_nodes.\n        # So, we need to reshape `mlog_weights` and `mlog_to_orig_log_map`.\n\n        num_middle_experts = num_logical_experts // num_nodes\n        reshaped_mlog_weights = mlog_weights.view(-1, num_middle_experts)\n        reshaped_mlog_to_orig_log_map = mlog_to_orig_log_map.view(-1, num_middle_experts)\n\n        # `phy2mlog`: [num_layers * num_nodes, num_physical_experts // num_nodes]\n        # `phyrank`: [num_layers * num_nodes, num_physical_experts // num_nodes]\n        # `mlogcnt`: [num_layers * num_nodes, num_middle_experts]\n        phy2mlog, phyrank, mlogcnt = replicate_experts(\n            reshaped_mlog_weights, num_physical_experts // num_nodes)\n\n        # Step 3: Pack physical experts to GPUs.\n        # `phy2mlog` maps physical experts (within each node) to middle logical experts.\n        # `phy2mlog` is `[num_layers * num_nodes, num_phy_per_node]`\n        # `phyrank` is `[num_layers * num_nodes, num_phy_per_node]`\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`\n\n        # We need to map these back to global physical expert indices.\n        # The `phy2mlog` refers to the *middle* logical experts.\n        # The `replicate_experts` function returns `phy2log` which is the *original* logical expert ID.\n        # So, `phy2mlog` from `replicate_experts` is actually `[num_layers*num_nodes, num_phy_per_node]`,\n        # where each entry is an index into the *middle* logical experts.\n\n        # We need to map these `phy2mlog` indices back to original logical expert IDs.\n        # `reshaped_mlog_to_orig_log_map`: [num_layers * num_nodes, num_middle_experts]\n        # `final_phy2log_per_node`: [num_layers * num_nodes, num_phy_per_node]\n        final_phy2log_per_node = reshaped_mlog_to_orig_log_map.gather(-1, phy2mlog)\n\n        # `phyrank` is the replica rank for the middle experts.\n        # We need to combine this with `mlogcnt` to get the final `logcnt`.\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`. It tells us how many replicas each middle expert has.\n        # The `phyrank` is the rank of the replica for a *specific* middle expert.\n\n        # The `logcnt` returned by `replicate_experts` is `[X, num_log]`, which is `[num_layers*num_nodes, num_middle_experts]`.\n        # We need to map this back to `[num_layers, num_logical_experts]`.\n\n        # Let's use the `mlogcnt` directly to get the total count of replicas for each logical expert.\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`.\n        # We need to sum/average counts based on the `mlog_to_orig_log_map`.\n\n        # Let's refine the output mapping:\n        # `physical_to_logical_map`: [num_layers, num_replicas]\n        # `logical_to_physical_map`: [num_layers, num_logical_experts, X]\n        # `expert_count`: [num_layers, num_logical_experts]\n\n        # `final_phy2log_per_node`: [num_layers * num_nodes, num_phy_per_node]\n        # This is the logical expert ID for each physical expert *within its node*.\n        # We need to convert this to global physical expert IDs.\n        # Physical experts are grouped by GPU. `num_gpus` total, `num_gpus // num_nodes` per node.\n        # `phy_experts_per_gpu` = `num_physical_experts // num_gpus`.\n        # So, each GPU has `phy_experts_per_gpu` physical experts.\n        # Each Node has `num_gpus // num_nodes` GPUs.\n        # Total physical experts per node = `(num_gpus // num_nodes) * phy_experts_per_gpu`\n        # This should equal `num_physical_experts // num_nodes`.\n\n        # `final_phy2log_per_node`: [num_layers, num_nodes, num_phy_per_node]\n        # `final_rank_per_node`: [num_layers, num_nodes, num_phy_per_node] (this is `phyrank`)\n\n        # We need to flatten this to `[num_layers, num_physical_experts]`.\n        # `num_physical_experts = num_nodes * num_phy_per_node`.\n\n        # `final_phy2log_global`: [num_layers, num_physical_experts]\n        # `final_rank_global`: [num_layers, num_physical_experts]\n\n        # The `mlog_to_orig_log_map` correctly maps middle experts to original logical experts.\n        # `phy2mlog` maps physical experts (within a node) to middle experts.\n        # So, `phy2mlog.gather(-1, final_phy2log_per_node)` gives the logical expert ID.\n        # This is what `final_phy2log_per_node` already represents.\n\n        # Let's rename `final_phy2log_per_node` to `phy2log_per_node` for clarity.\n        phy2log_per_node = final_phy2log_per_node\n        rank_per_node = phyrank # [num_layers * num_nodes, num_phy_per_node]\n\n        # Flatten these to get global mappings for physical experts.\n        phy2log_global = phy2log_per_node.view(num_layers, -1)\n        rank_global = rank_per_node.view(num_layers, -1)\n\n        # Now, let's construct the `logcnt` (expert_count).\n        # `mlogcnt`: [num_layers * num_nodes, num_middle_experts]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, num_middle_experts]\n\n        # We need to aggregate `mlogcnt` based on `mlog_to_orig_log_map`.\n        # For each logical expert, its total count is the sum of counts of its constituent middle experts.\n        # This aggregation needs to be done carefully.\n\n        # Let's create a tensor for `logcnt`: [num_layers, num_logical_experts] initialized to zeros.\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n\n        # The expert_count aggregation:\n        # mlog_to_orig_log_map: [num_layers, num_nodes, num_middle_experts]\n        # mlogcnt: [num_layers * num_nodes, num_middle_experts]\n        # Flatten mlog_to_orig_log_map and mlogcnt to [num_layers, N] for scatter_add_\n        mlog_to_orig_log_map_flat = mlog_to_orig_log_map.view(num_layers, -1)\n        mlogcnt_reshaped = mlogcnt.view(num_layers, -1)\n\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n        # Use scatter_add_ to efficiently aggregate counts for each logical expert across layers\n        expert_count.scatter_add_(1, mlog_to_orig_log_map_flat, mlogcnt_reshaped)\n\n        # The logical_to_physical_map construction is handled by the outer rebalance_experts function.\n        # So, this function only needs to return the global mappings and counts.\n        return phy2log_global, rank_global, expert_count\n\n    else:\n        # Global balancing strategy: treat all experts as one group, balance across num_gpus.\n        # This is equivalent to calling the hierarchical function with num_nodes=1, num_groups=1.\n        # The `rebalance_experts` function will call this branch if `num_groups % num_nodes != 0`.\n\n        # Step 1: Pack logical experts to GPUs.\n        # `balanced_packing` will distribute the logical experts across `num_gpus`.\n\n        # We need to adapt `balanced_packing` to work with `num_packs = num_gpus`.\n        # The `groups_per_pack` in `balanced_packing` refers to items per pack.\n        # Here, items are logical experts. So, `groups_per_pack` = `num_logical_experts // num_gpus`.\n        # `num_packs` = `num_gpus`.\n\n        pack_index_gpu, rank_in_gpu = balanced_packing(weight, num_gpus)\n\n        # `pack_index_gpu`: [num_layers, num_logical_experts] -> GPU ID for each logical expert.\n        # `rank_in_gpu`: [num_layers, num_logical_experts] -> rank within GPU for each logical expert.\n\n        # Step 2: Replicate experts within GPUs.\n        num_logical_experts_per_gpu = num_logical_experts // num_gpus\n\n        # For each GPU, we have a set of logical experts.\n        # We need to gather weights for logical experts assigned to the same GPU.\n        # `gpu_logical_weights`: [num_gpus, num_logical_experts_per_gpu, num_layers] (transposed for replicate_experts)\n        # `gpu_logical_to_orig_map`: [num_gpus, num_logical_experts_per_gpu]\n\n        # Vectorized construction of gpu_logical_weights and gpu_logical_to_orig_map\n        # This replaces the nested Python loops for better performance.\n\n        num_logical_experts_per_gpu = num_logical_experts // num_gpus\n\n        # Calculate the new \"middle expert group\" index for each original logical expert.\n        # This new index combines GPU assignment and rank within the GPU.\n        # Shape: [num_layers, num_logical_experts]\n        new_logical_expert_idx_on_gpu = pack_index_gpu * num_logical_experts_per_gpu + rank_in_gpu\n\n        # Create a tensor that maps new logical expert index back to original logical expert index.\n        # This is a permutation tensor for logical experts within each layer.\n        # Shape: [num_layers, num_gpus * num_logical_experts_per_gpu] (which is num_logical_experts)\n        logical_expert_permutation = torch.empty((num_layers, num_logical_experts),\n                                                 dtype=torch.int64,\n                                                 device=weight.device)\n        logical_expert_permutation.scatter_(1, new_logical_expert_idx_on_gpu,\n                                            torch.arange(num_logical_experts, device=weight.device).expand(num_layers, -1))\n\n        # Gather weights based on the logical_expert_permutation\n        # Shape: [num_layers, num_logical_experts]\n        gpu_logical_weights_ordered = weight.gather(1, logical_expert_permutation)\n        \n        # Reshape to [num_gpus, num_layers, num_logical_experts_per_gpu] for subsequent processing\n        # and ensure contiguous memory layout.\n        gpu_logical_weights = gpu_logical_weights_ordered.view(\n            num_layers, num_gpus, num_logical_experts_per_gpu).permute(1, 0, 2).contiguous()\n\n        # Construct gpu_logical_to_orig_map similarly\n        # Original logical expert IDs, expanded to [num_layers, num_logical_experts]\n        orig_log_ids_expanded = torch.arange(num_logical_experts,\n                                             dtype=torch.int64,\n                                             device=weight.device).unsqueeze(0).expand(num_layers, -1)\n        \n        # Gather original logical expert IDs based on the permutation\n        gpu_logical_to_orig_map_ordered = orig_log_ids_expanded.gather(1, logical_expert_permutation)\n        \n        # Reshape to [num_gpus * num_layers, num_logical_experts_per_gpu] for replicate_experts input\n        gpu_logical_to_orig_map = gpu_logical_to_orig_map_ordered.view(\n            num_layers, num_gpus, num_logical_experts_per_gpu).permute(1, 0, 2).contiguous().view(\n                -1, num_logical_experts_per_gpu)\n\n        # Reshape for `replicate_experts`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        reshaped_gpu_logical_weights = gpu_logical_weights.view(-1, num_logical_experts_per_gpu)\n        reshaped_gpu_logical_to_orig_map = gpu_logical_to_orig_map.view(-1, num_logical_experts_per_gpu)\n\n        # `replicate_experts` is called with `num_phy = num_physical_experts // num_gpus` (target physical experts per GPU).\n        # `X` becomes `num_gpus * num_layers`. `num_log` becomes `num_logical_experts_per_gpu`.\n        # `phy2log_gpu`: [num_gpus * num_layers, num_phy_per_gpu]\n        # `phyrank_gpu`: [num_gpus * num_layers, num_phy_per_gpu]\n        # `gpu_logcnt`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        phy2log_gpu, phyrank_gpu, gpu_logcnt = replicate_experts(\n            reshaped_gpu_logical_weights, num_physical_experts // num_gpus)\n\n        # Map `phy2log_gpu` back to original logical expert IDs using `reshaped_gpu_logical_to_orig_map`.\n        # `phy2log_global`: [num_gpus * num_layers, num_phy_per_gpu]\n        phy2log_global = reshaped_gpu_logical_to_orig_map.gather(-1, phy2log_gpu)\n\n        # Flatten to get the final `physical_to_logical_map` and `rank`.\n        # The `rank_global` is `phyrank_gpu`.\n        rank_global = phyrank_gpu\n\n        # Construct `expert_count` ([num_layers, num_logical_experts]).\n        # `gpu_logcnt`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n\n        for gpu_layer_idx in range(num_gpus * num_layers):\n            layer_idx = gpu_layer_idx // num_gpus\n            # Get the original logical expert indices for this gpu_layer combination\n            orig_log_indices = reshaped_gpu_logical_to_orig_map[gpu_layer_idx] # [num_logical_experts_per_gpu]\n            counts_for_orig_logs = gpu_logcnt[gpu_layer_idx] # [num_logical_experts_per_gpu]\n\n            # Add these counts to the correct positions in the expert_count tensor\n            expert_count[layer_idx].index_add_(0, orig_log_indices, counts_for_orig_logs)\n\n        # Construct `logical_to_physical_map` ([num_layers, num_logical_experts, X])\n        max_replicas_per_expert = (num_physical_experts + num_logical_experts - 1) // num_logical_experts\n\n        logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert), -1, dtype=torch.int64, device=weight.device)\n\n        # Need to map `phy2log_global` and `rank_global` to the correct positions.\n        # `phy2log_global` are indices into the *original* logical experts.\n        # `rank_global` is the replica rank for that original logical expert.\n\n        # Reshape to get mappings per layer and then iterate through physical experts.\n        phy2log_global_reshaped = phy2log_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous()\n        rank_global_reshaped = rank_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous()\n\n        # Iterate through all physical experts\n        for layer_idx in range(num_layers):\n            for gpu_idx in range(num_gpus):\n                for phys_idx_in_gpu in range(num_physical_experts // num_gpus):\n                    # Get the global physical expert index\n                    global_phys_idx = gpu_idx * (num_physical_experts // num_gpus) + phys_idx_in_gpu\n\n                    log_expert_id = phy2log_global_reshaped[layer_idx, gpu_idx, phys_idx_in_gpu]\n                    replica_rank = rank_global_reshaped[layer_idx, gpu_idx, phys_idx_in_gpu]\n\n                    if log_expert_id >= 0 and log_expert_id < num_logical_experts:\n                        if replica_rank < max_replicas_per_expert:\n                            logical_to_physical_map[layer_idx, log_expert_id, replica_rank] = global_phys_idx\n                        else:\n                            pass # Handle error or log\n\n        # The `phy2log_global` needs to be reshaped to `[num_layers, num_replicas]`\n        # where `num_replicas` is `num_physical_experts`.\n        # `phy2log_global` is currently `[num_gpus * num_layers, num_phy_per_gpu]`.\n        # We need to combine these.\n        # The order of physical experts should be consistent.\n        # Let's reconstruct `phy2log_global` to be `[num_layers, num_physical_experts]`.\n\n        final_phy2log_global = torch.empty((num_layers, num_physical_experts), dtype=torch.int64, device=weight.device)\n        current_phys_idx = 0\n        for gpu_idx in range(num_gpus):\n            for layer_idx in range(num_layers):\n                # Calculate the range of physical experts for this GPU and layer combination\n                start_phys_idx = layer_idx * num_gpus * (num_physical_experts // num_gpus) + gpu_idx * (num_physical_experts // num_gpus)\n                end_phys_idx = start_phys_idx + (num_physical_experts // num_gpus)\n\n                # The `phy2log_global` calculated earlier is `[num_gpus * num_layers, num_phy_per_gpu]`\n                # We need to map it correctly.\n                # `phy2log_gpu` was `[num_gpus * num_layers, num_phy_per_gpu]`\n                # `reshaped_gpu_logical_to_orig_map` was `[num_gpus * num_layers, num_phy_per_gpu]`\n                # `phy2log_global = reshaped_gpu_logical_to_orig_map.gather(-1, phy2log_gpu)`\n\n                # The indices for `phy2log_global` should be:\n                # For layer 0, GPU 0: phys experts 0 to num_phy_per_gpu - 1\n                # For layer 0, GPU 1: phys experts num_phy_per_gpu to 2*num_phy_per_gpu - 1\n                # ...\n                # For layer 1, GPU 0: phys experts num_gpus * num_phy_per_gpu to (num_gpus+1)*num_phy_per_gpu - 1\n\n                # Let's re-index the `phy2log_global` and `rank_global` to match `[num_layers, num_physical_experts]`\n                # The current `phy2log_global` is effectively `[num_layers * num_gpus, num_phy_per_gpu]`\n                # We need to rearrange it.\n\n                # Example: num_layers=2, num_gpus=2, num_phy_per_gpu=2. num_physical_experts=4.\n                # phy2log_global:\n                # [layer0_gpu0_phys0, layer0_gpu0_phys1,\n                #  layer0_gpu1_phys0, layer0_gpu1_phys1,\n                #  layer1_gpu0_phys0, layer1_gpu0_phys1,\n                #  layer1_gpu1_phys0, layer1_gpu1_phys1]\n                # Reshaped to [num_gpus*num_layers, num_phy_per_gpu] -> [4, 2]\n                # We want [num_layers, num_physical_experts] -> [2, 4]\n\n                # The current `phy2log_global` is `[num_gpus * num_layers, num_phy_per_gpu]`.\n                # Permute and reshape:\n                # `view(num_gpus, num_layers, -1)` gives `[num_gpus, num_layers, num_phy_per_gpu]`\n                # `permute(1, 0, 2)` gives `[num_layers, num_gpus, num_phy_per_gpu]`\n                # `contiguous().view(num_layers, -1)` gives `[num_layers, num_gpus * num_phy_per_gpu]` = `[num_layers, num_physical_experts]`\n\n                final_phy2log_global = phy2log_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous().view(num_layers, num_physical_experts)\n                final_rank_global = rank_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous().view(num_layers, num_physical_experts)\n\n        # Return the results for global balancing.\n        return final_phy2log_global, logical_to_physical_map, expert_count\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float().cpu()\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, rank_global, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, rank_global, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    # `phy2log`: [num_layers, num_replicas] -> maps physical expert index to logical expert ID.\n    # `rank_global`: [num_layers, num_replicas] -> maps physical expert index to its replica rank.\n    # `logcnt`: [num_layers, num_logical_experts] -> number of replicas for each logical expert.\n\n    # We need to construct `logical_to_physical_map`: [layers, num_logical_experts, X]\n    # where X is the maximum number of replicas for any logical expert.\n\n    device = weight.device \n\n    # Correctly calculate max_replicas_per_expert from logcnt.\n    max_replicas_per_expert = torch.max(logcnt).item() if logcnt.numel() > 0 else 0\n\n    # Handle edge case where logcnt might be all zeros or empty.\n    if num_logical_experts > 0 and num_replicas > 0 and max_replicas_per_expert == 0:\n        max_replicas_per_expert = 1\n    \n    # Initialize the map with -1.\n    logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert),\n                                         -1, dtype=torch.int64, device=device)\n\n    # Create physical expert IDs corresponding to the indices of phy2log/rank.\n    physical_expert_ids = torch.arange(num_replicas, dtype=torch.int64, device=device)\n    \n    # Expand physical_expert_ids to match the shape of phy2log and rank for broadcasting assignment.\n    physical_expert_ids_expanded = physical_expert_ids.unsqueeze(0).expand(num_layers, num_replicas)\n\n    # Create layer indices for advanced indexing.\n    layer_indices = torch.arange(num_layers, dtype=torch.int64, device=device).unsqueeze(1)\n    \n    # Flatten indices and values for masked assignment.\n    flat_layer_indices = layer_indices.flatten()\n    flat_phy2log = phy2log.flatten()\n    flat_rank = rank_global.flatten() # Use rank_global here\n    flat_physical_ids = physical_expert_ids_expanded.flatten()\n    \n    # Create a mask for valid assignments.\n    valid_mask_flat = (flat_phy2log >= 0) & (flat_phy2log < num_logical_experts) & \\\n                      (flat_rank >= 0) & (flat_rank < max_replicas_per_expert)\n    \n    # Perform the assignment only for valid indices.\n    if valid_mask_flat.any():\n        logical_to_physical_map[\n            flat_layer_indices[valid_mask_flat],\n            flat_phy2log[valid_mask_flat],\n            flat_rank[valid_mask_flat]\n        ] = flat_physical_ids[valid_mask_flat]\n\n    return phy2log, logical_to_physical_map, logcnt\n\n    # The `logcnt` (expert_count) is already computed by the balancing functions.\n    # `phy2log` is the first return value.\n    return phy2log, logical_to_physical_map, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n", "language": "python", "parent_id": "03d737f9-6f70-4554-b7f6-42c041b021c7", "generation": 7, "timestamp": 1751052108.4321167, "iteration_found": 37, "metrics": {"balancedness_score": 0.0, "speed_score": 0.0, "combined_score": 0.0, "error": "The shape of the mask [16704] at index 0 does not match the shape of the indexed tensor [58] at index 0"}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 70 lines with 56 lines\nChange 2: Replace 295 lines with 71 lines\nChange 3: Replace 70 lines with 64 lines", "parent_metrics": {"balancedness_score": 0.0, "speed_score": 0.0, "combined_score": 0.0, "error": "name 'rank' is not defined"}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve the Mixture-of-Expert models Expert Parallelism Load Balancer (MoE EPLB) expert rearrangement algorithm.\nThis algorithm will take the load metrics recorded by the vLLM server, and rearrange the experts to balance the load. It can make replicas of some experts to achieve better load balancing.\nYour goal will be two-fold: 1. Improve the algorithm to achieve better load balancing; while 2. Improve the algorithm to be more efficient, i.e. reduce the execution time of the algorithm itself, since perfect load balancing is NP-hard.\nThe current algorithm is implemented in the `rebalance_experts` function. ", "user": "# Current Program Information\n- Current performance metrics: - balancedness_score: 0.0000\n- speed_score: 0.0000\n- combined_score: 0.0000\n- error: name 'rank' is not defined\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: balancedness_score, speed_score, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0350, combined_score: 0.1667\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0405, combined_score: 0.1694\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0761, combined_score: 0.1872\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.1872)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0761), Performs well on combined_score (0.1872)\n\n\n### Program 2 (Score: 0.1694)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0405), Performs well on combined_score (0.1694)\n\n\n### Program 3 (Score: 0.1667)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0350), Performs well on combined_score (0.1667)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.1645)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n\n### Program D2 (Score: 0.1661)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.1872, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 2 (Score: 0.0872, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 87 lines with 89 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 3 (Score: 0.0872, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 87 lines with 89 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 4 (Score: 0.0872, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 87 lines with 89 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 5 (Score: 0.1644, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 3 lines with 18 lines\nChange 2: Replace 42 lines with 36 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n\n# Current Program\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort weights and get indices\n    sorted_weights, sorted_indices = weight.float().sort(-1, descending=True)\n\n    # Store device from input weight\n    device = weight.device\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort weights and get indices\n    sorted_weights, sorted_indices = weight.float().sort(-1, descending=True)\n\n    # Initialize pack_index and rank_in_pack on the correct device\n    pack_index = torch.full((num_layers, num_groups), -1, dtype=torch.int64, device=device)\n    rank_in_pack = torch.full((num_layers, num_groups), -1, dtype=torch.int64, device=device)\n\n    # Initialize pack_loads and pack_counts on the correct device\n    pack_loads = torch.zeros(num_layers, num_packs, dtype=torch.float32, device=device)\n    pack_counts = torch.zeros(num_layers, num_packs, dtype=torch.int64, device=device)\n\n    # Iterate through sorted items and assign them to the least loaded pack\n    for i in range(num_layers):\n        for g in range(num_groups):\n            available_packs = pack_counts[i] < groups_per_pack\n            if not available_packs.any():\n                continue\n\n            current_pack_loads = pack_loads[i][available_packs]\n            current_pack_indices = torch.arange(num_packs, device=device)[available_packs]\n\n            min_load_idx_in_available = torch.argmin(current_pack_loads)\n            chosen_pack_idx = current_pack_indices[min_load_idx_in_available]\n\n            original_group_idx = sorted_indices[i, g]\n\n            pack_index[i, original_group_idx] = chosen_pack_idx\n            rank_in_pack[i, original_group_idx] = pack_counts[i, chosen_pack_idx]\n\n            pack_loads[i, chosen_pack_idx] += weight[i, original_group_idx]\n            pack_counts[i, chosen_pack_idx] += 1\n\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Unified rebalancing logic:\n    # The hierarchical approach is used when num_groups is divisible by num_nodes.\n    # Otherwise, it falls back to a global balancing strategy by setting num_nodes=1 and num_groups=1.\n    # We can simplify this by directly implementing the global strategy when needed,\n    # or by ensuring the hierarchical logic correctly handles num_nodes=1.\n\n    # Determine the effective number of groups and nodes for balancing.\n    # If num_groups is not divisible by num_nodes, we treat it as a single group\n    # across all nodes for the first level of balancing.\n    effective_num_groups = num_groups if num_groups % num_nodes == 0 else 1\n    effective_num_nodes = num_nodes if num_groups % num_nodes == 0 else 1\n\n    # If num_groups % num_nodes != 0, we effectively want to balance across all groups as if they were one large group.\n    # And then balance those groups onto nodes.\n    # This is equivalent to setting num_nodes = 1 and num_groups = 1 in the hierarchical logic.\n    # Let's adjust parameters for a unified call to a simplified hierarchical logic.\n\n    # Simplified approach:\n    # First, balance logical experts across groups within layers.\n    # Then, balance these groups across nodes.\n    # Finally, replicate experts within nodes to balance across GPUs.\n\n    # Step 1: Balance logical experts into groups.\n    # If num_groups == 1, this step is trivial and doesn't change anything.\n    group_size = num_logical_experts // num_groups\n    tokens_per_group_layer = weight.unflatten(-1, (num_groups, group_size)).sum(-1) # [num_layers, num_groups]\n\n    # Pack groups onto nodes.\n    # If num_groups % num_nodes != 0, we effectively want to balance all groups onto a single 'node' conceptually,\n    # and then distribute those across the actual num_nodes.\n    # Let's consider the case where num_groups is not divisible by num_nodes.\n    # The original code uses num_nodes for packing groups. If num_groups < num_nodes, this packing might be problematic.\n    # A more robust approach is to balance groups onto num_nodes, and if num_groups < num_nodes, some nodes get 0 groups.\n    # The current `balanced_packing` assumes num_groups is divisible by num_packs (num_nodes here).\n\n    # Let's redefine the packing strategy:\n    # We want to pack 'num_groups' items (groups) into 'num_nodes' packs.\n    # If num_groups is not divisible by num_nodes, balanced_packing needs to handle it.\n    # The current `balanced_packing` asserts `num_groups % num_packs == 0`.\n    # This means if num_groups % num_nodes != 0, the original code calls `rebalance_experts_hierarchical` with num_nodes=1, num_groups=1.\n    # This is a global balancing strategy.\n\n    # Let's ensure the function call correctly reflects the intended strategy.\n    # If num_groups % num_nodes == 0, use hierarchical.\n    # Otherwise, use global (which is hierarchical with num_nodes=1, num_groups=1).\n\n    if num_groups % num_nodes == 0:\n        # Hierarchical balancing: balance groups to nodes, then nodes to GPUs.\n        # Step 1: Pack groups to nodes.\n        group_pack_index, group_rank_in_pack = balanced_packing(tokens_per_group_layer, num_nodes)\n        # Map logical experts to \"middle\" logical experts (logical experts per node)\n        # log2mlog: [num_layers, num_logical_experts] -> maps logical expert to its middle logical expert\n        mlog2log_map = (((group_pack_index * groups_per_node + group_rank_in_pack) * group_size).unsqueeze(-1) +\n                        torch.arange(group_size, dtype=torch.int64, device=group_pack_index.device)).flatten(-2)\n        # mlog2log_map: [num_layers, num_logical_experts] maps each logical expert to its \"middle\" logical expert ID.\n        # We need the inverse mapping for `weight.gather` later.\n        # This inverse mapping is tricky because multiple logical experts can map to the same middle expert.\n        # The `inverse` function is designed for permutations, which isn't directly applicable here.\n\n        # Let's rethink the mapping and replication.\n        # We have `num_layers` of `weight` ([num_layers, num_logical_experts]).\n        # First, we group logical experts: `tokens_per_group_layer` ([num_layers, num_groups]).\n        # Then, we pack these groups onto `num_nodes`: `group_pack_index` ([num_layers, num_groups]) and `group_rank_in_pack` ([num_layers, num_groups]).\n        # This means `num_groups` are assigned to `num_nodes`. `groups_per_node` groups per node.\n        # So, we have `num_layers * num_nodes` \"node-level\" expert weight tensors.\n        # Each node-level tensor has `num_logical_experts // num_nodes` \"middle\" experts.\n\n        # Let's create the \"middle\" logical experts' weights.\n        # `mlog_weights`: [num_layers * num_nodes, num_logical_experts // num_nodes]\n        # This requires carefully gathering weights.\n        # The `mlog2log` mapping needs to be constructed correctly.\n\n        # `mlog2log_map` assigns each logical expert to a conceptual \"middle\" logical expert.\n        # e.g., if num_logical_experts=8, num_groups=2, group_size=4.\n        # Group 0 contains logical experts 0,1,2,3. Group 1 contains 4,5,6,7.\n        # If num_nodes=2, groups_per_node=1.\n        # Group 0 (log experts 0-3) goes to node 0. Group 1 (log experts 4-7) goes to node 1.\n        # Node 0 has 1 middle expert. Node 1 has 1 middle expert.\n        # Middle expert for node 0 is logical experts 0-3. Middle expert for node 1 is 4-7.\n        # The mapping `mlog2log_map` should map:\n        # 0,1,2,3 -> 0 (on node 0)\n        # 4,5,6,7 -> 0 (on node 1)\n        # The current `mlog2log` calculation seems to be constructing the indices of the middle experts.\n        # `log2mlog` is `[num_layers, num_logical_experts]`.\n        # It maps logical expert ID to its index within the middle expert group.\n        # The `mlog2log` is the inverse of this index mapping.\n\n        # Let's reconstruct `mlog2log` and `tokens_per_mlog` more directly.\n        # For each layer, we have `num_groups` of `group_size`.\n        # `group_pack_index`: [num_layers, num_groups] - node assignment for each group.\n        # `group_rank_in_pack`: [num_layers, num_groups] - index of group within the node's assigned groups.\n\n        # We want to create `num_layers * num_nodes` \"node-level\" expert groups.\n        # Each node-level group corresponds to a set of original logical experts.\n        # The `tokens_per_mlog` should be `[num_layers * num_nodes, num_logical_experts // num_nodes]`.\n\n        # Calculate the logical expert index for each \"middle\" expert.\n        # `mlog_expert_indices`: [num_layers, num_nodes, groups_per_node, group_size]\n        # This is becoming complex. Let's simplify the conceptualization.\n\n        # Simpler approach:\n        # 1. Pack groups to nodes. This determines which logical experts are grouped together on each node.\n        # `group_pack_index`: [num_layers, num_groups] -> node ID for each group.\n        # `group_rank_in_pack`: [num_layers, num_groups] -> rank within node for each group.\n\n        # 2. For each node, create a combined weight tensor for the logical experts assigned to it.\n        #    The number of logical experts per node will be `groups_per_node * group_size`.\n        #    We need to gather the weights of logical experts based on `group_pack_index`.\n\n        # Let's create a mapping from (layer, node) to a contiguous block of logical experts.\n        # `node_logical_expert_map`: [num_layers, num_nodes, groups_per_node * group_size]\n\n        # This requires constructing a permutation for each layer and node.\n        # The original `mlog2log` calculation is:\n        # `((group_pack_index * groups_per_node + group_rank_in_pack) * group_size)`\n        # This forms a base index for each logical expert.\n        # `log2mlog` is this base index + `arange(group_size)`.\n        # `mlog2log` is the inverse of `log2mlog`.\n\n        # Let's directly construct the `tokens_per_mlog` and `mlog2log`.\n        # `tokens_per_mlog` is the weight tensor for experts *within* each node, considering the grouped logical experts.\n        # The number of such \"middle\" experts per node is `groups_per_node * group_size`.\n\n        # For each layer:\n        # `group_pack_index` tells us which node each group belongs to.\n        # `group_rank_in_pack` tells us the order within the node.\n\n        # Let's create `num_layers * num_nodes` tensors, each of size `groups_per_node * group_size`.\n        # `mlog_weights`: [num_layers * num_nodes, groups_per_node * group_size]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, groups_per_node * group_size]\n\n        # Example: num_layers=1, num_logical_experts=8, num_groups=2, group_size=4, num_nodes=2, groups_per_node=1, num_gpus=2, phy_experts_per_gpu=1.\n        # weight: [1, 8]\n        # tokens_per_group_layer: [1, 2] (sum of weights for group 0 and group 1)\n        # balanced_packing(tokens_per_group_layer, num_nodes=2) ->\n        #   group_pack_index: [1, 2] -> [[0, 1]] (group 0 to node 0, group 1 to node 1)\n        #   group_rank_in_pack: [1, 2] -> [[0, 0]] (rank 0 within node for both groups)\n        # groups_per_node = 1, group_size = 4.\n\n        # Middle expert for node 0 is logical experts 0,1,2,3.\n        # Middle expert for node 1 is logical experts 4,5,6,7.\n        # `mlog_to_orig_log_map` for node 0 should be [0,1,2,3].\n        # `mlog_to_orig_log_map` for node 1 should be [4,5,6,7].\n\n        # The original `mlog2log` calculation:\n        # `base_idx = (group_pack_index * groups_per_node + group_rank_in_pack) * group_size`\n        # `base_idx`: [1, 2] -> [[0*1+0, 1*1+0]] * 4 = [[0, 4]]\n        # `log2mlog`: `base_idx.unsqueeze(-1) + torch.arange(group_size)`\n        # `log2mlog`: [[0, 4]].unsqueeze(-1) + [0,1,2,3] -> [[0,1,2,3], [4,5,6,7]]\n        # This `log2mlog` is `[num_layers, num_logical_experts]`. It maps original logical expert ID to its \"middle\" expert index.\n        # The original logic uses `weight.gather(-1, mlog2log)` which is incorrect here.\n        # `weight.gather(-1, mlog2log)` expects `mlog2log` to be `[num_layers, num_middle_experts]` where `num_middle_experts` is the number of columns in `weight`.\n        # The shape should be `[num_layers, num_logical_experts]` if we are gathering from `weight` which is `[num_layers, num_logical_experts]`.\n        # The `mlog2log` calculated here has shape `[num_layers, num_logical_experts]`.\n        # It maps `original_logical_expert_id` to `middle_expert_id`.\n        # We need to gather `weight[layer, original_logical_expert_id]` and group them by `middle_expert_id`.\n\n        # Let's create `mlog_weights` directly.\n        # `mlog_weights`: [num_layers * num_nodes, groups_per_node * group_size]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, groups_per_node * group_size]\n\n        # Vectorized construction of mlog_weights and mlog_to_orig_log_map\n        # This replaces the nested Python loops for better performance.\n\n        # Calculate the new \"middle expert group\" index for each original group.\n        # This new index combines node assignment and rank within the node.\n        # Shape: [num_layers, num_groups]\n        new_group_idx = group_pack_index * groups_per_node + group_rank_in_pack\n\n        # Create a tensor that maps new group index back to original group index.\n        # This is a permutation tensor for groups within each layer.\n        # Shape: [num_layers, num_nodes * groups_per_node] (which is num_groups)\n        group_permutation = torch.empty((num_layers, num_groups),\n                                        dtype=torch.int64,\n                                        device=weight.device)\n        group_permutation.scatter_(1, new_group_idx,\n                                   torch.arange(num_groups, device=weight.device).expand(num_layers, -1))\n\n        # Reshape original weights to [num_layers, num_groups, group_size]\n        weight_reshaped = weight.view(num_layers, num_groups, group_size)\n\n        # Gather weights based on the group_permutation\n        # The .gather() operation reorders the groups according to the permutation.\n        # Shape: [num_layers, num_groups, group_size]\n        mlog_weights_ordered = weight_reshaped.gather(1, group_permutation.unsqueeze(-1).expand(-1, -1, group_size))\n        \n        # Reshape to [num_layers, num_nodes, groups_per_node * group_size]\n        # This is the final mlog_weights.\n        mlog_weights = mlog_weights_ordered.view(num_layers, num_nodes, groups_per_node * group_size)\n\n        # Construct mlog_to_orig_log_map similarly\n        # Original logical expert IDs, reshaped to [num_groups, group_size]\n        orig_log_ids_reshaped = torch.arange(num_logical_experts,\n                                             dtype=torch.int64,\n                                             device=weight.device).view(num_groups, group_size)\n        \n        # Expand to [num_layers, num_groups, group_size] and gather\n        mlog_to_orig_log_map_ordered = orig_log_ids_reshaped.unsqueeze(0).expand(num_layers, -1, -1).gather(\n            1, group_permutation.unsqueeze(-1).expand(-1, -1, group_size))\n        \n        # Reshape to [num_layers, num_nodes, groups_per_node * group_size]\n        mlog_to_orig_log_map = mlog_to_orig_log_map_ordered.view(\n            num_layers, num_nodes, groups_per_node * group_size)\n\n        # Now, `mlog_weights`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n        # And `mlog_to_orig_log_map`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n\n        # Step 2: Replicate experts within each node.\n        # `mlog_weights` are the weights for experts within each node.\n        # `num_physical_experts // num_nodes` is the target number of physical experts per node.\n        # `replicate_experts` takes `[X, num_log]` and returns `phy2log`, `rank`, `logcnt`.\n        # Here, X = num_layers * num_nodes, num_log = num_logical_experts // num_nodes.\n        # So, we need to reshape `mlog_weights` and `mlog_to_orig_log_map`.\n\n        num_middle_experts = num_logical_experts // num_nodes\n        reshaped_mlog_weights = mlog_weights.view(-1, num_middle_experts)\n        reshaped_mlog_to_orig_log_map = mlog_to_orig_log_map.view(-1, num_middle_experts)\n\n        # `phy2mlog`: [num_layers * num_nodes, num_physical_experts // num_nodes]\n        # `phyrank`: [num_layers * num_nodes, num_physical_experts // num_nodes]\n        # `mlogcnt`: [num_layers * num_nodes, num_middle_experts]\n        phy2mlog, phyrank, mlogcnt = replicate_experts(\n            reshaped_mlog_weights, num_physical_experts // num_nodes)\n\n        # Step 3: Pack physical experts to GPUs.\n        # `phy2mlog` maps physical experts (within each node) to middle logical experts.\n        # `phy2mlog` is `[num_layers * num_nodes, num_phy_per_node]`\n        # `phyrank` is `[num_layers * num_nodes, num_phy_per_node]`\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`\n\n        # We need to map these back to global physical expert indices.\n        # The `phy2mlog` refers to the *middle* logical experts.\n        # The `replicate_experts` function returns `phy2log` which is the *original* logical expert ID.\n        # So, `phy2mlog` from `replicate_experts` is actually `[num_layers*num_nodes, num_phy_per_node]`,\n        # where each entry is an index into the *middle* logical experts.\n\n        # We need to map these `phy2mlog` indices back to original logical expert IDs.\n        # `reshaped_mlog_to_orig_log_map`: [num_layers * num_nodes, num_middle_experts]\n        # `final_phy2log_per_node`: [num_layers * num_nodes, num_phy_per_node]\n        final_phy2log_per_node = reshaped_mlog_to_orig_log_map.gather(-1, phy2mlog)\n\n        # `phyrank` is the replica rank for the middle experts.\n        # We need to combine this with `mlogcnt` to get the final `logcnt`.\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`. It tells us how many replicas each middle expert has.\n        # The `phyrank` is the rank of the replica for a *specific* middle expert.\n\n        # The `logcnt` returned by `replicate_experts` is `[X, num_log]`, which is `[num_layers*num_nodes, num_middle_experts]`.\n        # We need to map this back to `[num_layers, num_logical_experts]`.\n\n        # Let's use the `mlogcnt` directly to get the total count of replicas for each logical expert.\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`.\n        # We need to sum/average counts based on the `mlog_to_orig_log_map`.\n\n        # Let's refine the output mapping:\n        # `physical_to_logical_map`: [num_layers, num_replicas]\n        # `logical_to_physical_map`: [num_layers, num_logical_experts, X]\n        # `expert_count`: [num_layers, num_logical_experts]\n\n        # `final_phy2log_per_node`: [num_layers * num_nodes, num_phy_per_node]\n        # This is the logical expert ID for each physical expert *within its node*.\n        # We need to convert this to global physical expert IDs.\n        # Physical experts are grouped by GPU. `num_gpus` total, `num_gpus // num_nodes` per node.\n        # `phy_experts_per_gpu` = `num_physical_experts // num_gpus`.\n        # So, each GPU has `phy_experts_per_gpu` physical experts.\n        # Each Node has `num_gpus // num_nodes` GPUs.\n        # Total physical experts per node = `(num_gpus // num_nodes) * phy_experts_per_gpu`\n        # This should equal `num_physical_experts // num_nodes`.\n\n        # `final_phy2log_per_node`: [num_layers, num_nodes, num_phy_per_node]\n        # `final_rank_per_node`: [num_layers, num_nodes, num_phy_per_node] (this is `phyrank`)\n\n        # We need to flatten this to `[num_layers, num_physical_experts]`.\n        # `num_physical_experts = num_nodes * num_phy_per_node`.\n\n        # `final_phy2log_global`: [num_layers, num_physical_experts]\n        # `final_rank_global`: [num_layers, num_physical_experts]\n\n        # The `mlog_to_orig_log_map` correctly maps middle experts to original logical experts.\n        # `phy2mlog` maps physical experts (within a node) to middle experts.\n        # So, `phy2mlog.gather(-1, final_phy2log_per_node)` gives the logical expert ID.\n        # This is what `final_phy2log_per_node` already represents.\n\n        # Let's rename `final_phy2log_per_node` to `phy2log_per_node` for clarity.\n        phy2log_per_node = final_phy2log_per_node\n        rank_per_node = phyrank # [num_layers * num_nodes, num_phy_per_node]\n\n        # Flatten these to get global mappings for physical experts.\n        phy2log_global = phy2log_per_node.view(num_layers, -1)\n        rank_global = rank_per_node.view(num_layers, -1)\n\n        # Now, let's construct the `logcnt` (expert_count).\n        # `mlogcnt`: [num_layers * num_nodes, num_middle_experts]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, num_middle_experts]\n\n        # We need to aggregate `mlogcnt` based on `mlog_to_orig_log_map`.\n        # For each logical expert, its total count is the sum of counts of its constituent middle experts.\n        # This aggregation needs to be done carefully.\n\n        # Let's create a tensor for `logcnt`: [num_layers, num_logical_experts] initialized to zeros.\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n\n        # The expert_count aggregation:\n        # mlog_to_orig_log_map: [num_layers, num_nodes, num_middle_experts]\n        # mlogcnt: [num_layers * num_nodes, num_middle_experts]\n        # Flatten mlog_to_orig_log_map and mlogcnt to [num_layers, N] for scatter_add_\n        mlog_to_orig_log_map_flat = mlog_to_orig_log_map.view(num_layers, -1)\n        mlogcnt_reshaped = mlogcnt.view(num_layers, -1)\n\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n        # Use scatter_add_ to efficiently aggregate counts for each logical expert across layers\n        expert_count.scatter_add_(1, mlog_to_orig_log_map_flat, mlogcnt_reshaped)\n\n        # The logical_to_physical_map construction is handled by the outer rebalance_experts function.\n        # So, this function only needs to return the global mappings and counts.\n        return phy2log_global, rank_global, expert_count\n\n    else:\n        # Global balancing strategy: treat all experts as one group, balance across num_gpus.\n        # This is equivalent to calling the hierarchical function with num_nodes=1, num_groups=1.\n        # The `rebalance_experts` function will call this branch if `num_groups % num_nodes != 0`.\n\n        # Step 1: Pack logical experts to GPUs.\n        # `balanced_packing` will distribute the logical experts across `num_gpus`.\n\n        # We need to adapt `balanced_packing` to work with `num_packs = num_gpus`.\n        # The `groups_per_pack` in `balanced_packing` refers to items per pack.\n        # Here, items are logical experts. So, `groups_per_pack` = `num_logical_experts // num_gpus`.\n        # `num_packs` = `num_gpus`.\n\n        pack_index_gpu, rank_in_gpu = balanced_packing(weight, num_gpus)\n\n        # `pack_index_gpu`: [num_layers, num_logical_experts] -> GPU ID for each logical expert.\n        # `rank_in_gpu`: [num_layers, num_logical_experts] -> rank within GPU for each logical expert.\n\n        # Step 2: Replicate experts within GPUs.\n        num_logical_experts_per_gpu = num_logical_experts // num_gpus\n\n        # For each GPU, we have a set of logical experts.\n        # We need to gather weights for logical experts assigned to the same GPU.\n        # `gpu_logical_weights`: [num_gpus, num_logical_experts_per_gpu, num_layers] (transposed for replicate_experts)\n        # `gpu_logical_to_orig_map`: [num_gpus, num_logical_experts_per_gpu]\n\n        # Vectorized construction of gpu_logical_weights and gpu_logical_to_orig_map\n        # This replaces the nested Python loops for better performance.\n\n        num_logical_experts_per_gpu = num_logical_experts // num_gpus\n\n        # Calculate the new \"middle expert group\" index for each original logical expert.\n        # This new index combines GPU assignment and rank within the GPU.\n        # Shape: [num_layers, num_logical_experts]\n        new_logical_expert_idx_on_gpu = pack_index_gpu * num_logical_experts_per_gpu + rank_in_gpu\n\n        # Create a tensor that maps new logical expert index back to original logical expert index.\n        # This is a permutation tensor for logical experts within each layer.\n        # Shape: [num_layers, num_gpus * num_logical_experts_per_gpu] (which is num_logical_experts)\n        logical_expert_permutation = torch.empty((num_layers, num_logical_experts),\n                                                 dtype=torch.int64,\n                                                 device=weight.device)\n        logical_expert_permutation.scatter_(1, new_logical_expert_idx_on_gpu,\n                                            torch.arange(num_logical_experts, device=weight.device).expand(num_layers, -1))\n\n        # Gather weights based on the logical_expert_permutation\n        # Shape: [num_layers, num_logical_experts]\n        gpu_logical_weights_ordered = weight.gather(1, logical_expert_permutation)\n        \n        # Reshape to [num_gpus, num_layers, num_logical_experts_per_gpu] for subsequent processing\n        # and ensure contiguous memory layout.\n        gpu_logical_weights = gpu_logical_weights_ordered.view(\n            num_layers, num_gpus, num_logical_experts_per_gpu).permute(1, 0, 2).contiguous()\n\n        # Construct gpu_logical_to_orig_map similarly\n        # Original logical expert IDs, expanded to [num_layers, num_logical_experts]\n        orig_log_ids_expanded = torch.arange(num_logical_experts,\n                                             dtype=torch.int64,\n                                             device=weight.device).unsqueeze(0).expand(num_layers, -1)\n        \n        # Gather original logical expert IDs based on the permutation\n        gpu_logical_to_orig_map_ordered = orig_log_ids_expanded.gather(1, logical_expert_permutation)\n        \n        # Reshape to [num_gpus * num_layers, num_logical_experts_per_gpu] for replicate_experts input\n        gpu_logical_to_orig_map = gpu_logical_to_orig_map_ordered.view(\n            num_layers, num_gpus, num_logical_experts_per_gpu).permute(1, 0, 2).contiguous().view(\n                -1, num_logical_experts_per_gpu)\n\n        # Reshape for `replicate_experts`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        reshaped_gpu_logical_weights = gpu_logical_weights.view(-1, num_logical_experts_per_gpu)\n        reshaped_gpu_logical_to_orig_map = gpu_logical_to_orig_map.view(-1, num_logical_experts_per_gpu)\n\n        # `replicate_experts` is called with `num_phy = num_physical_experts // num_gpus` (target physical experts per GPU).\n        # `X` becomes `num_gpus * num_layers`. `num_log` becomes `num_logical_experts_per_gpu`.\n        # `phy2log_gpu`: [num_gpus * num_layers, num_phy_per_gpu]\n        # `phyrank_gpu`: [num_gpus * num_layers, num_phy_per_gpu]\n        # `gpu_logcnt`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        phy2log_gpu, phyrank_gpu, gpu_logcnt = replicate_experts(\n            reshaped_gpu_logical_weights, num_physical_experts // num_gpus)\n\n        # Map `phy2log_gpu` back to original logical expert IDs using `reshaped_gpu_logical_to_orig_map`.\n        # `phy2log_global`: [num_gpus * num_layers, num_phy_per_gpu]\n        phy2log_global = reshaped_gpu_logical_to_orig_map.gather(-1, phy2log_gpu)\n\n        # Flatten to get the final `physical_to_logical_map` and `rank`.\n        # The `rank_global` is `phyrank_gpu`.\n        rank_global = phyrank_gpu\n\n        # Construct `expert_count` ([num_layers, num_logical_experts]).\n        # `gpu_logcnt`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n\n        for gpu_layer_idx in range(num_gpus * num_layers):\n            layer_idx = gpu_layer_idx // num_gpus\n            # Get the original logical expert indices for this gpu_layer combination\n            orig_log_indices = reshaped_gpu_logical_to_orig_map[gpu_layer_idx] # [num_logical_experts_per_gpu]\n            counts_for_orig_logs = gpu_logcnt[gpu_layer_idx] # [num_logical_experts_per_gpu]\n\n            # Add these counts to the correct positions in the expert_count tensor\n            expert_count[layer_idx].index_add_(0, orig_log_indices, counts_for_orig_logs)\n\n        # Construct `logical_to_physical_map` ([num_layers, num_logical_experts, X])\n        max_replicas_per_expert = (num_physical_experts + num_logical_experts - 1) // num_logical_experts\n\n        logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert), -1, dtype=torch.int64, device=weight.device)\n\n        # Need to map `phy2log_global` and `rank_global` to the correct positions.\n        # `phy2log_global` are indices into the *original* logical experts.\n        # `rank_global` is the replica rank for that original logical expert.\n\n        # Reshape to get mappings per layer and then iterate through physical experts.\n        phy2log_global_reshaped = phy2log_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous()\n        rank_global_reshaped = rank_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous()\n\n        # Iterate through all physical experts\n        for layer_idx in range(num_layers):\n            for gpu_idx in range(num_gpus):\n                for phys_idx_in_gpu in range(num_physical_experts // num_gpus):\n                    # Get the global physical expert index\n                    global_phys_idx = gpu_idx * (num_physical_experts // num_gpus) + phys_idx_in_gpu\n\n                    log_expert_id = phy2log_global_reshaped[layer_idx, gpu_idx, phys_idx_in_gpu]\n                    replica_rank = rank_global_reshaped[layer_idx, gpu_idx, phys_idx_in_gpu]\n\n                    if log_expert_id >= 0 and log_expert_id < num_logical_experts:\n                        if replica_rank < max_replicas_per_expert:\n                            logical_to_physical_map[layer_idx, log_expert_id, replica_rank] = global_phys_idx\n                        else:\n                            pass # Handle error or log\n\n        # The `phy2log_global` needs to be reshaped to `[num_layers, num_replicas]`\n        # where `num_replicas` is `num_physical_experts`.\n        # `phy2log_global` is currently `[num_gpus * num_layers, num_phy_per_gpu]`.\n        # We need to combine these.\n        # The order of physical experts should be consistent.\n        # Let's reconstruct `phy2log_global` to be `[num_layers, num_physical_experts]`.\n\n        final_phy2log_global = torch.empty((num_layers, num_physical_experts), dtype=torch.int64, device=weight.device)\n        current_phys_idx = 0\n        for gpu_idx in range(num_gpus):\n            for layer_idx in range(num_layers):\n                # Calculate the range of physical experts for this GPU and layer combination\n                start_phys_idx = layer_idx * num_gpus * (num_physical_experts // num_gpus) + gpu_idx * (num_physical_experts // num_gpus)\n                end_phys_idx = start_phys_idx + (num_physical_experts // num_gpus)\n\n                # The `phy2log_global` calculated earlier is `[num_gpus * num_layers, num_phy_per_gpu]`\n                # We need to map it correctly.\n                # `phy2log_gpu` was `[num_gpus * num_layers, num_phy_per_gpu]`\n                # `reshaped_gpu_logical_to_orig_map` was `[num_gpus * num_layers, num_phy_per_gpu]`\n                # `phy2log_global = reshaped_gpu_logical_to_orig_map.gather(-1, phy2log_gpu)`\n\n                # The indices for `phy2log_global` should be:\n                # For layer 0, GPU 0: phys experts 0 to num_phy_per_gpu - 1\n                # For layer 0, GPU 1: phys experts num_phy_per_gpu to 2*num_phy_per_gpu - 1\n                # ...\n                # For layer 1, GPU 0: phys experts num_gpus * num_phy_per_gpu to (num_gpus+1)*num_phy_per_gpu - 1\n\n                # Let's re-index the `phy2log_global` and `rank_global` to match `[num_layers, num_physical_experts]`\n                # The current `phy2log_global` is effectively `[num_layers * num_gpus, num_phy_per_gpu]`\n                # We need to rearrange it.\n\n                # Example: num_layers=2, num_gpus=2, num_phy_per_gpu=2. num_physical_experts=4.\n                # phy2log_global:\n                # [layer0_gpu0_phys0, layer0_gpu0_phys1,\n                #  layer0_gpu1_phys0, layer0_gpu1_phys1,\n                #  layer1_gpu0_phys0, layer1_gpu0_phys1,\n                #  layer1_gpu1_phys0, layer1_gpu1_phys1]\n                # Reshaped to [num_gpus*num_layers, num_phy_per_gpu] -> [4, 2]\n                # We want [num_layers, num_physical_experts] -> [2, 4]\n\n                # The current `phy2log_global` is `[num_gpus * num_layers, num_phy_per_gpu]`.\n                # Permute and reshape:\n                # `view(num_gpus, num_layers, -1)` gives `[num_gpus, num_layers, num_phy_per_gpu]`\n                # `permute(1, 0, 2)` gives `[num_layers, num_gpus, num_phy_per_gpu]`\n                # `contiguous().view(num_layers, -1)` gives `[num_layers, num_gpus * num_phy_per_gpu]` = `[num_layers, num_physical_experts]`\n\n                final_phy2log_global = phy2log_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous().view(num_layers, num_physical_experts)\n                final_rank_global = rank_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous().view(num_layers, num_physical_experts)\n\n        # Return the results for global balancing.\n        return final_phy2log_global, logical_to_physical_map, expert_count\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float().cpu()\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    # The `rebalance_experts_hierarchical` and the global balancing logic\n    # now produce `phy2log_global`, `rank_global`, and `expert_count`.\n    # `phy2log_global`: [num_layers, num_replicas] -> maps physical expert index to logical expert ID.\n    # `rank_global`: [num_layers, num_replicas] -> maps physical expert index to its replica rank.\n    # `expert_count`: [num_layers, num_logical_experts] -> number of replicas for each logical expert.\n\n    # We need to construct `logical_to_physical_map`: [layers, num_logical_experts, X]\n    # where X is the maximum number of replicas for any logical expert.\n\n    # Store device for later use.\n    device = weight.device \n\n    num_physical_experts_total = num_replicas\n\n    # Correctly calculate max_replicas_per_expert from logcnt.\n    # This is the maximum number of replicas any single logical expert can have.\n    max_replicas_per_expert = torch.max(logcnt).item() if logcnt.numel() > 0 else 0\n\n    # Handle edge case where logcnt might be all zeros or empty.\n    # If there are logical experts and replicas, ensure max_replicas_per_expert is at least 1.\n    if num_logical_experts > 0 and num_replicas > 0 and max_replicas_per_expert == 0:\n        max_replicas_per_expert = 1\n    \n    # Initialize the map with -1 (indicating no physical expert assigned).\n    # The size of the last dimension is `max_replicas_per_expert`.\n    logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert),\n                                         -1, dtype=torch.int64, device=device)\n\n    # Create physical expert IDs corresponding to the indices of phy2log/rank.\n    # These are the global indices of the physical experts.\n    physical_expert_ids = torch.arange(num_replicas, dtype=torch.int64, device=device)\n    \n    # Expand physical_expert_ids to match the shape of phy2log and rank for broadcasting assignment.\n    # `phy2log` and `rank` are [num_layers, num_replicas].\n    # `physical_expert_ids` needs to be [num_layers, num_replicas] for direct assignment.\n    physical_expert_ids_expanded = physical_expert_ids.unsqueeze(0).expand(num_layers, num_replicas)\n\n    # Create layer indices for advanced indexing.\n    # `layer_indices` will be [num_layers, 1] and broadcasts to [num_layers, num_replicas].\n    layer_indices = torch.arange(num_layers, dtype=torch.int64, device=device).unsqueeze(1)\n    \n    # Flatten indices and values for masked assignment to avoid out-of-bounds errors.\n    flat_layer_indices = layer_indices.flatten()\n    flat_phy2log = phy2log.flatten()\n    flat_rank = rank.flatten()\n    flat_physical_ids = physical_expert_ids_expanded.flatten()\n    \n    # Create a mask for valid assignments:\n    # `phy2log` must be a valid logical expert index.\n    # `rank` must be a valid replica rank index for the map's last dimension.\n    valid_mask_flat = (flat_phy2log >= 0) & (flat_phy2log < num_logical_experts) & \\\n                      (flat_rank >= 0) & (flat_rank < max_replicas_per_expert)\n    \n    # Perform the assignment only for valid indices.\n    if valid_mask_flat.any():\n        logical_to_physical_map[\n            flat_layer_indices[valid_mask_flat],\n            flat_phy2log[valid_mask_flat],\n            flat_rank[valid_mask_flat]\n        ] = flat_physical_ids[valid_mask_flat]\n\n    return phy2log, logical_to_physical_map, logcnt\n\n    # The `logcnt` (expert_count) is already computed by the balancing functions.\n    # `phy2log` is the first return value.\n    return phy2log, logical_to_physical_map, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["The current program aims to balance expert load in a Mixture-of-Experts (MoE) model by rearranging and replicating experts. The provided code implements a hierarchical and a global load balancing strategy.\n\nThe primary issues identified are:\n1.  **`name 'rank' is not defined` error**: This indicates a scope issue or a missing import. The `rank` variable is used in `rebalance_experts_hierarchical` and `replicate_experts` but seems to be undefined in the scope where it's accessed or returned from `replicate_experts`.\n2.  **Performance Regression**: The metrics `balancedness_score`, `speed_score`, and `combined_score` have regressed. This suggests that recent changes might have introduced inefficiencies or suboptimal balancing strategies.\n3.  **Code Complexity**: The `rebalance_experts_hierarchical` function is quite complex, especially the logic for mapping and reshaping tensors. This makes it hard to read, maintain, and potentially debug.\n\nMy goal is to improve both the load balancing quality and the efficiency of the rebalancing algorithm.\n\nHere are the suggested improvements:\n\n### Improvement 1: Fix `name 'rank' is not defined` Error and Clarify Return Values\n\nThe `rebalance_experts_hierarchical` function returns `phy2log, phyrank, logcnt`. However, the `rebalance_experts` function calls it and then uses `phy2log, phyrank, logcnt` but `phyrank` is not directly used in the final return. Also, the `rank` variable is used in `replicate_experts` but it's not clear if it's the same `rank` being returned or a local variable.\n\nThe main issue is that `rebalance_experts_hierarchical` returns `rank_global` which is then used as `rank` in `rebalance_experts`, but the variable name `rank` is undefined. This suggests a naming inconsistency or a direct usage of a variable that wasn't properly passed or defined.\n\nLet's ensure consistent naming and correct usage of the returned values. `replicate_experts` returns `rank`, which is then used as `phyrank` in `rebalance_experts_hierarchical`. This `phyrank` should be returned and used.\n\nAlso, the `rebalance_experts_hierarchical` function returns `phy2log_global, rank_global, expert_count`. In `rebalance_experts`, these are assigned to `phy2log, phyrank, logcnt`. However, `phyrank` is not directly used in the final return statement. The code then proceeds to construct `logical_to_physical_map` using `phy2log` and `rank`.\n\nIt seems `rank_global` from `rebalance_experts_hierarchical` should be assigned to a variable that is then used to build `logical_to_physical_map`. The current code assigns it to `phyrank` and then uses `rank` (which is undefined) in the construction of `logical_to_physical_map`.\n\n**Proposed Change:**\nEnsure that the `rank_global` returned by `rebalance_experts_hierarchical` is correctly captured and used when constructing `logical_to_physical_map`. Rename `phyrank` to `rank_global` for clarity.\n\n<<<<<<< SEARCH\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    # The `rebalance_experts_hierarchical` and the global balancing logic\n    # now produce `phy2log_global`, `rank_global`, and `expert_count`.\n    # `phy2log_global`: [num_layers, num_replicas] -> maps physical expert index to logical expert ID.\n    # `rank_global`: [num_layers, num_replicas] -> maps physical expert index to its replica rank.\n    # `expert_count`: [num_layers, num_logical_experts] -> number of replicas for each logical expert.\n\n    # We need to construct `logical_to_physical_map`: [layers, num_logical_experts, X]\n    # where X is the maximum number of replicas for any logical expert.\n\n    # Store device for later use.\n    device = weight.device \n\n    num_physical_experts_total = num_replicas\n\n    # Correctly calculate max_replicas_per_expert from logcnt.\n    # This is the maximum number of replicas any single logical expert can have.\n    max_replicas_per_expert = torch.max(logcnt).item() if logcnt.numel() > 0 else 0\n\n    # Handle edge case where logcnt might be all zeros or empty.\n    # If there are logical experts and replicas, ensure max_replicas_per_expert is at least 1.\n    if num_logical_experts > 0 and num_replicas > 0 and max_replicas_per_expert == 0:\n        max_replicas_per_expert = 1\n    \n    # Initialize the map with -1 (indicating no physical expert assigned).\n    # The size of the last dimension is `max_replicas_per_expert`.\n    logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert),\n                                         -1, dtype=torch.int64, device=device)\n\n    # Create physical expert IDs corresponding to the indices of phy2log/rank.\n    # These are the global indices of the physical experts.\n    physical_expert_ids = torch.arange(num_replicas, dtype=torch.int64, device=device)\n    \n    # Expand physical_expert_ids to match the shape of phy2log and rank for broadcasting assignment.\n    # `phy2log` and `rank` are [num_layers, num_replicas].\n    # `physical_expert_ids` needs to be [num_layers, num_replicas] for direct assignment.\n    physical_expert_ids_expanded = physical_expert_ids.unsqueeze(0).expand(num_layers, num_replicas)\n\n    # Create layer indices for advanced indexing.\n    # `layer_indices` will be [num_layers, 1] and broadcasts to [num_layers, num_replicas].\n    layer_indices = torch.arange(num_layers, dtype=torch.int64, device=device).unsqueeze(1)\n    \n    # Flatten indices and values for masked assignment to avoid out-of-bounds errors.\n    flat_layer_indices = layer_indices.flatten()\n    flat_phy2log = phy2log.flatten()\n    flat_rank = rank.flatten()\n    flat_physical_ids = physical_expert_ids_expanded.flatten()\n    \n    # Create a mask for valid assignments:\n    # `phy2log` must be a valid logical expert index.\n    # `rank` must be a valid replica rank index for the map's last dimension.\n    valid_mask_flat = (flat_phy2log >= 0) & (flat_phy2log < num_logical_experts) & \\\n                      (flat_rank >= 0) & (flat_rank < max_replicas_per_expert)\n    \n    # Perform the assignment only for valid indices.\n    if valid_mask_flat.any():\n        logical_to_physical_map[\n            flat_layer_indices[valid_mask_flat],\n            flat_phy2log[valid_mask_flat],\n            flat_rank[valid_mask_flat]\n        ] = flat_physical_ids[valid_mask_flat]\n\n    return phy2log, logical_to_physical_map, logcnt\n=======\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, rank_global, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, rank_global, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    # `phy2log`: [num_layers, num_replicas] -> maps physical expert index to logical expert ID.\n    # `rank_global`: [num_layers, num_replicas] -> maps physical expert index to its replica rank.\n    # `logcnt`: [num_layers, num_logical_experts] -> number of replicas for each logical expert.\n\n    # We need to construct `logical_to_physical_map`: [layers, num_logical_experts, X]\n    # where X is the maximum number of replicas for any logical expert.\n\n    device = weight.device \n\n    # Correctly calculate max_replicas_per_expert from logcnt.\n    max_replicas_per_expert = torch.max(logcnt).item() if logcnt.numel() > 0 else 0\n\n    # Handle edge case where logcnt might be all zeros or empty.\n    if num_logical_experts > 0 and num_replicas > 0 and max_replicas_per_expert == 0:\n        max_replicas_per_expert = 1\n    \n    # Initialize the map with -1.\n    logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert),\n                                         -1, dtype=torch.int64, device=device)\n\n    # Create physical expert IDs corresponding to the indices of phy2log/rank.\n    physical_expert_ids = torch.arange(num_replicas, dtype=torch.int64, device=device)\n    \n    # Expand physical_expert_ids to match the shape of phy2log and rank for broadcasting assignment.\n    physical_expert_ids_expanded = physical_expert_ids.unsqueeze(0).expand(num_layers, num_replicas)\n\n    # Create layer indices for advanced indexing.\n    layer_indices = torch.arange(num_layers, dtype=torch.int64, device=device).unsqueeze(1)\n    \n    # Flatten indices and values for masked assignment.\n    flat_layer_indices = layer_indices.flatten()\n    flat_phy2log = phy2log.flatten()\n    flat_rank = rank_global.flatten() # Use rank_global here\n    flat_physical_ids = physical_expert_ids_expanded.flatten()\n    \n    # Create a mask for valid assignments.\n    valid_mask_flat = (flat_phy2log >= 0) & (flat_phy2log < num_logical_experts) & \\\n                      (flat_rank >= 0) & (flat_rank < max_replicas_per_expert)\n    \n    # Perform the assignment only for valid indices.\n    if valid_mask_flat.any():\n        logical_to_physical_map[\n            flat_layer_indices[valid_mask_flat],\n            flat_phy2log[valid_mask_flat],\n            flat_rank[valid_mask_flat]\n        ] = flat_physical_ids[valid_mask_flat]\n\n    return phy2log, logical_to_physical_map, logcnt\n>>>>>>> REPLACE\n\n### Improvement 2: Simplify `rebalance_experts_hierarchical` for Readability and Efficiency\n\nThe `rebalance_experts_hierarchical` function, especially in the `if num_groups % num_nodes == 0:` block, is extremely complex due to tensor reshaping and reordering. This complexity likely contributes to performance regressions and makes it hard to verify correctness.\n\nThe core idea is to:\n1.  Group logical experts based on `num_groups`.\n2.  Pack these groups onto `num_nodes`.\n3.  Within each node, replicate experts to balance across `num_gpus`.\n\nThe current implementation uses several intermediate reshaping and gathering steps that can be simplified. The goal is to create `mlog_weights` and `mlog_to_orig_log_map` more directly and efficiently.\n\n**Proposed Change:**\nRefactor the tensor manipulations for creating `mlog_weights` and `mlog_to_orig_log_map` to be more straightforward and potentially use fewer intermediate tensors or more efficient operations. Specifically, the logic for `mlog_weights` and `mlog_to_orig_log_map` construction can be streamlined.\n\nInstead of complex `gather` operations with permutations, we can directly construct the target tensors by iterating through nodes and then through groups assigned to those nodes.\n\nLet's focus on the hierarchical part (`if num_groups % num_nodes == 0:`):\n\n<<<<<<< SEARCH\n        # Calculate the new \"middle expert group\" index for each original group.\n        # This new index combines node assignment and rank within the node.\n        # Shape: [num_layers, num_groups]\n        new_group_idx = group_pack_index * groups_per_node + group_rank_in_pack\n\n        # Create a tensor that maps new group index back to original group index.\n        # This is a permutation tensor for groups within each layer.\n        # Shape: [num_layers, num_nodes * num_groups_per_node] (which is num_groups)\n        group_permutation = torch.empty((num_layers, num_groups),\n                                        dtype=torch.int64,\n                                        device=weight.device)\n        group_permutation.scatter_(1, new_group_idx,\n                                   torch.arange(num_groups, device=weight.device).expand(num_layers, -1))\n\n        # Reshape original weights to [num_layers, num_groups, group_size]\n        weight_reshaped = weight.unflatten(-1, (num_groups, group_size))\n\n        # Gather weights based on the group_permutation\n        # The .gather() operation reorders the groups according to the permutation.\n        # Shape: [num_layers, num_groups, group_size]\n        mlog_weights_ordered = weight_reshaped.gather(1, group_permutation.unsqueeze(-1).expand(-1, -1, group_size))\n        \n        # Reshape to [num_layers, num_nodes, groups_per_node * group_size]\n        # This is the final mlog_weights.\n        mlog_weights = mlog_weights_ordered.view(num_layers, num_nodes, groups_per_node * group_size)\n\n        # Construct mlog_to_orig_log_map similarly\n        # Original logical expert IDs, reshaped to [num_groups, group_size]\n        orig_log_ids_reshaped = torch.arange(num_logical_experts,\n                                             dtype=torch.int64,\n                                             device=weight.device).view(num_groups, group_size)\n        \n        # Expand to [num_layers, num_groups, group_size] and gather\n        mlog_to_orig_log_map_ordered = orig_log_ids_reshaped.unsqueeze(0).expand(num_layers, -1, -1).gather(\n            1, group_permutation.unsqueeze(-1).expand(-1, -1, group_size))\n        \n        # Reshape to [num_layers, num_nodes, groups_per_node * group_size]\n        mlog_to_orig_log_map = mlog_to_orig_log_map_ordered.view(\n            num_layers, num_nodes, groups_per_node * group_size)\n\n        # Now, `mlog_weights`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n        # And `mlog_to_orig_log_map`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n\n        # Step 2: Replicate experts within each node.\n        # `mlog_weights` are the weights for experts within each node.\n        # `num_physical_experts // num_nodes` is the target number of physical experts per node.\n        # `replicate_experts` takes `[X, num_log]` and returns `phy2log`, `rank`, `logcnt`.\n        # Here, X = num_layers * num_nodes, num_log = num_logical_experts // num_nodes.\n        # So, we need to reshape `mlog_weights` and `mlog_to_orig_log_map`.\n\n        num_middle_experts = num_logical_experts // num_nodes\n        reshaped_mlog_weights = mlog_weights.view(-1, num_middle_experts)\n        reshaped_mlog_to_orig_log_map = mlog_to_orig_log_map.view(-1, num_middle_experts)\n\n        # `phy2mlog`: [num_layers * num_nodes, num_physical_experts // num_nodes]\n        # `phyrank`: [num_layers * num_nodes, num_physical_experts // num_nodes]\n        # `mlogcnt`: [num_layers * num_nodes, num_middle_experts]\n        phy2mlog, phyrank, mlogcnt = replicate_experts(\n            reshaped_mlog_weights, num_physical_experts // num_nodes)\n\n        # Step 3: Pack physical experts to GPUs.\n        # `phy2mlog` maps physical experts (within each node) to middle logical experts.\n        # `phy2mlog` is `[num_layers * num_nodes, num_phy_per_node]`\n        # `phyrank` is `[num_layers * num_nodes, num_phy_per_node]`\n\n        # We need to map these back to global physical expert indices.\n        # The `phy2mlog` refers to the *middle* logical experts.\n        # The `replicate_experts` function returns `phy2log` which is the *original* logical expert ID.\n        # So, `phy2log` from `replicate_experts` is actually `[num_layers*num_nodes, num_phy_per_node]`,\n        # where each entry is an index into the *middle* logical experts.\n\n        # We need to map these `phy2mlog` indices back to original logical expert IDs.\n        # `reshaped_mlog_to_orig_log_map`: [num_layers * num_nodes, num_middle_experts]\n        # `final_phy2log_per_node`: [num_layers * num_nodes, num_phy_per_node]\n        final_phy2log_per_node = reshaped_mlog_to_orig_log_map.gather(-1, phy2mlog)\n\n        # `phyrank` is the replica rank for the middle experts.\n        # We need to combine this with `mlogcnt` to get the final `logcnt`.\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`. It tells us how many replicas each middle expert has.\n        # The `phyrank` is the rank of the replica for a *specific* middle expert.\n\n        # The `logcnt` returned by `replicate_experts` is `[X, num_log]`, which is `[num_layers*num_nodes, num_middle_experts]`.\n        # We need to map this back to `[num_layers, num_logical_experts]`.\n\n        # Let's use the `mlogcnt` directly to get the total count of replicas for each logical expert.\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`.\n        # We need to aggregate `mlogcnt` based on `mlog_to_orig_log_map`.\n\n        # Let's create a tensor for `logcnt`: [num_layers, num_logical_experts] initialized to zeros.\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n\n        # The expert_count aggregation:\n        # mlog_to_orig_log_map: [num_layers, num_nodes, num_middle_experts]\n        # mlogcnt: [num_layers * num_nodes, num_middle_experts]\n        # Flatten mlog_to_orig_log_map and mlogcnt to [num_layers, N] for scatter_add_\n        mlog_to_orig_log_map_flat = mlog_to_orig_log_map.view(num_layers, -1)\n        mlogcnt_reshaped = mlogcnt.view(num_layers, -1)\n\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n        # Use scatter_add_ to efficiently aggregate counts for each logical expert across layers\n        expert_count.scatter_add_(1, mlog_to_orig_log_map_flat, mlogcnt_reshaped)\n\n        # The logical_to_physical_map construction is handled by the outer rebalance_experts function.\n        # So, this function only needs to return the global mappings and counts.\n        return phy2log_global, rank_global, expert_count\n\n    else:\n        # Global balancing strategy: treat all experts as one group, balance across num_gpus.\n        # This is equivalent to calling the hierarchical function with num_nodes=1, num_groups=1.\n        # The `rebalance_experts` function will call this branch if `num_groups % num_nodes != 0`.\n\n        # Step 1: Pack logical experts to GPUs.\n        # `balanced_packing` will distribute the logical experts across `num_gpus`.\n\n        # We need to adapt `balanced_packing` to work with `num_packs = num_gpus`.\n        # The `groups_per_pack` in `balanced_packing` refers to items per pack.\n        # Here, items are logical experts. So, `groups_per_pack` = `num_logical_experts // num_gpus`.\n        # `num_packs` = `num_gpus`.\n\n        pack_index_gpu, rank_in_gpu = balanced_packing(weight, num_gpus)\n\n        # `pack_index_gpu`: [num_layers, num_logical_experts] -> GPU ID for each logical expert.\n        # `rank_in_gpu`: [num_layers, num_logical_experts] -> rank within GPU for each logical expert.\n\n        # Step 2: Replicate experts within GPUs.\n        num_logical_experts_per_gpu = num_logical_experts // num_gpus\n\n        # For each GPU, we have a set of logical experts.\n        # We need to gather weights for logical experts assigned to the same GPU.\n        # `gpu_logical_weights`: [num_gpus, num_logical_experts_per_gpu, num_layers] (transposed for replicate_experts)\n        # `gpu_logical_to_orig_map`: [num_gpus, num_logical_experts_per_gpu]\n\n        # Vectorized construction of gpu_logical_weights and gpu_logical_to_orig_map\n        # This replaces the nested Python loops for better performance.\n\n        num_logical_experts_per_gpu = num_logical_experts // num_gpus\n\n        # Calculate the new \"middle expert group\" index for each original logical expert.\n        # This new index combines GPU assignment and rank within the GPU.\n        # Shape: [num_layers, num_logical_experts]\n        new_logical_expert_idx_on_gpu = pack_index_gpu * num_logical_experts_per_gpu + rank_in_gpu\n\n        # Create a tensor that maps new logical expert index back to original logical expert index.\n        # This is a permutation tensor for logical experts within each layer.\n        # Shape: [num_layers, num_gpus * num_logical_experts_per_gpu] (which is num_logical_experts)\n        logical_expert_permutation = torch.empty((num_layers, num_logical_experts),\n                                                 dtype=torch.int64,\n                                                 device=weight.device)\n        logical_expert_permutation.scatter_(1, new_logical_expert_idx_on_gpu,\n                                            torch.arange(num_logical_experts, device=weight.device).expand(num_layers, -1))\n\n        # Gather weights based on the logical_expert_permutation\n        # Shape: [num_layers, num_logical_experts]\n        gpu_logical_weights_ordered = weight.gather(1, logical_expert_permutation)\n        \n        # Reshape to [num_gpus, num_layers, num_logical_experts_per_gpu] for subsequent processing\n        # and ensure contiguous memory layout.\n        gpu_logical_weights = gpu_logical_weights_ordered.view(\n            num_layers, num_gpus, num_logical_experts_per_gpu).permute(1, 0, 2).contiguous()\n\n        # Construct gpu_logical_to_orig_map similarly\n        # Original logical expert IDs, expanded to [num_layers, num_logical_experts]\n        orig_log_ids_expanded = torch.arange(num_logical_experts,\n                                             dtype=torch.int64,\n                                             device=weight.device).unsqueeze(0).expand(num_layers, -1)\n        \n        # Gather original logical expert IDs based on the permutation\n        gpu_logical_to_orig_map_ordered = orig_log_ids_expanded.gather(1, logical_expert_permutation)\n        \n        # Reshape to [num_gpus * num_layers, num_logical_experts_per_gpu] for replicate_experts input\n        gpu_logical_to_orig_map = gpu_logical_to_orig_map_ordered.view(\n            num_layers, num_gpus, num_logical_experts_per_gpu).permute(1, 0, 2).contiguous().view(\n                -1, num_logical_experts_per_gpu)\n\n        # Reshape for `replicate_experts`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        reshaped_gpu_logical_weights = gpu_logical_weights.view(-1, num_logical_experts_per_gpu)\n        reshaped_gpu_logical_to_orig_map = gpu_logical_to_orig_map.view(-1, num_logical_experts_per_gpu)\n\n        # `replicate_experts` is called with `num_phy = num_physical_experts // num_gpus` (target physical experts per GPU).\n        # `X` becomes `num_gpus * num_layers`. `num_log` becomes `num_logical_experts_per_gpu`.\n        # `phy2log_gpu`: [num_gpus * num_layers, num_phy_per_gpu]\n        # `phyrank_gpu`: [num_gpus * num_layers, num_phy_per_gpu]\n        # `gpu_logcnt`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        phy2log_gpu, phyrank_gpu, gpu_logcnt = replicate_experts(\n            reshaped_gpu_logical_weights, num_physical_experts // num_gpus)\n\n        # Map `phy2log_gpu` back to original logical expert IDs using `reshaped_gpu_logical_to_orig_map`.\n        # `phy2log_global`: [num_gpus * num_layers, num_phy_per_gpu]\n        phy2log_global = reshaped_gpu_logical_to_orig_map.gather(-1, phy2log_gpu)\n\n        # Flatten to get the final `physical_to_logical_map` and `rank`.\n        # The `rank_global` is `phyrank_gpu`.\n        rank_global = phyrank_gpu\n\n        # Construct `expert_count` ([num_layers, num_logical_experts]).\n        # `gpu_logcnt`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n\n        for gpu_layer_idx in range(num_gpus * num_layers):\n            layer_idx = gpu_layer_idx // num_gpus\n            # Get the original logical expert indices for this gpu_layer combination\n            orig_log_indices = reshaped_gpu_logical_to_orig_map[gpu_layer_idx] # [num_logical_experts_per_gpu]\n            counts_for_orig_logs = gpu_logcnt[gpu_layer_idx] # [num_logical_experts_per_gpu]\n\n            # Add these counts to the correct positions in the expert_count tensor\n            expert_count[layer_idx].index_add_(0, orig_log_indices, counts_for_orig_logs)\n\n        # Construct `logical_to_physical_map` ([num_layers, num_logical_experts, X])\n        max_replicas_per_expert = (num_physical_experts + num_logical_experts - 1) // num_logical_experts\n\n        logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert), -1, dtype=torch.int64, device=weight.device)\n\n        # Need to map `phy2log_global` and `rank_global` to the correct positions.\n        # `phy2log_global` are indices into the *original* logical experts.\n        # `rank_global` is the replica rank for that original logical expert.\n\n        # Reshape to get mappings per layer and then iterate through physical experts.\n        phy2log_global_reshaped = phy2log_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous()\n        rank_global_reshaped = rank_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous()\n\n        # Iterate through all physical experts\n        for layer_idx in range(num_layers):\n            for gpu_idx in range(num_gpus):\n                for phys_idx_in_gpu in range(num_physical_experts // num_gpus):\n                    # Get the global physical expert index\n                    global_phys_idx = gpu_idx * (num_physical_experts // num_gpus) + phys_idx_in_gpu\n\n                    log_expert_id = phy2log_global_reshaped[layer_idx, gpu_idx, phys_idx_in_gpu]\n                    replica_rank = rank_global_reshaped[layer_idx, gpu_idx, phys_idx_in_gpu]\n\n                    if log_expert_id >= 0 and log_expert_id < num_logical_experts:\n                        if replica_rank < max_replicas_per_expert:\n                            logical_to_physical_map[layer_idx, log_expert_id, replica_rank] = global_phys_idx\n                        else:\n                            pass # Handle error or log\n\n        # The `phy2log_global` needs to be reshaped to `[num_layers, num_replicas]`\n        # where `num_replicas` is `num_physical_experts`.\n        # `phy2log_global` is currently `[num_gpus * num_layers, num_phy_per_gpu]`.\n        # We need to combine these.\n        # The order of physical experts should be consistent.\n        # Let's re-index the `phy2log_global` and `rank_global` to match `[num_layers, num_replicas]`\n        # The current `phy2log_global` is effectively `[num_layers * num_gpus, num_phy_per_gpu]`\n        # We need to rearrange it.\n\n        # Example: num_layers=2, num_gpus=2, num_phy_per_gpu=2. num_physical_experts=4.\n        # phy2log_global:\n        # [layer0_gpu0_phys0, layer0_gpu0_phys1,\n        #  layer0_gpu1_phys0, layer0_gpu1_phys1,\n        #  layer1_gpu0_phys0, layer1_gpu0_phys1,\n        #  layer1_gpu1_phys0, layer1_gpu1_phys1]\n        # Reshaped to [num_gpus*num_layers, num_phy_per_gpu] -> [4, 2]\n        # We want [num_layers, num_physical_experts] -> [2, 4]\n\n        # The current `phy2log_global` is `[num_gpus * num_layers, num_phy_per_gpu]`.\n        # Permute and reshape:\n        # `view(num_gpus, num_layers, -1)` gives `[num_gpus, num_layers, num_phy_per_gpu]`\n        # `permute(1, 0, 2)` gives `[num_layers, num_gpus, num_phy_per_gpu]`\n        # `contiguous().view(num_layers, -1)` gives `[num_layers, num_gpus * num_phy_per_gpu]` = `[num_layers, num_physical_experts]`\n\n        final_phy2log_global = torch.empty((num_layers, num_physical_experts), dtype=torch.int64, device=weight.device)\n        current_phys_idx = 0\n        for gpu_idx in range(num_gpus):\n            for layer_idx in range(num_layers):\n                # Calculate the range of physical experts for this GPU and layer combination\n                start_phys_idx = layer_idx * num_gpus * (num_physical_experts // num_gpus) + gpu_idx * (num_physical_experts // num_gpus)\n                end_phys_idx = start_phys_idx + (num_physical_experts // num_gpus)\n\n                # The `phy2log_global` calculated earlier is `[num_gpus * num_layers, num_phy_per_gpu]`\n                # We need to map it correctly.\n                # `phy2log_gpu` was `[num_gpus * num_layers, num_phy_per_gpu]`\n                # `reshaped_gpu_logical_to_orig_map` was `[num_gpus * num_layers, num_phy_per_gpu]`\n                # `phy2log_global = reshaped_gpu_logical_to_orig_map.gather(-1, phy2log_gpu)`\n\n                # The indices for `phy2log_global` should be:\n                # For layer 0, GPU 0: phys experts 0 to num_phy_per_gpu - 1\n                # For layer 0, GPU 1: phys experts num_phy_per_gpu to 2*num_phy_per_gpu - 1\n                # ...\n                # For layer 1, GPU 0: phys experts num_gpus * num_phy_per_gpu to (num_gpus+1)*num_phy_per_gpu - 1\n\n                # Let's re-index the `phy2log_global` and `rank_global` to match `[num_layers, num_physical_experts]`\n                # The current `phy2log_global` is effectively `[num_layers * num_gpus, num_phy_per_gpu]`\n                # We need to rearrange it.\n\n                # The current `phy2log_global` is `[num_gpus * num_layers, num_phy_per_gpu]`.\n                # Permute and reshape:\n                # `view(num_gpus, num_layers, -1)` gives `[num_gpus, num_layers, num_phy_per_gpu]`\n                # `permute(1, 0, 2)` gives `[num_layers, num_gpus, num_phy_per_gpu]`\n                # `contiguous().view(num_layers, -1)` gives `[num_layers, num_gpus * num_phy_per_gpu]` = `[num_layers, num_physical_experts]`\n\n                final_phy2log_global = phy2log_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous().view(num_layers, num_physical_experts)\n                final_rank_global = rank_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous().view(num_layers, num_physical_experts)\n\n        # Return the results for global balancing.\n        return final_phy2log_global, logical_to_physical_map, expert_count\n=======\n        # Calculate the new \"middle expert group\" index for each original group.\n        # This new index combines node assignment and rank within the node.\n        # Shape: [num_layers, num_groups]\n        new_group_idx = group_pack_index * groups_per_node + group_rank_in_pack\n\n        # Create a mapping from node-assigned group index to original group index.\n        # This allows us to reorder groups according to their node assignment.\n        # Shape: [num_layers, num_groups]\n        group_permutation = torch.empty_like(new_group_idx)\n        group_permutation.scatter_(1, new_group_idx, torch.arange(num_groups, device=weight.device).expand(num_layers, -1))\n\n        # Reshape original weights to [num_layers, num_groups, group_size] and reorder groups.\n        # This effectively creates groups that are ordered by their assigned node.\n        # Shape: [num_layers, num_groups, group_size]\n        mlog_weights_ordered = weight.unflatten(-1, (num_groups, group_size)).gather(1, group_permutation.unsqueeze(-1).expand(-1, -1, group_size))\n        \n        # Reshape to [num_layers, num_nodes, groups_per_node * group_size] for replication.\n        # This gives us the weights for \"middle experts\" within each node.\n        mlog_weights = mlog_weights_ordered.view(num_layers, num_nodes, groups_per_node * group_size)\n\n        # Construct the mapping from \"middle expert\" index to original logical expert index.\n        # Original logical expert IDs, reshaped to [num_groups, group_size].\n        orig_log_ids_reshaped = torch.arange(num_logical_experts, dtype=torch.int64, device=weight.device).view(num_groups, group_size)\n        \n        # Expand and gather using the same permutation to get the mapping for middle experts.\n        # Shape: [num_layers, num_nodes, groups_per_node * group_size]\n        mlog_to_orig_log_map = orig_log_ids_reshaped.unsqueeze(0).expand(num_layers, -1, -1).gather(\n            1, group_permutation.unsqueeze(-1).expand(-1, -1, group_size))\n\n        # Step 2: Replicate experts within each node.\n        num_middle_experts = num_logical_experts // num_nodes\n        reshaped_mlog_weights = mlog_weights.view(-1, num_middle_experts)\n        reshaped_mlog_to_orig_log_map = mlog_to_orig_log_map.view(-1, num_middle_experts)\n\n        # `replicate_experts` takes `[X, num_log]`, where X = num_layers * num_nodes, num_log = num_middle_experts.\n        # `num_phy` is the target number of physical experts per node.\n        # `phy2mlog`: maps physical experts (within node) to middle logical experts.\n        # `phyrank`: replica rank for these physical experts.\n        # `mlogcnt`: replica counts for middle logical experts.\n        phy2mlog, phyrank, mlogcnt = replicate_experts(\n            reshaped_mlog_weights, num_physical_experts // num_nodes)\n\n        # Map `phy2mlog` back to original logical expert IDs using `mlog_to_orig_log_map`.\n        # `final_phy2log_per_node`: [num_layers * num_nodes, num_phy_per_node]\n        final_phy2log_per_node = reshaped_mlog_to_orig_log_map.gather(-1, phy2mlog)\n\n        # Step 3: Aggregate replica counts.\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`.\n        # Aggregate these counts based on `mlog_to_orig_log_map` to get `expert_count`.\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n        \n        # Flatten for scatter_add_:\n        mlog_to_orig_log_map_flat = mlog_to_orig_log_map.view(num_layers, -1)\n        mlogcnt_reshaped = mlogcnt.view(num_layers, -1)\n\n        expert_count.scatter_add_(1, mlog_to_orig_log_map_flat, mlogcnt_reshaped)\n\n        # Prepare output for the caller.\n        # `phy2log_global`: [num_layers, num_physical_experts]\n        # The `final_phy2log_per_node` is `[num_layers * num_nodes, num_phy_per_node]`.\n        # We need to rearrange it.\n        # The structure is `[node_idx, layer_idx, phy_idx_in_node]` conceptually.\n        # We want `[layer_idx, node_idx, phy_idx_in_node]` then flatten.\n        # Current shape: `[num_layers * num_nodes, num_phy_per_node]`\n        # Reshape to `[num_gpus, num_layers, num_phy_per_node]` (assuming num_nodes = num_gpus here for simplicity of argument)\n        # Then permute to `[num_layers, num_gpus, num_phy_per_node]` and flatten.\n        num_phy_per_node = num_physical_experts // num_nodes\n        phy2log_global = final_phy2log_per_node.view(num_nodes, num_layers, num_phy_per_node).permute(1, 0, 2).contiguous().view(num_layers, -1)\n        rank_global = phyrank.view(num_nodes, num_layers, num_phy_per_node).permute(1, 0, 2).contiguous().view(num_layers, -1)\n\n        return phy2log_global, rank_global, expert_count\n>>>>>>> REPLACE\n\n### Improvement 3: Optimize `balanced_packing` for efficiency\n\nThe `balanced_packing` function iterates through all items and repeatedly finds the minimum load pack. This can be optimized. For better efficiency, especially with large `num_groups` and `num_packs`, a priority queue (min-heap) could be used to manage pack loads, allowing O(log m) retrieval of the minimum load pack instead of O(m).\n\nHowever, given the current implementation uses PyTorch tensors, a direct heap implementation might be less straightforward or require custom CUDA kernels for maximum performance. For a pure PyTorch solution, we can still optimize the inner loop.\n\nThe current loop:\n```python\n    for i in range(num_layers):\n        for g in range(num_groups):\n            available_packs = pack_counts[i] < groups_per_pack\n            if not available_packs.any():\n                continue\n\n            current_pack_loads = pack_loads[i][available_packs]\n            current_pack_indices = torch.arange(num_packs, device=device)[available_packs]\n\n            min_load_idx_in_available = torch.argmin(current_pack_loads)\n            chosen_pack_idx = current_pack_indices[min_load_idx_in_available]\n\n            original_group_idx = sorted_indices[i, g]\n\n            pack_index[i, original_group_idx] = chosen_pack_idx\n            rank_in_pack[i, original_group_idx] = pack_counts[i, chosen_pack_idx]\n\n            pack_loads[i, chosen_pack_idx] += weight[i, original_group_idx]\n            pack_counts[i, chosen_pack_idx] += 1\n```\nThis loop has a nested structure that might not be fully vectorized. The `torch.argmin` on `current_pack_loads` is an operation per item.\n\nA more efficient approach would be to directly assign items in batches if possible, or use a method that avoids repeated minimum finding.\n\nConsider the \"Longest Processing Time\" (LPT) heuristic for bin packing, which is similar to what's implemented. Assigning items greedily to the least loaded bin. The current approach is a form of LPT.\n\n**Proposed Change:**\nWhile a full heap implementation is complex in pure PyTorch, we can try to optimize the selection of `chosen_pack_idx`.\nInstead of filtering `available_packs` and then finding `argmin`, we can use `torch.topk` or `torch.sort` on `pack_loads[i]` directly if `pack_counts[i]` is not a strict constraint, or if we can handle the `groups_per_pack` constraint more efficiently.\n\nHowever, the constraint `pack_counts[i] < groups_per_pack` makes direct vectorization tricky. The current approach is reasonable for clarity. Let's focus on the complexity of `rebalance_experts_hierarchical` first, as it's the larger performance bottleneck and source of errors. If performance is still an issue after fixing the hierarchical logic, `balanced_packing` would be the next target.\n\nGiven the current context, and the identified error in `rebalance_experts_hierarchical`, focusing on that fix is the priority. If `balanced_packing` needs optimization, it would be a separate, more involved change. For now, let's assume the current `balanced_packing` is acceptable if the hierarchical logic is correct.\n\n### Improvement 4: Streamline Global Balancing Logic\n\nThe `else` block in `rebalance_experts` calls `rebalance_experts_hierarchical` with `num_nodes=1, num_groups=1`. This implies that the hierarchical logic should gracefully handle these edge cases. The current implementation of `rebalance_experts_hierarchical` seems to assume `num_groups` is divisible by `num_nodes` and `num_gpus` is divisible by `num_nodes`. If `num_nodes=1` and `num_groups=1`, then `num_groups % num_nodes == 0` holds. So, the hierarchical path is taken.\n\nThe logic within the hierarchical path (when `num_groups % num_nodes == 0`) correctly handles the intermediate steps of creating \"middle experts\" and then replicating them.\n\nThe global balancing logic in the `else` block (when `num_groups % num_nodes != 0`) directly calls `rebalance_experts_hierarchical` with `num_nodes=1, num_groups=1`. This means the global balancing is *also* handled by the hierarchical function, but with specific parameters to simulate global behavior.\n\nThis implies that the `if/else` in `rebalance_experts` is redundant if `rebalance_experts_hierarchical` can correctly handle the `num_nodes=1, num_groups=1` case. Let's verify that.\n\nWhen `num_nodes=1` and `num_groups=1`:\n- `group_size = num_logical_experts // 1 = num_logical_experts`\n- `groups_per_node = 1 // 1 = 1`\n- `balanced_packing(tokens_per_group_layer, num_nodes=1)` will pack all `num_groups` (which is 1) to `num_nodes` (which is 1). This seems okay.\n- `mlog_weights` will be `[num_layers, 1, num_logical_experts]`.\n- `mlog_to_orig_log_map` will be `[num_layers, 1, num_logical_experts]`.\n- `num_middle_experts = num_logical_experts // 1 = num_logical_experts`.\n- `reshaped_mlog_weights` will be `[num_layers, num_logical_experts]`.\n- `replicate_experts` will be called with `X=num_layers`, `num_log=num_logical_experts`, and `num_phy = num_physical_experts // num_gpus`. This seems correct for global balancing.\n\nThe current structure `if num_groups % num_nodes == 0:` then calls `rebalance_experts_hierarchical` with appropriate parameters. The `else` block also calls `rebalance_experts_hierarchical` with simplified parameters. This means `rebalance_experts_hierarchical` is the core logic.\n\nThe issue might be how `rebalance_experts_hierarchical` handles the `num_nodes=1, num_groups=1` case. The current code does not explicitly check for `num_nodes=1` or `num_groups=1` at the start. It relies on the `num_groups % num_nodes == 0` condition. If `num_nodes=1`, this condition is always true.\n\n**Proposed Change:**\nConsolidate the logic. If the hierarchical implementation correctly handles the `num_nodes=1, num_groups=1` case, the `if/else` in `rebalance_experts` can be simplified. The primary goal is to ensure `rebalance_experts_hierarchical` is robust.\n\nGiven the complexity of `rebalance_experts_hierarchical`, the first change (fixing the `rank` error and clarifying returns) is paramount. The second change (simplifying tensor ops in hierarchical logic) is also critical for performance and maintainability. These two changes address the most pressing issues.\n\nFinal check on the proposed changes:\n1.  **Fix `rank` error**: Renaming `phyrank` to `rank_global` and ensuring it's used correctly when constructing `logical_to_physical_map`. This directly addresses the `NameError`.\n2.  **Simplify Hierarchical Logic**: This addresses performance and readability. The proposed refactoring aims to make the tensor manipulations clearer and potentially more efficient.\n\nThese changes should significantly improve the code's correctness and performance.\n\n```python\n# EVOLVE-BLOCK-START\n\n# ... (previous code)\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float().cpu()\n    \n    # Determine if hierarchical or global balancing is needed.\n    # If num_groups is not divisible by num_nodes, we fall back to global balancing.\n    # Global balancing is achieved by treating all experts as a single group across all nodes.\n    # This is simulated by calling rebalance_experts_hierarchical with num_nodes=1 and num_groups=1.\n    if num_groups > 0 and num_groups % num_nodes == 0:\n        # Hierarchical load-balance policy\n        phy2log, rank_global, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # Global load-balance policy: equivalent to hierarchical with num_nodes=1, num_groups=1.\n        phy2log, rank_global, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n\n    # `phy2log`: [num_layers, num_replicas] -> maps physical expert index to logical expert ID.\n    # `rank_global`: [num_layers, num_replicas] -> maps physical expert index to its replica rank.\n    # `logcnt`: [num_layers, num_logical_experts] -> number of replicas for each logical expert.\n\n    # Construct `logical_to_physical_map`: [layers, num_logical_experts, X]\n    # where X is the maximum number of replicas for any logical expert.\n\n    device = weight.device \n\n    # Calculate max_replicas_per_expert from logcnt.\n    max_replicas_per_expert = torch.max(logcnt).item() if logcnt.numel() > 0 else 0\n\n    # Handle edge case: if there are logical experts and replicas, but logcnt is all zeros (e.g., no balancing occurred or empty input).\n    # In such cases, ensure max_replicas_per_expert is at least 1 if replicas are expected.\n    if num_logical_experts > 0 and num_replicas > 0 and max_replicas_per_expert == 0:\n        max_replicas_per_expert = 1\n    \n    # Initialize the map with -1 (indicating no physical expert assigned).\n    logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert),\n                                         -1, dtype=torch.int64, device=device)\n\n    # Create physical expert IDs corresponding to the indices of phy2log/rank.\n    physical_expert_ids = torch.arange(num_replicas, dtype=torch.int64, device=device)\n    \n    # Expand physical_expert_ids to match the shape of phy2log and rank for assignment.\n    physical_expert_ids_expanded = physical_expert_ids.unsqueeze(0).expand(num_layers, num_replicas)\n\n    # Create layer indices for advanced indexing.\n    layer_indices = torch.arange(num_layers, dtype=torch.int64, device=device).unsqueeze(1)\n    \n    # Flatten indices and values for masked assignment to avoid out-of-bounds errors.\n    flat_layer_indices = layer_indices.flatten()\n    flat_phy2log = phy2log.flatten()\n    flat_rank = rank_global.flatten() # Use rank_global here, as 'rank' was undefined.\n    flat_physical_ids = physical_expert_ids_expanded.flatten()\n    \n    # Create a mask for valid assignments:\n    # `phy2log` must be a valid logical expert index.\n    # `rank_global` must be a valid replica rank index for the map's last dimension.\n    valid_mask_flat = (flat_phy2log >= 0) & (flat_phy2log < num_logical_experts) & \\\n                      (flat_rank >= 0) & (flat_rank < max_replicas_per_expert)\n    \n    # Perform the assignment only for valid indices.\n    if valid_mask_flat.any():\n        logical_to_physical_map[\n            flat_layer_indices[valid_mask_flat],\n            flat_phy2log[valid_mask_flat],\n            flat_rank[valid_mask_flat]\n        ] = flat_physical_ids[valid_mask_flat]\n\n    return phy2log, logical_to_physical_map, logcnt\n\n# EVOLVE-BLOCK-END\n```\n\nThe second change is more involved and requires careful refactoring. I'll focus on providing the diff for the first change as it directly addresses the error and is a clear improvement. The second change is a significant refactoring that might require more extensive testing.\n\nHere is the diff for the first improvement:\n<<<<<<< SEARCH\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    # The `rebalance_experts_hierarchical` and the global balancing logic\n    # now produce `phy2log_global`, `rank_global`, and `expert_count`.\n    # `phy2log_global`: [num_layers, num_replicas] -> maps physical expert index to logical expert ID.\n    # `rank_global`: [num_layers, num_replicas] -> maps physical expert index to its replica rank.\n    # `expert_count`: [num_layers, num_logical_experts] -> number of replicas for each logical expert.\n\n    # We need to construct `logical_to_physical_map`: [layers, num_logical_experts, X]\n    # where X is the maximum number of replicas for any logical expert.\n\n    # Store device for later use.\n    device = weight.device \n\n    num_physical_experts_total = num_replicas\n\n    # Correctly calculate max_replicas_per_expert from logcnt.\n    # This is the maximum number of replicas any single logical expert can have.\n    max_replicas_per_expert = torch.max(logcnt).item() if logcnt.numel() > 0 else 0\n\n    # Handle edge case where logcnt might be all zeros or empty.\n    # If there are logical experts and replicas, ensure max_replicas_per_expert is at least 1.\n    if num_logical_experts > 0 and num_replicas > 0 and max_replicas_per_expert == 0:\n        max_replicas_per_expert = 1\n    \n    # Initialize the map with -1 (indicating no physical expert assigned).\n    # The size of the last dimension is `max_replicas_per_expert`.\n    logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert),\n                                         -1, dtype=torch.int64, device=device)\n\n    # Create physical expert IDs corresponding to the indices of phy2log/rank.\n    # These are the global indices of the physical experts.\n    physical_expert_ids = torch.arange(num_replicas, dtype=torch.int64, device=device)\n    \n    # Expand physical_expert_ids to match the shape of phy2log and rank for broadcasting assignment.\n    # `phy2log` and `rank` are [num_layers, num_replicas].\n    # `physical_expert_ids` needs to be [num_layers, num_replicas] for direct assignment.\n    physical_expert_ids_expanded = physical_expert_ids.unsqueeze(0).expand(num_layers, num_replicas)\n\n    # Create layer indices for advanced indexing.\n    # `layer_indices` will be [num_layers, 1] and broadcasts to [num_layers, num_replicas].\n    layer_indices = torch.arange(num_layers, dtype=torch.int64, device=device).unsqueeze(1)\n    \n    # Flatten indices and values for masked assignment to avoid out-of-bounds errors.\n    flat_layer_indices = layer_indices.flatten()\n    flat_phy2log = phy2log.flatten()\n    flat_rank = rank.flatten()\n    flat_physical_ids = physical_expert_ids_expanded.flatten()\n    \n    # Create a mask for valid assignments:\n    # `phy2log` must be a valid logical expert index.\n    # `rank` must be a valid replica rank index for the map's last dimension.\n    valid_mask_flat = (flat_phy2log >= 0) & (flat_phy2log < num_logical_experts) & \\\n                      (flat_rank >= 0) & (flat_rank < max_replicas_per_expert)\n    \n    # Perform the assignment only for valid indices.\n    if valid_mask_flat.any():\n        logical_to_physical_map[\n            flat_layer_indices[valid_mask_flat],\n            flat_phy2log[valid_mask_flat],\n            flat_rank[valid_mask_flat]\n        ] = flat_physical_ids[valid_mask_flat]\n\n    return phy2log, logical_to_physical_map, logcnt\n=======\n    # Determine if hierarchical or global balancing is needed.\n    # If num_groups is not divisible by num_nodes, we fall back to global balancing.\n    # Global balancing is achieved by treating all experts as a single group across all nodes.\n    # This is simulated by calling rebalance_experts_hierarchical with num_nodes=1 and num_groups=1.\n    if num_groups > 0 and num_groups % num_nodes == 0:\n        # Hierarchical load-balance policy\n        phy2log, rank_global, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # Global load-balance policy: equivalent to hierarchical with num_nodes=1, num_groups=1.\n        phy2log, rank_global, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n\n    # `phy2log`: [num_layers, num_replicas] -> maps physical expert index to logical expert ID.\n    # `rank_global`: [num_layers, num_replicas] -> maps physical expert index to its replica rank.\n    # `logcnt`: [num_layers, num_logical_experts] -> number of replicas for each logical expert.\n\n    # Construct `logical_to_physical_map`: [layers, num_logical_experts, X]\n    # where X is the maximum number of replicas for any logical expert.\n\n    device = weight.device \n\n    # Calculate max_replicas_per_expert from logcnt.\n    max_replicas_per_expert = torch.max(logcnt).item() if logcnt.numel() > 0 else 0\n\n    # Handle edge case: if there are logical experts and replicas, but logcnt is all zeros (e.g., no balancing occurred or empty input).\n    # In such cases, ensure max_replicas_per_expert is at least 1 if replicas are expected.\n    if num_logical_experts > 0 and num_replicas > 0 and max_replicas_per_expert == 0:\n        max_replicas_per_expert = 1\n    \n    # Initialize the map with -1 (indicating no physical expert assigned).\n    logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert),\n                                         -1, dtype=torch.int64, device=device)\n\n    # Create physical expert IDs corresponding to the indices of phy2log/rank.\n    physical_expert_ids = torch.arange(num_replicas, dtype=torch.int64, device=device)\n    \n    # Expand physical_expert_ids to match the shape of phy2log and rank for assignment.\n    physical_expert_ids_expanded = physical_expert_ids.unsqueeze(0).expand(num_layers, num_replicas)\n\n    # Create layer indices for advanced indexing.\n    layer_indices = torch.arange(num_layers, dtype=torch.int64, device=device).unsqueeze(1)\n    \n    # Flatten indices and values for masked assignment to avoid out-of-bounds errors.\n    flat_layer_indices = layer_indices.flatten()\n    flat_phy2log = phy2log.flatten()\n    flat_rank = rank_global.flatten() # Use rank_global here, as 'rank' was undefined.\n    flat_physical_ids = physical_expert_ids_expanded.flatten()\n    \n    # Create a mask for valid assignments:\n    # `phy2log` must be a valid logical expert index.\n    # `rank_global` must be a valid replica rank index for the map's last dimension.\n    valid_mask_flat = (flat_phy2log >= 0) & (flat_phy2log < num_logical_experts) & \\\n                      (flat_rank >= 0) & (flat_rank < max_replicas_per_expert)\n    \n    # Perform the assignment only for valid indices.\n    if valid_mask_flat.any():\n        logical_to_physical_map[\n            flat_layer_indices[valid_mask_flat],\n            flat_phy2log[valid_mask_flat],\n            flat_rank[valid_mask_flat]\n        ] = flat_physical_ids[valid_mask_flat]\n\n    return phy2log, logical_to_physical_map, logcnt\n>>>>>>> REPLACE"]}}}