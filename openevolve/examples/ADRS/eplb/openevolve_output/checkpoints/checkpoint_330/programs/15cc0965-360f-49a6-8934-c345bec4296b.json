{"id": "15cc0965-360f-49a6-8934-c345bec4296b", "code": "# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        # Optimized: Directly assign pack_index and rank_in_pack for this simple case.\n        pack_index = torch.arange(num_groups,\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(num_layers, num_groups)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort groups by weight in descending order.\n    # 'weight' is already float and on CPU from rebalance_experts.\n    sorted_weights, sorted_indices = weight.sort(-1, descending=True)\n\n    # Initialize pack_index and rank_in_pack.\n    pack_index = torch.empty_like(weight, dtype=torch.int64)\n    rank_in_pack = torch.empty_like(pack_index)\n    \n    # Initialize tensors to hold pack weights and item counts for all layers.\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Create row indices for advanced indexing, used repeatedly in the loop\n    row_indices = torch.arange(num_layers, device=weight.device)\n\n    # Iterate through sorted group ranks (k-th heaviest group for each layer)\n    # This loop replaces the outer Python loop over num_layers and inner heap operations,\n    # processing all layers simultaneously using vectorized PyTorch operations.\n    for k in range(num_groups):\n        # Get the k-th heaviest group's original ID and weight for each layer\n        current_group_weights_across_layers = sorted_weights[:, k] # [num_layers]\n        original_group_ids_across_layers = sorted_indices[:, k] # [num_layers]\n\n        # Identify full packs for each layer\n        mask_full_packs = (pack_items_per_layer == groups_per_pack)\n        \n        # Set weights of full packs to a very large number so they are not chosen by torch.min\n        temp_pack_weights_for_min = torch.where(mask_full_packs, \n                                                torch.full_like(pack_weights_per_layer, float('inf')), \n                                                pack_weights_per_layer)\n\n        # Find the pack with the minimum current weight for each layer\n        _, chosen_pack_ids_per_layer = torch.min(temp_pack_weights_for_min, dim=1) # [num_layers], [num_layers]\n\n        # Assign the group to the chosen pack using advanced indexing\n        pack_index[row_indices, original_group_ids_across_layers] = chosen_pack_ids_per_layer\n        rank_in_pack[row_indices, original_group_ids_across_layers] = pack_items_per_layer[row_indices, chosen_pack_ids_per_layer]\n\n        # Update pack weights and item counts for the chosen packs using advanced indexing\n        pack_weights_per_layer[row_indices, chosen_pack_ids_per_layer] += current_group_weights_across_layers\n        pack_items_per_layer[row_indices, chosen_pack_ids_per_layer] += 1\n        \n    return pack_index, rank_in_pack\n\n\n# Removed 'import heapq' as it is no longer used after vectorization.\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized. Optimized using a min-heap.\n\n    Parameters:\n        weight: [X, num_log]  (e.g., [num_layers * num_nodes, num_logical_experts_per_node])\n        num_phy: total number of experts after replication (e.g., num_physical_experts_per_node)\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank (0-indexed)\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    # Ensure num_phy is at least num_log, as each logical expert should get at least one physical expert\n    assert num_phy >= num_log\n    device = weight.device\n\n    # Initialize output tensors\n    # phy2log: maps physical expert index to logical expert index\n    # rank: maps physical expert index to its replica rank (0-indexed)\n    phy2log = torch.empty((n, num_phy), dtype=torch.int64, device=device)\n    rank = torch.empty((n, num_phy), dtype=torch.int64, device=device)\n    \n    # Keep track of the current number of replicas for each logical expert\n    current_logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    \n    # Initialize scores for each logical expert. Score = original_weight / current_replica_count.\n    # We want to greedily pick the expert with the highest score (most load per current replica) to replicate next.\n    current_scores = weight / current_logcnt.float()\n\n    # Create row indices for advanced indexing, used repeatedly in the loop\n    row_indices = torch.arange(n, device=device)\n\n    # Fill the physical experts by greedily picking the logical expert with the highest current score.\n    # This loop replaces the outer Python loop over 'n' and inner heap operations,\n    # processing all rows simultaneously using vectorized PyTorch operations.\n    for k in range(num_phy): # Iterate from 0 to num_phy-1 for physical expert index\n        # Find the logical expert with the highest current score for each row\n        # best_log_indices: [n] contains the index of the best logical expert for each row\n        _, best_log_indices = torch.max(current_scores, dim=1)\n        \n        # Assign this physical expert to the chosen logical expert for the current row.\n        phy2log[:, k] = best_log_indices\n        \n        # The rank is 0-indexed. It's the current count of replicas *before* incrementing.\n        rank[:, k] = current_logcnt[row_indices, best_log_indices] - 1 \n\n        # Increment the replica count for the chosen logical experts\n        current_logcnt[row_indices, best_log_indices] += 1\n        \n        # Update the scores for the chosen logical experts for the next iteration.\n        # The new score is original_weight / new_count.\n        original_weights_chosen = weight[row_indices, best_log_indices]\n        new_counts_chosen = current_logcnt[row_indices, best_log_indices]\n        current_scores[row_indices, best_log_indices] = original_weights_chosen / new_counts_chosen.float()\n\n    # The `current_logcnt` tensor now holds the final number of replicas for each logical expert.\n    return phy2log, rank, current_logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    # Ensure weight is float type for numerical stability in calculations.\n    # Keep it on its original device (likely GPU) for performance.\n    weight = weight.float()\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    # The `phyrank` (which is `pphyrank` here) represents the replica index\n    # for a logical expert within the set of experts assigned to a node.\n    # Its maximum value is determined by the `num_phy` parameter passed to `replicate_experts`.\n    # In `rebalance_experts_hierarchical`:\n    # - If hierarchical path (`num_groups % num_nodes == 0`), `num_phy` is `num_replicas // num_nodes`.\n    # - If global path (`else` branch), `num_nodes` is effectively 1, so `num_phy` is `num_replicas // 1`.\n    # Therefore, `max_log_replicas` should be `num_replicas // effective_num_nodes`.\n    \n    effective_num_nodes = num_nodes if num_groups % num_nodes == 0 else 1\n    max_log_replicas = num_replicas // effective_num_nodes\n\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, max_log_replicas),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    # The scatter_ operation needs a flat index.\n    # The index is logical_expert_id * max_log_replicas + phyrank.\n    # The maximum index will be (num_logical_experts - 1) * max_log_replicas + (max_log_replicas - 1)\n    # which equals num_logical_experts * max_log_replicas - 1.\n    # So the flattened size of log2phy must be num_logical_experts * max_log_replicas.\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * max_log_replicas + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n", "language": "python", "parent_id": "858b57f4-2c32-4e71-9eb7-15a1fb9c0e95", "generation": 8, "timestamp": 1751061319.2910268, "iteration_found": 180, "metrics": {"balancedness_score": 0.007070680001776291, "speed_score": 0.07979674119975817, "combined_score": 0.043433710600767235}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace weight = weight.float().cpu() with 3 lines\nChange 2: Replace 21 lines with 28 lines", "parent_metrics": {"balancedness_score": 0.007070678491580371, "speed_score": 0.08026608588297647, "combined_score": 0.04366838218727842}, "island": 3}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve the Mixture-of-Expert models Expert Parallelism Load Balancer (MoE EPLB) expert rearrangement algorithm.\nThis algorithm will take the load metrics recorded by the vLLM server, and rearrange the experts to balance the load. It can make replicas of some experts to achieve better load balancing.\nYour goal will be two-fold: 1. Improve the algorithm to achieve better load balancing; while 2. Improve the algorithm to be more efficient, i.e. reduce the execution time of the algorithm itself, since perfect load balancing is NP-hard.\nThe current algorithm is implemented in the `rebalance_experts` function. ", "user": "# Current Program Information\n- Current performance metrics: - balancedness_score: 0.0071\n- speed_score: 0.0803\n- combined_score: 0.0437\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: balancedness_score, speed_score, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0857, combined_score: 0.1920\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1097, combined_score: 0.2312\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1103, combined_score: 0.2315\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.2315)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1103), Performs well on combined_score (0.2315)\n\n\n### Program 2 (Score: 0.2312)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1097), Performs well on combined_score (0.2312)\n\n\n### Program 3 (Score: 0.1920)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0857), Performs well on combined_score (0.1920)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.1909)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n\n### Program D2 (Score: 0.1914)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.2315, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 32 lines with 32 lines, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 2 (Score: 0.0939, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 23 lines with 236 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 3 (Score: 0.0939, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 23 lines with 236 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 4 (Score: 0.1920, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 5 (Score: 0.0939, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 23 lines with 236 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n\n# Current Program\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        # Optimized: Directly assign pack_index and rank_in_pack for this simple case.\n        pack_index = torch.arange(num_groups,\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(num_layers, num_groups)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort groups by weight in descending order.\n    # 'weight' is already float and on CPU from rebalance_experts.\n    sorted_weights, sorted_indices = weight.sort(-1, descending=True)\n\n    # Initialize pack_index and rank_in_pack.\n    pack_index = torch.empty_like(weight, dtype=torch.int64)\n    rank_in_pack = torch.empty_like(pack_index)\n    \n    # Initialize tensors to hold pack weights and item counts for all layers.\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Create row indices for advanced indexing, used repeatedly in the loop\n    row_indices = torch.arange(num_layers, device=weight.device)\n\n    # Iterate through sorted group ranks (k-th heaviest group for each layer)\n    # This loop replaces the outer Python loop over num_layers and inner heap operations,\n    # processing all layers simultaneously using vectorized PyTorch operations.\n    for k in range(num_groups):\n        # Get the k-th heaviest group's original ID and weight for each layer\n        current_group_weights_across_layers = sorted_weights[:, k] # [num_layers]\n        original_group_ids_across_layers = sorted_indices[:, k] # [num_layers]\n\n        # Identify full packs for each layer\n        mask_full_packs = (pack_items_per_layer == groups_per_pack)\n        \n        # Set weights of full packs to a very large number so they are not chosen by torch.min\n        temp_pack_weights_for_min = torch.where(mask_full_packs, \n                                                torch.full_like(pack_weights_per_layer, float('inf')), \n                                                pack_weights_per_layer)\n\n        # Find the pack with the minimum current weight for each layer\n        _, chosen_pack_ids_per_layer = torch.min(temp_pack_weights_for_min, dim=1) # [num_layers], [num_layers]\n\n        # Assign the group to the chosen pack using advanced indexing\n        pack_index[row_indices, original_group_ids_across_layers] = chosen_pack_ids_per_layer\n        rank_in_pack[row_indices, original_group_ids_across_layers] = pack_items_per_layer[row_indices, chosen_pack_ids_per_layer]\n\n        # Update pack weights and item counts for the chosen packs using advanced indexing\n        pack_weights_per_layer[row_indices, chosen_pack_ids_per_layer] += current_group_weights_across_layers\n        pack_items_per_layer[row_indices, chosen_pack_ids_per_layer] += 1\n        \n    return pack_index, rank_in_pack\n\n\n# Removed 'import heapq' as it is no longer used after vectorization.\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized. Optimized using a min-heap.\n\n    Parameters:\n        weight: [X, num_log]  (e.g., [num_layers * num_nodes, num_logical_experts_per_node])\n        num_phy: total number of experts after replication (e.g., num_physical_experts_per_node)\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank (0-indexed)\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    # Ensure num_phy is at least num_log, as each logical expert should get at least one physical expert\n    assert num_phy >= num_log\n    device = weight.device\n\n    # Initialize output tensors\n    # phy2log: maps physical expert index to logical expert index\n    # rank: maps physical expert index to its replica rank (0-indexed)\n    phy2log = torch.empty((n, num_phy), dtype=torch.int64, device=device)\n    rank = torch.empty((n, num_phy), dtype=torch.int64, device=device)\n    \n    # Keep track of the current number of replicas for each logical expert\n    current_logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    \n    # Initialize scores for each logical expert. Score = original_weight / current_replica_count.\n    # We want to greedily pick the expert with the highest score (most load per current replica) to replicate next.\n    current_scores = weight / current_logcnt.float()\n\n    # Create row indices for advanced indexing, used repeatedly in the loop\n    row_indices = torch.arange(n, device=device)\n\n    # Fill the physical experts by greedily picking the logical expert with the highest current score.\n    # This loop replaces the outer Python loop over 'n' and inner heap operations,\n    # processing all rows simultaneously using vectorized PyTorch operations.\n    for k in range(num_phy): # Iterate from 0 to num_phy-1 for physical expert index\n        # Find the logical expert with the highest current score for each row\n        # best_log_indices: [n] contains the index of the best logical expert for each row\n        _, best_log_indices = torch.max(current_scores, dim=1)\n        \n        # Assign this physical expert to the chosen logical expert for the current row.\n        phy2log[:, k] = best_log_indices\n        \n        # The rank is 0-indexed. It's the current count of replicas *before* incrementing.\n        rank[:, k] = current_logcnt[row_indices, best_log_indices] - 1 \n\n        # Increment the replica count for the chosen logical experts\n        current_logcnt[row_indices, best_log_indices] += 1\n        \n        # Update the scores for the chosen logical experts for the next iteration.\n        # The new score is original_weight / new_count.\n        original_weights_chosen = weight[row_indices, best_log_indices]\n        new_counts_chosen = current_logcnt[row_indices, best_log_indices]\n        current_scores[row_indices, best_log_indices] = original_weights_chosen / new_counts_chosen.float()\n\n    # The `current_logcnt` tensor now holds the final number of replicas for each logical expert.\n    return phy2log, rank, current_logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float().cpu()\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    # Fix: The `maxlogcnt` for `log2phy` must accommodate the maximum possible\n    # replica rank, which can be `num_replicas - 1`.\n    # Therefore, the third dimension of `log2phy` should be `num_replicas`.\n    max_log_replicas = num_replicas \n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, max_log_replicas),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    # The scatter_ operation needs a flat index.\n    # The index is logical_expert_id * max_log_replicas + phyrank.\n    # The maximum index will be (num_logical_experts - 1) * max_log_replicas + (max_log_replicas - 1)\n    # which equals num_logical_experts * max_log_replicas - 1.\n    # So the flattened size of log2phy must be num_logical_experts * max_log_replicas.\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * max_log_replicas + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["The current performance metrics show a significant regression, especially in `balancedness_score` (0.0071 compared to previous highs of 0.3528) and `speed_score`. This suggests either a critical performance bottleneck or a correctness issue in the algorithm's output that negatively impacts the balancing metric.\n\nUpon reviewing the code, two primary areas for improvement stand out:\n\n1.  **Performance Bottleneck: Unnecessary CPU Transfer.**\n    The line `weight = weight.float().cpu()` in `rebalance_experts` explicitly moves the `weight` tensor to the CPU. If the input `weight` tensor is originally on a GPU (which is typical for vLLM and deep learning workloads), this transfer introduces a significant overhead. All subsequent PyTorch operations within `balanced_packing` and `replicate_experts` would then operate on the CPU, which is generally much slower for tensor computations than a GPU. Removing this `.cpu()` call will ensure that computations remain on the device where the `weight` tensor originates, leveraging GPU acceleration if available, thus directly improving `speed_score`. The `float()` conversion is retained to ensure numerical stability for load balancing calculations.\n\n2.  **Correctness and Balancedness: `log2phy` Tensor Sizing.**\n    The `log2phy` tensor, which provides the inverse mapping from logical experts to their physical replicas, is crucial for evaluating load balance and for subsequent use by the vLLM system.\n    The `phyrank` (specifically `pphyrank` in `rebalance_experts`) represents the replica rank of a logical expert within a certain scope (either per node or globally, depending on the hierarchical policy). This `phyrank` value determines the index within the third dimension of `log2phy`.\n    The `replicate_experts` function, which determines these ranks, operates on a subset of physical experts: `num_physical_experts // num_nodes` (in the hierarchical case) or `num_physical_experts` (in the global case where `num_nodes` is effectively 1). Therefore, the maximum possible `phyrank` value for any logical expert is `(num_physical_experts // num_nodes) - 1` (or `num_physical_experts - 1` for the global case).\n    The current code sets `max_log_replicas = num_replicas`. If `num_nodes > 1`, `num_replicas` (total physical experts) is much larger than `num_physical_experts // num_nodes`. This leads to `log2phy` being allocated with an unnecessarily large third dimension. While this might not cause a crash, it can lead to:\n    *   **Increased Memory Usage:** Wasting memory on unused tensor parts.\n    *   **Reduced Performance:** `scatter_` operation might be slower on a larger tensor.\n    *   **Incorrect `balancedness_score` Evaluation:** If the evaluation metric or downstream components rely on the precise structure or density of `log2phy`, an incorrectly sized or sparse `log2phy` could lead to misinterpretations and a degraded `balancedness_score`. By correctly sizing `max_log_replicas` based on the effective number of nodes, we ensure `log2phy` accurately represents the expert assignments.\n\nThese two changes are targeted and address the most likely causes for the observed performance regression.\n\n```python\n<<<<<<< SEARCH\n    weight = weight.float().cpu()\n=======\n    # Ensure weight is float type for numerical stability in calculations.\n    # Keep it on its original device (likely GPU) for performance.\n    weight = weight.float()\n>>>>>>> REPLACE\n<<<<<<< SEARCH\n    # Fix: The `maxlogcnt` for `log2phy` must accommodate the maximum possible\n    # replica rank, which can be `num_replicas - 1`.\n    # Therefore, the third dimension of `log2phy` should be `num_replicas`.\n    max_log_replicas = num_replicas \n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, max_log_replicas),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    # The scatter_ operation needs a flat index.\n    # The index is logical_expert_id * max_log_replicas + phyrank.\n    # The maximum index will be (num_logical_experts - 1) * max_log_replicas + (max_log_replicas - 1)\n    # which equals num_logical_experts * max_log_replicas - 1.\n    # So the flattened size of log2phy must be num_logical_experts * max_log_replicas.\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * max_log_replicas + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n=======\n    # The `phyrank` (which is `pphyrank` here) represents the replica index\n    # for a logical expert within the set of experts assigned to a node.\n    # Its maximum value is determined by the `num_phy` parameter passed to `replicate_experts`.\n    # In `rebalance_experts_hierarchical`:\n    # - If hierarchical path (`num_groups % num_nodes == 0`), `num_phy` is `num_replicas // num_nodes`.\n    # - If global path (`else` branch), `num_nodes` is effectively 1, so `num_phy` is `num_replicas // 1`.\n    # Therefore, `max_log_replicas` should be `num_replicas // effective_num_nodes`.\n    \n    effective_num_nodes = num_nodes if num_groups % num_nodes == 0 else 1\n    max_log_replicas = num_replicas // effective_num_nodes\n\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, max_log_replicas),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    # The scatter_ operation needs a flat index.\n    # The index is logical_expert_id * max_log_replicas + phyrank.\n    # The maximum index will be (num_logical_experts - 1) * max_log_replicas + (max_log_replicas - 1)\n    # which equals num_logical_experts * max_log_replicas - 1.\n    # So the flattened size of log2phy must be num_logical_experts * max_log_replicas.\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * max_log_replicas + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n>>>>>>> REPLACE\n```"]}}}