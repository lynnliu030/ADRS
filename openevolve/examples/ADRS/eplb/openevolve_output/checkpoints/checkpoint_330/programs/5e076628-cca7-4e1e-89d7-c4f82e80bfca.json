{"id": "5e076628-cca7-4e1e-89d7-c4f82e80bfca", "code": "# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    indices = weight.float().sort(-1, descending=True).indices\n    pack_index = torch.full_like(weight,\n                                 fill_value=-1,\n                                 dtype=torch.int64,\n                                 device=weight.device)\n    rank_in_pack = torch.full_like(pack_index, fill_value=-1, device=weight.device)\n    # Initialize tensors to hold pack weights and item counts for all layers.\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Use JIT-compiled helper for the main packing loop for performance.\n    pack_index, rank_in_pack = _balanced_packing_jit_helper(\n        weight, num_packs, num_layers, num_groups, groups_per_pack,\n        indices, pack_index, rank_in_pack, pack_weights_per_layer, pack_items_per_layer\n    )\n    return pack_index, rank_in_pack\n\n@torch.jit.script\ndef _balanced_packing_jit_helper(\n    weight_tensor: torch.Tensor,\n    num_packs: int,\n    num_layers: int,\n    num_groups: int,\n    groups_per_pack: int,\n    indices_tensor: torch.Tensor,\n    pack_index: torch.Tensor,\n    rank_in_pack: torch.Tensor,\n    pack_weights_per_layer: torch.Tensor,\n    pack_items_per_layer: torch.Tensor\n) -> tuple[torch.Tensor, torch.Tensor]:\n    for i in range(num_layers):\n        current_layer_indices = indices_tensor[i]\n        for k in range(num_groups):\n            group_id = current_layer_indices[k]\n\n            current_pack_weights = pack_weights_per_layer[i]\n            current_pack_items = pack_items_per_layer[i]\n\n            available_mask = (current_pack_items < groups_per_pack)\n            candidate_weights_temp = torch.where(available_mask, current_pack_weights, float('inf'))\n            \n            pack = torch.argmin(candidate_weights_temp)\n\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            pack_weights_per_layer[i, pack] += weight_tensor[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        # To minimize the maximum load, we should add replicas to the\n        # logical experts with the minimum current average load.\n        # Reverting to original behavior as suggested by performance metrics.\n        # This strategy aims to reduce the load of the currently most loaded experts.\n        # Reverting to .max as suggested by historical performance data.\n        # This aims to distribute replicas to experts that are currently most loaded.\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float().cpu()\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n", "language": "python", "parent_id": "aa17b780-32f8-4429-9385-82a1c44802eb", "generation": 5, "timestamp": 1751050832.1902606, "iteration_found": 20, "metrics": {"balancedness_score": 0.29830835153138974, "speed_score": 0.02626057525548571, "combined_score": 0.16228446339343772}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 2 lines with 3 lines\nChange 2: Replace 6 lines with 8 lines", "parent_metrics": {"balancedness_score": 0.14515957884623945, "speed_score": 0.029323714000434098, "combined_score": 0.08724164642333677}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve the Mixture-of-Expert models Expert Parallelism Load Balancer (MoE EPLB) expert rearrangement algorithm.\nThis algorithm will take the load metrics recorded by the vLLM server, and rearrange the experts to balance the load. It can make replicas of some experts to achieve better load balancing.\nYour goal will be two-fold: 1. Improve the algorithm to achieve better load balancing; while 2. Improve the algorithm to be more efficient, i.e. reduce the execution time of the algorithm itself, since perfect load balancing is NP-hard.\nThe current algorithm is implemented in the `rebalance_experts` function. ", "user": "# Current Program Information\n- Current performance metrics: - balancedness_score: 0.1452\n- speed_score: 0.0293\n- combined_score: 0.0872\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: balancedness_score, speed_score, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0340, combined_score: 0.1661\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0350, combined_score: 0.1667\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0405, combined_score: 0.1694\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.1694)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0405), Performs well on combined_score (0.1694)\n\n\n### Program 2 (Score: 0.1667)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0350), Performs well on combined_score (0.1667)\n\n\n### Program 3 (Score: 0.1661)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0340), Performs well on combined_score (0.1661)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.1614)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n\n### Program D2 (Score: 0.1610)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.1694, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 79 lines with 108 lines\nChange 2: Replace 24 lines with 43 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 2 (Score: 0.0000, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 241 lines with 356 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 3 (Score: 0.0000, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 74 lines with 92 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 4 (Score: 0.1610, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 34 lines with 47 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 5 (Score: 0.1667, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n\n# Current Program\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    indices = weight.float().sort(-1, descending=True).indices\n    pack_index = torch.full_like(weight,\n                                 fill_value=-1,\n                                 dtype=torch.int64,\n                                 device=weight.device)\n    rank_in_pack = torch.full_like(pack_index, fill_value=-1, device=weight.device)\n    # Initialize tensors to hold pack weights and item counts for all layers.\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Use JIT-compiled helper for the main packing loop for performance.\n    pack_index, rank_in_pack = _balanced_packing_jit_helper(\n        weight, num_packs, num_layers, num_groups, groups_per_pack,\n        indices, pack_index, rank_in_pack, pack_weights_per_layer, pack_items_per_layer\n    )\n    return pack_index, rank_in_pack\n\n@torch.jit.script\ndef _balanced_packing_jit_helper(\n    weight_tensor: torch.Tensor,\n    num_packs: int,\n    num_layers: int,\n    num_groups: int,\n    groups_per_pack: int,\n    indices_tensor: torch.Tensor,\n    pack_index: torch.Tensor,\n    rank_in_pack: torch.Tensor,\n    pack_weights_per_layer: torch.Tensor,\n    pack_items_per_layer: torch.Tensor\n) -> tuple[torch.Tensor, torch.Tensor]:\n    for i in range(num_layers):\n        current_layer_indices = indices_tensor[i]\n        for k in range(num_groups):\n            group_id = current_layer_indices[k]\n\n            current_pack_weights = pack_weights_per_layer[i]\n            current_pack_items = pack_items_per_layer[i]\n\n            available_mask = (current_pack_items < groups_per_pack)\n            candidate_weights_temp = torch.where(available_mask, current_pack_weights, float('inf'))\n            \n            pack = torch.argmin(candidate_weights_temp)\n\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            pack_weights_per_layer[i, pack] += weight_tensor[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        # To minimize the maximum load, we should add replicas to the\n        # logical experts with the minimum current average load.\n        # Reverting to original behavior as suggested by performance metrics.\n        # This strategy aims to reduce the load of the currently most loaded experts.\n        # Changed from .max to .min based on performance regression hint.\n        redundant_indices = (weight / logcnt).min(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float().cpu()\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["The current `rebalance_experts` function uses a hierarchical approach when `num_groups % num_nodes == 0`, and falls back to a global approach (which is essentially a simplified hierarchical approach with `num_groups=1`, `num_nodes=1`) otherwise. This fallback might not be optimal.\n\nThe performance metrics show a regression across the board, suggesting that the recent changes might have introduced inefficiencies or suboptimal balancing strategies. Specifically, the `balancedness_score` and `speed_score` have decreased.\n\nHere are some proposed improvements:\n\n1.  **Refine the fallback strategy:** When the condition `num_groups % num_nodes == 0` is not met, the current implementation calls `rebalance_experts_hierarchical` with `num_groups=1`, `num_nodes=1`. This effectively treats all experts as a single group and attempts to balance them globally across all GPUs. A more robust global balancing strategy could be implemented to handle cases where the hierarchical structure doesn't apply cleanly, potentially improving the `balancedness_score`.\n\n2.  **Optimize `replicate_experts`:** The `replicate_experts` function iterates through logical experts to decide where to add replicas. The current logic `(weight / logcnt).min(dim=-1).indices` aims to add replicas to the logical experts with the minimum *current average load*. This greedy approach might not always lead to the most balanced distribution of *total* load across physical experts. A more sophisticated approach could consider the *impact* of adding a replica to a specific logical expert on the overall load distribution. However, given the performance regression hint that suggests reverting to previous behavior related to `.max` vs `.min`, it's crucial to tread carefully. The current implementation uses `.min`, which was stated as a change to address performance regression. Let's assume the regression was *not* due to this change, and focus on other aspects first. If further analysis shows this is the bottleneck, we can revisit.\n\n3.  **Improve `balanced_packing` efficiency:** The `balanced_packing` function uses a JIT-compiled helper. While JIT compilation is good for performance, the inner loop iterates `num_groups` times. If `num_groups` is large, this could still be a bottleneck. We can explore if there are more vectorized operations or alternative algorithms for packing that might be faster. However, without specific profiling data, it's hard to say if this is the primary bottleneck.\n\nLet's focus on refining the fallback strategy first, as it's a clear structural point of potential improvement. The current fallback essentially ignores the `num_groups` information when it's not perfectly divisible by `num_nodes`. A better approach would be to still try to group experts into `num_nodes` effective groups, even if the original `num_groups` don't align perfectly.\n\nThe current `rebalance_experts` function:\n```python\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n```\nThe fallback `rebalance_experts_hierarchical(weight, num_replicas, 1, 1, num_gpus)` means `num_groups` becomes 1 and `num_nodes` becomes 1. This effectively makes the `rebalance_experts_hierarchical` function operate as a global balancer across all `num_gpus`.\n\nA potential improvement would be to still leverage the `num_nodes` information when `num_groups % num_nodes != 0`. We could try to create `num_nodes` \"super-groups\" of experts and then balance within those.\n\nLet's consider a modified `rebalance_experts` that attempts a more granular global balancing when the hierarchical condition isn't met.\n\n**Proposed Change:**\n\nInstead of completely resetting `num_groups` and `num_nodes` to 1 and 1, we can try to balance the experts into `num_nodes` packs first, and then within those packs, balance them across the `num_gpus`. This would still be a form of hierarchical balancing but adapted to the non-divisible case.\n\nHowever, given the performance regression hint and the overall goal to improve *both* balancedness and speed, a simpler approach might be to first ensure the current implementation is as clean as possible and then consider more complex strategies. The prompt also mentions simplifying the code for readability and maintainability.\n\nLet's look at the structure of `rebalance_experts_hierarchical`. It seems to be designed to pack groups to nodes, then replicate experts within nodes, and then pack physical experts to GPUs.\n\nIf `num_groups % num_nodes != 0`, the current fallback uses `num_groups = 1` and `num_nodes = 1`. This means the first step of `rebalance_experts_hierarchical` (packing groups to nodes) is effectively skipped because there's only one \"group\" and one \"node\". The code then proceeds to replicate experts for `num_logical_experts // 1` (i.e., `num_logical_experts`) and then packs these across `num_gpus // 1` (i.e., `num_gpus`).\n\nThis seems to be a valid global strategy, but it might not be as efficient or as balanced as a strategy that *tries* to leverage the server architecture more.\n\nGiven the specific instruction to \"Consider reverting or revising recent changes in these areas\" due to regression, and the fact that the current implementation has `balancedness_score: 0.1452`, `speed_score: 0.0293`, `combined_score: 0.0872`, while the top-performing programs had `combined_score: 0.1694` (with `balancedness_score: 0.2983`, `speed_score: 0.0405`), there's a significant gap. The regression is substantial.\n\nLet's focus on making the existing `rebalance_experts_hierarchical` more robust and potentially simplifying its logic if possible, while ensuring it handles the non-hierarchical case better.\n\nThe core idea of `rebalance_experts_hierarchical` is to leverage the faster intra-node communication. When `num_groups % num_nodes != 0`, we can't perfectly assign `groups_per_node`.\n\nLet's reconsider the `replicate_experts` function. The comment \"Reverting to original behavior as suggested by performance metrics. This strategy aims to reduce the load of the currently most loaded experts. Changed from .max to .min based on performance regression hint.\" is key. If the regression happened *after* changing from `.max` to `.min`, then reverting to `.max` might be the solution. However, the current code *is* using `.min`. This implies the regression might be from *somewhere else*, or that the `.min` strategy itself, while intended to help, caused other issues.\n\nLet's try to address the \"simplify the code to improve readability and maintainability\" aspect first, as it's a direct request. The `rebalance_experts_hierarchical` function has several nested calculations and views. Simplifying these could make it easier to reason about and debug.\n\nOne area for potential simplification and improvement in `rebalance_experts_hierarchical` is in how it constructs the `log2mlog` and `pphy2mlog` mappings. These involve complex reshaping and scattering operations.\n\nLet's focus on the `replicate_experts` function's logic for a moment. The goal is to minimize the *maximum* load. The current approach adds replicas to the logical expert with the minimum *average* load. This is a greedy approach. A common heuristic for minimizing the maximum load is to add a replica to the expert that *currently has the minimum total load* (or, equivalently, the minimum average load if counts are equal). The current code does this by `(weight / logcnt).min(dim=-1).indices`.\n\nIf the regression is indeed linked to `replicate_experts`, and the comment suggests a change from `.max` to `.min`, perhaps the original `.max` was better for some reason, or the `.min` change introduced a subtle bug or a suboptimal strategy in conjunction with other parts.\n\nLet's focus on the fallback mechanism in `rebalance_experts`. The current fallback is to call `rebalance_experts_hierarchical` with `num_groups=1` and `num_nodes=1`. This is a global load balancing.\n\nA potentially better global strategy, if the hierarchical one doesn't fit, is to:\n1.  Calculate the total load per logical expert across all groups.\n2.  Use `replicate_experts` on this global load to determine the number of replicas per logical expert.\n3.  Distribute these replicas across the physical experts.\n\nThe current fallback effectively does this, but it does so by calling the *hierarchical* function with parameters that disable its hierarchy.\n\nLet's propose a change to the `rebalance_experts` function to create a dedicated global balancing function if the hierarchical conditions are not met, or to modify how `rebalance_experts_hierarchical` is called.\n\nConsider the scenario where `num_groups % num_nodes != 0`. The current code calls `rebalance_experts_hierarchical(weight, num_replicas, 1, 1, num_gpus)`. This means `num_groups` becomes 1 and `num_nodes` becomes 1.\nInside `rebalance_experts_hierarchical`:\n- `num_groups` is 1.\n- `group_size` is `num_logical_experts // 1 = num_logical_experts`.\n- `num_nodes` is 1.\n- `groups_per_node` is `1 // 1 = 1`.\n- `num_gpus % num_nodes == 0` is true (e.g., `10 % 1 == 0`).\n- `num_physical_experts % num_gpus == 0` is true.\n- `phy_experts_per_gpu` is `num_physical_experts // num_gpus`.\n\nStep 1: `tokens_per_group = weight.unflatten(-1, (1, num_logical_experts)).sum(-1)` -> `tokens_per_group` becomes `weight`.\n   `group_pack_index, group_rank_in_pack = balanced_packing(weight, 1)` -> Packs all experts into 1 \"node\".\n   `log2mlog = (((group_pack_index * 1 + group_rank_in_pack) * num_logical_experts).unsqueeze(-1) + torch.arange(num_logical_experts, ...)).flatten(-2)` -> This seems to create an identity mapping if `balanced_packing` with `num_packs=1` works as expected.\n   `mlog2log = inverse(log2mlog)` -> Identity mapping.\n\nStep 2: `tokens_per_mlog = weight.gather(-1, mlog2log).view(-1, num_logical_experts // 1)` -> `tokens_per_mlog` is `weight`.\n   `phy2mlog, phyrank, logcnt = replicate_experts(tokens_per_mlog, num_physical_experts // 1)` -> `replicate_experts(weight, num_physical_experts // num_gpus)` is called. This is the core replication step.\n\nStep 3: `tokens_per_phy = (tokens_per_mlog / logcnt).gather(-1, phy2mlog)` -> This seems to use the results from `replicate_experts`.\n   `pack_index, rank_in_pack = balanced_packing(tokens_per_phy, num_gpus // 1)` -> `balanced_packing` is called with `num_packs = num_gpus`. This attempts to pack the replicated experts across GPUs.\n\nThe logic seems sound for a global strategy, but the regression suggests it's not optimal.\n\nLet's propose a change to simplify the fallback and potentially make it more direct if the hierarchical structure doesn't apply.\n\n**Proposal 1: Simplify and potentially refine the global fallback.**\n\nInstead of calling `rebalance_experts_hierarchical` with modified parameters, we could extract the core logic for global balancing. However, the current structure *is* a global balancing strategy when `num_groups=1, num_nodes=1`.\n\nLet's focus on the `replicate_experts` function, as it's a critical part of balancing. The comment about `.max` vs `.min` is a strong hint. If the regression happened *after* changing from `.max` to `.min`, then reverting to `.max` is a prime candidate.\n\nThe current `replicate_experts` uses `(weight / logcnt).min(dim=-1).indices`.\nIf the previous best performance was achieved with `.max`, let's try that.\n\n<<<<<<< SEARCH\n        # Changed from .max to .min based on performance regression hint.\n        redundant_indices = (weight / logcnt).min(dim=-1).indices\n=======\n        # Reverting to .max as suggested by historical performance data.\n        # This aims to distribute replicas to experts that are currently most loaded.\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n>>>>>>> REPLACE\n\n**Reasoning for this change:**\nThe comment in the `replicate_experts` function explicitly mentions a change from `.max` to `.min` and links it to performance regression. While the current code uses `.min`, the history shows that `.max` might have led to better overall scores in previous iterations. The goal of `replicate_experts` is to minimize the maximum load. A greedy strategy to achieve this is to add a replica to the expert that is currently *most overloaded*. The term \"overloaded\" can be interpreted in different ways:\n*   Minimum average load (`.min(weight / logcnt)`): This aims to bring up the least loaded experts.\n*   Maximum average load (`.max(weight / logcnt)`): This aims to reduce the load on the most loaded experts.\n\nGiven the objective is to minimize the *maximum* load, adding to the currently *most loaded* experts is a more direct heuristic. The phrasing \"This strategy aims to reduce the load of the currently most loaded experts. Changed from .max to .min based on performance regression hint\" is a bit contradictory. If the goal is to reduce the load of the most loaded, `.max` makes sense. If the regression was *caused* by changing *to* `.min`, then reverting to `.max` is the logical step. Assuming the latter, this change directly addresses the potential cause of the performance regression.\n\nThis change directly targets the `speed_score` and `balancedness_score` by potentially improving the load distribution during expert replication, which is a core part of the balancing algorithm. It also aligns with the historical performance data suggesting `.max` was better.\n\nLet's also consider simplifying the fallback logic in `rebalance_experts` for better readability and maintainability. The current fallback `rebalance_experts_hierarchical(weight, num_replicas, 1, 1, num_gpus)` is a bit of a hack to force a global strategy. It might be clearer to have an explicit `rebalance_experts_global` function or to adjust the parameters more thoughtfully.\n\nHowever, given the strong hint about the `.max` vs `.min` in `replicate_experts`, that seems like the most critical immediate change to test for performance improvement. If this doesn't yield significant gains, we can explore other structural changes.\n\nLet's stick with the `.max` change for `replicate_experts` as the primary suggestion.\n\nOne minor point of readability: the `rebalance_experts` function converts `weight` to `float().cpu()`. This might be a performance bottleneck if the weight tensor is large and on GPU. If the operations within `rebalance_experts_hierarchical` and `replicate_experts` can handle GPU tensors and mixed precision, this conversion could be avoided or done more selectively. However, without knowing the downstream usage and the exact requirements, it's safer to keep this for now.\n\nLet's re-examine the `replicate_experts` function:\n```python\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    # ...\n    for i in range(num_log, num_phy):\n        # To minimize the maximum load, we should add replicas to the\n        # logical experts with the minimum current average load.\n        # Reverting to original behavior as suggested by performance metrics.\n        # This strategy aims to reduce the load of the currently most loaded experts.\n        # Changed from .max to .min based on performance regression hint.\n        redundant_indices = (weight / logcnt).min(dim=-1).indices\n        # ...\n```\nThe comment is indeed confusing. \"To minimize the maximum load, we should add replicas to the logical experts with the minimum current average load.\" This statement itself is a bit counter-intuitive for minimizing the *maximum* load. Usually, you'd add to the *most* loaded to bring it down. The subsequent sentence \"This strategy aims to reduce the load of the currently most loaded experts.\" contradicts the \"minimum current average load\" statement. The final sentence \"Changed from .max to .min based on performance regression hint\" is the most concrete piece of information. If the regression happened *after* changing from `.max` to `.min`, then reverting to `.max` is the most direct path to potentially recover performance.\n\nLet's assume the intention of the comment was: \"To minimize the maximum load, we should add replicas to the logical experts that are currently most loaded. The previous implementation used `.max` here, which achieved better performance. A recent change to `.min` might have caused a regression.\"\n\nTherefore, the proposed change to use `.max` seems the most aligned with the provided information.\n\n```python\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    indices = weight.float().sort(-1, descending=True).indices\n    pack_index = torch.full_like(weight,\n                                 fill_value=-1,\n                                 dtype=torch.int64,\n                                 device=weight.device)\n    rank_in_pack = torch.full_like(pack_index, fill_value=-1, device=weight.device)\n    # Initialize tensors to hold pack weights and item counts for all layers.\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Use JIT-compiled helper for the main packing loop for performance.\n    pack_index, rank_in_pack = _balanced_packing_jit_helper(\n        weight, num_packs, num_layers, num_groups, groups_per_pack,\n        indices, pack_index, rank_in_pack, pack_weights_per_layer, pack_items_per_layer\n    )\n    return pack_index, rank_in_pack\n\n@torch.jit.script\ndef _balanced_packing_jit_helper(\n    weight_tensor: torch.Tensor,\n    num_packs: int,\n    num_layers: int,\n    num_groups: int,\n    groups_per_pack: int,\n    indices_tensor: torch.Tensor,\n    pack_index: torch.Tensor,\n    rank_in_pack: torch.Tensor,\n    pack_weights_per_layer: torch.Tensor,\n    pack_items_per_layer: torch.Tensor\n) -> tuple[torch.Tensor, torch.Tensor]:\n    for i in range(num_layers):\n        current_layer_indices = indices_tensor[i]\n        for k in range(num_groups):\n            group_id = current_layer_indices[k]\n\n            current_pack_weights = pack_weights_per_layer[i]\n            current_pack_items = pack_items_per_layer[i]\n\n            available_mask = (current_pack_items < groups_per_pack)\n            candidate_weights_temp = torch.where(available_mask, current_pack_weights, float('inf'))\n            \n            pack = torch.argmin(candidate_weights_temp)\n\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            pack_weights_per_layer[i, pack] += weight_tensor[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        # To minimize the maximum load, we should add replicas to the\n        # logical experts with the minimum current average load.\n        # Reverting to original behavior as suggested by performance metrics.\n        # This strategy aims to reduce the load of the currently most loaded experts.\n        # Changed from .max to .min based on performance regression hint.\n        redundant_indices = (weight / logcnt).min(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float().cpu()\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n```\n\nThe diff is applied to the `replicate_experts` function.\n\nI am confident this change will improve performance.\n\n\n```diff\n<<<<<<< SEARCH\n        # To minimize the maximum load, we should add replicas to the\n        # logical experts with the minimum current average load.\n        # Reverting to original behavior as suggested by performance metrics.\n        # This strategy aims to reduce the load of the currently most loaded experts.\n        # Changed from .max to .min based on performance regression hint.\n        redundant_indices = (weight / logcnt).min(dim=-1).indices\n=======\n        # To minimize the maximum load, we should add replicas to the\n        # logical experts with the minimum current average load.\n        # Reverting to original behavior as suggested by performance metrics.\n        # This strategy aims to reduce the load of the currently most loaded experts.\n        # Changed from .max to .min based on performance regression hint.\n        # Based on performance regression, we revert to using .max to add replicas\n        # to the most loaded experts, aiming to reduce the maximum load.\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n>>>>>>> REPLACE\n```"]}}}