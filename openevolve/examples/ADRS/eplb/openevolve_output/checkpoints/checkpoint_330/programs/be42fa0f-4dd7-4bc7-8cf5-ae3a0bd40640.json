{"id": "be42fa0f-4dd7-4bc7-8cf5-ae3a0bd40640", "code": "# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort groups by weight in descending order.\n    # 'weight' is already float and on CPU from rebalance_experts,\n    # so no need for .float().cpu() here.\n    indices = weight.sort(-1, descending=True).indices\n\n    # Initialize pack_index and rank_in_pack.\n    # These will be fully populated, so `empty_like` is sufficient.\n    # They will be on the same device as 'weight' (i.e., CPU).\n    pack_index = torch.empty_like(weight, dtype=torch.int64)\n    rank_in_pack = torch.empty_like(pack_index)\n    \n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These tensors are on the same device as 'weight' (which is CPU).\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id in indices[i]:\n            # Get current state for this layer's packs.\n            current_pack_weights = pack_weights_per_layer[i]\n            current_pack_items = pack_items_per_layer[i]\n\n            # Find available packs (those not yet full).\n            # `available_mask` is a boolean tensor.\n            available_mask = (current_pack_items < groups_per_pack)\n\n            # Find the pack with the minimum current weight among available ones.\n            # This approach avoids creating a large temporary tensor with float('inf').\n            available_pack_indices = torch.nonzero(available_mask).squeeze(-1)\n            \n            # If no available packs, something is wrong with the logic or inputs.\n            # This assert ensures correctness under expected conditions.\n            assert available_pack_indices.numel() > 0, \"No available packs found, but expected to find one.\"\n\n            # Get weights of only the available packs\n            weights_of_available_packs = current_pack_weights[available_pack_indices]\n            \n            # Find the index within the `available_pack_indices` list that has the minimum weight.\n            # Use torch.min to get both the minimum value and its index.\n            min_weight, local_min_idx = torch.min(weights_of_available_packs, dim=0)\n            \n            # Map this local index back to the global pack index.\n            pack = available_pack_indices[local_min_idx]\n\n            # Assert that a valid pack was found and it has space.\n            # This assertion is crucial for correctness but can be removed in production if confident.\n            # assert current_pack_items[pack] < groups_per_pack\n\n            # Assign the group to the chosen pack.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: Construct redundant experts within nodes and map to physical experts\n    # Calculate token counts per logical expert group within each node.\n    # `weight` is [num_layers, num_logical_experts]\n    # `mlog2log` maps logical experts to their groups within a node.\n    # We need to get the weight for logical experts that fall into each group.\n    # `log2mlog` maps logical experts to their allocated physical expert within a node.\n    \n    # First, determine the logical experts that belong to each node.\n    # `group_pack_index` maps each group to a node.\n    # `groups_per_node` is the number of groups per node.\n    # `group_size` is the number of logical experts per group.\n    \n    # Create a mapping from logical expert index to node index.\n    logical_expert_to_node = torch.div(torch.arange(num_logical_experts, device=weight.device), group_size, rounding_mode='floor')\n    logical_expert_to_node = torch.div(logical_expert_to_node, groups_per_node, rounding_mode='floor')\n    \n    # Prepare weights for replication within each node.\n    # We need `num_layers` x `num_nodes` x `num_logical_experts // num_groups`\n    # where each element is the weight of a logical expert within a node.\n    node_logical_weights = torch.zeros(num_layers, num_nodes, num_logical_experts // num_groups, device=weight.device)\n    \n    for layer_idx in range(num_layers):\n        for group_idx in range(num_groups):\n            node_idx = group_idx // groups_per_node\n            start_logical_idx = group_idx * group_size\n            end_logical_idx = start_logical_idx + group_size\n            \n            # Map logical experts to their position within the node's logical experts\n            logical_expert_indices_in_node = torch.arange(group_size, device=weight.device)\n            \n            node_logical_weights[layer_idx, node_idx, logical_expert_indices_in_node] = weight[layer_idx, start_logical_idx:end_logical_idx]\n\n    # Reshape to [num_layers * num_nodes, num_logical_experts // num_groups] for replicate_experts\n    node_logical_weights_flat = node_logical_weights.view(-1, num_logical_experts // num_groups)\n    \n    # Replicate experts within each node.\n    # `phy2mlog` maps physical experts within a node to logical experts within that node.\n    # `mlogcnt` is the count of replicas for each logical expert within a node.\n    phy2mlog_per_node, mlogcnt_per_node, _ = replicate_experts(\n        node_logical_weights_flat, num_physical_experts // num_nodes)\n    \n    # Map physical experts within a node back to their original logical expert IDs.\n    # `phy2log_per_node` will have shape [num_layers * num_nodes, num_physical_experts // num_nodes]\n    # where each entry is the original logical expert ID.\n    \n    # We need to map the `phy2mlog_per_node` indices back to the original logical expert indices.\n    # The `replicate_experts` function returns `phy2log` which is the logical expert ID.\n    # So `phy2mlog_per_node` is already the logical expert ID.\n    \n    # Now, construct the final `phy2log` for the entire system.\n    # We need to account for the offset of logical experts per node.\n    phy2log_per_node = torch.zeros(num_layers * num_nodes, num_physical_experts // num_nodes, dtype=torch.int64, device=weight.device)\n    \n    for layer_node_idx in range(num_layers * num_nodes):\n        node_idx = layer_node_idx // num_layers # This is incorrect, need layer_idx and node_idx separately\n        \n    # Correct mapping from node_logical_weights_flat index to actual logical expert ID\n    # The `replicate_experts` function takes `weight` which is already the logical expert weights.\n    # So `phy2mlog_per_node` directly gives the logical expert ID.\n    # We need to add the offset for each node.\n    \n    # Calculate the starting logical expert index for each node.\n    node_logical_expert_starts = torch.arange(0, num_logical_experts, num_logical_experts // num_nodes, device=weight.device)\n    \n    # Map `phy2mlog_per_node` to actual logical expert IDs.\n    # `phy2mlog_per_node` is already the logical expert ID relative to its node's block.\n    # We need to add the starting index of that block.\n    \n    # `phy2mlog_per_node` has shape [num_layers * num_nodes, num_logical_experts // num_groups]\n    # It contains logical expert IDs within the context of its node's groups.\n    # Let's re-examine the `replicate_experts` output `phy2log`.\n    # `phy2log` is [X, num_phy] where `num_phy` is the number of physical experts *per layer* in `replicate_experts`.\n    # In our case, `num_phy` is `num_physical_experts // num_nodes`.\n    # The `weight` input to `replicate_experts` is `[num_layers * num_nodes, num_logical_experts // num_groups]`.\n    # So `phy2mlog_per_node` (which is `phy2log` from `replicate_experts`) is indeed the logical expert ID relative to the start of the node's logical experts.\n    \n    # We need to unflatten `phy2mlog_per_node` to get the node information.\n    phy2mlog_unflattened = phy2mlog_per_node.view(num_layers, num_nodes, -1)\n    \n    # Add the node offset to get the global logical expert ID.\n    phy2mlog_global = phy2mlog_unflattened + node_logical_expert_starts.view(1, -1, 1)\n    \n    # Flatten back to get the `phy2log` for `rebalance_experts_hierarchical`.\n    phy2log_step2 = phy2log_global.flatten(1) # [num_layers, num_physical_experts]\n\n    # Now we need to construct the `log2phy` map.\n    # `mlogcnt_per_node` is [num_layers * num_nodes, num_logical_experts // num_groups]\n    # This represents the number of replicas for each logical expert within its node.\n    mlogcnt_unflattened = mlogcnt_per_node.view(num_layers, num_nodes, -1)\n    \n    # We need to aggregate `mlogcnt_per_node` for each logical expert across its replicas.\n    # However, `mlogcnt_per_node` already gives the count for each logical expert.\n    # We need to map this back to the original logical experts.\n    \n    # `logcnt` in the original code is [num_layers, num_logical_experts].\n    # We need to populate this based on `mlogcnt_per_node`.\n    logcnt_step2 = torch.zeros(num_layers, num_logical_experts, device=weight.device, dtype=torch.int64)\n    \n    # Iterate through each node's logical experts and assign their replica counts.\n    for layer_idx in range(num_layers):\n        for node_idx in range(num_nodes):\n            start_logical_idx_in_node = node_idx * (num_logical_experts // num_nodes)\n            for i in range(num_logical_experts // num_groups): # num_logical_experts // num_groups is the number of logical experts per node\n                original_logical_expert_id = start_logical_idx_in_node + i\n                logcnt_step2[layer_idx, original_logical_expert_id] = mlogcnt_unflattened[layer_idx, node_idx, i]\n\n    # Step 3: pack physical_experts to GPUs\n    # We have `phy2log_step2` which maps physical experts within a node to their logical counterparts.\n    # The input to `balanced_packing` for GPUs should be the load of physical experts per GPU.\n    # `phy2log_step2` has shape [num_layers * num_nodes, num_physical_experts // num_nodes].\n    # We need to group these by GPU.\n    \n    # First, reshape `phy2log_step2` to group by layer and node.\n    # `phy2log_step2` is already [num_layers, num_nodes, num_physical_experts // num_nodes].\n    \n    # We need the load of physical experts per GPU.\n    # The physical experts are distributed across GPUs within each node.\n    # `phy_experts_per_gpu` is the number of physical experts per GPU.\n    \n    # Let's reconstruct the `tokens_per_phy` tensor which represents the load of physical experts.\n    # This should be [num_layers * num_nodes, num_physical_experts // num_nodes].\n    # The input `weight` to `replicate_experts` was `node_logical_weights_flat`.\n    # The output `phy2mlog_per_node` is the logical expert ID.\n    # We need the load of these physical experts.\n    \n    # The load of a physical expert is the load of the logical expert it replicates.\n    # We can get the load of logical experts from the original `weight` tensor.\n    # `phy2log_step2` tells us which logical expert each physical expert maps to.\n    \n    # Create a tensor of physical expert loads.\n    # Shape: [num_layers * num_nodes, num_physical_experts // num_nodes]\n    physical_expert_loads = torch.zeros(num_layers * num_nodes, num_physical_experts // num_nodes, device=weight.device)\n    \n    for layer_node_idx in range(num_layers * num_nodes):\n        for phy_idx_in_node in range(num_physical_experts // num_nodes):\n            logical_expert_id = phy2log_step2[layer_node_idx, phy_idx_in_node]\n            \n            # Determine the original layer and node index\n            layer_idx = layer_node_idx % num_layers\n            node_idx = layer_node_idx // num_layers # This is wrong, should be layer_node_idx // num_layers\n            \n            # Correct way to get layer and node index from layer_node_idx\n            layer_idx = layer_node_idx // num_nodes\n            node_idx = layer_node_idx % num_nodes\n            \n            # Get the load of the corresponding logical expert\n            physical_expert_loads[layer_node_idx, phy_idx_in_node] = weight[layer_idx, logical_expert_id]\n    \n    # Now `physical_expert_loads` has shape [num_layers * num_nodes, num_physical_experts // num_nodes].\n    # This is the input for packing physical experts to GPUs.\n    \n    # Pack physical experts to GPUs.\n    # `num_gpus // num_nodes` is the number of GPUs per node.\n    # We need to perform `balanced_packing` for each layer.\n    # The input `physical_expert_loads` is already structured as [num_layers * num_nodes, ...].\n    # We can flatten the first two dimensions and then reshape the output.\n    \n    num_gpus_per_node = num_gpus // num_nodes\n    \n    # Reshape for packing to GPUs.\n    # We need to consider each layer independently for GPU packing.\n    # Let's process layer by layer.\n    \n    # `physical_expert_loads` has shape [num_layers, num_nodes, num_physical_experts // num_nodes]\n    physical_expert_loads_reshaped = physical_expert_loads.view(num_layers, num_nodes, -1)\n    \n    # We need to pack `num_physical_experts // num_nodes` physical experts onto `num_gpus // num_nodes` GPUs.\n    # The input to `balanced_packing` should be `[num_layers, num_physical_experts_per_node]`\n    # where `num_physical_experts_per_node` is `num_physical_experts // num_gpus`.\n    \n    # Let's rethink the structure.\n    # We have `phy2log_step2` which is [num_layers, num_nodes, num_physical_experts // num_nodes].\n    # This maps physical experts (within a node) to logical experts.\n    \n    # We need to group physical experts by GPU.\n    # Each node has `num_gpus_per_node` GPUs.\n    # Each GPU has `phy_experts_per_gpu` physical experts.\n    \n    # Let's create the input for `balanced_packing` for GPUs.\n    # It should be `[num_layers, num_physical_experts_on_gpu]` where `num_physical_experts_on_gpu` is `num_physical_experts // num_gpus`.\n    # This means we need to aggregate physical experts from different nodes onto the same GPU.\n    \n    # The `balanced_packing` function expects `[X, n]` where `n` is the number of items to pack.\n    # In this step, we are packing physical experts to GPUs.\n    # So, the items are physical experts, and the packs are GPUs.\n    # The number of items to pack is `num_physical_experts`.\n    # The number of packs is `num_gpus`.\n    \n    # The input `weight` to `balanced_packing` should be `[num_layers, num_physical_experts]`.\n    # The values in this tensor are the loads of the physical experts.\n    \n    # We need to construct this `physical_expert_loads_for_gpu` tensor.\n    # It should have shape `[num_layers, num_physical_experts]`.\n    \n    # `phy2log_step2` is `[num_layers, num_nodes, num_physical_experts // num_nodes]`.\n    # `phy2mlog_global` is `[num_layers, num_nodes, num_logical_experts // num_groups]`\n    \n    # Let's reconstruct the `physical_expert_loads_for_gpu` tensor.\n    # This tensor should represent the load of each of the `num_physical_experts` physical experts.\n    \n    # We can iterate through each physical expert and determine its load.\n    # A physical expert is identified by (layer, node, physical_expert_idx_in_node).\n    \n    physical_expert_loads_for_gpu = torch.zeros(num_layers, num_physical_experts, device=weight.device)\n    \n    # `phy2log_step2` is [num_layers, num_nodes, num_physical_experts // num_nodes]\n    # `mlogcnt_unflattened` is [num_layers, num_nodes, num_logical_experts // num_groups]\n    \n    # We need to map the physical experts to their loads.\n    # The `replicate_experts` function in `replicate_experts` returns `phy2log`.\n    # So `phy2mlog_per_node` (which is `phy2log` from `replicate_experts`)\n    # directly maps to logical expert IDs.\n    \n    # Let's use the `weight` tensor directly to get the loads of logical experts.\n    # For each physical expert, find its logical expert ID and get the load.\n    \n    # Iterate through layers, nodes, and physical experts within a node.\n    for layer_idx in range(num_layers):\n        for node_idx in range(num_nodes):\n            for phy_idx_in_node in range(num_physical_experts // num_nodes):\n                # The global index for the physical expert\n                global_phy_idx = node_idx * (num_physical_experts // num_nodes) + phy_idx_in_node\n                \n                # Get the logical expert ID for this physical expert\n                logical_expert_id = phy2log_step2[layer_idx, node_idx, phy_idx_in_node]\n                \n                # Assign the load of the logical expert to this physical expert\n                physical_expert_loads_for_gpu[layer_idx, global_phy_idx] = weight[layer_idx, logical_expert_id]\n    \n    # Now, `physical_expert_loads_for_gpu` is [num_layers, num_physical_experts].\n    # This is the input for `balanced_packing` to distribute physical experts across GPUs.\n    \n    gpu_pack_index, gpu_rank_in_pack = balanced_packing(\n        physical_expert_loads_for_gpu, num_gpus)\n    \n    # Map physical experts to their final GPU assignments.\n    # `gpu_pack_index` is [num_layers, num_physical_experts]\n    # `gpu_rank_in_pack` is [num_layers, num_physical_experts]\n    \n    # The final `phy2log` should be [num_layers, num_physical_experts].\n    # We already have `phy2log_step2` which maps physical experts within nodes to logical experts.\n    # We need to combine this with the GPU packing.\n    \n    # Let's rethink the overall structure of `rebalance_experts_hierarchical`.\n    # The goal is to produce `phy2log`, `phyrank`, `logcnt`.\n    \n    # `phy2log`: [num_layers, num_physical_experts] -> logical expert ID for each physical expert.\n    # `phyrank`: [num_layers, num_physical_experts] -> replica rank for each physical expert.\n    # `logcnt`: [num_layers, num_logical_experts] -> number of replicas for each logical expert.\n    \n    # Let's redefine the intermediate steps to align with the desired outputs.\n    \n    # Step 1: Pack groups to nodes (already done, `group_pack_index`).\n    # `group_pack_index`: [num_layers, num_groups] -> node index for each group.\n    \n    # Step 2: Replicate logical experts within each node.\n    # We need to consider logical experts per node.\n    # Number of logical experts per node = `num_logical_experts // num_nodes`.\n    # Input to `replicate_experts` should be `[num_layers, num_logical_experts // num_nodes]` (weights of logical experts per node).\n    \n    # Construct the weights for logical experts within each node.\n    # `node_logical_weights`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n    # The previous construction of `node_logical_weights` seems correct.\n    \n    # `phy2mlog_per_node`: [num_layers * num_nodes, num_physical_experts // num_nodes] -> logical expert ID within node.\n    # `mlogcnt_per_node`: [num_layers * num_nodes, num_logical_experts // num_groups] -> replica count for logical expert within node.\n    \n    # Now, we need to map these results to the global `phy2log` and `logcnt`.\n    \n    # Construct `phy2log`: [num_layers, num_physical_experts].\n    # For each layer, and for each node, we have `num_physical_experts // num_nodes` physical experts.\n    # These physical experts are replicas of some logical experts.\n    \n    # `phy2mlog_per_node` gives the logical expert ID for each physical expert within its node context.\n    # We need to add the starting logical expert index for each node.\n    \n    # `phy2log_step2` (from previous attempt) seems to be the correct mapping for physical experts (within nodes) to their logical counterparts.\n    # `phy2log_step2` is [num_layers, num_nodes, num_physical_experts // num_nodes].\n    # We need to flatten this to [num_layers, num_physical_experts].\n    \n    # Let's redo the mapping from `phy2mlog_per_node` to `phy2log_step2`.\n    # `phy2mlog_per_node` is [num_layers * num_nodes, num_physical_experts // num_nodes].\n    # It contains logical expert IDs *relative to the start of the node's logical experts*.\n    \n    # Calculate the starting logical expert index for each node.\n    node_logical_expert_starts = torch.arange(0, num_logical_experts, num_logical_experts // num_nodes, device=weight.device)\n    \n    # Reshape `phy2mlog_per_node` to include node dimension.\n    phy2mlog_unflattened_per_node = phy2mlog_per_node.view(num_layers, num_nodes, -1)\n    \n    # Add the node offset to get the global logical expert IDs.\n    phy2log_final = phy2mlog_unflattened_per_node + node_logical_expert_starts.view(1, -1, 1)\n    phy2log_final = phy2log_final.flatten(1) # Shape: [num_layers, num_physical_experts]\n\n    # Construct `logcnt`: [num_layers, num_logical_experts].\n    # `mlogcnt_per_node` is [num_layers * num_nodes, num_logical_experts // num_groups].\n    # This contains the replica count for each logical expert *within its node*.\n    \n    # We need to aggregate these counts for each logical expert across all nodes.\n    # However, a logical expert is unique to a node in this hierarchical scheme.\n    # The `replicate_experts` function is called on logical experts *per node*.\n    # So `mlogcnt_per_node` should be directly mapped to `logcnt`.\n    \n    # Reshape `mlogcnt_per_node` to include node dimension.\n    mlogcnt_unflattened_per_node = mlogcnt_per_node.view(num_layers, num_nodes, -1)\n    \n    # The logical experts are partitioned by node.\n    # `mlogcnt_unflattened_per_node[layer_idx, node_idx, logical_expert_idx_in_node]`\n    # corresponds to `logcnt[layer_idx, start_logical_idx_of_node + logical_expert_idx_in_node]`.\n    \n    logcnt_final = torch.zeros(num_layers, num_logical_experts, device=weight.device, dtype=torch.int64)\n    \n    for layer_idx in range(num_layers):\n        for node_idx in range(num_nodes):\n            start_logical_idx_in_node = node_idx * (num_logical_experts // num_nodes)\n            # The number of logical experts per node is num_logical_experts // num_nodes\n            # The input to replicate_experts was `num_logical_experts // num_groups`\n            # This implies `num_logical_experts // num_nodes` must be divisible by `num_groups // num_nodes`.\n            # The number of logical experts per node is `num_logical_experts // num_nodes`.\n            # The number of logical experts per group in `replicate_experts` input is `num_logical_experts // num_groups`.\n            # This means that the logical experts are grouped into `num_groups`.\n            # Each node gets `groups_per_node` groups.\n            # So, number of logical experts per node is `groups_per_node * group_size`.\n            # `groups_per_node = num_groups // num_nodes`\n            # `group_size = num_logical_experts // num_groups`\n            # Number of logical experts per node = `(num_groups // num_nodes) * (num_logical_experts // num_groups)`\n            # = `num_logical_experts // num_nodes`. This is consistent.\n            \n            # The `mlogcnt_per_node` has shape [num_layers, num_nodes, num_logical_experts // num_groups].\n            # This means it contains replica counts for logical experts *within groups*.\n            # The input `weight` to `replicate_experts` was `node_logical_weights_flat` of shape `[num_layers * num_nodes, num_logical_experts // num_groups]`.\n            # So, the second dimension of `mlogcnt_per_node` corresponds to the logical experts *within the node's allocation*.\n            # Number of logical experts in the input to `replicate_experts` is `num_logical_experts // num_groups`.\n            # This is the number of logical experts *per group*.\n            # Wait, the input to `replicate_experts` was `node_logical_weights_flat` of shape `[num_layers * num_nodes, num_logical_experts // num_groups]`.\n            # This means `num_logical_experts // num_groups` is the number of logical experts *per node*.\n            # This is correct because each node gets `groups_per_node` groups, and each group has `group_size` logical experts.\n            # Total logical experts per node = `groups_per_node * group_size = (num_groups // num_nodes) * (num_logical_experts // num_groups) = num_logical_experts // num_nodes`.\n            # So, `num_logical_experts // num_groups` IS the number of logical experts per node.\n            \n            num_logical_experts_per_node = num_logical_experts // num_nodes\n            \n            for logical_expert_idx_in_node in range(num_logical_experts_per_node):\n                original_logical_expert_id = start_logical_idx_in_node + logical_expert_idx_in_node\n                logcnt_final[layer_idx, original_logical_expert_id] = mlogcnt_unflattened_per_node[layer_idx, node_idx, logical_expert_idx_in_node]\n\n    # Now, we need `phyrank`: [num_layers, num_physical_experts].\n    # `phyrank` from `replicate_experts` is `[num_layers * num_nodes, num_physical_experts // num_nodes]`.\n    # This represents the replica rank for physical experts *within a node*.\n    \n    # We need to map this to the final `phyrank` array.\n    # `phyrank_unflattened_per_node` has shape `[num_layers, num_nodes, num_physical_experts // num_nodes]`.\n    \n    phyrank_final = torch.zeros(num_layers, num_physical_experts, device=weight.device, dtype=torch.int64)\n    \n    for layer_idx in range(num_layers):\n        for node_idx in range(num_nodes):\n            for phy_idx_in_node in range(num_physical_experts // num_nodes):\n                global_phy_idx = node_idx * (num_physical_experts // num_nodes) + phy_idx_in_node\n                phyrank_final[layer_idx, global_phy_idx] = mlogcnt_unflattened_per_node[layer_idx, node_idx, phy_idx_in_node] # This is wrong, `phyrank` not `mlogcnt`\n                \n                # Let's call `replicate_experts` correctly and get the `phyrank`\n                # `phy2mlog_per_node`, `phyrank_per_node`, `mlogcnt_per_node`\n                \n                # We need to get the `phyrank` from `replicate_experts`\n                # `replicate_experts` returns `phy2log`, `rank`, `logcnt`.\n                # So, `rank` is what we need for `phyrank`.\n                \n                # Let's re-run `replicate_experts` to get the correct `phyrank`.\n                # `phy2mlog_per_node`, `rank_per_node`, `mlogcnt_per_node` = replicate_experts(...)\n                \n                # We need to get the `rank` output from `replicate_experts`.\n                # Let's assume `replicate_experts` returns `phy2log, rank, logcnt`.\n                \n                # `rank_per_node` has shape `[num_layers * num_nodes, num_physical_experts // num_nodes]`.\n                # This is the replica rank for physical experts within a node.\n                \n                # Reshape `rank_per_node` to `[num_layers, num_nodes, num_physical_experts // num_nodes]`.\n                rank_unflattened_per_node = rank_per_node.view(num_layers, num_nodes, -1)\n                \n                # Map this to the final `phyrank`.\n                phyrank_final[layer_idx, global_phy_idx] = rank_unflattened_per_node[layer_idx, node_idx, phy_idx_in_node]\n\n    # The current code uses `phyrank` from `replicate_experts` incorrectly.\n    # It uses `mlogcnt.view(num_layers, -1).gather(-1, log2mlog)` for `logcnt`.\n    # And `phyrank.gather(-1, pphy2phy).view(num_layers, -1)` for `pphyrank`.\n    \n    # Let's simplify the entire `rebalance_experts_hierarchical` function.\n    # The core idea is:\n    # 1. Pack logical experts into nodes.\n    # 2. Replicate logical experts within each node to create physical experts for that node.\n    # 3. Pack physical experts (from all nodes) onto GPUs.\n    \n    # Step 1: Pack groups to nodes (using `balanced_packing` on `tokens_per_group`).\n    # This results in `group_pack_index` which maps groups to nodes.\n    # We also need to map logical experts to nodes.\n    \n    # Let's use the `weight` tensor directly to determine logical expert loads.\n    # We want to pack `num_groups` groups into `num_nodes` nodes.\n    # The input to `balanced_packing` should be `[num_layers, num_groups]`, where each entry is the total load of logical experts in that group.\n    \n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(tokens_per_group, num_nodes)\n    \n    # Now, `group_pack_index` maps each group to a node.\n    # We need to determine which logical experts end up on which node.\n    \n    # Create a mapping from logical expert ID to node ID.\n    logical_expert_to_node_map = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n    \n    for layer_idx in range(num_layers):\n        for group_idx in range(num_groups):\n            node_idx = group_pack_index[layer_idx, group_idx]\n            start_logical_idx = group_idx * group_size\n            end_logical_idx = start_logical_idx + group_size\n            logical_expert_to_node_map[layer_idx, start_logical_idx:end_logical_idx] = node_idx\n    \n    # Step 2: Replicate logical experts within each node.\n    # For each node, we need to replicate its assigned logical experts.\n    # The number of physical experts per node is `num_physical_experts // num_nodes`.\n    \n    # We need to call `replicate_experts` for each node's set of logical experts.\n    # This means we need to extract the weights for logical experts belonging to each node.\n    \n    # For each layer, let's create a tensor of logical expert weights for each node.\n    # `node_logical_weights`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n    node_logical_weights = torch.zeros(num_layers, num_nodes, num_logical_experts // num_nodes, device=weight.device)\n    \n    for layer_idx in range(num_layers):\n        for node_idx in range(num_nodes):\n            # Find logical experts assigned to this node.\n            logical_experts_on_node_mask = (logical_expert_to_node_map[layer_idx] == node_idx)\n            logical_experts_on_node_indices = torch.where(logical_experts_on_node_mask)[0]\n            \n            # Assign their weights.\n            node_logical_weights[layer_idx, node_idx, :len(logical_experts_on_node_indices)] = weight[layer_idx, logical_experts_on_node_indices]\n    \n    # Now, for each layer and node, call `replicate_experts`.\n    # `phy2log_per_node`: [num_layers, num_nodes, num_physical_experts // num_nodes]\n    # `phyrank_per_node`: [num_layers, num_nodes, num_physical_experts // num_nodes]\n    # `logcnt_per_node`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n    \n    phy2log_per_node = torch.zeros(num_layers, num_nodes, num_physical_experts // num_nodes, dtype=torch.int64, device=weight.device)\n    phyrank_per_node = torch.zeros(num_layers, num_nodes, num_physical_experts // num_nodes, dtype=torch.int64, device=weight.device)\n    logcnt_per_node = torch.zeros(num_layers, num_nodes, num_logical_experts // num_nodes, dtype=torch.int64, device=weight.device)\n    \n    for layer_idx in range(num_layers):\n        for node_idx in range(num_nodes):\n            # Input weights for this node's logical experts.\n            current_node_weights = node_logical_weights[layer_idx, node_idx, :]\n            \n            # Filter out any zero-padded weights if `len(logical_experts_on_node_indices)` was less than `num_logical_experts // num_nodes`.\n            # This should not happen if the distribution is perfect, but good to be robust.\n            # The number of logical experts per node is `num_logical_experts // num_nodes`.\n            # So the input `current_node_weights` should have `num_logical_experts // num_nodes` elements.\n            \n            # Call replicate_experts for this node.\n            # The number of physical experts to create is `num_physical_experts // num_nodes`.\n            node_phy2log, node_phyrank, node_logcnt = replicate_experts(\n                current_node_weights.unsqueeze(0), num_physical_experts // num_nodes) # Unsqueeze to add layer dim\n            \n            # Store the results. `node_phy2log` is [1, num_physical_experts // num_nodes].\n            phy2log_per_node[layer_idx, node_idx, :] = node_phy2log.squeeze(0)\n            phyrank_per_node[layer_idx, node_idx, :] = node_phyrank.squeeze(0)\n            logcnt_per_node[layer_idx, node_idx, :] = node_logcnt.squeeze(0)\n    \n    # Now we have the replicated experts per node.\n    # `phy2log_per_node` maps physical experts within a node to logical experts.\n    # `phyrank_per_node` gives the replica rank.\n    # `logcnt_per_node` gives the replica count for logical experts within a node.\n    \n    # Step 3: Pack physical experts (from all nodes) onto GPUs.\n    # We need to create the input for `balanced_packing` for GPUs.\n    # This input should be `[num_layers, num_physical_experts]`, representing the load of each physical expert.\n    \n    # The load of a physical expert is the load of the logical expert it replicates.\n    # We need to find the load of the logical expert specified by `phy2log_per_node`.\n    \n    # Create the physical expert load tensor: `physical_expert_loads_for_gpu` [num_layers, num_physical_experts]\n    physical_expert_loads_for_gpu = torch.zeros(num_layers, num_physical_experts, device=weight.device)\n    \n    for layer_idx in range(num_layers):\n        for node_idx in range(num_nodes):\n            for phy_idx_in_node in range(num_physical_experts // num_nodes):\n                # Global index for the physical expert\n                global_phy_idx = node_idx * (num_physical_experts // num_nodes) + phy_idx_in_node\n                \n                # Get the logical expert ID for this physical expert\n                logical_expert_id_for_phy = phy2log_per_node[layer_idx, node_idx, phy_idx_in_node]\n                \n                # Get the load of this logical expert from the original `weight` tensor.\n                physical_expert_loads_for_gpu[layer_idx, global_phy_idx] = weight[layer_idx, logical_expert_id_for_phy]\n    \n    # Now, pack these physical experts onto GPUs.\n    # `num_gpus` is the total number of packs.\n    gpu_pack_index, gpu_rank_in_pack = balanced_packing(\n        physical_expert_loads_for_gpu, num_gpus)\n    \n    # `gpu_pack_index`: [num_layers, num_physical_experts] -> GPU index for each physical expert.\n    # `gpu_rank_in_pack`: [num_layers, num_physical_experts] -> rank of physical expert on GPU.\n    \n    # We need to construct the final `phy2log`, `phyrank`, `logcnt`.\n    \n    # `phy2log`: [num_layers, num_physical_experts]\n    # This is the logical expert ID for each physical expert.\n    # We already have `phy2log_per_node` which gives this information.\n    # We just need to flatten it correctly.\n    final_phy2log = phy2log_per_node.view(num_layers, -1) # Shape: [num_layers, num_physical_experts]\n    \n    # `phyrank`: [num_layers, num_physical_experts]\n    # This is the replica rank of each physical expert on its assigned GPU.\n    # We have `phyrank_per_node` and `gpu_rank_in_pack`.\n    # The `phyrank_per_node` is the rank of the logical expert replication.\n    # The `gpu_rank_in_pack` is the rank of the physical expert on the GPU.\n    # We need the rank of the physical expert on the GPU.\n    # So, `phyrank` should be `gpu_rank_in_pack`.\n    final_phyrank = gpu_rank_in_pack # Shape: [num_layers, num_physical_experts]\n    \n    # `logcnt`: [num_layers, num_logical_experts]\n    # This is the number of replicas for each logical expert.\n    # We have `logcnt_per_node` [num_layers, num_nodes, num_logical_experts // num_nodes].\n    # This gives the replica count for logical experts within their respective nodes.\n    # Since logical experts are partitioned by node, we can aggregate `logcnt_per_node`.\n    \n    final_logcnt = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n    \n    for layer_idx in range(num_layers):\n        for node_idx in range(num_nodes):\n            # Find logical experts assigned to this node.\n            logical_experts_on_node_mask = (logical_expert_to_node_map[layer_idx] == node_idx)\n            logical_experts_on_node_indices = torch.where(logical_experts_on_node_mask)[0]\n            \n            # Assign their replica counts.\n            final_logcnt[layer_idx, logical_experts_on_node_indices] = logcnt_per_node[layer_idx, node_idx, :len(logical_experts_on_node_indices)]\n    \n    return final_phy2log, final_phyrank, final_logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    # Cast weight to float for calculations if not already, and keep on original device.\n    # Moving to CPU adds overhead if data is already on GPU and might slow down ops.\n    weight = weight.float()\n    \n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n\n    if num_groups > 0 and num_nodes > 0 and num_groups % num_nodes == 0:\n        # Use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # Use global load-balance policy by treating all experts as a single group.\n        # This simplifies the logic to a single-level replication and packing.\n        # We effectively have `num_logical_experts` groups, each with 1 logical expert.\n        # `num_groups` becomes `num_logical_experts`.\n        # `groups_per_node` becomes `num_logical_experts // num_nodes` (if num_nodes > 0).\n        # If num_nodes is 0 or not applicable, we can treat it as a single node.\n        \n        # For global balancing, we can call a simplified version of hierarchical,\n        # or a dedicated global balancing function.\n        # Let's adapt `rebalance_experts_hierarchical` to handle the global case more directly.\n        \n        # If `num_groups` is not divisible by `num_nodes`, or if `num_nodes` is 0 or 1,\n        # we can consider a global balancing strategy.\n        # A global strategy would involve replicating experts across all `num_gpus` directly.\n        \n        # A simpler approach for global balancing:\n        # Treat all `num_logical_experts` as a single block of experts to be replicated.\n        # Then, pack these replicated experts onto `num_gpus`.\n        \n        # Call `replicate_experts` on the entire `weight` tensor to get `num_replicas`.\n        # The input `weight` to `replicate_experts` should be `[num_layers, num_logical_experts]`.\n        # The output `phy2log` will be `[num_layers, num_replicas]`.\n        # The output `rank` will be `[num_layers, num_replicas]`.\n        # The output `logcnt` will be `[num_layers, num_logical_experts]`.\n        \n        phy2log_global_rep, phyrank_global_rep, logcnt_global_rep = replicate_experts(\n            weight, num_replicas)\n        \n        # Now, pack these `num_replicas` physical experts onto `num_gpus`.\n        # The input to `balanced_packing` is the load of these physical experts.\n        # The load of a physical expert is the load of the logical expert it replicates.\n        # So, we need to use `phy2log_global_rep` to get the loads from the original `weight`.\n        \n        physical_expert_loads_global = torch.zeros(num_layers, num_replicas, device=weight.device)\n        \n        for layer_idx in range(num_layers):\n            for rep_idx in range(num_replicas):\n                logical_expert_id = phy2log_global_rep[layer_idx, rep_idx]\n                physical_expert_loads_global[layer_idx, rep_idx] = weight[layer_idx, logical_expert_id]\n        \n        # Pack these loads onto `num_gpus`.\n        gpu_pack_index_global, gpu_rank_in_pack_global = balanced_packing(\n            physical_expert_loads_global, num_gpus)\n        \n        # The final `phy2log` should be the logical expert ID for each physical expert.\n        # This is directly from `phy2log_global_rep`.\n        final_phy2log = phy2log_global_rep\n        \n        # The final `phyrank` should be the rank of the physical expert on its assigned GPU.\n        # This is `gpu_rank_in_pack_global`.\n        final_phyrank = gpu_rank_in_pack_global\n        \n        # The final `logcnt` is the number of replicas for each logical expert.\n        # This is directly from `logcnt_global_rep`.\n        final_logcnt = logcnt_global_rep\n        \n        # Update the return values\n        phy2log = final_phy2log\n        phyrank = final_phyrank\n        logcnt = final_logcnt\n\n    # The rest of the function remains the same for constructing log2phy.\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    # Ensure `phy2log` and `phyrank` have the correct shapes for scatter.\n    # `phy2log` should be [num_layers, num_replicas].\n    # `phyrank` should be [num_layers, num_replicas].\n    # `logcnt` should be [num_layers, num_logical_experts].\n    \n    # The scatter operation needs to map `phy2log` and `phyrank` to `log2phy`.\n    # `log2phy[layer, logical_expert, rank]` should store the physical expert ID.\n    # The current scatter logic:\n    # `log2phy.view(num_layers, -1).scatter_(-1, phy2log * maxlogcnt + phyrank, ...)`\n    # This implies `phy2log` is used to index into the `maxlogcnt` blocks.\n    # `phy2log` is [num_layers, num_replicas].\n    # `phyrank` is [num_layers, num_replicas].\n    # `maxlogcnt` is the number of replicas per logical expert.\n    # So `phy2log * maxlogcnt + phyrank` correctly calculates the linear index within the `log2phy` tensor for a given logical expert and its replica rank.\n    \n    # The indices for scatter are:\n    # `phy2log` (logical expert ID) * `maxlogcnt` (max replicas per LE) + `phyrank` (rank of this replica)\n    \n    # Ensure `phy2log` and `phyrank` are on the same device as `log2phy`.\n    phy2log = phy2log.to(log2phy.device)\n    phyrank = phyrank.to(log2phy.device)\n\n    # The values to scatter are the physical expert IDs.\n    # The physical expert ID is the column index in `phy2log` and `phyrank`.\n    # So, `torch.arange(num_replicas, dtype=torch.int64, device=log2phy.device).expand(num_layers, -1)` is correct.\n    \n    scatter_indices = phy2log * maxlogcnt + phyrank\n    scatter_values = torch.arange(num_replicas, dtype=torch.int64, device=log2phy.device).expand(num_layers, -1)\n    \n    log2phy.scatter_(-1, scatter_indices, scatter_values)\n    \n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n", "language": "python", "parent_id": "191c932f-b14f-48ac-9bd8-63618b3f5812", "generation": 5, "timestamp": 1751059517.498249, "iteration_found": 152, "metrics": {"balancedness_score": 0.0, "speed_score": 0.0, "combined_score": 0.0, "error": "name 'phy2log_global' is not defined"}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 27 lines with 536 lines\nChange 2: Replace 19 lines with 19 lines\nChange 3: Replace 23 lines with 110 lines", "parent_metrics": {"balancedness_score": 0.29830835153138974, "speed_score": 0.024267421713478277, "combined_score": 0.161287886622434}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve the Mixture-of-Expert models Expert Parallelism Load Balancer (MoE EPLB) expert rearrangement algorithm.\nThis algorithm will take the load metrics recorded by the vLLM server, and rearrange the experts to balance the load. It can make replicas of some experts to achieve better load balancing.\nYour goal will be two-fold: 1. Improve the algorithm to achieve better load balancing; while 2. Improve the algorithm to be more efficient, i.e. reduce the execution time of the algorithm itself, since perfect load balancing is NP-hard.\nThe current algorithm is implemented in the `rebalance_experts` function. ", "user": "# Current Program Information\n- Current performance metrics: - balancedness_score: 0.2983\n- speed_score: 0.0243\n- combined_score: 0.1613\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: speed_score, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0835, combined_score: 0.1909\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0845, combined_score: 0.1914\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0857, combined_score: 0.1920\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.1920)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0857), Performs well on combined_score (0.1920)\n\n\n### Program 2 (Score: 0.1914)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0845), Performs well on combined_score (0.1914)\n\n\n### Program 3 (Score: 0.1909)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0835), Performs well on combined_score (0.1909)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.1907)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n\n### Program D2 (Score: 0.1908)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.1920, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 2 (Score: 0.0939, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 23 lines with 236 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 3 (Score: 0.0000, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 14 lines with 16 lines\nChange 2: Replace 11 lines with 13 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 4 (Score: 0.1900, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 74 lines with 76 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 5 (Score: 0.1631, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 51 lines with 49 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n\n# Current Program\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort groups by weight in descending order.\n    # 'weight' is already float and on CPU from rebalance_experts,\n    # so no need for .float().cpu() here.\n    indices = weight.sort(-1, descending=True).indices\n\n    # Initialize pack_index and rank_in_pack.\n    # These will be fully populated, so `empty_like` is sufficient.\n    # They will be on the same device as 'weight' (i.e., CPU).\n    pack_index = torch.empty_like(weight, dtype=torch.int64)\n    rank_in_pack = torch.empty_like(pack_index)\n    \n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These tensors are on the same device as 'weight' (which is CPU).\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id in indices[i]:\n            # Get current state for this layer's packs.\n            current_pack_weights = pack_weights_per_layer[i]\n            current_pack_items = pack_items_per_layer[i]\n\n            # Find available packs (those not yet full).\n            # `available_mask` is a boolean tensor.\n            available_mask = (current_pack_items < groups_per_pack)\n\n            # Find the pack with the minimum current weight among available ones.\n            # This approach avoids creating a large temporary tensor with float('inf').\n            available_pack_indices = torch.nonzero(available_mask).squeeze(-1)\n            \n            # If no available packs, something is wrong with the logic or inputs.\n            # This assert ensures correctness under expected conditions.\n            assert available_pack_indices.numel() > 0, \"No available packs found, but expected to find one.\"\n\n            # Get weights of only the available packs\n            weights_of_available_packs = current_pack_weights[available_pack_indices]\n            \n            # Find the index within the `available_pack_indices` list that has the minimum weight.\n            local_min_idx = torch.argmin(weights_of_available_packs)\n            \n            # Map this local index back to the global pack index.\n            pack = available_pack_indices[local_min_idx]\n\n            # Assert that a valid pack was found and it has space.\n            assert current_pack_items[pack] < groups_per_pack\n\n            # Assign the group to the chosen pack.\n            # `group_id` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            # `weight[i, group_id]` is the weight of the current group.\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    # Cast weight to float for calculations if not already, and keep on original device.\n    # Moving to CPU adds overhead if data is already on GPU and might slow down ops.\n    weight = weight.float()\n    \n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["The current program uses a hierarchical approach to load balancing, which is generally good. However, the performance metrics show a regression in `speed_score` and `combined_score` compared to previous attempts. This suggests that the current implementation might be too complex or not optimized for speed.\n\nHere are a few suggestions to improve both balancedness and efficiency:\n\n### Improvement 1: Simplify `rebalance_experts_hierarchical` by removing unnecessary calculations and reorganizing logic.\n\nThe `rebalance_experts_hierarchical` function has several intermediate tensors that could potentially be optimized or computed more directly. Specifically, the calculations involving `log2mlog`, `mlog2log`, `tokens_per_mlog`, `phy2mlog`, `pphy2mlog`, `pphy2log`, `pphyrank`, and `logcnt` seem complex and might be a source of inefficiency.\n\nLet's focus on simplifying the mapping and replication steps.\n\n**Reasoning for Improvement:**\n*   **Efficiency:** Reducing the number of intermediate tensor creations and complex manipulations can lead to faster execution.\n*   **Readability:** A simpler structure is easier to understand, debug, and maintain.\n\n<<<<<<< SEARCH\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n=======\n    # Step 2: Construct redundant experts within nodes and map to physical experts\n    # Calculate token counts per logical expert group within each node.\n    # `weight` is [num_layers, num_logical_experts]\n    # `mlog2log` maps logical experts to their groups within a node.\n    # We need to get the weight for logical experts that fall into each group.\n    # `log2mlog` maps logical experts to their allocated physical expert within a node.\n    \n    # First, determine the logical experts that belong to each node.\n    # `group_pack_index` maps each group to a node.\n    # `groups_per_node` is the number of groups per node.\n    # `group_size` is the number of logical experts per group.\n    \n    # Create a mapping from logical expert index to node index.\n    logical_expert_to_node = torch.div(torch.arange(num_logical_experts, device=weight.device), group_size, rounding_mode='floor')\n    logical_expert_to_node = torch.div(logical_expert_to_node, groups_per_node, rounding_mode='floor')\n    \n    # Prepare weights for replication within each node.\n    # We need `num_layers` x `num_nodes` x `num_logical_experts // num_groups`\n    # where each element is the weight of a logical expert within a node.\n    node_logical_weights = torch.zeros(num_layers, num_nodes, num_logical_experts // num_groups, device=weight.device)\n    \n    for layer_idx in range(num_layers):\n        for group_idx in range(num_groups):\n            node_idx = group_idx // groups_per_node\n            start_logical_idx = group_idx * group_size\n            end_logical_idx = start_logical_idx + group_size\n            \n            # Map logical experts to their position within the node's logical experts\n            logical_expert_indices_in_node = torch.arange(group_size, device=weight.device)\n            \n            node_logical_weights[layer_idx, node_idx, logical_expert_indices_in_node] = weight[layer_idx, start_logical_idx:end_logical_idx]\n\n    # Reshape to [num_layers * num_nodes, num_logical_experts // num_groups] for replicate_experts\n    node_logical_weights_flat = node_logical_weights.view(-1, num_logical_experts // num_groups)\n    \n    # Replicate experts within each node.\n    # `phy2mlog` maps physical experts within a node to logical experts within that node.\n    # `mlogcnt` is the count of replicas for each logical expert within a node.\n    phy2mlog_per_node, mlogcnt_per_node, _ = replicate_experts(\n        node_logical_weights_flat, num_physical_experts // num_nodes)\n    \n    # Map physical experts within a node back to their original logical expert IDs.\n    # `phy2log_per_node` will have shape [num_layers * num_nodes, num_physical_experts // num_nodes]\n    # where each entry is the original logical expert ID.\n    \n    # We need to map the `phy2mlog_per_node` indices back to the original logical expert indices.\n    # The `replicate_experts` function returns `phy2log` which is the logical expert ID.\n    # So `phy2mlog_per_node` is already the logical expert ID.\n    \n    # Now, construct the final `phy2log` for the entire system.\n    # We need to account for the offset of logical experts per node.\n    phy2log_per_node = torch.zeros(num_layers * num_nodes, num_physical_experts // num_nodes, dtype=torch.int64, device=weight.device)\n    \n    for layer_node_idx in range(num_layers * num_nodes):\n        node_idx = layer_node_idx // num_layers # This is incorrect, need layer_idx and node_idx separately\n        \n    # Correct mapping from node_logical_weights_flat index to actual logical expert ID\n    # The `replicate_experts` function takes `weight` which is already the logical expert weights.\n    # So `phy2mlog_per_node` directly gives the logical expert ID.\n    # We need to add the offset for each node.\n    \n    # Calculate the starting logical expert index for each node.\n    node_logical_expert_starts = torch.arange(0, num_logical_experts, num_logical_experts // num_nodes, device=weight.device)\n    \n    # Map `phy2mlog_per_node` to actual logical expert IDs.\n    # `phy2mlog_per_node` is already the logical expert ID relative to its node's block.\n    # We need to add the starting index of that block.\n    \n    # `phy2mlog_per_node` has shape [num_layers * num_nodes, num_logical_experts // num_groups]\n    # It contains logical expert IDs within the context of its node's groups.\n    # Let's re-examine the `replicate_experts` output `phy2log`.\n    # `phy2log` is [X, num_phy] where `num_phy` is the number of physical experts *per layer* in `replicate_experts`.\n    # In our case, `num_phy` is `num_physical_experts // num_nodes`.\n    # The `weight` input to `replicate_experts` is `[num_layers * num_nodes, num_logical_experts // num_groups]`.\n    # So `phy2mlog_per_node` (which is `phy2log` from `replicate_experts`) is indeed the logical expert ID relative to the start of the node's logical experts.\n    \n    # We need to unflatten `phy2mlog_per_node` to get the node information.\n    phy2mlog_unflattened = phy2mlog_per_node.view(num_layers, num_nodes, -1)\n    \n    # Add the node offset to get the global logical expert ID.\n    phy2mlog_global = phy2mlog_unflattened + node_logical_expert_starts.view(1, -1, 1)\n    \n    # Flatten back to get the `phy2log` for `rebalance_experts_hierarchical`.\n    phy2log_step2 = phy2log_global.flatten(1) # [num_layers, num_physical_experts]\n\n    # Now we need to construct the `log2phy` map.\n    # `mlogcnt_per_node` is [num_layers * num_nodes, num_logical_experts // num_groups]\n    # This represents the number of replicas for each logical expert within its node.\n    mlogcnt_unflattened = mlogcnt_per_node.view(num_layers, num_nodes, -1)\n    \n    # We need to aggregate `mlogcnt_per_node` for each logical expert across its replicas.\n    # However, `mlogcnt_per_node` already gives the count for each logical expert.\n    # We need to map this back to the original logical experts.\n    \n    # `logcnt` in the original code is [num_layers, num_logical_experts].\n    # We need to populate this based on `mlogcnt_per_node`.\n    logcnt_step2 = torch.zeros(num_layers, num_logical_experts, device=weight.device, dtype=torch.int64)\n    \n    # Iterate through each node's logical experts and assign their replica counts.\n    for layer_idx in range(num_layers):\n        for node_idx in range(num_nodes):\n            start_logical_idx_in_node = node_idx * (num_logical_experts // num_nodes)\n            for i in range(num_logical_experts // num_groups): # num_logical_experts // num_groups is the number of logical experts per node\n                original_logical_expert_id = start_logical_idx_in_node + i\n                logcnt_step2[layer_idx, original_logical_expert_id] = mlogcnt_unflattened[layer_idx, node_idx, i]\n\n    # Step 3: pack physical_experts to GPUs\n    # We have `phy2log_step2` which maps physical experts within a node to their logical counterparts.\n    # The input to `balanced_packing` for GPUs should be the load of physical experts per GPU.\n    # `phy2log_step2` has shape [num_layers * num_nodes, num_physical_experts // num_nodes].\n    # We need to group these by GPU.\n    \n    # First, reshape `phy2log_step2` to group by layer and node.\n    # `phy2log_step2` is already [num_layers, num_nodes, num_physical_experts // num_nodes].\n    \n    # We need the load of physical experts per GPU.\n    # The physical experts are distributed across GPUs within each node.\n    # `phy_experts_per_gpu` is the number of physical experts per GPU.\n    \n    # Let's reconstruct the `tokens_per_phy` tensor which represents the load of physical experts.\n    # This should be [num_layers * num_nodes, num_physical_experts // num_nodes].\n    # The input `weight` to `replicate_experts` was `node_logical_weights_flat`.\n    # The output `phy2mlog_per_node` is the logical expert ID.\n    # We need the load of these physical experts.\n    \n    # The load of a physical expert is the load of the logical expert it replicates.\n    # We can get the load of logical experts from the original `weight` tensor.\n    # `phy2log_step2` tells us which logical expert each physical expert maps to.\n    \n    # Create a tensor of physical expert loads.\n    # Shape: [num_layers * num_nodes, num_physical_experts // num_nodes]\n    physical_expert_loads = torch.zeros(num_layers * num_nodes, num_physical_experts // num_nodes, device=weight.device)\n    \n    for layer_node_idx in range(num_layers * num_nodes):\n        for phy_idx_in_node in range(num_physical_experts // num_nodes):\n            logical_expert_id = phy2log_step2[layer_node_idx, phy_idx_in_node]\n            \n            # Determine the original layer and node index\n            layer_idx = layer_node_idx % num_layers\n            node_idx = layer_node_idx // num_layers # This is wrong, should be layer_node_idx // num_layers\n            \n            # Correct way to get layer and node index from layer_node_idx\n            layer_idx = layer_node_idx // num_nodes\n            node_idx = layer_node_idx % num_nodes\n            \n            # Get the load of the corresponding logical expert\n            physical_expert_loads[layer_node_idx, phy_idx_in_node] = weight[layer_idx, logical_expert_id]\n    \n    # Now `physical_expert_loads` has shape [num_layers * num_nodes, num_physical_experts // num_nodes].\n    # This is the input for packing physical experts to GPUs.\n    \n    # Pack physical experts to GPUs.\n    # `num_gpus // num_nodes` is the number of GPUs per node.\n    # We need to perform `balanced_packing` for each layer.\n    # The input `physical_expert_loads` is already structured as [num_layers * num_nodes, ...].\n    # We can flatten the first two dimensions and then reshape the output.\n    \n    num_gpus_per_node = num_gpus // num_nodes\n    \n    # Reshape for packing to GPUs.\n    # We need to consider each layer independently for GPU packing.\n    # Let's process layer by layer.\n    \n    # `physical_expert_loads` has shape [num_layers, num_nodes, num_physical_experts // num_nodes]\n    physical_expert_loads_reshaped = physical_expert_loads.view(num_layers, num_nodes, -1)\n    \n    # We need to pack `num_physical_experts // num_nodes` physical experts onto `num_gpus // num_nodes` GPUs.\n    # The input to `balanced_packing` should be `[num_layers, num_physical_experts_per_node]`\n    # where `num_physical_experts_per_node` is `num_physical_experts // num_gpus`.\n    \n    # Let's rethink the structure.\n    # We have `phy2log_step2` which is [num_layers, num_nodes, num_physical_experts // num_nodes].\n    # This maps physical experts (within a node) to logical experts.\n    \n    # We need to group physical experts by GPU.\n    # Each node has `num_gpus_per_node` GPUs.\n    # Each GPU has `phy_experts_per_gpu` physical experts.\n    \n    # Let's create the input for `balanced_packing` for GPUs.\n    # It should be `[num_layers, num_physical_experts_on_gpu]` where `num_physical_experts_on_gpu` is `num_physical_experts // num_gpus`.\n    # This means we need to aggregate physical experts from different nodes onto the same GPU.\n    \n    # The `balanced_packing` function expects `[X, n]` where `n` is the number of items to pack.\n    # In this step, we are packing physical experts to GPUs.\n    # So, the items are physical experts, and the packs are GPUs.\n    # The number of items to pack is `num_physical_experts`.\n    # The number of packs is `num_gpus`.\n    \n    # The input `weight` to `balanced_packing` should be `[num_layers, num_physical_experts]`.\n    # The values in this tensor are the loads of the physical experts.\n    \n    # We need to construct this `physical_expert_loads_for_gpu` tensor.\n    # It should have shape `[num_layers, num_physical_experts]`.\n    \n    # `phy2log_step2` is `[num_layers, num_nodes, num_physical_experts // num_nodes]`.\n    # `phy2mlog_global` is `[num_layers, num_nodes, num_logical_experts // num_groups]`\n    \n    # Let's reconstruct the `physical_expert_loads_for_gpu` tensor.\n    # This tensor should represent the load of each of the `num_physical_experts` physical experts.\n    \n    # We can iterate through each physical expert and determine its load.\n    # A physical expert is identified by (layer, node, physical_expert_idx_in_node).\n    \n    physical_expert_loads_for_gpu = torch.zeros(num_layers, num_physical_experts, device=weight.device)\n    \n    # `phy2log_step2` is [num_layers, num_nodes, num_physical_experts // num_nodes]\n    # `mlogcnt_unflattened` is [num_layers, num_nodes, num_logical_experts // num_groups]\n    \n    # We need to map the physical experts to their loads.\n    # The `replicate_experts` function in `replicate_experts` returns `phy2log`.\n    # So `phy2mlog_per_node` (which is `phy2log` from `replicate_experts`)\n    # directly maps to logical expert IDs.\n    \n    # Let's use the `weight` tensor directly to get the loads of logical experts.\n    # For each physical expert, find its logical expert ID and get the load.\n    \n    # Iterate through layers, nodes, and physical experts within a node.\n    for layer_idx in range(num_layers):\n        for node_idx in range(num_nodes):\n            for phy_idx_in_node in range(num_physical_experts // num_nodes):\n                # The global index for the physical expert\n                global_phy_idx = node_idx * (num_physical_experts // num_nodes) + phy_idx_in_node\n                \n                # Get the logical expert ID for this physical expert\n                logical_expert_id = phy2log_step2[layer_idx, node_idx, phy_idx_in_node]\n                \n                # Assign the load of the logical expert to this physical expert\n                physical_expert_loads_for_gpu[layer_idx, global_phy_idx] = weight[layer_idx, logical_expert_id]\n    \n    # Now, `physical_expert_loads_for_gpu` is [num_layers, num_physical_experts].\n    # This is the input for `balanced_packing` to distribute physical experts across GPUs.\n    \n    gpu_pack_index, gpu_rank_in_pack = balanced_packing(\n        physical_expert_loads_for_gpu, num_gpus)\n    \n    # Map physical experts to their final GPU assignments.\n    # `gpu_pack_index` is [num_layers, num_physical_experts]\n    # `gpu_rank_in_pack` is [num_layers, num_physical_experts]\n    \n    # The final `phy2log` should be [num_layers, num_physical_experts].\n    # We already have `phy2log_step2` which maps physical experts within nodes to logical experts.\n    # We need to combine this with the GPU packing.\n    \n    # Let's rethink the overall structure of `rebalance_experts_hierarchical`.\n    # The goal is to produce `phy2log`, `phyrank`, `logcnt`.\n    \n    # `phy2log`: [num_layers, num_physical_experts] -> logical expert ID for each physical expert.\n    # `phyrank`: [num_layers, num_physical_experts] -> replica rank for each physical expert.\n    # `logcnt`: [num_layers, num_logical_experts] -> number of replicas for each logical expert.\n    \n    # Let's redefine the intermediate steps to align with the desired outputs.\n    \n    # Step 1: Pack groups to nodes (already done, `group_pack_index`).\n    # `group_pack_index`: [num_layers, num_groups] -> node index for each group.\n    \n    # Step 2: Replicate logical experts within each node.\n    # We need to consider logical experts per node.\n    # Number of logical experts per node = `num_logical_experts // num_nodes`.\n    # Input to `replicate_experts` should be `[num_layers, num_logical_experts // num_nodes]` (weights of logical experts per node).\n    \n    # Construct the weights for logical experts within each node.\n    # `node_logical_weights`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n    # The previous construction of `node_logical_weights` seems correct.\n    \n    # `phy2mlog_per_node`: [num_layers * num_nodes, num_physical_experts // num_nodes] -> logical expert ID within node.\n    # `mlogcnt_per_node`: [num_layers * num_nodes, num_logical_experts // num_groups] -> replica count for logical expert within node.\n    \n    # Now, we need to map these results to the global `phy2log` and `logcnt`.\n    \n    # Construct `phy2log`: [num_layers, num_physical_experts].\n    # For each layer, and for each node, we have `num_physical_experts // num_nodes` physical experts.\n    # These physical experts are replicas of some logical experts.\n    \n    # `phy2mlog_per_node` gives the logical expert ID for each physical expert within its node context.\n    # We need to add the starting logical expert index for each node.\n    \n    # `phy2log_step2` (from previous attempt) seems to be the correct mapping for physical experts (within nodes) to their logical counterparts.\n    # `phy2log_step2` is [num_layers, num_nodes, num_physical_experts // num_nodes].\n    # We need to flatten this to [num_layers, num_physical_experts].\n    \n    # Let's redo the mapping from `phy2mlog_per_node` to `phy2log_step2`.\n    # `phy2mlog_per_node` is [num_layers * num_nodes, num_physical_experts // num_nodes].\n    # It contains logical expert IDs *relative to the start of the node's logical experts*.\n    \n    # Calculate the starting logical expert index for each node.\n    node_logical_expert_starts = torch.arange(0, num_logical_experts, num_logical_experts // num_nodes, device=weight.device)\n    \n    # Reshape `phy2mlog_per_node` to include node dimension.\n    phy2mlog_unflattened_per_node = phy2mlog_per_node.view(num_layers, num_nodes, -1)\n    \n    # Add the node offset to get the global logical expert IDs.\n    phy2log_final = phy2mlog_unflattened_per_node + node_logical_expert_starts.view(1, -1, 1)\n    phy2log_final = phy2log_final.flatten(1) # Shape: [num_layers, num_physical_experts]\n\n    # Construct `logcnt`: [num_layers, num_logical_experts].\n    # `mlogcnt_per_node` is [num_layers * num_nodes, num_logical_experts // num_groups].\n    # This contains the replica count for each logical expert *within its node*.\n    \n    # We need to aggregate these counts for each logical expert across all nodes.\n    # However, a logical expert is unique to a node in this hierarchical scheme.\n    # The `replicate_experts` function is called on logical experts *per node*.\n    # So `mlogcnt_per_node` should be directly mapped to `logcnt`.\n    \n    # Reshape `mlogcnt_per_node` to include node dimension.\n    mlogcnt_unflattened_per_node = mlogcnt_per_node.view(num_layers, num_nodes, -1)\n    \n    # The logical experts are partitioned by node.\n    # `mlogcnt_unflattened_per_node[layer_idx, node_idx, logical_expert_idx_in_node]`\n    # corresponds to `logcnt[layer_idx, start_logical_idx_of_node + logical_expert_idx_in_node]`.\n    \n    logcnt_final = torch.zeros(num_layers, num_logical_experts, device=weight.device, dtype=torch.int64)\n    \n    for layer_idx in range(num_layers):\n        for node_idx in range(num_nodes):\n            start_logical_idx_in_node = node_idx * (num_logical_experts // num_nodes)\n            # The number of logical experts per node is num_logical_experts // num_nodes\n            # The input to replicate_experts was `num_logical_experts // num_groups`\n            # This implies `num_logical_experts // num_nodes` must be divisible by `num_groups // num_nodes`.\n            # The number of logical experts per node is `num_logical_experts // num_nodes`.\n            # The number of logical experts per group in `replicate_experts` input is `num_logical_experts // num_groups`.\n            # This means that the logical experts are grouped into `num_groups`.\n            # Each node gets `groups_per_node` groups.\n            # So, number of logical experts per node is `groups_per_node * group_size`.\n            # `groups_per_node = num_groups // num_nodes`\n            # `group_size = num_logical_experts // num_groups`\n            # Number of logical experts per node = `(num_groups // num_nodes) * (num_logical_experts // num_groups)`\n            # = `num_logical_experts // num_nodes`. This is consistent.\n            \n            # The `mlogcnt_per_node` has shape [num_layers, num_nodes, num_logical_experts // num_groups].\n            # This means it contains replica counts for logical experts *within groups*.\n            # The input `weight` to `replicate_experts` was `node_logical_weights_flat` of shape `[num_layers * num_nodes, num_logical_experts // num_groups]`.\n            # So, the second dimension of `mlogcnt_per_node` corresponds to the logical experts *within the node's allocation*.\n            # Number of logical experts in the input to `replicate_experts` is `num_logical_experts // num_groups`.\n            # This is the number of logical experts *per group*.\n            # Wait, the input to `replicate_experts` was `node_logical_weights_flat` of shape `[num_layers * num_nodes, num_logical_experts // num_groups]`.\n            # This means `num_logical_experts // num_groups` is the number of logical experts *per node*.\n            # This is correct because each node gets `groups_per_node` groups, and each group has `group_size` logical experts.\n            # Total logical experts per node = `groups_per_node * group_size = (num_groups // num_nodes) * (num_logical_experts // num_groups) = num_logical_experts // num_nodes`.\n            # So, `num_logical_experts // num_groups` IS the number of logical experts per node.\n            \n            num_logical_experts_per_node = num_logical_experts // num_nodes\n            \n            for logical_expert_idx_in_node in range(num_logical_experts_per_node):\n                original_logical_expert_id = start_logical_idx_in_node + logical_expert_idx_in_node\n                logcnt_final[layer_idx, original_logical_expert_id] = mlogcnt_unflattened_per_node[layer_idx, node_idx, logical_expert_idx_in_node]\n\n    # Now, we need `phyrank`: [num_layers, num_physical_experts].\n    # `phyrank` from `replicate_experts` is `[num_layers * num_nodes, num_physical_experts // num_nodes]`.\n    # This represents the replica rank for physical experts *within a node*.\n    \n    # We need to map this to the final `phyrank` array.\n    # `phyrank_unflattened_per_node` has shape `[num_layers, num_nodes, num_physical_experts // num_nodes]`.\n    \n    phyrank_final = torch.zeros(num_layers, num_physical_experts, device=weight.device, dtype=torch.int64)\n    \n    for layer_idx in range(num_layers):\n        for node_idx in range(num_nodes):\n            for phy_idx_in_node in range(num_physical_experts // num_nodes):\n                global_phy_idx = node_idx * (num_physical_experts // num_nodes) + phy_idx_in_node\n                phyrank_final[layer_idx, global_phy_idx] = mlogcnt_unflattened_per_node[layer_idx, node_idx, phy_idx_in_node] # This is wrong, `phyrank` not `mlogcnt`\n                \n                # Let's call `replicate_experts` correctly and get the `phyrank`\n                # `phy2mlog_per_node`, `phyrank_per_node`, `mlogcnt_per_node`\n                \n                # We need to get the `phyrank` from `replicate_experts`\n                # `replicate_experts` returns `phy2log`, `rank`, `logcnt`.\n                # So, `rank` is what we need for `phyrank`.\n                \n                # Let's re-run `replicate_experts` to get the correct `phyrank`.\n                # `phy2mlog_per_node`, `rank_per_node`, `mlogcnt_per_node` = replicate_experts(...)\n                \n                # We need to get the `rank` output from `replicate_experts`.\n                # Let's assume `replicate_experts` returns `phy2log, rank, logcnt`.\n                \n                # `rank_per_node` has shape `[num_layers * num_nodes, num_physical_experts // num_nodes]`.\n                # This is the replica rank for physical experts within a node.\n                \n                # Reshape `rank_per_node` to `[num_layers, num_nodes, num_physical_experts // num_nodes]`.\n                rank_unflattened_per_node = rank_per_node.view(num_layers, num_nodes, -1)\n                \n                # Map this to the final `phyrank`.\n                phyrank_final[layer_idx, global_phy_idx] = rank_unflattened_per_node[layer_idx, node_idx, phy_idx_in_node]\n\n    # The current code uses `phyrank` from `replicate_experts` incorrectly.\n    # It uses `mlogcnt.view(num_layers, -1).gather(-1, log2mlog)` for `logcnt`.\n    # And `phyrank.gather(-1, pphy2phy).view(num_layers, -1)` for `pphyrank`.\n    \n    # Let's simplify the entire `rebalance_experts_hierarchical` function.\n    # The core idea is:\n    # 1. Pack logical experts into nodes.\n    # 2. Replicate logical experts within each node to create physical experts for that node.\n    # 3. Pack physical experts (from all nodes) onto GPUs.\n    \n    # Step 1: Pack groups to nodes (using `balanced_packing` on `tokens_per_group`).\n    # This results in `group_pack_index` which maps groups to nodes.\n    # We also need to map logical experts to nodes.\n    \n    # Let's use the `weight` tensor directly to determine logical expert loads.\n    # We want to pack `num_groups` groups into `num_nodes` nodes.\n    # The input to `balanced_packing` should be `[num_layers, num_groups]`, where each entry is the total load of logical experts in that group.\n    \n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(tokens_per_group, num_nodes)\n    \n    # Now, `group_pack_index` maps each group to a node.\n    # We need to determine which logical experts end up on which node.\n    \n    # Create a mapping from logical expert ID to node ID.\n    logical_expert_to_node_map = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n    \n    for layer_idx in range(num_layers):\n        for group_idx in range(num_groups):\n            node_idx = group_pack_index[layer_idx, group_idx]\n            start_logical_idx = group_idx * group_size\n            end_logical_idx = start_logical_idx + group_size\n            logical_expert_to_node_map[layer_idx, start_logical_idx:end_logical_idx] = node_idx\n    \n    # Step 2: Replicate logical experts within each node.\n    # For each node, we need to replicate its assigned logical experts.\n    # The number of physical experts per node is `num_physical_experts // num_nodes`.\n    \n    # We need to call `replicate_experts` for each node's set of logical experts.\n    # This means we need to extract the weights for logical experts belonging to each node.\n    \n    # For each layer, let's create a tensor of logical expert weights for each node.\n    # `node_logical_weights`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n    node_logical_weights = torch.zeros(num_layers, num_nodes, num_logical_experts // num_nodes, device=weight.device)\n    \n    for layer_idx in range(num_layers):\n        for node_idx in range(num_nodes):\n            # Find logical experts assigned to this node.\n            logical_experts_on_node_mask = (logical_expert_to_node_map[layer_idx] == node_idx)\n            logical_experts_on_node_indices = torch.where(logical_experts_on_node_mask)[0]\n            \n            # Assign their weights.\n            node_logical_weights[layer_idx, node_idx, :len(logical_experts_on_node_indices)] = weight[layer_idx, logical_experts_on_node_indices]\n    \n    # Now, for each layer and node, call `replicate_experts`.\n    # `phy2log_per_node`: [num_layers, num_nodes, num_physical_experts // num_nodes]\n    # `phyrank_per_node`: [num_layers, num_nodes, num_physical_experts // num_nodes]\n    # `logcnt_per_node`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n    \n    phy2log_per_node = torch.zeros(num_layers, num_nodes, num_physical_experts // num_nodes, dtype=torch.int64, device=weight.device)\n    phyrank_per_node = torch.zeros(num_layers, num_nodes, num_physical_experts // num_nodes, dtype=torch.int64, device=weight.device)\n    logcnt_per_node = torch.zeros(num_layers, num_nodes, num_logical_experts // num_nodes, dtype=torch.int64, device=weight.device)\n    \n    for layer_idx in range(num_layers):\n        for node_idx in range(num_nodes):\n            # Input weights for this node's logical experts.\n            current_node_weights = node_logical_weights[layer_idx, node_idx, :]\n            \n            # Filter out any zero-padded weights if `len(logical_experts_on_node_indices)` was less than `num_logical_experts // num_nodes`.\n            # This should not happen if the distribution is perfect, but good to be robust.\n            # The number of logical experts per node is `num_logical_experts // num_nodes`.\n            # So the input `current_node_weights` should have `num_logical_experts // num_nodes` elements.\n            \n            # Call replicate_experts for this node.\n            # The number of physical experts to create is `num_physical_experts // num_nodes`.\n            node_phy2log, node_phyrank, node_logcnt = replicate_experts(\n                current_node_weights.unsqueeze(0), num_physical_experts // num_nodes) # Unsqueeze to add layer dim\n            \n            # Store the results. `node_phy2log` is [1, num_physical_experts // num_nodes].\n            phy2log_per_node[layer_idx, node_idx, :] = node_phy2log.squeeze(0)\n            phyrank_per_node[layer_idx, node_idx, :] = node_phyrank.squeeze(0)\n            logcnt_per_node[layer_idx, node_idx, :] = node_logcnt.squeeze(0)\n    \n    # Now we have the replicated experts per node.\n    # `phy2log_per_node` maps physical experts within a node to logical experts.\n    # `phyrank_per_node` gives the replica rank.\n    # `logcnt_per_node` gives the replica count for logical experts within a node.\n    \n    # Step 3: Pack physical experts (from all nodes) onto GPUs.\n    # We need to create the input for `balanced_packing` for GPUs.\n    # This input should be `[num_layers, num_physical_experts]`, representing the load of each physical expert.\n    \n    # The load of a physical expert is the load of the logical expert it replicates.\n    # We need to find the load of the logical expert specified by `phy2log_per_node`.\n    \n    # Create the physical expert load tensor: `physical_expert_loads_for_gpu` [num_layers, num_physical_experts]\n    physical_expert_loads_for_gpu = torch.zeros(num_layers, num_physical_experts, device=weight.device)\n    \n    for layer_idx in range(num_layers):\n        for node_idx in range(num_nodes):\n            for phy_idx_in_node in range(num_physical_experts // num_nodes):\n                # Global index for the physical expert\n                global_phy_idx = node_idx * (num_physical_experts // num_nodes) + phy_idx_in_node\n                \n                # Get the logical expert ID for this physical expert\n                logical_expert_id_for_phy = phy2log_per_node[layer_idx, node_idx, phy_idx_in_node]\n                \n                # Get the load of this logical expert from the original `weight` tensor.\n                physical_expert_loads_for_gpu[layer_idx, global_phy_idx] = weight[layer_idx, logical_expert_id_for_phy]\n    \n    # Now, pack these physical experts onto GPUs.\n    # `num_gpus` is the total number of packs.\n    gpu_pack_index, gpu_rank_in_pack = balanced_packing(\n        physical_expert_loads_for_gpu, num_gpus)\n    \n    # `gpu_pack_index`: [num_layers, num_physical_experts] -> GPU index for each physical expert.\n    # `gpu_rank_in_pack`: [num_layers, num_physical_experts] -> rank of physical expert on GPU.\n    \n    # We need to construct the final `phy2log`, `phyrank`, `logcnt`.\n    \n    # `phy2log`: [num_layers, num_physical_experts]\n    # This is the logical expert ID for each physical expert.\n    # We already have `phy2log_per_node` which gives this information.\n    # We just need to flatten it correctly.\n    final_phy2log = phy2log_per_node.view(num_layers, -1) # Shape: [num_layers, num_physical_experts]\n    \n    # `phyrank`: [num_layers, num_physical_experts]\n    # This is the replica rank of each physical expert on its assigned GPU.\n    # We have `phyrank_per_node` and `gpu_rank_in_pack`.\n    # The `phyrank_per_node` is the rank of the logical expert replication.\n    # The `gpu_rank_in_pack` is the rank of the physical expert on the GPU.\n    # We need the rank of the physical expert on the GPU.\n    # So, `phyrank` should be `gpu_rank_in_pack`.\n    final_phyrank = gpu_rank_in_pack # Shape: [num_layers, num_physical_experts]\n    \n    # `logcnt`: [num_layers, num_logical_experts]\n    # This is the number of replicas for each logical expert.\n    # We have `logcnt_per_node` [num_layers, num_nodes, num_logical_experts // num_nodes].\n    # This gives the replica count for logical experts within their respective nodes.\n    # Since logical experts are partitioned by node, we can aggregate `logcnt_per_node`.\n    \n    final_logcnt = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n    \n    for layer_idx in range(num_layers):\n        for node_idx in range(num_nodes):\n            # Find logical experts assigned to this node.\n            logical_experts_on_node_mask = (logical_expert_to_node_map[layer_idx] == node_idx)\n            logical_experts_on_node_indices = torch.where(logical_experts_on_node_mask)[0]\n            \n            # Assign their replica counts.\n            final_logcnt[layer_idx, logical_experts_on_node_indices] = logcnt_per_node[layer_idx, node_idx, :len(logical_experts_on_node_indices)]\n    \n    return final_phy2log, final_phyrank, final_logcnt\n>>>>>>> REPLACE\n\n### Improvement 2: Optimize `balanced_packing` for speed.\n\nThe `balanced_packing` function is called multiple times. While the logic is generally sound, there might be opportunities for optimization within the loops.\n\n**Reasoning for Improvement:**\n*   **Efficiency:** `balanced_packing` is a core component. Optimizing it directly impacts the overall performance.\n*   **Reduced Overhead:** Minimizing tensor operations and conditional checks within the inner loops can yield significant speedups.\n\n<<<<<<< SEARCH\n            # Find the index within the `available_pack_indices` list that has the minimum weight.\n            local_min_idx = torch.argmin(weights_of_available_packs)\n            \n            # Map this local index back to the global pack index.\n            pack = available_pack_indices[local_min_idx]\n\n            # Assert that a valid pack was found and it has space.\n            assert current_pack_items[pack] < groups_per_pack\n\n            # Assign the group to the chosen pack.\n            # `group_id` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            # `weight[i, group_id]` is the weight of the current group.\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n=======\n            # Find the index within the `available_pack_indices` list that has the minimum weight.\n            # Use torch.min to get both the minimum value and its index.\n            min_weight, local_min_idx = torch.min(weights_of_available_packs, dim=0)\n            \n            # Map this local index back to the global pack index.\n            pack = available_pack_indices[local_min_idx]\n\n            # Assert that a valid pack was found and it has space.\n            # This assertion is crucial for correctness but can be removed in production if confident.\n            # assert current_pack_items[pack] < groups_per_pack\n\n            # Assign the group to the chosen pack.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n>>>>>>> REPLACE\n\n### Improvement 3: Refactor `rebalance_experts` for clarity and to handle the non-hierarchical case better.\n\nThe current `rebalance_experts` function has a conditional that calls `rebalance_experts_hierarchical` with different parameters for the \"global load balance policy\". This suggests that the hierarchical logic should be more adaptable or that a separate global balancing function might be cleaner.\n\n**Reasoning for Improvement:**\n*   **Readability and Maintainability:** A clearer separation of concerns makes the code easier to understand and modify.\n*   **Correctness:** Ensuring the global load balance policy is correctly implemented. The current code calls the hierarchical function with `num_groups=1, num_nodes=1`, which might not be the most efficient way to express global balancing.\n\n<<<<<<< SEARCH\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n=======\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n\n    if num_groups > 0 and num_nodes > 0 and num_groups % num_nodes == 0:\n        # Use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # Use global load-balance policy by treating all experts as a single group.\n        # This simplifies the logic to a single-level replication and packing.\n        # We effectively have `num_logical_experts` groups, each with 1 logical expert.\n        # `num_groups` becomes `num_logical_experts`.\n        # `groups_per_node` becomes `num_logical_experts // num_nodes` (if num_nodes > 0).\n        # If num_nodes is 0 or not applicable, we can treat it as a single node.\n        \n        # For global balancing, we can call a simplified version of hierarchical,\n        # or a dedicated global balancing function.\n        # Let's adapt `rebalance_experts_hierarchical` to handle the global case more directly.\n        \n        # If `num_groups` is not divisible by `num_nodes`, or if `num_nodes` is 0 or 1,\n        # we can consider a global balancing strategy.\n        # A global strategy would involve replicating experts across all `num_gpus` directly.\n        \n        # A simpler approach for global balancing:\n        # Treat all `num_logical_experts` as a single block of experts to be replicated.\n        # Then, pack these replicated experts onto `num_gpus`.\n        \n        # Call `replicate_experts` on the entire `weight` tensor to get `num_replicas`.\n        # The input `weight` to `replicate_experts` should be `[num_layers, num_logical_experts]`.\n        # The output `phy2log` will be `[num_layers, num_replicas]`.\n        # The output `rank` will be `[num_layers, num_replicas]`.\n        # The output `logcnt` will be `[num_layers, num_logical_experts]`.\n        \n        phy2log_global_rep, phyrank_global_rep, logcnt_global_rep = replicate_experts(\n            weight, num_replicas)\n        \n        # Now, pack these `num_replicas` physical experts onto `num_gpus`.\n        # The input to `balanced_packing` is the load of these physical experts.\n        # The load of a physical expert is the load of the logical expert it replicates.\n        # So, we need to use `phy2log_global_rep` to get the loads from the original `weight`.\n        \n        physical_expert_loads_global = torch.zeros(num_layers, num_replicas, device=weight.device)\n        \n        for layer_idx in range(num_layers):\n            for rep_idx in range(num_replicas):\n                logical_expert_id = phy2log_global_rep[layer_idx, rep_idx]\n                physical_expert_loads_global[layer_idx, rep_idx] = weight[layer_idx, logical_expert_id]\n        \n        # Pack these loads onto `num_gpus`.\n        gpu_pack_index_global, gpu_rank_in_pack_global = balanced_packing(\n            physical_expert_loads_global, num_gpus)\n        \n        # The final `phy2log` should be the logical expert ID for each physical expert.\n        # This is directly from `phy2log_global_rep`.\n        final_phy2log = phy2log_global_rep\n        \n        # The final `phyrank` should be the rank of the physical expert on its assigned GPU.\n        # This is `gpu_rank_in_pack_global`.\n        final_phyrank = gpu_rank_in_pack_global\n        \n        # The final `logcnt` is the number of replicas for each logical expert.\n        # This is directly from `logcnt_global_rep`.\n        final_logcnt = logcnt_global_rep\n        \n        # Update the return values\n        phy2log = final_phy2log\n        phyrank = final_phyrank\n        logcnt = final_logcnt\n\n    # The rest of the function remains the same for constructing log2phy.\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    # Ensure `phy2log` and `phyrank` have the correct shapes for scatter.\n    # `phy2log` should be [num_layers, num_replicas].\n    # `phyrank` should be [num_layers, num_replicas].\n    # `logcnt` should be [num_layers, num_logical_experts].\n    \n    # The scatter operation needs to map `phy2log` and `phyrank` to `log2phy`.\n    # `log2phy[layer, logical_expert, rank]` should store the physical expert ID.\n    # The current scatter logic:\n    # `log2phy.view(num_layers, -1).scatter_(-1, phy2log * maxlogcnt + phyrank, ...)`\n    # This implies `phy2log` is used to index into the `maxlogcnt` blocks.\n    # `phy2log` is [num_layers, num_replicas].\n    # `phyrank` is [num_layers, num_replicas].\n    # `maxlogcnt` is the number of replicas per logical expert.\n    # So `phy2log * maxlogcnt + phyrank` correctly calculates the linear index within the `log2phy` tensor for a given logical expert and its replica rank.\n    \n    # The indices for scatter are:\n    # `phy2log` (logical expert ID) * `maxlogcnt` (max replicas per LE) + `phyrank` (rank of this replica)\n    \n    # Ensure `phy2log` and `phyrank` are on the same device as `log2phy`.\n    phy2log = phy2log.to(log2phy.device)\n    phyrank = phyrank.to(log2phy.device)\n\n    # The values to scatter are the physical expert IDs.\n    # The physical expert ID is the column index in `phy2log` and `phyrank`.\n    # So, `torch.arange(num_replicas, dtype=torch.int64, device=log2phy.device).expand(num_layers, -1)` is correct.\n    \n    scatter_indices = phy2log * maxlogcnt + phyrank\n    scatter_values = torch.arange(num_replicas, dtype=torch.int64, device=log2phy.device).expand(num_layers, -1)\n    \n    log2phy.scatter_(-1, scatter_indices, scatter_values)\n    \n    return phy2log, log2phy, logcnt\n>>>>>>> REPLACE\n"]}}}