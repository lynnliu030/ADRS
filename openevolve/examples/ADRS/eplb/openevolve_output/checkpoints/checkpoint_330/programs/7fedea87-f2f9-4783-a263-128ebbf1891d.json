{"id": "7fedea87-f2f9-4783-a263-128ebbf1891d", "code": "# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    # Removed assert num_groups % num_packs == 0 to allow non-divisible cases\n    # Calculate target items per pack for uneven distribution\n    base_groups_per_pack = num_groups // num_packs\n    remainder_groups = num_groups % num_packs\n\n    # max_items_per_pack will be a tensor of shape [num_packs]\n    # The first 'remainder_groups' packs get base + 1 items, the rest get base items.\n    max_items_per_pack = torch.full((num_packs,), base_groups_per_pack,\n                                    dtype=torch.int64, device=weight.device)\n    if remainder_groups > 0:\n        max_items_per_pack[:remainder_groups] += 1\n\n    # Special case: if num_groups <= num_packs, each pack gets at most one item.\n    # The original `if groups_per_pack == 1` handled `num_groups == num_packs`.\n    # This generalized logic handles `num_groups < num_packs` too,\n    # where some packs will remain empty.\n    if num_groups <= num_packs:\n        # If there are fewer or equal items than packs, assign each item to its own pack.\n        # This is a simplified assignment since there's plenty of space.\n        pack_index = torch.full_like(weight,\n                                     fill_value=-1,\n                                     dtype=torch.int64,\n                                     device=weight.device)\n        # Assign group_id to pack_id (0 to num_groups-1)\n        pack_index[:, :num_groups] = torch.arange(num_groups, dtype=torch.int64, device=weight.device)\n        rank_in_pack = torch.zeros_like(pack_index, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    indices = weight.float().sort(-1, descending=True).indices\n    pack_index = torch.full_like(weight,\n                                 fill_value=-1,\n                                 dtype=torch.int64,\n                                 device=weight.device)\n    rank_in_pack = torch.full_like(pack_index,\n                                   fill_value=-1,\n                                   device=weight.device)\n    # Initialize tensors to hold pack weights and item counts for all layers.\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Pre-create an infinite scalar tensor on the correct device to avoid repeated creation in the inner loop\n    inf_scalar_tensor = torch.tensor(float('inf'), device=weight.device)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        # Make a copy of max_items_per_pack for current layer's capacity tracking.\n        current_max_items_per_pack = max_items_per_pack.clone()\n        for group_id in indices[i]:\n            # Get current state for this layer's packs.\n            current_pack_weights = pack_weights_per_layer[i]\n            current_pack_items = pack_items_per_layer[i]\n\n            # Find available packs (those not yet full).\n            # `available_mask` is a boolean tensor.\n            # Use current_max_items_per_pack for capacity check\n            available_mask = (current_pack_items < current_max_items_per_pack)\n\n            # Find the pack that, after adding the current group's weight,\n            # results in the minimum maximum pack weight.\n            # This is a more sophisticated greedy choice that aims to minimize the peak load.\n            potential_new_weights = torch.where(\n                available_mask,\n                current_pack_weights + weight[i, group_id],\n                inf_scalar_tensor\n            )\n            # We want to select the pack that minimizes the *maximum* weight among all packs *after* assignment.\n            # This is equivalent to picking the pack that results in the smallest value for\n            # torch.max(torch.where(available_mask, current_pack_weights + weight[i, group_id], inf_scalar_tensor))\n            # However, a simpler greedy heuristic that often works well is to select the pack\n            # which, after adding the current item, results in the minimum *individual* pack weight.\n            # This is what the following line does:\n            pack = torch.argmin(potential_new_weights).item()\n\n            # No assert needed here, as the available_mask ensures a valid pack is chosen.\n            # If all packs are full, available_mask would be all False,\n            # and torch.where would make all weights inf, resulting in argmin(0).\n            # This implies an error state where num_groups > sum(max_items_per_pack).\n            # But sum(max_items_per_pack) is always num_groups, so this won't happen.\n\n            # Assign the group to the chosen pack.\n            # `group_id` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            # `weight[i, group_id]` is the weight of the current group.\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    # Calculate initial average loads for all logical experts\n    # Avoid division by zero if logcnt is 0 (though it's initialized to 1)\n    avg_loads = weight / torch.clamp(logcnt, min=1e-9)\n\n    for i in range(num_log, num_phy):\n        # Find the logical expert with the maximum current average load\n        # This is the core of the LPT greedy strategy.\n        # Using topk is generally efficient for finding the largest elements.\n        # We need to find the index of the maximum average load.\n        # `torch.topk(avg_loads, 1, dim=-1)` returns values and indices.\n        # We need the index of the maximum.\n        _, redundant_indices_flat = torch.topk(avg_loads.flatten(), 1, largest=True)\n        # Convert flat index to multi-dimensional index for `redundant_indices`\n        # This is tricky because `avg_loads` is [n, num_log]\n        # Let's stick to the current `max(dim=-1)` for simplicity and correctness\n        # as it directly gives the indices per layer.\n\n        # Reverting to the original method for finding the index, as\n        # managing dynamic average loads and their indices across layers\n        # can be complex and error-prone for potentially marginal gains.\n        # The current approach is clear and directly implements the LPT heuristic.\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n\n        # Assign the selected logical expert to the current physical expert slot\n        phy2log[:, i] = redundant_indices\n        # Assign the replica rank (which is the current count of replicas for that logical expert)\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        # Increment the replica count for the chosen logical expert\n        logcnt[arangen, redundant_indices] += 1\n\n        # Update the average loads for the next iteration.\n        # This is where the optimization could be.\n        # Instead of recalculating `weight / logcnt` entirely, we can update\n        # the average load for the expert that just received a replica.\n        # However, `logcnt` is updated per layer, so updating `avg_loads`\n        # needs careful indexing.\n\n        # For efficiency and simplicity, let's stick to the current recalculation.\n        # If performance becomes a critical issue, profiling this line would be key.\n        # A potential speedup might come from pre-sorting logical experts by their initial load.\n        # But the dynamic nature of logcnt makes this difficult to leverage directly.\n\n        # Let's consider a small optimization: if we are adding many replicas,\n        # the load distribution might stabilize.\n        # However, the greedy approach must make a decision at each step.\n\n        # Final decision: The current implementation of `replicate_experts` is\n        # a standard and reasonably efficient greedy approach. Significant\n        # speedups might require a different algorithmic approach (e.g.,\n        # approximation algorithms for bin packing or flow-based methods),\n        # which could increase complexity. For now, we'll keep the logic as is\n        # but focus on the `balanced_packing` refinement.\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float()\n    # Use hierarchical load-balance policy regardless of num_groups % num_nodes\n    # The balanced_packing function has been updated to handle non-divisible cases gracefully.\n    phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n        weight, num_replicas, num_groups, num_nodes, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n", "language": "python", "parent_id": "c4f12bbc-a970-42fb-8bbe-f50b421ead07", "generation": 7, "timestamp": 1751069003.9431024, "iteration_found": 301, "metrics": {"balancedness_score": 0.29830835153138974, "speed_score": 0.02782364866315802, "combined_score": 0.16306600009727387}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 5 lines with 15 lines\nChange 2: Replace 9 lines with 52 lines\nChange 3: Replace 5 lines with 10 lines", "parent_metrics": {"balancedness_score": 0.29830835153138974, "speed_score": 0.030118157372447014, "combined_score": 0.16421325445191837}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve the Mixture-of-Expert models Expert Parallelism Load Balancer (MoE EPLB) expert rearrangement algorithm.\nThis algorithm will take the load metrics recorded by the vLLM server, and rearrange the experts to balance the load. It can make replicas of some experts to achieve better load balancing.\nYour goal will be two-fold: 1. Improve the algorithm to achieve better load balancing; while 2. Improve the algorithm to be more efficient, i.e. reduce the execution time of the algorithm itself, since perfect load balancing is NP-hard.\nThe current algorithm is implemented in the `rebalance_experts` function. ", "user": "# Current Program Information\n- Current performance metrics: - balancedness_score: 0.2983\n- speed_score: 0.0301\n- combined_score: 0.1642\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: balancedness_score, speed_score, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1125, combined_score: 0.2326\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1126, combined_score: 0.2327\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1143, combined_score: 0.2335\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.2335)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1143), Performs well on combined_score (0.2335)\n\n\n### Program 2 (Score: 0.2327)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1126), Performs well on combined_score (0.2327)\n\n\n### Program 3 (Score: 0.2326)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1125), Performs well on combined_score (0.2326)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.2322)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n\n### Program D2 (Score: 0.2325)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.2335, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 32 lines with 17 lines, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 2 (Score: 0.1920, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 3 (Score: 0.1920, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 4 (Score: 0.1914, Type: Migrant)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 5 (Score: 0.2312, Type: Migrant)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative speed_score approach, Alternative combined_score approach, Comprehensive implementation\n\n\n\n# Current Program\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    # Removed assert num_groups % num_packs == 0 to allow non-divisible cases\n    # Calculate target items per pack for uneven distribution\n    base_groups_per_pack = num_groups // num_packs\n    remainder_groups = num_groups % num_packs\n\n    # max_items_per_pack will be a tensor of shape [num_packs]\n    # The first 'remainder_groups' packs get base + 1 items, the rest get base items.\n    max_items_per_pack = torch.full((num_packs,), base_groups_per_pack,\n                                    dtype=torch.int64, device=weight.device)\n    if remainder_groups > 0:\n        max_items_per_pack[:remainder_groups] += 1\n\n    # Special case: if num_groups <= num_packs, each pack gets at most one item.\n    # The original `if groups_per_pack == 1` handled `num_groups == num_packs`.\n    # This generalized logic handles `num_groups < num_packs` too,\n    # where some packs will remain empty.\n    if num_groups <= num_packs:\n        # If there are fewer or equal items than packs, assign each item to its own pack.\n        # This is a simplified assignment since there's plenty of space.\n        pack_index = torch.full_like(weight,\n                                     fill_value=-1,\n                                     dtype=torch.int64,\n                                     device=weight.device)\n        # Assign group_id to pack_id (0 to num_groups-1)\n        pack_index[:, :num_groups] = torch.arange(num_groups, dtype=torch.int64, device=weight.device)\n        rank_in_pack = torch.zeros_like(pack_index, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    indices = weight.float().sort(-1, descending=True).indices\n    pack_index = torch.full_like(weight,\n                                 fill_value=-1,\n                                 dtype=torch.int64,\n                                 device=weight.device)\n    rank_in_pack = torch.full_like(pack_index,\n                                   fill_value=-1,\n                                   device=weight.device)\n    # Initialize tensors to hold pack weights and item counts for all layers.\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Pre-create an infinite scalar tensor on the correct device to avoid repeated creation in the inner loop\n    inf_scalar_tensor = torch.tensor(float('inf'), device=weight.device)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        # Make a copy of max_items_per_pack for current layer's capacity tracking.\n        current_max_items_per_pack = max_items_per_pack.clone()\n        for group_id in indices[i]:\n            # Get current state for this layer's packs.\n            current_pack_weights = pack_weights_per_layer[i]\n            current_pack_items = pack_items_per_layer[i]\n\n            # Find available packs (those not yet full).\n            # `available_mask` is a boolean tensor.\n            # Use current_max_items_per_pack for capacity check\n            available_mask = (current_pack_items < current_max_items_per_pack)\n\n            # Find the pack with the minimum current weight among available ones.\n            # Use torch.where to set weights of unavailable packs to infinity for argmin.\n            pack = torch.argmin(\n                torch.where(available_mask, current_pack_weights, inf_scalar_tensor)\n            ).item()\n\n            # No assert needed here, as the available_mask ensures a valid pack is chosen.\n            # If all packs are full, available_mask would be all False,\n            # and torch.where would make all weights inf, resulting in argmin(0).\n            # This implies an error state where num_groups > sum(max_items_per_pack).\n            # But sum(max_items_per_pack) is always num_groups, so this won't happen.\n\n            # Assign the group to the chosen pack.\n            # `group_id` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            # `weight[i, group_id]` is the weight of the current group.\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        # To minimize the maximum load, we should add replicas to the\n        # logical experts with the maximum current average load. This\n        # aligns with the \"Longest Processing Time\" (LPT) greedy strategy\n        # for minimizing makespan, which is effective for load balancing.\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float()\n    # Use hierarchical load-balance policy regardless of num_groups % num_nodes\n    # The balanced_packing function has been updated to handle non-divisible cases gracefully.\n    phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n        weight, num_replicas, num_groups, num_nodes, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["The current program attempts to balance expert loads by first packing groups onto nodes, then replicating experts within nodes, and finally packing physical experts onto GPUs. The `balanced_packing` function uses a greedy approach to assign items to packs, aiming to balance the weights of the packs.\n\nHere are some suggestions for improvement:\n\n### Improvement 1: More Robust Handling of Uneven Distributions in `balanced_packing`\n\nThe `balanced_packing` function aims to distribute `num_groups` items across `num_packs`. While it handles cases where `num_groups` is not perfectly divisible by `num_packs` by assigning one extra item to the first `remainder_groups` packs, the greedy assignment within the loop might not always yield the *most* balanced distribution.\n\nA more robust approach would be to use a min-cost max-flow algorithm or a more sophisticated greedy strategy that considers the overall distribution more holistically. However, to maintain efficiency and simplify the code, we can refine the existing greedy strategy.\n\nThe current logic assigns the next item (group) to the pack that currently has the minimum weight, provided that pack is not full. This can be improved by:\n\n1.  **Prioritizing packs with fewer items:** Even if a pack has a slightly higher current weight, if it has significantly fewer items than other available packs, assigning the next item to it might lead to a better long-term balance.\n2.  **Considering the weight of the item being assigned:** The impact of assigning an item to a pack depends on the item's weight. High-weight items should ideally be placed in packs that have lower current weights.\n\nGiven the goal of improving efficiency and readability, a simpler refinement to the existing greedy approach is to ensure that when selecting a pack, we not only consider the current pack weight but also the *potential* increase in weight.\n\nLet's refine the selection of the `pack` within the loop. Instead of just picking the `argmin` of `current_pack_weights`, we can consider the *difference* in weights between the current pack and the next lightest available pack. This can help to avoid situations where a very heavy item is added to an already heavily loaded pack, potentially exacerbating imbalance.\n\nHowever, a simpler and more direct improvement for the greedy strategy is to ensure that the `max_items_per_pack` is correctly utilized and that the selection of the \"best\" pack is truly minimizing the *maximum* load after the assignment.\n\nThe current code:\n```python\n            # Find the pack with the minimum current weight among available ones.\n            # Use torch.where to set weights of unavailable packs to infinity for argmin.\n            pack = torch.argmin(\n                torch.where(available_mask, current_pack_weights, inf_scalar_tensor)\n            ).item()\n```\nThis finds the pack with the minimum current weight among available packs. A better strategy might be to find the pack that, *after* adding the current item, results in the smallest increase in the *maximum* pack weight.\n\nLet's try to implement a greedy approach that tries to minimize the maximum load *after* assigning the current item.\n\n**Reasoning for the change:**\nThe current greedy approach picks the pack with the minimum current weight. While this is a good heuristic, it doesn't directly account for the weight of the item being placed. A better greedy strategy would be to select the pack that, after adding the current item, results in the smallest *maximum* load among all packs. This is closer to a \"minimum makespan\" objective.\n\nLet's try to implement this by calculating the potential new weights for all available packs and then selecting the pack that minimizes the maximum of these potential weights.\n\n```python\n            # Find available packs (those not yet full).\n            available_mask = (current_pack_items < current_max_items_per_pack)\n\n            # Calculate potential new weights if the current group is added to each available pack.\n            potential_pack_weights = torch.where(\n                available_mask,\n                current_pack_weights + weight[i, group_id],\n                inf_scalar_tensor\n            )\n\n            # Find the pack that results in the minimum maximum weight after assignment.\n            # This is a more sophisticated greedy choice.\n            # We need to find the pack that minimizes the maximum of (current_pack_weights + weight[i, group_id])\n            # among the available packs.\n            # This can be achieved by iterating or by a more complex tensor operation.\n            # A simpler approach that is still greedy and often effective:\n            # Find the pack that has the minimum current weight, but also consider the item's weight.\n            # Let's stick to a simpler greedy choice for now to maintain performance and readability.\n            # The current implementation already picks the minimum weight pack.\n            # A potential improvement is to consider the *difference* between the current pack's weight\n            # and the maximum pack weight.\n\n            # Let's refine the selection to consider the maximum load *after* adding the item.\n            # Find the pack that, after adding the current group's weight, results in the minimum *maximum* pack weight.\n            # This is a bit more complex and might impact performance.\n            # For now, let's stick to the current greedy approach as it's simple and efficient.\n            # If we want to improve balancedness, we might need to explore more complex algorithms\n            # or adjust the objective.\n\n            # Reverting to the original logic for pack selection as a baseline for further iteration.\n            # The current greedy choice is to pick the pack with the minimum current weight.\n            pack = torch.argmin(\n                torch.where(available_mask, current_pack_weights, inf_scalar_tensor)\n            ).item()\n```\n\nThe above is a discussion and not a direct code change. Let's consider a more direct change to improve the greedy selection.\n\nThe original `balanced_packing` function uses a greedy approach that sorts items by weight and then assigns them to the least loaded pack that is not yet full. This is a standard greedy approach for the multiway number partitioning problem. While effective, it's not guaranteed to find the optimal solution.\n\nA minor improvement within the greedy framework could be to consider the *difference* between the heaviest and lightest packs when making a decision. However, this adds complexity.\n\nLet's focus on a potential simplification of the `balanced_packing` function that might improve efficiency and readability without sacrificing too much balancedness. The current implementation iterates through each layer and then through each group within that layer.\n\nThe core of the greedy assignment is:\n```python\n            # Find the pack with the minimum current weight among available ones.\n            pack = torch.argmin(\n                torch.where(available_mask, current_pack_weights, inf_scalar_tensor)\n            ).item()\n```\nThis line selects the pack with the minimum current weight among those that are not full.\n\nA potentially more balanced approach, though computationally more expensive, would be to sort the *available packs* by their current weight and then pick the one that results in the smallest *increase* in the *maximum* pack weight after adding the current item.\n\nHowever, the prompt emphasizes improving *efficiency* and *readability*. The current implementation is already quite efficient for a greedy approach. The complexity comes from the nested loops and tensor operations.\n\nLet's reconsider the `balanced_packing` function's logic. The goal is to pack `num_groups` into `num_packs`. The current code sorts groups by weight within each layer and then greedily assigns them.\n\nA key observation is that the `weight` tensor is `[num_layers, num_groups]`. The packing is done *per layer*. This means that the balancing is done independently for each layer.\n\nIf we want to improve *balancedness*, we might need to consider the total load across all layers, but that's a different problem. Within the per-layer packing, the current greedy strategy is reasonable.\n\nLet's look for opportunities to simplify or optimize the existing code.\n\nThe `max_items_per_pack` calculation is correct for distributing items as evenly as possible.\n\nThe special case `if num_groups <= num_packs:` is handled correctly.\n\nThe inner loop iterates through `indices[i]`, which are the group IDs sorted by weight for layer `i`.\n\nThe most computationally intensive part is the `torch.argmin` inside the inner loop. For each group in each layer, we are performing this operation.\n\nLet's consider if we can avoid some of these `argmin` operations or make them more efficient.\n\nOne potential area for improvement in `balanced_packing` could be how `current_pack_weights` and `current_pack_items` are accessed and updated. The current code uses `pack_weights_per_layer[i]` and `pack_items_per_layer[i]`.\n\nLet's try to optimize the selection of the `pack` within the inner loop. Instead of recalculating `available_mask` and then using `torch.where` with `inf_scalar_tensor` for `argmin`, we can maintain a list of available packs and their weights.\n\nThis might make the code more complex. Let's stick to improving the existing logic where possible.\n\nA subtle point in `balanced_packing`:\nThe line `pack = torch.argmin(torch.where(available_mask, current_pack_weights, inf_scalar_tensor)).item()` is critical. It finds the index of the pack with the minimum weight among the available ones.\n\nConsider the scenario where multiple packs have the same minimum weight. `argmin` will return the first one. This might not be ideal if one of those packs is already close to its capacity.\n\nA more robust greedy choice could be to select the available pack that, after adding the current item, has the minimum *new* weight.\n\nLet's try to implement this refinement for `balanced_packing`.\n\n```python\n<<<<<<< SEARCH\n            # Find the pack with the minimum current weight among available ones.\n            # Use torch.where to set weights of unavailable packs to infinity for argmin.\n            pack = torch.argmin(\n                torch.where(available_mask, current_pack_weights, inf_scalar_tensor)\n            ).item()\n=======\n            # Find the pack that, after adding the current group's weight,\n            # results in the minimum maximum pack weight.\n            # This is a more sophisticated greedy choice that aims to minimize the peak load.\n            potential_new_weights = torch.where(\n                available_mask,\n                current_pack_weights + weight[i, group_id],\n                inf_scalar_tensor\n            )\n            # We want to select the pack that minimizes the *maximum* weight among all packs *after* assignment.\n            # This is equivalent to picking the pack that results in the smallest value for\n            # torch.max(torch.where(available_mask, current_pack_weights + weight[i, group_id], inf_scalar_tensor))\n            # However, a simpler greedy heuristic that often works well is to select the pack\n            # which, after adding the current item, results in the minimum *individual* pack weight.\n            # This is what the following line does:\n            pack = torch.argmin(potential_new_weights).item()\n>>>>>>> REPLACE\n```\n\nThis change aims to improve the `balancedness_score` by making a more informed greedy choice. The `speed_score` might see a slight decrease due to the additional calculation, but the `combined_score` could improve if the balancedness improvement is significant.\n\n### Improvement 2: Streamlining `replicate_experts`\n\nThe `replicate_experts` function implements a greedy strategy (similar to LPT) to replicate experts to minimize the maximum load. The core logic involves iteratively adding replicas to the logical expert with the current highest average load.\n\n```python\n    for i in range(num_log, num_phy):\n        # To minimize the maximum load, we should add replicas to the\n        # logical experts with the maximum current average load. This\n        # aligns with the \"Longest Processing Time\" (LPT) greedy strategy\n        # for minimizing makespan, which is effective for load balancing.\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n```\n\nThe calculation `(weight / logcnt).max(dim=-1).indices` is performed in each iteration. This involves division and a reduction.\n\nA potential simplification for efficiency and readability: instead of recalculating `weight / logcnt` every time, we could maintain the current average loads and update them. However, this adds state management.\n\nLet's consider if the `logcnt` update can be made more efficient. The current update `logcnt[arangen, redundant_indices] += 1` is already quite efficient as it uses advanced indexing.\n\nThe main computation is finding `redundant_indices`. This is essentially finding the index of the maximum value in `weight / logcnt` along the `dim=-1` (logical experts) dimension.\n\nIf `num_phy` is much larger than `num_log`, this loop can be long.\n\nOne way to potentially improve efficiency is to use a priority queue (heap) to keep track of the logical experts' current loads. This would allow us to quickly find the expert with the maximum load. However, implementing a heap in PyTorch might not always be faster than the current tensor operations, especially if the number of logical experts is not extremely large.\n\nLet's look for a simpler optimization. The current code calculates `weight / logcnt`.\n\nConsider the case where `logcnt` is 1 for all experts initially. Then `weight / logcnt` is just `weight`. The first few iterations will pick experts with the highest `weight`. As `logcnt` increases, the average load calculation becomes more important.\n\nCould we pre-sort the logical experts by their initial weights? This might help in the early stages of replication.\n\nLet's analyze the current line: `redundant_indices = (weight / logcnt).max(dim=-1).indices`. This is already quite optimized for PyTorch.\n\nPerhaps the opportunity for improvement lies in how the `phy2log` and `rank` are populated.\n\nThe `phy2log[:, i] = redundant_indices` assigns the logical expert to the current physical expert slot.\nThe `rank[:, i] = logcnt[arangen, redundant_indices]` assigns the replica rank.\n\nLet's consider a more direct way to manage the replication. Instead of iterating and assigning one by one, could we determine the number of replicas for each logical expert more directly?\n\nThe problem of distributing `num_redundant` items (replicas) to `num_log` bins (logical experts) to minimize the maximum bin load is a variation of the multiway number partitioning problem. The LPT (Longest Processing Time) heuristic used here is a good approximation.\n\nIf we want to improve `speed_score`, we might need to reduce the number of operations. The current loop runs `num_redundant` times.\n\nA possible simplification: if `num_redundant` is large, we could potentially process replications in batches. However, the greedy nature of the algorithm makes batching tricky.\n\nLet's focus on the `replicate_experts` function and see if there's a way to simplify the logic or make the core operation more efficient.\n\nThe core operation is `(weight / logcnt).max(dim=-1)`. This is performed `num_redundant` times.\n\nWhat if we pre-calculate `weight / logcnt` and then use `torch.topk` to find the top `num_redundant` assignments? This isn't quite right because the `logcnt` changes dynamically.\n\nLet's consider the `replicate_experts` function again. The goal is to assign `num_phy - num_log` additional replicas. The current approach iteratively finds the logical expert with the highest average load and assigns a replica to it.\n\nA potential simplification for readability and maintainability could be to make the `replicate_experts` function more self-contained or to use more descriptive variable names. However, the current code is reasonably clear.\n\nLet's consider the performance aspect. The bottleneck is the loop and the `max(dim=-1)` operation within it.\n\nIf `num_log` is small and `num_phy` is large, this loop can be very long.\n\nCould we optimize the selection of `redundant_indices`?\nInstead of `(weight / logcnt).max(dim=-1).indices`, what if we use a different approach?\n\nConsider the total load `weight`. We want to distribute `num_phy` physical experts such that the maximum load is minimized.\n\nThe current method is an iterative greedy approach.\nLet's try to optimize the calculation of `redundant_indices`.\n\nIf we have the current `logcnt`, the average load for expert `j` is `weight[:, j] / logcnt[:, j]`. We want to find the `j` that maximizes this average load.\n\nA potential optimization for `replicate_experts`:\nInstead of recalculating `weight / logcnt` repeatedly, we could maintain a tensor of average loads.\n\n```python\n<<<<<<< SEARCH\n    for i in range(num_log, num_phy):\n        # To minimize the maximum load, we should add replicas to the\n        # logical experts with the maximum current average load. This\n        # aligns with the \"Longest Processing Time\" (LPT) greedy strategy\n        # for minimizing makespan, which is effective for load balancing.\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n=======\n    # Calculate initial average loads for all logical experts\n    # Avoid division by zero if logcnt is 0 (though it's initialized to 1)\n    avg_loads = weight / torch.clamp(logcnt, min=1e-9)\n\n    for i in range(num_log, num_phy):\n        # Find the logical expert with the maximum current average load\n        # This is the core of the LPT greedy strategy.\n        # Using topk is generally efficient for finding the largest elements.\n        # We need to find the index of the maximum average load.\n        # `torch.topk(avg_loads, 1, dim=-1)` returns values and indices.\n        # We need the index of the maximum.\n        _, redundant_indices_flat = torch.topk(avg_loads.flatten(), 1, largest=True)\n        # Convert flat index to multi-dimensional index for `redundant_indices`\n        # This is tricky because `avg_loads` is [n, num_log]\n        # Let's stick to the current `max(dim=-1)` for simplicity and correctness\n        # as it directly gives the indices per layer.\n\n        # Reverting to the original method for finding the index, as\n        # managing dynamic average loads and their indices across layers\n        # can be complex and error-prone for potentially marginal gains.\n        # The current approach is clear and directly implements the LPT heuristic.\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n\n        # Assign the selected logical expert to the current physical expert slot\n        phy2log[:, i] = redundant_indices\n        # Assign the replica rank (which is the current count of replicas for that logical expert)\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        # Increment the replica count for the chosen logical expert\n        logcnt[arangen, redundant_indices] += 1\n\n        # Update the average loads for the next iteration.\n        # This is where the optimization could be.\n        # Instead of recalculating `weight / logcnt` entirely, we can update\n        # the average load for the expert that just received a replica.\n        # However, `logcnt` is updated per layer, so updating `avg_loads`\n        # needs careful indexing.\n\n        # For efficiency and simplicity, let's stick to the current recalculation.\n        # If performance becomes a critical issue, profiling this line would be key.\n        # A potential speedup might come from pre-sorting logical experts by their initial load.\n        # But the dynamic nature of logcnt makes this difficult to leverage directly.\n\n        # Let's consider a small optimization: if we are adding many replicas,\n        # the load distribution might stabilize.\n        # However, the greedy approach must make a decision at each step.\n\n        # Final decision: The current implementation of `replicate_experts` is\n        # a standard and reasonably efficient greedy approach. Significant\n        # speedups might require a different algorithmic approach (e.g.,\n        # approximation algorithms for bin packing or flow-based methods),\n        # which could increase complexity. For now, we'll keep the logic as is\n        # but focus on the `balanced_packing` refinement.\n>>>>>>> REPLACE\n```\n\nUpon reflection, the `replicate_experts` function's core logic `(weight / logcnt).max(dim=-1).indices` is quite efficient using PyTorch's vectorized operations. Trying to optimize it further with explicit loops or priority queues might not yield significant gains and could reduce readability. The current implementation is a good balance of performance and clarity for the LPT heuristic.\n\nTherefore, the primary focus for improvement will be on the `balanced_packing` function.\n\nLet's finalize the change for `balanced_packing`.\n\n```python\n<<<<<<< SEARCH\n            # Find the pack with the minimum current weight among available ones.\n            # Use torch.where to set weights of unavailable packs to infinity for argmin.\n            pack = torch.argmin(\n                torch.where(available_mask, current_pack_weights, inf_scalar_tensor)\n            ).item()\n=======\n            # Find the pack that, after adding the current group's weight,\n            # results in the minimum *individual* pack weight among available packs.\n            # This is a greedy heuristic that aims to keep individual pack loads low.\n            # It's a slight variation on simply picking the minimum current weight.\n            potential_new_weights = torch.where(\n                available_mask,\n                current_pack_weights + weight[i, group_id],\n                inf_scalar_tensor  # Ensure unavailable packs are not chosen\n            )\n            pack = torch.argmin(potential_new_weights).item()\n>>>>>>> REPLACE\n```\n\nThis change aims to improve the balancedness by making a slightly more informed greedy choice. The `speed_score` might see a very minor decrease due to the extra addition and `argmin` operation, but the potential for improved balancedness is the primary goal here.\n\nThe current program's performance metrics show a regression. The proposed change in `balanced_packing` is a targeted improvement to the core greedy strategy for balancing, which directly addresses the `balancedness_score`. If this change leads to a significant improvement in `balancedness_score` that outweighs any minor decrease in `speed_score`, the `combined_score` should improve.\n\nLet's consider if there are any other obvious simplification opportunities.\nThe `rebalance_experts_hierarchical` function orchestrates the process. The mapping and inverse operations are standard.\n\nThe `rebalance_experts` function acts as an entry point and handles the final mapping construction.\n\nThe current code seems to have a good structure. The main area for improvement, based on the prompt and the nature of the algorithm, is the quality of the load balancing achieved by `balanced_packing`.\n\nFinal check on the `balanced_packing` refinement:\nThe original code selects the pack with the minimum *current* weight.\nThe proposed change selects the pack that, after adding the item, will have the minimum *new* weight. This is a more direct way to try and keep all pack weights low. It's still a greedy approach, but it's slightly more forward-looking.\n\nThe `inf_scalar_tensor` is used correctly to mask out unavailable packs for `argmin`.\n\nThis change is localized to a single line within the inner loop of `balanced_packing`, making it a focused improvement.\n\nOne more thought on `balanced_packing`: The initial sorting of `indices = weight.float().sort(-1, descending=True).indices` is crucial for the greedy strategy. This ensures that heavier groups are considered first. This part is correct.\n\nThe `max_items_per_pack` calculation correctly handles uneven distribution.\n\nThe core of the greedy choice is indeed the `argmin` line. The proposed change modifies how the \"best\" pack is chosen.\n\nLet's consider the `num_groups <= num_packs` case.\n`pack_index[:, :num_groups] = torch.arange(num_groups, dtype=torch.int64, device=weight.device)`\nThis assigns each of the `num_groups` to its own pack. This is correct.\n\nThe `balanced_packing` function itself is responsible for balancing the load across `num_packs`. The `rebalance_experts_hierarchical` function uses this to pack groups onto nodes and then uses `replicate_experts` and `balanced_packing` again for packing physical experts onto GPUs.\n\nThe change to `balanced_packing` is the most direct way to improve the \"balancedness_score\" as it's the fundamental packing routine.\n\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    # Removed assert num_groups % num_packs == 0 to allow non-divisible cases\n    # Calculate target items per pack for uneven distribution\n    base_groups_per_pack = num_groups // num_packs\n    remainder_groups = num_groups % num_packs\n\n    # max_items_per_pack will be a tensor of shape [num_packs]\n    # The first 'remainder_groups' packs get base + 1 items, the rest get base items.\n    max_items_per_pack = torch.full((num_packs,), base_groups_per_pack,\n                                    dtype=torch.int64, device=weight.device)\n    if remainder_groups > 0:\n        max_items_per_pack[:remainder_groups] += 1\n\n    # Special case: if num_groups <= num_packs, each pack gets at most one item.\n    # The original `if groups_per_pack == 1` handled `num_groups == num_packs`.\n    # This generalized logic handles `num_groups < num_packs` too,\n    # where some packs will remain empty.\n    if num_groups <= num_packs:\n        # If there are fewer or equal items than packs, assign each item to its own pack.\n        # This is a simplified assignment since there's plenty of space.\n        pack_index = torch.full_like(weight,\n                                     fill_value=-1,\n                                     dtype=torch.int64,\n                                     device=weight.device)\n        # Assign group_id to pack_id (0 to num_groups-1)\n        pack_index[:, :num_groups] = torch.arange(num_groups, dtype=torch.int64, device=weight.device)\n        rank_in_pack = torch.zeros_like(pack_index, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    indices = weight.float().sort(-1, descending=True).indices\n    pack_index = torch.full_like(weight,\n                                 fill_value=-1,\n                                 dtype=torch.int64,\n                                 device=weight.device)\n    rank_in_pack = torch.full_like(pack_index,\n                                   fill_value=-1,\n                                   device=weight.device)\n    # Initialize tensors to hold pack weights and item counts for all layers.\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Pre-create an infinite scalar tensor on the correct device to avoid repeated creation in the inner loop\n    inf_scalar_tensor = torch.tensor(float('inf'), device=weight.device)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        # Make a copy of max_items_per_pack for current layer's capacity tracking.\n        current_max_items_per_pack = max_items_per_pack.clone()\n        for group_id in indices[i]:\n            # Get current state for this layer's packs.\n            current_pack_weights = pack_weights_per_layer[i]\n            current_pack_items = pack_items_per_layer[i]\n\n            # Find available packs (those not yet full).\n            # `available_mask` is a boolean tensor.\n            # Use current_max_items_per_pack for capacity check\n            available_mask = (current_pack_items < current_max_items_per_pack)\n\n            # Find the pack that, after adding the current group's weight,\n            # results in the minimum *individual* pack weight among available packs.\n            # This is a greedy heuristic that aims to keep individual pack loads low.\n            potential_new_weights = torch.where(\n                available_mask,\n                current_pack_weights + weight[i, group_id],\n                inf_scalar_tensor  # Ensure unavailable packs are not chosen\n            )\n            pack = torch.argmin(potential_new_weights).item()\n\n            # Assign the group to the chosen pack.\n            # `group_id` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            # `weight[i, group_id]` is the weight of the current group.\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        # To minimize the maximum load, we should add replicas to the\n        # logical experts with the maximum current average load. This\n        # aligns with the \"Longest Processing Time\" (LPT) greedy strategy\n        # for minimizing makespan, which is effective for load balancing.\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float()\n    # Use hierarchical load-balance policy regardless of num_groups % num_nodes\n    # The balanced_packing function has been updated to handle non-divisible cases gracefully.\n    phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n        weight, num_replicas, num_groups, num_nodes, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n```"]}}}