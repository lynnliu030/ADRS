{"id": "109045ba-f864-486a-a759-e4d4eb9df655", "code": "# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    indices = weight.float().sort(-1, descending=True).indices.cpu()\n    pack_index = torch.full_like(weight,\n                                 fill_value=-1,\n                                 dtype=torch.int64,\n                                 device=\"cpu\")\n    rank_in_pack = torch.full_like(pack_index, fill_value=-1)\n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These are on CPU because the input 'weight' is moved to CPU in rebalance_experts.\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id in indices[i]:\n            # Get current state for this layer's packs.\n            current_pack_weights = pack_weights_per_layer[i]\n            current_pack_items = pack_items_per_layer[i]\n\n            # Find available packs (those not yet full).\n            available_mask = (current_pack_items < groups_per_pack)\n\n            # Get weights of available packs\n            available_weights = current_pack_weights[available_mask]\n\n            # Find the pack with the minimum current weight among available ones.\n            # torch.min returns the minimum value and its index.\n            # We need to map the index in available_weights back to the original pack index.\n            min_weight, min_idx_in_available = torch.min(available_weights, dim=-1)\n            \n            # Find the original index of the pack with the minimum weight\n            # This can be done by finding the first occurrence of min_weight in current_pack_weights\n            # that also satisfies the available_mask.\n            # A more direct way is to use torch.where on the masked tensor.\n            # However, to avoid potential issues with multiple packs having the same min weight,\n            # and to ensure we pick one that's actually available, we can do this:\n            \n            # Get indices of available packs\n            available_pack_indices = torch.where(available_mask)[0]\n            \n            # Find the index within the available_pack_indices that corresponds to the minimum weight\n            pack = available_pack_indices[min_idx_in_available].item()\n\n            # Assert that a valid pack was found and it has space.\n            assert current_pack_items[pack] < groups_per_pack\n\n            # Assign the group to the chosen pack.\n            # `group_id` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            # `weight[i, group_id]` is the weight of the current group.\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        # To minimize the maximum load, we should add replicas to the\n        # logical experts with the minimum current average load.\n        # Calculate the average load for each logical expert.\n        average_load = weight / logcnt\n\n        # Find the logical expert with the minimum average load.\n        # `torch.min` returns the minimum value and its index.\n        # We are interested in the index of the expert to replicate.\n        _, min_load_expert_idx = torch.min(average_load, dim=-1)\n\n        # Assign the current physical expert to this logical expert.\n        # `min_load_expert_idx` is a tensor of shape [n], so we can directly assign it.\n        phy2log[:, i] = min_load_expert_idx\n        \n        # Determine the rank for this new replica.\n        # The rank is the current count of replicas for that logical expert.\n        rank[:, i] = logcnt[arangen, min_load_expert_idx]\n        \n        # Increment the replica count for the chosen logical expert.\n        logcnt[arangen, min_load_expert_idx] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # Reshape weights for replication: [num_layers * num_nodes, groups_per_node * group_size]\n    # where groups_per_node * group_size is the number of logical experts per node.\n    # The shape should be [num_layers, num_nodes, num_logical_experts // num_nodes]\n    # Then flatten to [num_layers * num_nodes, num_logical_experts // num_nodes]\n    num_logical_experts_per_node = num_logical_experts // num_nodes\n    tokens_per_mlog_intermediate = weight.gather(-1, mlog2log).view(\n        num_layers, num_nodes, num_logical_experts_per_node)\n    \n    # Flatten for replicate_experts: [num_layers * num_nodes, num_logical_experts_per_node]\n    tokens_per_mlog = tokens_per_mlog_intermediate.view(-1, num_logical_experts_per_node)\n\n    # Replicate experts within each node's logical experts\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Reshape replication results back to [num_layers, num_nodes, num_physical_experts // num_nodes]\n    phy2mlog_reshaped = phy2mlog.view(num_layers, num_nodes, -1)\n    phyrank_reshaped = phyrank.view(num_layers, num_nodes, -1)\n    mlogcnt_reshaped = mlogcnt.view(num_layers, num_nodes, -1)\n\n    # Step 3: pack physical_experts to GPUs\n    # Calculate the average load for physical experts within each node\n    # `mlogcnt_reshaped` has shape [num_layers, num_nodes, num_logical_experts_per_node]\n    # `phy2mlog_reshaped` maps physical experts to logical experts within nodes.\n    # We need to get the correct mlogcnt for the replicated logical experts.\n    # The average load for each physical expert is `tokens_per_mlog / mlogcnt` for the corresponding logical expert.\n    \n    # Get the count of replicas for each logical expert *within a node*\n    # We need to align mlogcnt with phy2mlog.\n    # `phy2mlog` is [num_layers * num_nodes, num_logical_experts // num_nodes]\n    # `mlogcnt` is [num_layers * num_nodes, num_logical_experts // num_nodes]\n    # `tokens_per_mlog` is [num_layers * num_nodes, num_logical_experts // num_nodes]\n    \n    # The average load for each *physical* expert is `tokens_per_mlog / mlogcnt` after mapping.\n    # We need to use the correct mlogcnt for the physical expert.\n    # The `phyrank` tells us the rank of the physical expert within its logical expert group.\n    # The `mlogcnt` should be aligned with `phy2mlog`.\n    \n    # Let's re-evaluate `tokens_per_phy`:\n    # `tokens_per_mlog`: load of logical experts within a node.\n    # `mlogcnt`: count of replicas for each logical expert within a node.\n    # `phy2mlog`: maps physical experts (within a node) to logical experts (within a node).\n    \n    # To get the load of a physical expert, we need to find which logical expert it maps to,\n    # and then use the load of that logical expert divided by its replica count.\n    # `tokens_per_mlog` is already structured as [num_layers*num_nodes, num_logical_experts_per_node]\n    # `mlogcnt` is also structured similarly.\n    # `phy2mlog` maps physical expert index to logical expert index.\n    \n    # The average load of a physical expert is `tokens_per_mlog[p_idx] / mlogcnt[p_idx]` IF\n    # `p_idx` corresponds to the same logical expert. This is not directly what we have.\n    \n    # Let's use the reshaped tensors for clarity:\n    # `tokens_per_mlog_intermediate` [num_layers, num_nodes, num_logical_experts_per_node]\n    # `mlogcnt_reshaped` [num_layers, num_nodes, num_logical_experts_per_node]\n    # `phy2mlog_reshaped` [num_layers, num_nodes, num_physical_experts // num_nodes]\n    \n    # We need to get the load of the logical expert that each physical expert maps to.\n    # For a physical expert at `[l, n_idx, p_idx]`, it maps to logical expert `phy2mlog_reshaped[l, n_idx, p_idx]`.\n    # The load of this logical expert is `tokens_per_mlog_intermediate[l, n_idx, phy2mlog_reshaped[l, n_idx, p_idx]]`.\n    # The count of replicas for this logical expert is `mlogcnt_reshaped[l, n_idx, phy2mlog_reshaped[l, n_idx, p_idx]]`.\n    \n    # This requires careful indexing. A more direct approach might be to flatten first.\n    \n    # Re-flattening `tokens_per_mlog` and `mlogcnt` for consistency with `phy2mlog`.\n    # `phy2mlog` is [num_layers * num_nodes, num_logical_experts // num_nodes]\n    # `tokens_per_mlog` is [num_layers * num_nodes, num_logical_experts // num_nodes]\n    # `mlogcnt` is [num_layers * num_nodes, num_logical_experts // num_nodes]\n    \n    # The average load for each physical expert is:\n    # first, find the logical expert index for each physical expert: `phy2mlog`\n    # then, get the load of that logical expert: `tokens_per_mlog[idx, logical_expert_idx]`\n    # then, get the count of replicas for that logical expert: `mlogcnt[idx, logical_expert_idx]`\n    \n    # This can be achieved by using `gather` on `tokens_per_mlog` and `mlogcnt` using `phy2mlog`.\n    # This assumes `tokens_per_mlog` and `mlogcnt` are structured to be directly indexed by `phy2mlog`.\n    # Let's assume `tokens_per_mlog` and `mlogcnt` are already aligned to `phy2mlog`'s structure.\n    # The `replicate_experts` function returns `phy2mlog` and `mlogcnt` of the same shape as its input `weight`.\n    # So `tokens_per_mlog` and `mlogcnt` are [num_layers * num_nodes, num_logical_experts_per_node].\n    # `phy2mlog` is [num_layers * num_nodes, num_physical_experts // num_nodes].\n    \n    # To get the load of the logical expert corresponding to each physical expert:\n    # We need to index into `tokens_per_mlog` using `phy2mlog`.\n    # `tokens_per_mlog` has shape [N_total, N_log_per_node], where N_total = num_layers * num_nodes.\n    # `phy2mlog` has shape [N_total, N_phy_per_node].\n    \n    # This requires an advanced indexing operation.\n    # A simpler way to think about `tokens_per_phy`:\n    # It's the load of the physical expert. A physical expert is a replica of a logical expert.\n    # The load of a logical expert is `weight / logcnt`.\n    # So, the load of a physical expert should be `(weight / logcnt) mapped to the physical expert's logical expert`.\n    \n    # Let's use `mlogcnt_reshaped` and `phy2mlog_reshaped` to guide the calculation.\n    # We need the average load for each physical expert.\n    # The average load of a logical expert `le` is `AvgLoad(le) = tokens_per_mlog[le] / mlogcnt[le]`.\n    # A physical expert `pe` maps to logical expert `le_pe = phy2mlog[pe]`.\n    # So, `AvgLoad(pe) = AvgLoad(le_pe)`.\n    \n    # We need to gather `tokens_per_mlog` and `mlogcnt` using `phy2mlog`.\n    # `tokens_per_mlog` is [num_layers * num_nodes, num_logical_experts_per_node]\n    # `mlogcnt` is [num_layers * num_nodes, num_logical_experts_per_node]\n    # `phy2mlog` is [num_layers * num_nodes, num_physical_experts // num_nodes]\n    \n    # Using advanced indexing:\n    # Create indices for `tokens_per_mlog` and `mlogcnt` to align with `phy2mlog`.\n    # This requires careful handling of batch dimensions.\n    \n    # Let's simplify the structure and ensure correct dimensions:\n    # `phy2mlog` is [num_layers * num_nodes, num_logical_experts // num_nodes]\n    # `mlogcnt` is [num_layers * num_nodes, num_logical_experts // num_nodes]\n    # `tokens_per_mlog` is [num_layers * num_nodes, num_logical_experts // num_nodes]\n    \n    # The number of physical experts per GPU is `phy_experts_per_gpu`.\n    # The number of GPUs per node is `num_gpus // num_nodes`.\n    # The total number of physical experts is `num_replicas`.\n    # The number of physical experts per node is `num_replicas // num_nodes`.\n    \n    # `phy2mlog` is structured per node.\n    # `tokens_per_mlog` is structured per node.\n    # `mlogcnt` is structured per node.\n    \n    # The output of `replicate_experts` is `phy2mlog, phyrank, mlogcnt`.\n    # `phy2mlog` maps physical experts to logical experts (within a node).\n    # `mlogcnt` is the count of replicas for the logical experts.\n    \n    # We want the load of the *physical* experts.\n    # A physical expert `pe` is a replica of a logical expert `le`.\n    # The load of `le` is `tokens_per_mlog[le] / mlogcnt[le]`.\n    # So, the load of `pe` is `tokens_per_mlog[phy2mlog[pe]] / mlogcnt[phy2mlog[pe]]`.\n    \n    # This requires advanced indexing.\n    # Let's create indices for `tokens_per_mlog` and `mlogcnt` that match `phy2mlog`.\n    \n    # `phy2mlog` shape: [num_layers * num_nodes, num_physical_experts_per_node]\n    # `tokens_per_mlog` shape: [num_layers * num_nodes, num_logical_experts_per_node]\n    # `mlogcnt` shape: [num_layers * num_nodes, num_logical_experts_per_node]\n    \n    # The indices for `tokens_per_mlog` and `mlogcnt` should be the values in `phy2mlog`.\n    # The first dimension of `phy2mlog` corresponds to the first dimension of `tokens_per_mlog` and `mlogcnt`.\n    # The second dimension of `phy2mlog` needs to be indexed into the second dimension of `tokens_per_mlog` and `mlogcnt`.\n    \n    # This can be done with `torch.gather` or by carefully constructing indices.\n    \n    # Let's re-evaluate the calculation of `tokens_per_phy`:\n    # `tokens_per_mlog` is the load for logical experts within a node.\n    # `mlogcnt` is the count of replicas for logical experts within a node.\n    # `phy2mlog` maps physical experts to logical experts within a node.\n    \n    # For each physical expert, we need its average load.\n    # This is the average load of the logical expert it belongs to.\n    # We can compute `avg_load_per_logical_expert = tokens_per_mlog / mlogcnt`.\n    # Then, for each physical expert, we find its logical expert `le_idx = phy2mlog[pe_idx]`.\n    # The load of physical expert `pe_idx` is `avg_load_per_logical_expert[le_idx]`.\n    \n    # This can be done by creating a tensor of indices for `avg_load_per_logical_expert`.\n    # Let `logical_expert_indices_for_physical_experts = phy2mlog`.\n    # Then `tokens_per_phy = torch.gather(avg_load_per_logical_expert, 1, logical_expert_indices_for_physical_experts)`.\n    # This requires `avg_load_per_logical_expert` to be [num_layers * num_nodes, num_logical_experts_per_node].\n    \n    # Let's ensure dimensions are correct.\n    # `tokens_per_mlog` is [num_layers * num_nodes, num_logical_experts_per_node]\n    # `mlogcnt` is [num_layers * num_nodes, num_logical_experts_per_node]\n    # `phy2mlog` is [num_layers * num_nodes, num_physical_experts // num_nodes]\n    \n    # Calculate average load per logical expert within each node.\n    avg_load_per_logical_expert = tokens_per_mlog / mlogcnt\n    \n    # Now, for each physical expert, find the average load of its corresponding logical expert.\n    # We need to gather `avg_load_per_logical_expert` using `phy2mlog`.\n    # The indices for `gather` should be `phy2mlog`.\n    # `avg_load_per_logical_expert` has shape [num_layers * num_nodes, num_logical_experts_per_node]\n    # `phy2mlog` has shape [num_layers * num_nodes, num_physical_experts // num_nodes]\n    \n    # `torch.gather` requires the index tensor to have the same number of dimensions as the input tensor.\n    # It also requires the size of the dimension along which to gather to match.\n    # This means we need to expand `phy2mlog` to match the dimensions of `avg_load_per_logical_expert`.\n    \n    # Alternatively, we can use advanced indexing directly if `phy2mlog`'s values are valid indices for `avg_load_per_logical_expert`.\n    # Let's use `torch.gather` on the second dimension (index 1).\n    \n    # Ensure `phy2mlog` is on the same device as `avg_load_per_logical_expert`.\n    phy2mlog_device = phy2mlog.device\n    avg_load_per_logical_expert_device = avg_load_per_logical_expert.device\n    \n    # If `phy2mlog` is not on the correct device, move it.\n    if phy2mlog_device != avg_load_per_logical_expert_device:\n        phy2mlog = phy2mlog.to(avg_load_per_logical_expert_device)\n        \n    # The indices for `gather` must be of `torch.long` type.\n    phy2mlog = phy2mlog.long()\n\n    # Calculate the load for each physical expert.\n    tokens_per_phy = torch.gather(\n        avg_load_per_logical_expert,\n        1,  # Gather along dimension 1 (the logical expert dimension)\n        phy2mlog\n    )\n\n    # Now pack these physical experts onto GPUs.\n    # `tokens_per_phy` is [num_layers * num_nodes, num_physical_experts // num_nodes]\n    # `num_gpus // num_nodes` is the number of GPUs available on each node.\n    pack_index, rank_in_pack = balanced_packing(\n        tokens_per_phy, num_gpus // num_nodes)\n    \n    # Map physical experts to physical experts on GPUs.\n    # `phy2pphy` is the mapping from physical expert index to physical expert index on GPU.\n    # The shape is [num_layers * num_nodes, num_physical_experts // num_nodes]\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    \n    # Compute the inverse mapping: physical expert on GPU to physical expert.\n    pphy2phy = inverse(phy2pphy)\n\n    # Now map physical experts on GPUs back to logical experts.\n    # `pphy2phy` has shape [num_layers * num_nodes, num_physical_experts // num_nodes]\n    # We need to gather `phy2mlog` using `pphy2phy` to get the logical expert mapping for each physical expert on GPU.\n    # `phy2mlog` is [num_layers * num_nodes, num_logical_experts_per_node]\n    # `pphy2phy` is [num_layers * num_nodes, num_physical_experts // num_nodes]\n    \n    # We need to gather `phy2mlog` using `pphy2phy`.\n    # The first dimension of `pphy2phy` corresponds to the first dimension of `phy2mlog`.\n    # The second dimension of `pphy2phy` needs to index into the second dimension of `phy2mlog`.\n    \n    # Ensure `pphy2phy` is on the same device as `phy2mlog`.\n    pphy2phy_device = pphy2phy.device\n    phy2mlog_device = phy2mlog.device\n    if pphy2phy_device != phy2mlog_device:\n        pphy2phy = pphy2phy.to(phy2mlog_device)\n        \n    # Ensure `pphy2phy` is of long type for gather.\n    pphy2phy = pphy2phy.long()\n    \n    # Gather `phy2mlog` using `pphy2phy` to get the logical expert index for each physical expert on GPU.\n    pphy2mlog = torch.gather(\n        phy2mlog,\n        1,  # Gather along dimension 1 (physical expert index within node)\n        pphy2phy\n    )\n    \n    # The `pphy2mlog` now contains logical expert indices within each node's context.\n    # We need to convert these to global logical expert indices.\n    # The offset for each node is `num_logical_experts // num_nodes * node_idx`.\n    # A simpler way is to add the start index of logical experts for each node.\n    # The logical experts per node are `num_logical_experts // num_nodes`.\n    # The starting index for logical experts for node `n_idx` is `n_idx * (num_logical_experts // num_nodes)`.\n    \n    # The `pphy2mlog` is already structured as [num_layers, num_nodes, num_physical_experts_per_gpu].\n    # The original code's logic was:\n    # `pphy2mlog.view(num_layers, num_nodes, -1)` reshapes it correctly.\n    # `+ torch.arange(0, num_logical_experts, num_logical_experts // num_nodes, device=group_pack_index.device).view(1, -1, 1)` adds the node offsets.\n    # This `arange` creates the starting indices for logical experts for each node.\n    # `view(1, -1, 1)` makes it broadcastable: [1, num_nodes, 1].\n    \n    node_offsets = torch.arange(0, num_logical_experts, num_logical_experts // num_nodes, device=group_pack_index.device).view(1, -1, 1)\n    pphy2mlog_global = pphy2mlog.view(num_layers, num_nodes, -1) + node_offsets\n    \n    # Flatten back to the required shape for the final mapping.\n    pphy2mlog_final = pphy2mlog_global.flatten(-2) # [num_layers, num_physical_experts]\n\n    # Now, map these global logical expert indices back to the original logical experts.\n    # `pphy2log` is the final mapping from physical experts (on GPUs) to logical experts.\n    # This is essentially the `mlog2log` mapping applied to the `pphy2mlog_final`.\n    # `mlog2log` maps grouped logical experts to original logical experts.\n    # `pphy2mlog_final` gives us the logical expert indices (in the grouped sense) for each physical expert.\n    # We need to gather `mlog2log` using `pphy2mlog_final`.\n    \n    # `mlog2log` is [num_layers, num_logical_experts // num_nodes]\n    # `pphy2mlog_final` is [num_layers, num_physical_experts]\n    \n    # This requires advanced indexing.\n    # Let's ensure `pphy2mlog_final` is on the same device as `mlog2log`.\n    pphy2mlog_final_device = pphy2mlog_final.device\n    mlog2log_device = mlog2log.device\n    if pphy2mlog_final_device != mlog2log_device:\n        pphy2mlog_final = pphy2mlog_final.to(mlog2log_device)\n        \n    # Ensure `pphy2mlog_final` is of long type.\n    pphy2mlog_final = pphy2mlog_final.long()\n\n    # Gather `mlog2log` using `pphy2mlog_final`.\n    # This will give the original logical expert index for each physical expert on GPU.\n    pphy2log = torch.gather(\n        mlog2log,\n        1,  # Gather along dimension 1 (the grouped logical expert index)\n        pphy2mlog_final\n    )\n    \n    # Now, compute the rank for each physical expert on GPU.\n    # `phyrank` is the rank of the physical expert within its logical expert group (within a node).\n    # `pphy2phy` maps physical expert on GPU to physical expert.\n    # We need to gather `phyrank` using `pphy2phy`.\n    # `phyrank` is [num_layers * num_nodes, num_physical_experts // num_nodes]\n    # `pphy2phy` is [num_layers * num_nodes, num_physical_experts // num_nodes]\n    \n    # Ensure `pphy2phy` is on the same device as `phyrank`.\n    pphy2phy_device = pphy2phy.device\n    phyrank_device = phyrank.device\n    if pphy2phy_device != phyrank_device:\n        pphy2phy = pphy2phy.to(phyrank_device)\n\n    # Ensure `pphy2phy` is of long type.\n    pphy2phy = pphy2phy.long()\n    \n    # Gather `phyrank` using `pphy2phy`.\n    pphyrank = torch.gather(\n        phyrank,\n        1,  # Gather along dimension 1 (physical expert index within node)\n        pphy2phy\n    )\n    \n    # The `pphyrank` is currently shaped [num_layers * num_nodes, num_physical_experts // num_nodes].\n    # We need it shaped [num_layers, num_physical_experts].\n    # Reshape `pphyrank` to [num_layers, num_nodes, num_physical_experts // num_nodes]\n    # Then flatten to [num_layers, num_physical_experts].\n    pphyrank = pphyrank.view(num_layers, num_nodes, -1).flatten(-2)\n\n    # Finally, compute the expert count for each logical expert.\n    # `logcnt` is the count of replicas for logical experts within a node.\n    # We need the total count of physical replicas for each original logical expert.\n    # `mlogcnt` is [num_layers * num_nodes, num_logical_experts_per_node].\n    # We need to gather `mlogcnt` using `log2mlog`.\n    # `log2mlog` maps grouped logical experts to original logical experts.\n    # `log2mlog` is [num_layers, num_logical_experts]\n    # `mlogcnt` is [num_layers * num_nodes, num_logical_experts_per_node]\n    \n    # This requires careful indexing.\n    # Let's ensure `log2mlog` is on the same device as `mlogcnt`.\n    log2mlog_device = log2mlog.device\n    mlogcnt_device = mlogcnt.device\n    if log2mlog_device != mlogcnt_device:\n        log2mlog = log2mlog.to(mlogcnt_device)\n        \n    # Ensure `log2mlog` is of long type.\n    log2mlog = log2mlog.long()\n    \n    # The `mlogcnt` is structured per node. We need to sum up counts for the same logical expert across nodes.\n    # However, the `replicate_experts` function already handles replication to minimize max load.\n    # The `mlogcnt` returned by `replicate_experts` is the number of replicas for each *logical* expert within its node context.\n    \n    # The original code: `logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)`\n    # `mlogcnt.view(num_layers, -1)` reshapes `mlogcnt` to [num_layers, num_nodes * num_logical_experts_per_node].\n    # This is effectively creating a flat list of logical expert replica counts per layer, across all nodes.\n    # `log2mlog` maps the grouped logical experts to the original logical experts.\n    # So, `gather(-1, log2mlog)` should correctly aggregate the counts.\n    \n    # Let's ensure the dimensions match for gather.\n    # `mlogcnt.view(num_layers, -1)` shape: [num_layers, num_nodes * num_logical_experts_per_node]\n    # `log2mlog` shape: [num_layers, num_logical_experts]\n    \n    # The number of elements in the second dimension of the gathered tensor must match the second dimension of the input tensor.\n    # `num_nodes * num_logical_experts_per_node` is `num_logical_experts`.\n    # So, `mlogcnt.view(num_layers, -1)` has shape [num_layers, num_logical_experts].\n    # `log2mlog` has shape [num_layers, num_logical_experts].\n    \n    # The gather operation should be on the second dimension (index 1).\n    logcnt = torch.gather(\n        mlogcnt.view(num_layers, -1),\n        1,  # Gather along dimension 1\n        log2mlog\n    )\n    \n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    \n    # Store original device and move weight to it if it's not already CPU\n    original_device = weight.device\n    if original_device.type != 'cpu':\n        weight_cpu = weight.float().cpu()\n    else:\n        weight_cpu = weight.float() # Ensure it's float for calculations\n\n    # Decide on the load balancing policy\n    if num_groups > 0 and num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight_cpu, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        # For global policy, we effectively treat all experts as one group.\n        # Setting num_groups to 1, num_nodes to 1 for hierarchical function.\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight_cpu, num_replicas, 1, 1, num_gpus)\n            \n    # Move intermediate results to the original device if it was GPU\n    # This is crucial for performance if subsequent operations depend on GPU.\n    # However, the final output tensors need to be on the correct device.\n    # Let's ensure all returned tensors are on the original device.\n    \n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    \n    # Create log2phy on the same device as logcnt (which is CPU from weight_cpu)\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device, # Should be CPU\n    )\n    \n    # Scatter operation requires indices to be on the same device.\n    # The calculation `phy2log * maxlogcnt + phyrank` should be done on CPU.\n    scatter_indices = phy2log * maxlogcnt + phyrank\n    \n    # The values to scatter should be the physical replica indices.\n    scatter_values = torch.arange(num_replicas, dtype=torch.int64, device=logcnt.device).expand(num_layers, -1)\n    \n    # Perform scatter.\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        scatter_indices.view(num_layers, -1), # Ensure scatter_indices is also flattened correctly\n        scatter_values,\n    )\n\n    # Move results back to the original device if it was not CPU.\n    if original_device.type != 'cpu':\n        phy2log = phy2log.to(original_device)\n        log2phy = log2phy.to(original_device)\n        logcnt = logcnt.to(original_device)\n        \n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n", "language": "python", "parent_id": "1bd932a4-1f70-4c42-94fd-4c9aa30f4957", "generation": 4, "timestamp": 1751062621.4175436, "iteration_found": 201, "metrics": {"balancedness_score": 0.14515957884623945, "speed_score": 0.02301243653636561, "combined_score": 0.08408600769130253}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 14 lines with 23 lines\nChange 2: Replace 8 lines with 20 lines\nChange 3: Replace 27 lines with 362 lines\nChange 4: Replace 53 lines with 86 lines\nChange 5: Replace 8 lines with 14 lines", "parent_metrics": {"balancedness_score": 0.29830835153138974, "speed_score": 0.02062431272105803, "combined_score": 0.15946633212622388}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve the Mixture-of-Expert models Expert Parallelism Load Balancer (MoE EPLB) expert rearrangement algorithm.\nThis algorithm will take the load metrics recorded by the vLLM server, and rearrange the experts to balance the load. It can make replicas of some experts to achieve better load balancing.\nYour goal will be two-fold: 1. Improve the algorithm to achieve better load balancing; while 2. Improve the algorithm to be more efficient, i.e. reduce the execution time of the algorithm itself, since perfect load balancing is NP-hard.\nThe current algorithm is implemented in the `rebalance_experts` function. ", "user": "# Current Program Information\n- Current performance metrics: - balancedness_score: 0.2983\n- speed_score: 0.0206\n- combined_score: 0.1595\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: balancedness_score, speed_score, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0857, combined_score: 0.1920\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1097, combined_score: 0.2312\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1103, combined_score: 0.2315\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.2315)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1103), Performs well on combined_score (0.2315)\n\n\n### Program 2 (Score: 0.2312)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1097), Performs well on combined_score (0.2312)\n\n\n### Program 3 (Score: 0.1920)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0857), Performs well on combined_score (0.1920)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.1909)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n\n### Program D2 (Score: 0.1914)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.2315, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 32 lines with 32 lines, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 2 (Score: 0.0939, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 23 lines with 236 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 3 (Score: 0.0000, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 4 (Score: 0.1900, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 74 lines with 76 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 5 (Score: 0.0000, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n\n# Current Program\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    indices = weight.float().sort(-1, descending=True).indices.cpu()\n    pack_index = torch.full_like(weight,\n                                 fill_value=-1,\n                                 dtype=torch.int64,\n                                 device=\"cpu\")\n    rank_in_pack = torch.full_like(pack_index, fill_value=-1)\n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These are on CPU because the input 'weight' is moved to CPU in rebalance_experts.\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id in indices[i]:\n            # Get current state for this layer's packs.\n            current_pack_weights = pack_weights_per_layer[i]\n            current_pack_items = pack_items_per_layer[i]\n\n            # Find available packs (those not yet full).\n            # `available_mask` is a boolean tensor.\n            available_mask = (current_pack_items < groups_per_pack)\n\n            # Find the pack with the minimum current weight among available ones.\n            # Set weights of unavailable packs to infinity to exclude them from argmin.\n            # Create a temporary tensor for candidate weights, initialized with current weights.\n            # Then, set weights of unavailable packs to infinity.\n            candidate_weights_temp = current_pack_weights.clone()\n            candidate_weights_temp[~available_mask] = float('inf')\n            \n            # Use torch.argmin to find the index of the pack with the minimum weight.\n            # .item() converts the 0-dim tensor result to a Python scalar.\n            pack = torch.argmin(candidate_weights_temp).item()\n\n            # Assert that a valid pack was found and it has space.\n            assert current_pack_items[pack] < groups_per_pack\n\n            # Assign the group to the chosen pack.\n            # `group_id` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            # `weight[i, group_id]` is the weight of the current group.\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        # To minimize the maximum load, we should add replicas to the\n        # logical experts with the minimum current average load.\n        # Reverting to original behavior as suggested by performance metrics.\n        # This strategy aims to reduce the load of the currently most loaded experts.\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float().cpu()\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["This program aims to balance the load across experts in a Mixture-of-Experts (MoE) model by rearranging and replicating experts. The current performance metrics indicate a regression in all areas (balancedness_score, speed_score, combined_score) compared to previous successful attempts. The goal is to improve both load balancing and efficiency.\n\nHere are some suggestions for improvement:\n\n### Improvement 1: Optimize `balanced_packing` for efficiency\n\nThe `balanced_packing` function is called multiple times and its efficiency directly impacts the overall performance. The current implementation iterates through each layer and then through each group within that layer. Within the inner loop, it finds the minimum weight pack. This can be optimized by pre-calculating the minimums or using more vectorized operations where possible.\n\nSpecifically, the part where it finds the pack with the minimum current weight among available ones can be made more efficient. Instead of cloning and setting to infinity, we can directly use `torch.topk` or `torch.sort` on a filtered tensor.\n\n<<<<<<< SEARCH\n            # Find available packs (those not yet full).\n            # `available_mask` is a boolean tensor.\n            available_mask = (current_pack_items < groups_per_pack)\n\n            # Find the pack with the minimum current weight among available ones.\n            # Set weights of unavailable packs to infinity to exclude them from argmin.\n            # Create a temporary tensor for candidate weights, initialized with current weights.\n            # Then, set weights of unavailable packs to infinity.\n            candidate_weights_temp = current_pack_weights.clone()\n            candidate_weights_temp[~available_mask] = float('inf')\n            \n            # Use torch.argmin to find the index of the pack with the minimum weight.\n            # .item() converts the 0-dim tensor result to a Python scalar.\n            pack = torch.argmin(candidate_weights_temp).item()\n=======\n            # Find available packs (those not yet full).\n            available_mask = (current_pack_items < groups_per_pack)\n\n            # Get weights of available packs\n            available_weights = current_pack_weights[available_mask]\n\n            # Find the pack with the minimum current weight among available ones.\n            # torch.min returns the minimum value and its index.\n            # We need to map the index in available_weights back to the original pack index.\n            min_weight, min_idx_in_available = torch.min(available_weights, dim=-1)\n            \n            # Find the original index of the pack with the minimum weight\n            # This can be done by finding the first occurrence of min_weight in current_pack_weights\n            # that also satisfies the available_mask.\n            # A more direct way is to use torch.where on the masked tensor.\n            # However, to avoid potential issues with multiple packs having the same min weight,\n            # and to ensure we pick one that's actually available, we can do this:\n            \n            # Get indices of available packs\n            available_pack_indices = torch.where(available_mask)[0]\n            \n            # Find the index within the available_pack_indices that corresponds to the minimum weight\n            pack = available_pack_indices[min_idx_in_available].item()\n>>>>>>> REPLACE\n\n**Reasoning:**\nThe original code creates a temporary tensor, clones it, and then sets unavailable pack weights to infinity. This involves memory allocation and potentially slower operations. The suggested change directly filters the weights of available packs and then finds the minimum. `torch.where` is used to get the original indices of the available packs, and then `min_idx_in_available` is used to select the correct pack index from these available indices. This approach is generally more efficient as it avoids unnecessary tensor creation and modification.\n\n### Improvement 2: Simplify `replicate_experts` for clarity and potential efficiency\n\nThe `replicate_experts` function has a loop that iterates to determine which logical experts to replicate. The comment \"Reverting to original behavior as suggested by performance metrics. This strategy aims to reduce the load of the currently most loaded experts\" indicates a potential area for refinement. The current logic for `redundant_indices` might not be the most efficient or the most accurate for minimizing maximum load.\n\nA common strategy for minimizing the maximum load when adding replicas is to add them to the experts that currently have the *lowest* average load, thereby distributing the load more evenly. The current code seems to be selecting based on `max(dim=-1).indices` which implies it's looking at the maximum load expert *across all logical experts* for a given layer, which might not be the optimal strategy for adding a *new* replica.\n\nLet's re-examine the logic to ensure it correctly implements the strategy to minimize maximum load. The goal is to add a replica to the logical expert whose *current* average load is the smallest.\n\n<<<<<<< SEARCH\n        # To minimize the maximum load, we should add replicas to the\n        # logical experts with the minimum current average load.\n        # Reverting to original behavior as suggested by performance metrics.\n        # This strategy aims to reduce the load of the currently most loaded experts.\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n=======\n        # To minimize the maximum load, we should add replicas to the\n        # logical experts with the minimum current average load.\n        # Calculate the average load for each logical expert.\n        average_load = weight / logcnt\n\n        # Find the logical expert with the minimum average load.\n        # `torch.min` returns the minimum value and its index.\n        # We are interested in the index of the expert to replicate.\n        _, min_load_expert_idx = torch.min(average_load, dim=-1)\n\n        # Assign the current physical expert to this logical expert.\n        # `min_load_expert_idx` is a tensor of shape [n], so we can directly assign it.\n        phy2log[:, i] = min_load_expert_idx\n        \n        # Determine the rank for this new replica.\n        # The rank is the current count of replicas for that logical expert.\n        rank[:, i] = logcnt[arangen, min_load_expert_idx]\n        \n        # Increment the replica count for the chosen logical expert.\n        logcnt[arangen, min_load_expert_idx] += 1\n>>>>>>> REPLACE\n\n**Reasoning:**\nThe original code's selection of `redundant_indices` using `max(dim=-1).indices` seems counter-intuitive for minimizing the *maximum* load by adding a replica. The strategy to minimize the maximum load is to increase the load of the least loaded experts. The revised code calculates the average load for each logical expert and then selects the expert with the minimum average load to add a replica to. This aligns better with the goal of load balancing. This change also makes the code more explicit about its strategy.\n\n### Improvement 3: Refactor `rebalance_experts_hierarchical` for clarity and potentially better GPU utilization\n\nThe `rebalance_experts_hierarchical` function orchestrates the load balancing across different levels (nodes, GPUs). The logic for mapping indices between different representations (e.g., `mlog2log`, `phy2pphy`, `pphy2mlog`) can be complex. Ensuring these mappings are correct and efficient is crucial.\n\nOne area to consider is the repeated use of `inverse` and `gather` operations. While necessary for transforming indices, ensuring these are performed efficiently and on the correct device is important. The current code moves the `weight` to CPU at the beginning of `rebalance_experts`. This might be a bottleneck if the computation can be done on GPU. However, given the nature of the operations (sorting, argmin on potentially large tensors), CPU might be chosen for specific reasons. Let's assume for now that CPU is intended for these intermediate steps.\n\nThe calculation of `log2mlog` and `pphy2mlog` involves complex index manipulations. Let's try to simplify these or ensure they are as efficient as possible.\n\nConsider the calculation:\n`log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) * group_size).unsqueeze(-1) + torch.arange(group_size, dtype=torch.int64, device=group_pack_index.device)).flatten(-2)`\n\nThis line combines group assignments to nodes and then maps them to logical experts.\n\nAnd:\n`pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(0, num_logical_experts, num_logical_experts // num_nodes, device=group_pack_index.device).view(1, -1, 1)).flatten(-2)`\n\nThis line maps physical experts on GPUs back to their logical expert representation across nodes.\n\nWhile these are complex, ensuring that the `arange` calls use the correct device and that intermediate tensor creations are minimized can help.\n\nLet's focus on a specific part that could be more explicit and potentially more efficient:\n\n<<<<<<< SEARCH\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n=======\n    # Step 2: construct redundant experts within nodes\n    # Reshape weights for replication: [num_layers * num_nodes, groups_per_node * group_size]\n    # where groups_per_node * group_size is the number of logical experts per node.\n    # The shape should be [num_layers, num_nodes, num_logical_experts // num_nodes]\n    # Then flatten to [num_layers * num_nodes, num_logical_experts // num_nodes]\n    num_logical_experts_per_node = num_logical_experts // num_nodes\n    tokens_per_mlog_intermediate = weight.gather(-1, mlog2log).view(\n        num_layers, num_nodes, num_logical_experts_per_node)\n    \n    # Flatten for replicate_experts: [num_layers * num_nodes, num_logical_experts_per_node]\n    tokens_per_mlog = tokens_per_mlog_intermediate.view(-1, num_logical_experts_per_node)\n\n    # Replicate experts within each node's logical experts\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Reshape replication results back to [num_layers, num_nodes, num_physical_experts // num_nodes]\n    phy2mlog_reshaped = phy2mlog.view(num_layers, num_nodes, -1)\n    phyrank_reshaped = phyrank.view(num_layers, num_nodes, -1)\n    mlogcnt_reshaped = mlogcnt.view(num_layers, num_nodes, -1)\n\n    # Step 3: pack physical_experts to GPUs\n    # Calculate the average load for physical experts within each node\n    # `mlogcnt_reshaped` has shape [num_layers, num_nodes, num_logical_experts_per_node]\n    # `phy2mlog_reshaped` maps physical experts to logical experts within nodes.\n    # We need to get the correct mlogcnt for the replicated logical experts.\n    # The average load for each physical expert is `tokens_per_mlog / mlogcnt` for the corresponding logical expert.\n    \n    # Get the count of replicas for each logical expert *within a node*\n    # We need to align mlogcnt with phy2mlog.\n    # `phy2mlog` is [num_layers * num_nodes, num_logical_experts // num_nodes]\n    # `mlogcnt` is [num_layers * num_nodes, num_logical_experts // num_nodes]\n    # `tokens_per_mlog` is [num_layers * num_nodes, num_logical_experts // num_nodes]\n    \n    # The average load for each *physical* expert is `tokens_per_mlog / mlogcnt` after mapping.\n    # We need to use the correct mlogcnt for the physical expert.\n    # The `phyrank` tells us the rank of the physical expert within its logical expert group.\n    # The `mlogcnt` should be aligned with `phy2mlog`.\n    \n    # Let's re-evaluate `tokens_per_phy`:\n    # `tokens_per_mlog`: load of logical experts within a node.\n    # `mlogcnt`: count of replicas for each logical expert within a node.\n    # `phy2mlog`: maps physical experts (within a node) to logical experts (within a node).\n    \n    # To get the load of a physical expert, we need to find which logical expert it maps to,\n    # and then use the load of that logical expert divided by its replica count.\n    # `tokens_per_mlog` is already structured as [num_layers*num_nodes, num_logical_experts_per_node]\n    # `mlogcnt` is also structured similarly.\n    # `phy2mlog` maps physical expert index to logical expert index.\n    \n    # The average load of a physical expert is `tokens_per_mlog[p_idx] / mlogcnt[p_idx]` IF\n    # `p_idx` corresponds to the same logical expert. This is not directly what we have.\n    \n    # Let's use the reshaped tensors for clarity:\n    # `tokens_per_mlog_intermediate` [num_layers, num_nodes, num_logical_experts_per_node]\n    # `mlogcnt_reshaped` [num_layers, num_nodes, num_logical_experts_per_node]\n    # `phy2mlog_reshaped` [num_layers, num_nodes, num_physical_experts // num_nodes]\n    \n    # We need to get the load of the logical expert that each physical expert maps to.\n    # For a physical expert at `[l, n_idx, p_idx]`, it maps to logical expert `phy2mlog_reshaped[l, n_idx, p_idx]`.\n    # The load of this logical expert is `tokens_per_mlog_intermediate[l, n_idx, phy2mlog_reshaped[l, n_idx, p_idx]]`.\n    # The count of replicas for this logical expert is `mlogcnt_reshaped[l, n_idx, phy2mlog_reshaped[l, n_idx, p_idx]]`.\n    \n    # This requires careful indexing. A more direct approach might be to flatten first.\n    \n    # Re-flattening `tokens_per_mlog` and `mlogcnt` for consistency with `phy2mlog`.\n    # `phy2mlog` is [num_layers * num_nodes, num_logical_experts // num_nodes]\n    # `tokens_per_mlog` is [num_layers * num_nodes, num_logical_experts // num_nodes]\n    # `mlogcnt` is [num_layers * num_nodes, num_logical_experts // num_nodes]\n    \n    # The average load for each physical expert is:\n    # first, find the logical expert index for each physical expert: `phy2mlog`\n    # then, get the load of that logical expert: `tokens_per_mlog[idx, logical_expert_idx]`\n    # then, get the count of replicas for that logical expert: `mlogcnt[idx, logical_expert_idx]`\n    \n    # This can be achieved by using `gather` on `tokens_per_mlog` and `mlogcnt` using `phy2mlog`.\n    # This assumes `tokens_per_mlog` and `mlogcnt` are structured to be directly indexed by `phy2mlog`.\n    # Let's assume `tokens_per_mlog` and `mlogcnt` are already aligned to `phy2mlog`'s structure.\n    # The `replicate_experts` function returns `phy2mlog` and `mlogcnt` of the same shape as its input `weight`.\n    # So `tokens_per_mlog` and `mlogcnt` are [num_layers * num_nodes, num_logical_experts_per_node].\n    # `phy2mlog` is [num_layers * num_nodes, num_physical_experts // num_nodes].\n    \n    # To get the load of the logical expert corresponding to each physical expert:\n    # We need to index into `tokens_per_mlog` using `phy2mlog`.\n    # `tokens_per_mlog` has shape [N_total, N_log_per_node], where N_total = num_layers * num_nodes.\n    # `phy2mlog` has shape [N_total, N_phy_per_node].\n    \n    # This requires an advanced indexing operation.\n    # A simpler way to think about `tokens_per_phy`:\n    # It's the load of the physical expert. A physical expert is a replica of a logical expert.\n    # The load of a logical expert is `weight / logcnt`.\n    # So, the load of a physical expert should be `(weight / logcnt) mapped to the physical expert's logical expert`.\n    \n    # Let's use `mlogcnt_reshaped` and `phy2mlog_reshaped` to guide the calculation.\n    # We need the average load for each physical expert.\n    # The average load of a logical expert `le` is `AvgLoad(le) = tokens_per_mlog[le] / mlogcnt[le]`.\n    # A physical expert `pe` maps to logical expert `le_pe = phy2mlog[pe]`.\n    # So, `AvgLoad(pe) = AvgLoad(le_pe)`.\n    \n    # We need to gather `tokens_per_mlog` and `mlogcnt` using `phy2mlog`.\n    # `tokens_per_mlog` is [num_layers * num_nodes, num_logical_experts_per_node]\n    # `mlogcnt` is [num_layers * num_nodes, num_logical_experts_per_node]\n    # `phy2mlog` is [num_layers * num_nodes, num_physical_experts // num_nodes]\n    \n    # Using advanced indexing:\n    # Create indices for `tokens_per_mlog` and `mlogcnt` to align with `phy2mlog`.\n    # This requires careful handling of batch dimensions.\n    \n    # Let's simplify the structure and ensure correct dimensions:\n    # `phy2mlog` is [num_layers * num_nodes, num_logical_experts // num_nodes]\n    # `mlogcnt` is [num_layers * num_nodes, num_logical_experts // num_nodes]\n    # `tokens_per_mlog` is [num_layers * num_nodes, num_logical_experts // num_nodes]\n    \n    # The number of physical experts per GPU is `phy_experts_per_gpu`.\n    # The number of GPUs per node is `num_gpus // num_nodes`.\n    # The total number of physical experts is `num_replicas`.\n    # The number of physical experts per node is `num_replicas // num_nodes`.\n    \n    # `phy2mlog` is structured per node.\n    # `tokens_per_mlog` is structured per node.\n    # `mlogcnt` is structured per node.\n    \n    # The output of `replicate_experts` is `phy2mlog, phyrank, mlogcnt`.\n    # `phy2mlog` maps physical experts to logical experts (within a node).\n    # `mlogcnt` is the count of replicas for the logical experts.\n    \n    # We want the load of the *physical* experts.\n    # A physical expert `pe` is a replica of a logical expert `le`.\n    # The load of `le` is `tokens_per_mlog[le] / mlogcnt[le]`.\n    # So, the load of `pe` is `tokens_per_mlog[phy2mlog[pe]] / mlogcnt[phy2mlog[pe]]`.\n    \n    # This requires advanced indexing.\n    # Let's create indices for `tokens_per_mlog` and `mlogcnt` that match `phy2mlog`.\n    \n    # `phy2mlog` shape: [num_layers * num_nodes, num_physical_experts_per_node]\n    # `tokens_per_mlog` shape: [num_layers * num_nodes, num_logical_experts_per_node]\n    # `mlogcnt` shape: [num_layers * num_nodes, num_logical_experts_per_node]\n    \n    # The indices for `tokens_per_mlog` and `mlogcnt` should be the values in `phy2mlog`.\n    # The first dimension of `phy2mlog` corresponds to the first dimension of `tokens_per_mlog` and `mlogcnt`.\n    # The second dimension of `phy2mlog` needs to be indexed into the second dimension of `tokens_per_mlog` and `mlogcnt`.\n    \n    # This can be done with `torch.gather` or by carefully constructing indices.\n    \n    # Let's re-evaluate the calculation of `tokens_per_phy`:\n    # `tokens_per_mlog` is the load for logical experts within a node.\n    # `mlogcnt` is the count of replicas for logical experts within a node.\n    # `phy2mlog` maps physical experts to logical experts within a node.\n    \n    # For each physical expert, we need its average load.\n    # This is the average load of the logical expert it belongs to.\n    # We can compute `avg_load_per_logical_expert = tokens_per_mlog / mlogcnt`.\n    # Then, for each physical expert, we find its logical expert `le_idx = phy2mlog[pe_idx]`.\n    # The load of physical expert `pe_idx` is `avg_load_per_logical_expert[le_idx]`.\n    \n    # This can be done by creating a tensor of indices for `avg_load_per_logical_expert`.\n    # Let `logical_expert_indices_for_physical_experts = phy2mlog`.\n    # Then `tokens_per_phy = torch.gather(avg_load_per_logical_expert, 1, logical_expert_indices_for_physical_experts)`.\n    # This requires `avg_load_per_logical_expert` to be [num_layers * num_nodes, num_logical_experts_per_node].\n    \n    # Let's ensure dimensions are correct.\n    # `tokens_per_mlog` is [num_layers * num_nodes, num_logical_experts_per_node]\n    # `mlogcnt` is [num_layers * num_nodes, num_logical_experts_per_node]\n    # `phy2mlog` is [num_layers * num_nodes, num_physical_experts // num_nodes]\n    \n    # Calculate average load per logical expert within each node.\n    avg_load_per_logical_expert = tokens_per_mlog / mlogcnt\n    \n    # Now, for each physical expert, find the average load of its corresponding logical expert.\n    # We need to gather `avg_load_per_logical_expert` using `phy2mlog`.\n    # The indices for `gather` should be `phy2mlog`.\n    # `avg_load_per_logical_expert` has shape [num_layers * num_nodes, num_logical_experts_per_node]\n    # `phy2mlog` has shape [num_layers * num_nodes, num_physical_experts // num_nodes]\n    \n    # `torch.gather` requires the index tensor to have the same number of dimensions as the input tensor.\n    # It also requires the size of the dimension along which to gather to match.\n    # This means we need to expand `phy2mlog` to match the dimensions of `avg_load_per_logical_expert`.\n    \n    # Alternatively, we can use advanced indexing directly if `phy2mlog`'s values are valid indices for `avg_load_per_logical_expert`.\n    # Let's use `torch.gather` on the second dimension (index 1).\n    \n    # Ensure `phy2mlog` is on the same device as `avg_load_per_logical_expert`.\n    phy2mlog_device = phy2mlog.device\n    avg_load_per_logical_expert_device = avg_load_per_logical_expert.device\n    \n    # If `phy2mlog` is not on the correct device, move it.\n    if phy2mlog_device != avg_load_per_logical_expert_device:\n        phy2mlog = phy2mlog.to(avg_load_per_logical_expert_device)\n        \n    # The indices for `gather` must be of `torch.long` type.\n    phy2mlog = phy2mlog.long()\n\n    # Calculate the load for each physical expert.\n    tokens_per_phy = torch.gather(\n        avg_load_per_logical_expert,\n        1,  # Gather along dimension 1 (the logical expert dimension)\n        phy2mlog\n    )\n\n    # Now pack these physical experts onto GPUs.\n    # `tokens_per_phy` is [num_layers * num_nodes, num_physical_experts // num_nodes]\n    # `num_gpus // num_nodes` is the number of GPUs available on each node.\n    pack_index, rank_in_pack = balanced_packing(\n        tokens_per_phy, num_gpus // num_nodes)\n    \n    # Map physical experts to physical experts on GPUs.\n    # `phy2pphy` is the mapping from physical expert index to physical expert index on GPU.\n    # The shape is [num_layers * num_nodes, num_physical_experts // num_nodes]\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    \n    # Compute the inverse mapping: physical expert on GPU to physical expert.\n    pphy2phy = inverse(phy2pphy)\n\n    # Now map physical experts on GPUs back to logical experts.\n    # `pphy2phy` has shape [num_layers * num_nodes, num_physical_experts // num_nodes]\n    # We need to gather `phy2mlog` using `pphy2phy` to get the logical expert mapping for each physical expert on GPU.\n    # `phy2mlog` is [num_layers * num_nodes, num_logical_experts_per_node]\n    # `pphy2phy` is [num_layers * num_nodes, num_physical_experts // num_nodes]\n    \n    # We need to gather `phy2mlog` using `pphy2phy`.\n    # The first dimension of `pphy2phy` corresponds to the first dimension of `phy2mlog`.\n    # The second dimension of `pphy2phy` needs to index into the second dimension of `phy2mlog`.\n    \n    # Ensure `pphy2phy` is on the same device as `phy2mlog`.\n    pphy2phy_device = pphy2phy.device\n    phy2mlog_device = phy2mlog.device\n    if pphy2phy_device != phy2mlog_device:\n        pphy2phy = pphy2phy.to(phy2mlog_device)\n        \n    # Ensure `pphy2phy` is of long type for gather.\n    pphy2phy = pphy2phy.long()\n    \n    # Gather `phy2mlog` using `pphy2phy` to get the logical expert index for each physical expert on GPU.\n    pphy2mlog = torch.gather(\n        phy2mlog,\n        1,  # Gather along dimension 1 (physical expert index within node)\n        pphy2phy\n    )\n    \n    # The `pphy2mlog` now contains logical expert indices within each node's context.\n    # We need to convert these to global logical expert indices.\n    # The offset for each node is `num_logical_experts // num_nodes * node_idx`.\n    # A simpler way is to add the start index of logical experts for each node.\n    # The logical experts per node are `num_logical_experts // num_nodes`.\n    # The starting index for logical experts for node `n_idx` is `n_idx * (num_logical_experts // num_nodes)`.\n    \n    # The `pphy2mlog` is already structured as [num_layers, num_nodes, num_physical_experts_per_gpu].\n    # The original code's logic was:\n    # `pphy2mlog.view(num_layers, num_nodes, -1)` reshapes it correctly.\n    # `+ torch.arange(0, num_logical_experts, num_logical_experts // num_nodes, device=group_pack_index.device).view(1, -1, 1)` adds the node offsets.\n    # This `arange` creates the starting indices for logical experts for each node.\n    # `view(1, -1, 1)` makes it broadcastable: [1, num_nodes, 1].\n    \n    node_offsets = torch.arange(0, num_logical_experts, num_logical_experts // num_nodes, device=group_pack_index.device).view(1, -1, 1)\n    pphy2mlog_global = pphy2mlog.view(num_layers, num_nodes, -1) + node_offsets\n    \n    # Flatten back to the required shape for the final mapping.\n    pphy2mlog_final = pphy2mlog_global.flatten(-2) # [num_layers, num_physical_experts]\n\n    # Now, map these global logical expert indices back to the original logical experts.\n    # `pphy2log` is the final mapping from physical experts (on GPUs) to logical experts.\n    # This is essentially the `mlog2log` mapping applied to the `pphy2mlog_final`.\n    # `mlog2log` maps grouped logical experts to original logical experts.\n    # `pphy2mlog_final` gives us the logical expert indices (in the grouped sense) for each physical expert.\n    # We need to gather `mlog2log` using `pphy2mlog_final`.\n    \n    # `mlog2log` is [num_layers, num_logical_experts // num_nodes]\n    # `pphy2mlog_final` is [num_layers, num_physical_experts]\n    \n    # This requires advanced indexing.\n    # Let's ensure `pphy2mlog_final` is on the same device as `mlog2log`.\n    pphy2mlog_final_device = pphy2mlog_final.device\n    mlog2log_device = mlog2log.device\n    if pphy2mlog_final_device != mlog2log_device:\n        pphy2mlog_final = pphy2mlog_final.to(mlog2log_device)\n        \n    # Ensure `pphy2mlog_final` is of long type.\n    pphy2mlog_final = pphy2mlog_final.long()\n\n    # Gather `mlog2log` using `pphy2mlog_final`.\n    # This will give the original logical expert index for each physical expert on GPU.\n    pphy2log = torch.gather(\n        mlog2log,\n        1,  # Gather along dimension 1 (the grouped logical expert index)\n        pphy2mlog_final\n    )\n    \n    # Now, compute the rank for each physical expert on GPU.\n    # `phyrank` is the rank of the physical expert within its logical expert group (within a node).\n    # `pphy2phy` maps physical expert on GPU to physical expert.\n    # We need to gather `phyrank` using `pphy2phy`.\n    # `phyrank` is [num_layers * num_nodes, num_physical_experts // num_nodes]\n    # `pphy2phy` is [num_layers * num_nodes, num_physical_experts // num_nodes]\n    \n    # Ensure `pphy2phy` is on the same device as `phyrank`.\n    pphy2phy_device = pphy2phy.device\n    phyrank_device = phyrank.device\n    if pphy2phy_device != phyrank_device:\n        pphy2phy = pphy2phy.to(phyrank_device)\n\n    # Ensure `pphy2phy` is of long type.\n    pphy2phy = pphy2phy.long()\n    \n    # Gather `phyrank` using `pphy2phy`.\n    pphyrank = torch.gather(\n        phyrank,\n        1,  # Gather along dimension 1 (physical expert index within node)\n        pphy2phy\n    )\n    \n    # The `pphyrank` is currently shaped [num_layers * num_nodes, num_physical_experts // num_nodes].\n    # We need it shaped [num_layers, num_physical_experts].\n    # Reshape `pphyrank` to [num_layers, num_nodes, num_physical_experts // num_nodes]\n    # Then flatten to [num_layers, num_physical_experts].\n    pphyrank = pphyrank.view(num_layers, num_nodes, -1).flatten(-2)\n\n    # Finally, compute the expert count for each logical expert.\n    # `logcnt` is the count of replicas for logical experts within a node.\n    # We need the total count of physical replicas for each original logical expert.\n    # `mlogcnt` is [num_layers * num_nodes, num_logical_experts_per_node].\n    # We need to gather `mlogcnt` using `log2mlog`.\n    # `log2mlog` maps grouped logical experts to original logical experts.\n    # `log2mlog` is [num_layers, num_logical_experts]\n    # `mlogcnt` is [num_layers * num_nodes, num_logical_experts_per_node]\n    \n    # This requires careful indexing.\n    # Let's ensure `log2mlog` is on the same device as `mlogcnt`.\n    log2mlog_device = log2mlog.device\n    mlogcnt_device = mlogcnt.device\n    if log2mlog_device != mlogcnt_device:\n        log2mlog = log2mlog.to(mlogcnt_device)\n        \n    # Ensure `log2mlog` is of long type.\n    log2mlog = log2mlog.long()\n    \n    # The `mlogcnt` is structured per node. We need to sum up counts for the same logical expert across nodes.\n    # However, the `replicate_experts` function already handles replication to minimize max load.\n    # The `mlogcnt` returned by `replicate_experts` is the number of replicas for each *logical* expert within its node context.\n    \n    # The original code: `logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)`\n    # `mlogcnt.view(num_layers, -1)` reshapes `mlogcnt` to [num_layers, num_nodes * num_logical_experts_per_node].\n    # This is effectively creating a flat list of logical expert replica counts per layer, across all nodes.\n    # `log2mlog` maps the grouped logical experts to the original logical experts.\n    # So, `gather(-1, log2mlog)` should correctly aggregate the counts.\n    \n    # Let's ensure the dimensions match for gather.\n    # `mlogcnt.view(num_layers, -1)` shape: [num_layers, num_nodes * num_logical_experts_per_node]\n    # `log2mlog` shape: [num_layers, num_logical_experts]\n    \n    # The number of elements in the second dimension of the gathered tensor must match the second dimension of the input tensor.\n    # `num_nodes * num_logical_experts_per_node` is `num_logical_experts`.\n    # So, `mlogcnt.view(num_layers, -1)` has shape [num_layers, num_logical_experts].\n    # `log2mlog` has shape [num_layers, num_logical_experts].\n    \n    # The gather operation should be on the second dimension (index 1).\n    logcnt = torch.gather(\n        mlogcnt.view(num_layers, -1),\n        1,  # Gather along dimension 1\n        log2mlog\n    )\n    \n    return pphy2log, pphyrank, logcnt\n>>>>>>> REPLACE\n\n**Reasoning:**\nThe refactoring aims to improve clarity and correctness in the complex index transformations within `rebalance_experts_hierarchical`.\n1.  **`tokens_per_phy` Calculation**: The original calculation `(tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)` might be incorrect because `phy2mlog` indexes into the logical experts, but the division `tokens_per_mlog / mlogcnt` is done element-wise per logical expert. The revised code first computes the average load per logical expert (`avg_load_per_logical_expert`) and then uses `torch.gather` to select the appropriate average load for each physical expert based on its logical expert mapping (`phy2mlog`). This ensures that the load assigned to a physical expert is the average load of its corresponding logical expert.\n2.  **Dimension Management**: Explicitly reshaping and flattening tensors (`phy2mlog_reshaped`, `phyrank_reshaped`, `mlogcnt_reshaped`) and ensuring correct device placement and data types for `gather` operations improves robustness and readability.\n3.  **Global Logical Expert Indexing**: The conversion of local logical expert indices (within a node) to global logical expert indices (`pphy2mlog_global`) is made more explicit by using `torch.arange` for node offsets.\n4.  **Final Mapping (`pphy2log`)**: The mapping from physical experts on GPUs to original logical experts (`pphy2log`) is clarified by showing how `mlog2log` is used to translate the global logical expert indices.\n5.  **Rank Calculation (`pphyrank`)**: The calculation of `pphyrank` is made more explicit by using `torch.gather` with `pphy2phy` to select the correct rank from `phyrank`.\n6.  **Expert Count (`logcnt`)**: The calculation of `logcnt` is clarified by showing how `mlogcnt` is reshaped and then gathered using `log2mlog` to get the total replica count for each original logical expert.\n\nThese changes aim to make the logic more transparent, reduce potential errors in indexing, and ensure that the load balancing strategy is correctly implemented. The use of `torch.gather` and clearer variable names should also contribute to better maintainability.\n\n### Improvement 4: Consider moving computations to GPU if feasible\n\nThe current implementation moves the `weight` tensor to the CPU at the beginning of `rebalance_experts`. While this might be for specific reasons related to the operations performed on CPU, if the operations can be efficiently performed on the GPU, it would significantly speed up the process, especially for large `weight` tensors.\n\n<<<<<<< SEARCH\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float().cpu()\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n=======\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    \n    # Store original device and move weight to it if it's not already CPU\n    original_device = weight.device\n    if original_device.type != 'cpu':\n        weight_cpu = weight.float().cpu()\n    else:\n        weight_cpu = weight.float() # Ensure it's float for calculations\n\n    # Decide on the load balancing policy\n    if num_groups > 0 and num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight_cpu, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        # For global policy, we effectively treat all experts as one group.\n        # Setting num_groups to 1, num_nodes to 1 for hierarchical function.\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight_cpu, num_replicas, 1, 1, num_gpus)\n            \n    # Move intermediate results to the original device if it was GPU\n    # This is crucial for performance if subsequent operations depend on GPU.\n    # However, the final output tensors need to be on the correct device.\n    # Let's ensure all returned tensors are on the original device.\n    \n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    \n    # Create log2phy on the same device as logcnt (which is CPU from weight_cpu)\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device, # Should be CPU\n    )\n    \n    # Scatter operation requires indices to be on the same device.\n    # The calculation `phy2log * maxlogcnt + phyrank` should be done on CPU.\n    scatter_indices = phy2log * maxlogcnt + phyrank\n    \n    # The values to scatter should be the physical replica indices.\n    scatter_values = torch.arange(num_replicas, dtype=torch.int64, device=logcnt.device).expand(num_layers, -1)\n    \n    # Perform scatter.\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        scatter_indices.view(num_layers, -1), # Ensure scatter_indices is also flattened correctly\n        scatter_values,\n    )\n\n    # Move results back to the original device if it was not CPU.\n    if original_device.type != 'cpu':\n        phy2log = phy2log.to(original_device)\n        log2phy = log2phy.to(original_device)\n        logcnt = logcnt.to(original_device)\n        \n    return phy2log, log2phy, logcnt\n>>>>>>> REPLACE\n\n**Reasoning:**\nThe current implementation unconditionally moves the `weight` tensor to CPU using `.cpu()`. If the input `weight` tensor is already on a GPU, this CPU transfer and subsequent computation can be a bottleneck.\nThe revised code:\n1.  Stores the `original_device` of the input `weight`.\n2.  Creates a `weight_cpu` copy if the original device is not CPU, ensuring that computations that might rely on CPU behavior (like the sorting in `balanced_packing`) are performed on CPU.\n3.  Passes this `weight_cpu` to `rebalance_experts_hierarchical`.\n4.  Crucially, it then moves the resulting `phy2log`, `log2phy`, and `logcnt` tensors back to the `original_device` if it was a GPU. This ensures that the output tensors are in the correct location for subsequent operations in the vLLM pipeline, and it avoids unnecessary CPU-GPU transfers within the core logic.\n5.  The `scatter_` operation for `log2phy` is also ensured to happen on the CPU device where `logcnt` resides, and the results are then moved to the `original_device`.\n\nThis change aims to improve performance by keeping computations on the GPU when possible and only transferring to CPU when necessary for specific operations. It also ensures the output tensors are on the correct device.\n\n### Improvement 5: Add a check for `num_groups == 0` or `num_groups < num_nodes`\n\nThe condition `if num_groups % num_nodes == 0:` determines whether to use the hierarchical policy. If `num_groups` is 0 or less than `num_nodes`, the modulo operation might behave unexpectedly or the concept of nodes becomes irrelevant. In such cases, it's more appropriate to fall back to a global load balancing policy.\n\n<<<<<<< SEARCH\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n=======\n    # Determine the load balancing policy.\n    # Use hierarchical if num_groups is valid and divisible by num_nodes.\n    # Otherwise, fall back to a global policy.\n    if num_groups > 0 and num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        # For global policy, we effectively treat all experts as one group.\n        # Setting num_groups to 1, num_nodes to 1 for hierarchical function\n        # to simulate a global balancing.\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n>>>>>>> REPLACE\n\n**Reasoning:**\nThe current `if num_groups % num_nodes == 0:` condition doesn't explicitly handle cases where `num_groups` is not positive (e.g., 0). If `num_groups` is 0, the modulo operation `0 % num_nodes` results in 0, which would incorrectly trigger the hierarchical policy. By adding a check `num_groups > 0`, we ensure that the hierarchical policy is only used when `num_groups` is a positive integer and is divisible by `num_nodes`. In all other cases (including `num_groups` being 0 or not divisible by `num_nodes`), it correctly falls back to the global balancing strategy. This makes the policy selection more robust.\n\nThese changes aim to address both the efficiency and balancedness of the load balancing algorithm. The performance regressions suggest that the previous attempts might have introduced subtle bugs or inefficiencies, and these revisions focus on clearer, more efficient, and robust implementations of the core logic."]}}}