{"id": "cd64448e-6b3f-4d87-9ded-40d027251c77", "code": "# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort groups by weight in descending order.\n    # 'weight' is already float and on CPU from rebalance_experts,\n    # so no need for .float().cpu() here.\n    indices = weight.sort(-1, descending=True).indices\n\n    # Initialize pack_index and rank_in_pack.\n    # These will be fully populated, so `empty_like` is sufficient.\n    # They will be on the same device as 'weight' (i.e., CPU).\n    pack_index = torch.empty_like(weight, dtype=torch.int64)\n    rank_in_pack = torch.empty_like(pack_index)\n    \n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These tensors are on the same device as 'weight' (which is CPU).\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id in indices[i]:\n            # Get current state for this layer's packs.\n            current_pack_weights = pack_weights_per_layer[i]\n            current_pack_items = pack_items_per_layer[i]\n\n            # Find the pack with the minimum current weight among available ones.\n            # `available_mask` is a boolean tensor.\n            available_mask = (current_pack_items < groups_per_pack)\n            \n            # Get the indices of available packs.\n            available_pack_indices = torch.where(available_mask)[0]\n\n            # If there are available packs, find the one with the minimum weight.\n            if available_pack_indices.numel() > 0:\n                # Get weights of only the available packs\n                weights_of_available_packs = current_pack_weights[available_pack_indices]\n                \n                # Find the index within `available_pack_indices` that has the minimum weight.\n                local_min_idx = torch.argmin(weights_of_available_packs)\n                \n                # Map this local index back to the global pack index.\n                pack = available_pack_indices[local_min_idx].item()\n            else:\n                # This case should ideally not be reached if inputs are valid and logic is correct.\n                # For robustness, we can break or raise an error, but here we assume valid inputs.\n                # If it happens, it means no pack has capacity, which implies an issue with `groups_per_pack` or `current_pack_items`.\n                # For now, we'll assume it won't happen based on the problem constraints.\n                # If this assertion fails, it means there's a problem with the logic or input assumptions.\n                raise RuntimeError(\"No available packs found, but expected to find one.\")\n\n            # Assign the group to the chosen pack.\n            # `group_id` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            # `weight[i, group_id]` is the weight of the current group.\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    # Calculate the ratio of weight to current replica count for each logical expert.\n    # This is done once per iteration of the outer loop.\n    ratios = weight / logcnt\n\n    # In each step, we want to find the logical expert that, if replicated,\n    # would result in the smallest increase in maximum load. This is approximated\n    # by picking the logical expert with the highest current weight / replica_count ratio.\n    # We are essentially trying to balance the load by adding a replica to the\n    # \"most loaded\" logical expert (relative to its current number of replicas).\n\n    # To efficiently find the best expert to replicate across all layers,\n    # we can use `torch.topk` on the flattened ratios.\n    # `torch.topk` returns the values and indices of the k largest elements.\n    # Here, k=1 because we want to replicate one expert at a time.\n    \n    # Flatten ratios to find the global max ratio across all layers and logical experts.\n    flat_ratios = ratios.flatten()\n    \n    # We need to find the logical expert index that corresponds to the max ratio.\n    # The structure of `ratios` is [num_layers, num_logical_experts].\n    # We need to determine which logical expert to replicate for each layer.\n    # A more direct approach for this specific problem is to iterate through layers\n    # and find the best expert for each layer independently, as the current code does,\n    # but optimize the inner part.\n\n    # The current loop is already somewhat optimized by operating on `weight` and `logcnt`\n    # which are [n, num_log]. Let's refine the selection process within the loop.\n\n    # For each layer `l`, we want to find the logical expert `log_idx` that maximizes `weight[l, log_idx] / logcnt[l, log_idx]`.\n    # This is equivalent to finding the `log_idx` that minimizes `logcnt[l, log_idx] / weight[l, log_idx]` if weight is non-zero,\n    # or maximizing `weight[l, log_idx] / logcnt[l, log_idx]`.\n\n    # The current approach:\n    # `redundant_indices = (weight / logcnt).max(dim=-1).indices`\n    # This line correctly finds the index of the logical expert with the maximum ratio for each layer.\n    # The issue might be the repeated computation of `weight / logcnt` if it's not optimized by PyTorch.\n\n    # Let's stick to the current logic for finding the index, as it's already per-layer.\n    # The main potential for speedup is if the operations within the loop are more efficient.\n    # The current operations are element-wise division and `max(dim=-1)`.\n    # These are generally efficient operations in PyTorch.\n\n    # The number of iterations is `num_redundant_experts`. If this number is large,\n    # the loop itself becomes a bottleneck.\n\n    # Consider a different approach: instead of iterative greedy selection,\n    # can we directly determine the final assignment? This is complex because\n    # adding a replica changes the \"best\" candidate for subsequent additions.\n\n    # For now, let's focus on ensuring the inner operations are as lean as possible.\n    # The current implementation of `replicate_experts` is fundamentally an iterative greedy approach.\n    # The `weight / logcnt` calculation is done inside the loop. We can move it outside if `logcnt` doesn't change.\n    # However, `logcnt` *does* change in each iteration.\n\n    # Let's re-examine the `replicate_experts` function's goal:\n    # \"Replicate `num_log` experts to `num_phy` replicas, such that the maximum load of all replicas is minimized.\"\n    # The current greedy strategy is a common heuristic for this.\n\n    # A potential optimization: Instead of recalculating `weight / logcnt` repeatedly,\n    # we can maintain a priority queue (or a sorted list) for each layer, storing\n    # (ratio, logical_expert_index). When we replicate an expert, we update its\n    # ratio in the priority queue.\n\n    # However, implementing a priority queue in pure PyTorch for this might be\n    # more complex than it's worth for a SEARCH/REPLACE diff.\n\n    # Let's try to optimize the current loop by ensuring `weight` and `logcnt`\n    # are on the CPU, as `rebalance_experts` already moves `weight` to CPU.\n    # `weight` is already on CPU. `logcnt` is also created on CPU.\n\n    # The most significant factor influencing speed here is the number of times the loop runs\n    # (`num_redundant_experts`) and the complexity of the operations inside.\n    # The operations are `weight / logcnt`, `.max(dim=-1)`, and indexing.\n    # These are already quite optimized.\n\n    # Let's consider the structure of `logcnt`: `logcnt[arangen, redundant_indices] += 1`.\n    # `arangen` is `torch.arange(n)`. `redundant_indices` is `[n]`.\n    # This indexing is efficient.\n\n    # The core issue might be the iterative nature. If `num_redundant_experts` is large,\n    # this will be slow.\n\n    # Let's try to optimize the calculation of `redundant_indices` slightly by\n    # ensuring that division by zero is handled gracefully (though `logcnt` starts at 1).\n    # The current code assumes `logcnt` will not be zero.\n\n    # A more direct approach to find the max ratio per layer:\n    for i in range(num_log, num_phy):\n        # Calculate ratios for the current state of logcnt.\n        # Ensure we handle potential division by zero if logcnt could be 0, though it starts at 1.\n        # Use a small epsilon to avoid division by zero if logcnt becomes zero (shouldn't happen here).\n        # The current code `weight / logcnt` is fine as logcnt starts at 1.\n        \n        # Find the logical expert with the maximum weight-to-replica ratio for each layer.\n        # `torch.max` returns values and indices. We need indices.\n        _, redundant_indices = torch.max(weight / logcnt, dim=-1)\n        \n        # Assign the chosen logical expert to the current physical expert slot.\n        phy2log[:, i] = redundant_indices\n        \n        # Record the replica rank for this physical expert.\n        # `logcnt[arangen, redundant_indices]` gets the current count for the chosen experts.\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        \n        # Increment the replica count for the chosen logical experts.\n        logcnt[arangen, redundant_indices] += 1\n        \n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float().cpu()\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n", "language": "python", "parent_id": "5a6b2e36-7429-44cf-b3ac-9812d4fa4a12", "generation": 3, "timestamp": 1751056662.459704, "iteration_found": 110, "metrics": {"balancedness_score": 0.29830835153138974, "speed_score": 0.02628728441614004, "combined_score": 0.16229781797376489}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 33 lines with 34 lines\nChange 2: Replace 6 lines with 108 lines\nChange 3: Replace 33 lines with 31 lines\nChange 4: Replace 6 lines with 18 lines", "parent_metrics": {"balancedness_score": 0.29830835153138974, "speed_score": 0.023694861587179625, "combined_score": 0.1610016065592847}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve the Mixture-of-Expert models Expert Parallelism Load Balancer (MoE EPLB) expert rearrangement algorithm.\nThis algorithm will take the load metrics recorded by the vLLM server, and rearrange the experts to balance the load. It can make replicas of some experts to achieve better load balancing.\nYour goal will be two-fold: 1. Improve the algorithm to achieve better load balancing; while 2. Improve the algorithm to be more efficient, i.e. reduce the execution time of the algorithm itself, since perfect load balancing is NP-hard.\nThe current algorithm is implemented in the `rebalance_experts` function. ", "user": "# Current Program Information\n- Current performance metrics: - balancedness_score: 0.2983\n- speed_score: 0.0237\n- combined_score: 0.1610\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: speed_score, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0818, combined_score: 0.1900\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0830, combined_score: 0.1907\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0835, combined_score: 0.1909\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.1909)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0835), Performs well on combined_score (0.1909)\n\n\n### Program 2 (Score: 0.1907)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0830), Performs well on combined_score (0.1907)\n\n\n### Program 3 (Score: 0.1900)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0818), Performs well on combined_score (0.1900)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.1852)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n\n### Program D2 (Score: 0.1872)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.1909, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 3 lines with 4 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 2 (Score: 0.0939, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 23 lines with 236 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 3 (Score: 0.1615, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 17 lines with 18 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 4 (Score: 0.0000, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 31 lines with 52 lines\nChange 2: Replace 12 lines with 68 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 5 (Score: 0.0000, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 47 lines with 49 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n\n# Current Program\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort groups by weight in descending order.\n    # 'weight' is already float and on CPU from rebalance_experts,\n    # so no need for .float().cpu() here.\n    indices = weight.sort(-1, descending=True).indices\n\n    # Initialize pack_index and rank_in_pack.\n    # These will be fully populated, so `empty_like` is sufficient.\n    # They will be on the same device as 'weight' (i.e., CPU).\n    pack_index = torch.empty_like(weight, dtype=torch.int64)\n    rank_in_pack = torch.empty_like(pack_index)\n    \n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These tensors are on the same device as 'weight' (which is CPU).\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id in indices[i]:\n            # Get current state for this layer's packs.\n            current_pack_weights = pack_weights_per_layer[i]\n            current_pack_items = pack_items_per_layer[i]\n\n            # Find available packs (those not yet full).\n            # `available_mask` is a boolean tensor.\n            available_mask = (current_pack_items < groups_per_pack)\n\n            # Find the pack with the minimum current weight among available ones.\n            # This approach avoids creating a large temporary tensor with float('inf').\n            available_pack_indices = torch.nonzero(available_mask).squeeze(-1)\n            \n            # If no available packs, something is wrong with the logic or inputs.\n            # This assert ensures correctness under expected conditions.\n            assert available_pack_indices.numel() > 0, \"No available packs found, but expected to find one.\"\n\n            # Get weights of only the available packs\n            weights_of_available_packs = current_pack_weights[available_pack_indices]\n            \n            # Find the index within the `available_pack_indices` list that has the minimum weight.\n            local_min_idx = torch.argmin(weights_of_available_packs).item()\n            \n            # Map this local index back to the global pack index.\n            pack = available_pack_indices[local_min_idx].item()\n\n            # Assert that a valid pack was found and it has space.\n            assert current_pack_items[pack] < groups_per_pack\n\n            # Assign the group to the chosen pack.\n            # `group_id` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            # `weight[i, group_id]` is the weight of the current group.\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float().cpu()\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["The current program achieves a balancedness_score of 0.2983, but the speed_score is low at 0.0237, resulting in a combined_score of 0.1610. The historical data indicates that previous attempts (Attempts 1-3) had significantly better speed_scores (around 0.08), suggesting that recent changes might have negatively impacted performance.\n\nThe primary goal is to improve both balancedness and speed. Since perfect load balancing is NP-hard, we need to strike a balance. The current implementation has a complex hierarchical approach (`rebalance_experts_hierarchical`) which might be a bottleneck.\n\nHere are some proposed improvements:\n\n### Improvement 1: Simplify `balanced_packing` for efficiency\n\nThe `balanced_packing` function, called multiple times, has a few areas that could be optimized for speed. Specifically, the inner loop iterates through `indices[i]` and then finds the minimum weight among available packs. This can be made more efficient.\n\n**Reasoning:**\n*   **Reduced Tensor Operations:** The current code uses `torch.nonzero` and then indexing into `current_pack_weights` to find the minimum. This can be streamlined.\n*   **Pre-sorting:** The `indices` are already sorted by weight. We can leverage this to directly assign groups to packs in a more direct manner, potentially avoiding repeated searches for the minimum.\n\nLet's focus on optimizing the inner loop of `balanced_packing` to be more direct and potentially avoid some of the intermediate tensor creations.\n\n<<<<<<< SEARCH\n            # Find available packs (those not yet full).\n            # `available_mask` is a boolean tensor.\n            available_mask = (current_pack_items < groups_per_pack)\n\n            # Find the pack with the minimum current weight among available ones.\n            # This approach avoids creating a large temporary tensor with float('inf').\n            available_pack_indices = torch.nonzero(available_mask).squeeze(-1)\n            \n            # If no available packs, something is wrong with the logic or inputs.\n            # This assert ensures correctness under expected conditions.\n            assert available_pack_indices.numel() > 0, \"No available packs found, but expected to find one.\"\n\n            # Get weights of only the available packs\n            weights_of_available_packs = current_pack_weights[available_pack_indices]\n            \n            # Find the index within the `available_pack_indices` list that has the minimum weight.\n            local_min_idx = torch.argmin(weights_of_available_packs).item()\n            \n            # Map this local index back to the global pack index.\n            pack = available_pack_indices[local_min_idx].item()\n\n            # Assert that a valid pack was found and it has space.\n            assert current_pack_items[pack] < groups_per_pack\n\n            # Assign the group to the chosen pack.\n            # `group_id` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            # `weight[i, group_id]` is the weight of the current group.\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n=======\n            # Find the pack with the minimum current weight among available ones.\n            # `available_mask` is a boolean tensor.\n            available_mask = (current_pack_items < groups_per_pack)\n            \n            # Get the indices of available packs.\n            available_pack_indices = torch.where(available_mask)[0]\n\n            # If there are available packs, find the one with the minimum weight.\n            if available_pack_indices.numel() > 0:\n                # Get weights of only the available packs\n                weights_of_available_packs = current_pack_weights[available_pack_indices]\n                \n                # Find the index within `available_pack_indices` that has the minimum weight.\n                local_min_idx = torch.argmin(weights_of_available_packs)\n                \n                # Map this local index back to the global pack index.\n                pack = available_pack_indices[local_min_idx].item()\n            else:\n                # This case should ideally not be reached if inputs are valid and logic is correct.\n                # For robustness, we can break or raise an error, but here we assume valid inputs.\n                # If it happens, it means no pack has capacity, which implies an issue with `groups_per_pack` or `current_pack_items`.\n                # For now, we'll assume it won't happen based on the problem constraints.\n                # If this assertion fails, it means there's a problem with the logic or input assumptions.\n                raise RuntimeError(\"No available packs found, but expected to find one.\")\n\n            # Assign the group to the chosen pack.\n            # `group_id` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            # `weight[i, group_id]` is the weight of the current group.\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n>>>>>>> REPLACE\n\n### Improvement 2: Optimize `replicate_experts` for efficiency\n\nThe `replicate_experts` function iterates `num_phy - num_log` times. Inside the loop, it calculates `(weight / logcnt).max(dim=-1).indices`. This operation can be computationally intensive, especially for large `num_log`. We can explore a more efficient way to find the expert to replicate.\n\n**Reasoning:**\n*   **Vectorization:** The current loop structure is not fully vectorized. We can try to achieve a more vectorized approach.\n*   **Greedy Strategy:** The current greedy strategy is to pick the expert with the highest weight-to-replica-count ratio. While this is a good heuristic, the implementation could be more efficient.\n\nLet's consider a more optimized way to select the expert to replicate. Instead of iterating and finding `max` repeatedly, we can maintain a data structure that allows for faster retrieval of the best candidate. However, given the constraints of modifying existing code with SEARCH/REPLACE, we'll focus on a more direct calculation if possible.\n\nOne potential optimization is to pre-calculate the ratios and then use `torch.topk` or similar to find the best candidates.\n\n<<<<<<< SEARCH\n    for i in range(num_log, num_phy):\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n=======\n    # Calculate the ratio of weight to current replica count for each logical expert.\n    # This is done once per iteration of the outer loop.\n    ratios = weight / logcnt\n\n    # In each step, we want to find the logical expert that, if replicated,\n    # would result in the smallest increase in maximum load. This is approximated\n    # by picking the logical expert with the highest current weight / replica_count ratio.\n    # We are essentially trying to balance the load by adding a replica to the\n    # \"most loaded\" logical expert (relative to its current number of replicas).\n\n    # To efficiently find the best expert to replicate across all layers,\n    # we can use `torch.topk` on the flattened ratios.\n    # `torch.topk` returns the values and indices of the k largest elements.\n    # Here, k=1 because we want to replicate one expert at a time.\n    \n    # Flatten ratios to find the global max ratio across all layers and logical experts.\n    flat_ratios = ratios.flatten()\n    \n    # We need to find the logical expert index that corresponds to the max ratio.\n    # The structure of `ratios` is [num_layers, num_logical_experts].\n    # We need to determine which logical expert to replicate for each layer.\n    # A more direct approach for this specific problem is to iterate through layers\n    # and find the best expert for each layer independently, as the current code does,\n    # but optimize the inner part.\n\n    # The current loop is already somewhat optimized by operating on `weight` and `logcnt`\n    # which are [n, num_log]. Let's refine the selection process within the loop.\n\n    # For each layer `l`, we want to find the logical expert `log_idx` that maximizes `weight[l, log_idx] / logcnt[l, log_idx]`.\n    # This is equivalent to finding the `log_idx` that minimizes `logcnt[l, log_idx] / weight[l, log_idx]` if weight is non-zero,\n    # or maximizing `weight[l, log_idx] / logcnt[l, log_idx]`.\n\n    # The current approach:\n    # `redundant_indices = (weight / logcnt).max(dim=-1).indices`\n    # This line correctly finds the index of the logical expert with the maximum ratio for each layer.\n    # The issue might be the repeated computation of `weight / logcnt` if it's not optimized by PyTorch.\n\n    # Let's stick to the current logic for finding the index, as it's already per-layer.\n    # The main potential for speedup is if the operations within the loop are more efficient.\n    # The current operations are element-wise division and `max(dim=-1)`.\n    # These are generally efficient operations in PyTorch.\n\n    # The number of iterations is `num_redundant_experts`. If this number is large,\n    # the loop itself becomes a bottleneck.\n\n    # Consider a different approach: instead of iterative greedy selection,\n    # can we directly determine the final assignment? This is complex because\n    # adding a replica changes the \"best\" candidate for subsequent additions.\n\n    # For now, let's focus on ensuring the inner operations are as lean as possible.\n    # The current implementation of `replicate_experts` is fundamentally an iterative greedy approach.\n    # The `weight / logcnt` calculation is done inside the loop. We can move it outside if `logcnt` doesn't change.\n    # However, `logcnt` *does* change in each iteration.\n\n    # Let's re-examine the `replicate_experts` function's goal:\n    # \"Replicate `num_log` experts to `num_phy` replicas, such that the maximum load of all replicas is minimized.\"\n    # The current greedy strategy is a common heuristic for this.\n\n    # A potential optimization: Instead of recalculating `weight / logcnt` repeatedly,\n    # we can maintain a priority queue (or a sorted list) for each layer, storing\n    # (ratio, logical_expert_index). When we replicate an expert, we update its\n    # ratio in the priority queue.\n\n    # However, implementing a priority queue in pure PyTorch for this might be\n    # more complex than it's worth for a SEARCH/REPLACE diff.\n\n    # Let's try to optimize the current loop by ensuring `weight` and `logcnt`\n    # are on the CPU, as `rebalance_experts` already moves `weight` to CPU.\n    # `weight` is already on CPU. `logcnt` is also created on CPU.\n\n    # The most significant factor influencing speed here is the number of times the loop runs\n    # (`num_redundant_experts`) and the complexity of the operations inside.\n    # The operations are `weight / logcnt`, `.max(dim=-1)`, and indexing.\n    # These are already quite optimized.\n\n    # Let's consider the structure of `logcnt`: `logcnt[arangen, redundant_indices] += 1`.\n    # `arangen` is `torch.arange(n)`. `redundant_indices` is `[n]`.\n    # This indexing is efficient.\n\n    # The core issue might be the iterative nature. If `num_redundant_experts` is large,\n    # this will be slow.\n\n    # Let's try to optimize the calculation of `redundant_indices` slightly by\n    # ensuring that division by zero is handled gracefully (though `logcnt` starts at 1).\n    # The current code assumes `logcnt` will not be zero.\n\n    # A more direct approach to find the max ratio per layer:\n    for i in range(num_log, num_phy):\n        # Calculate ratios for the current state of logcnt.\n        # Ensure we handle potential division by zero if logcnt could be 0, though it starts at 1.\n        # Use a small epsilon to avoid division by zero if logcnt becomes zero (shouldn't happen here).\n        # The current code `weight / logcnt` is fine as logcnt starts at 1.\n        \n        # Find the logical expert with the maximum weight-to-replica ratio for each layer.\n        # `torch.max` returns values and indices. We need indices.\n        _, redundant_indices = torch.max(weight / logcnt, dim=-1)\n        \n        # Assign the chosen logical expert to the current physical expert slot.\n        phy2log[:, i] = redundant_indices\n        \n        # Record the replica rank for this physical expert.\n        # `logcnt[arangen, redundant_indices]` gets the current count for the chosen experts.\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        \n        # Increment the replica count for the chosen logical experts.\n        logcnt[arangen, redundant_indices] += 1\n        \n    return phy2log, rank, logcnt\n>>>>>>> REPLACE\n\n### Improvement 3: Simplify `rebalance_experts` entry point\n\nThe `rebalance_experts` function has a conditional call to `rebalance_experts_hierarchical`. If `num_groups % num_nodes != 0`, it calls `rebalance_experts_hierarchical` with `num_groups=1`, `num_nodes=1`. This effectively falls back to a global load balancing policy, but it still uses the hierarchical function's structure.\n\n**Reasoning:**\n*   **Code Duplication/Complexity:** It might be cleaner to have a separate function for the global policy or to ensure the hierarchical function handles `num_nodes=1` gracefully without the need for a special case.\n*   **Readability:** Simplifying the entry point can improve understanding.\n\nGiven the current performance metrics and the history, it seems the hierarchical approach is intended to be beneficial. The regression in speed_score suggests that the current implementation of `rebalance_experts_hierarchical` or its interaction with other parts might be the issue, rather than the choice between hierarchical and global.\n\nHowever, for code clarity and maintainability, if the global policy is significantly different, it might be better to have a dedicated function. But for now, let's focus on the core functions.\n\nThe current call:\n`rebalance_experts_hierarchical(weight, num_replicas, 1, 1, num_gpus)`\nwhen `num_groups % num_nodes != 0`.\nThis should work, but it's worth noting that `rebalance_experts_hierarchical` has assertions like `num_groups % num_nodes == 0` and `num_gpus % num_nodes == 0`. When `num_nodes=1`, these assertions hold if `num_groups` is any integer and `num_gpus` is any integer.\n\nLet's reconsider the \"Areas identified for improvement\": \"Consider simplifying the code to improve readability and maintainability.\"\n\nThe current `rebalance_experts` function is quite concise. The complexity lies within `rebalance_experts_hierarchical`.\n\nLet's focus on the two specific functions `balanced_packing` and `replicate_experts` for direct performance improvements. The suggestions above aim to make these core components more efficient.\n\nThe historical data shows a significant drop in `speed_score` from ~0.08 to ~0.02. This suggests that the recent changes might have introduced overhead. The most complex part is `rebalance_experts_hierarchical`.\n\nLet's look at the logic within `rebalance_experts_hierarchical`:\n1.  `tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)`: This involves unflattening and summing, which is generally efficient.\n2.  `balanced_packing(tokens_per_group, num_nodes)`: This is where our first optimization targets.\n3.  `mlog2log = ...`: Indexing operations.\n4.  `tokens_per_mlog = weight.gather(-1, mlog2log).view(-1, num_logical_experts // num_nodes)`: Gather and view operations.\n5.  `replicate_experts(...)`: This is where our second optimization targets.\n6.  `tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)`: Division, gather, and potentially broadcasting.\n7.  `balanced_packing(tokens_per_phy, num_gpus // num_nodes)`: Second call to `balanced_packing`.\n8.  `phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack`: Arithmetic operations.\n9.  `pphy2mlog = phy2mlog.gather(-1, pphy2phy)`: Gather operation.\n10. `pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + ...)`: View, addition, flattening.\n11. `pphy2log = mlog2log.gather(-1, pphy2mlog)`: Gather operation.\n12. `pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)`: Gather and view.\n13. `logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)`: View and gather.\n\nThe two calls to `balanced_packing` and one call to `replicate_experts` are the most computationally intensive parts. Our proposed changes focus on these.\n\nThe \"Areas identified for improvement\" also mentions \"Metrics showing regression: speed_score, combined_score. Consider reverting or revising recent changes in these areas.\" Without knowing what those recent changes were, it's hard to revert. However, the suggested optimizations are aimed at improving speed.\n\nLet's refine the `balanced_packing` optimization slightly. The current loop iterates over `indices[i]`. `indices[i]` is a 1D tensor of group IDs sorted by weight. We can iterate through these sorted group IDs.\n\nThe logic `torch.where(available_mask)[0]` followed by `torch.argmin` is fine, but perhaps we can make it more direct.\n\nConsider the structure of `balanced_packing`:\n`weight`: [num_layers, num_groups]\n`num_packs`\n\nInside the loop `for i in range(num_layers):`\n`indices[i]`: indices of groups sorted by weight for layer `i`.\n`group_id` iterates through `indices[i]`.\n\nThe goal is to assign `group_id` to a pack such that `pack_items_per_layer[i, pack]` is minimized, and then update `pack_weights_per_layer[i, pack]`.\n\nThe current approach finds the pack with the minimum current weight among available ones. This is a greedy approach.\n\nLet's try to optimize the inner loop of `balanced_packing` by using a more direct assignment strategy that leverages the sorted `indices`.\n\nConsider this alternative for the inner loop of `balanced_packing` (within `for group_id in indices[i]:`):\n\nInstead of finding the minimum weight pack each time, we can maintain the current load of packs and assign the next group (from the sorted list) to the least loaded pack that still has capacity.\n\n```python\n# Inside the loop `for i in range(num_layers):`\n    # For each layer, process groups in sorted order of their weights.\n    # `indices[i]` is a 1D tensor of group IDs for the current layer.\n    for group_id in indices[i]:\n        # Get current state for this layer's packs.\n        current_pack_weights = pack_weights_per_layer[i]\n        current_pack_items = pack_items_per_layer[i]\n\n        # Find available packs (those not yet full).\n        available_mask = (current_pack_items < groups_per_pack)\n        \n        # Get the indices of available packs.\n        available_pack_indices = torch.where(available_mask)[0]\n\n        # If there are available packs, find the one with the minimum weight.\n        if available_pack_indices.numel() > 0:\n            # Get weights of only the available packs\n            weights_of_available_packs = current_pack_weights[available_pack_indices]\n            \n            # Find the index within `available_pack_indices` that has the minimum weight.\n            local_min_idx = torch.argmin(weights_of_available_packs)\n            \n            # Map this local index back to the global pack index.\n            pack = available_pack_indices[local_min_idx].item()\n        else:\n            # This case should ideally not be reached if inputs are valid and logic is correct.\n            raise RuntimeError(\"No available packs found, but expected to find one.\")\n\n        # Assign the group to the chosen pack.\n        pack_index[i, group_id] = pack\n        rank_in_pack[i, group_id] = current_pack_items[pack]\n\n        # Update pack weights and item counts for the chosen pack.\n        pack_weights_per_layer[i, pack] += weight[i, group_id]\n        pack_items_per_layer[i, pack] += 1\n```\nThis is essentially the same as the previous refined version. The key is the efficiency of `torch.where`, `weights_of_available_packs = current_pack_weights[available_pack_indices]`, and `torch.argmin`.\n\nLet's consider an alternative strategy for `balanced_packing` to potentially improve both balancedness and speed by using a more deterministic assignment based on sorted order.\n\nIf we have `N` items and `M` bins, and each bin must have `N/M` items, and we sort items by weight, we can fill bins in a round-robin fashion with decreasing weights.\n\nExample: Items with weights [10, 9, 8, 7, 6, 5], num_packs = 2, groups_per_pack = 3.\nSorted indices: [0, 1, 2, 3, 4, 5] (assuming weights are already sorted)\nPack 0 gets items with original weights: 10, 8, 6\nPack 1 gets items with original weights: 9, 7, 5\n\nThis is a form of the \"multiway number partitioning\" problem, which is NP-hard. The current greedy approach (assign to least loaded available bin) is a common heuristic.\n\nThe original code seems to implement this greedy strategy. The performance regression might be due to subtle changes in how PyTorch handles these operations or increased overhead in other parts of the system that are not shown.\n\nGiven the limited scope of SEARCH/REPLACE, the most impactful changes would be to ensure the core functions are as efficient as possible. The proposed changes to `balanced_packing` and `replicate_experts` are attempts at this.\n\nThe prompt also mentions: \"Consider simplifying the code to improve readability and maintainability\". The current code is relatively readable, but the hierarchical logic can be dense.\n\nLet's re-evaluate the `replicate_experts` optimization. The current `max(dim=-1)` is indeed a key operation.\n\nIf we are to improve `speed_score`, we must reduce the computational cost of `rebalance_experts_hierarchical` and its sub-functions. The current approach seems to be a good heuristic for balancedness.\n\nPerhaps the regression is due to the overhead of moving tensors to CPU (`weight = weight.float().cpu()`). While this is often necessary for certain operations, it can be a bottleneck. However, the historical data suggests that previous versions with similar CPU movement performed better, so this is unlikely to be the sole cause.\n\nLet's stick with the proposed optimizations for `balanced_packing` and `replicate_experts` as they target the most computationally intensive parts. The changes are minor refinements to existing logic rather than a complete overhaul, which is suitable for this format.\n\nFinal check on the proposed changes:\n\n1.  **`balanced_packing`:** The change aims to use `torch.where` and `torch.argmin` on a subset of weights, which is a common and generally efficient pattern. The previous version used `torch.nonzero` and then indexing, which might have slightly more overhead. The logic remains the same (greedy assignment to least loaded available pack).\n2.  **`replicate_experts`:** The change is primarily a clarification and slight restructuring of how `torch.max` is used. It doesn't fundamentally alter the algorithm but ensures clarity and potentially avoids redundant calculations if PyTorch's optimizer doesn't fully optimize the original form. The core logic of finding the max ratio per layer remains.\n\nThese changes are incremental and aim to slightly improve efficiency without sacrificing the balancedness achieved by the greedy heuristics. If the speed regression is due to something else (e.g., overhead in `rebalance_experts_hierarchical`'s setup or finalization), these specific changes might not fully address it, but they target the most likely computational bottlenecks within the provided code.\n\nOne last consideration: the `rebalance_experts` function itself.\nIt converts `weight` to float and CPU. This is done once.\nThen it conditionally calls `rebalance_experts_hierarchical`.\nFinally, it constructs `log2phy` using `scatter_`. This `scatter_` operation is efficient.\n\nThe structure seems sound. The performance issue is likely in the core algorithms.\n\nThe provided \"Top Performing Programs\" all have `balancedness_score: 0.2983`. This suggests that the current level of balancedness is already quite good and hard to improve further without potentially hurting speed. The main focus should be on the `speed_score`.\n\nThe exploratory programs (D1, D2) and inspiration programs show alternative approaches. Without knowing their specific changes, it's hard to draw direct conclusions. However, Inspiration 1 has a \"Modification: Change 1: Replace 3 lines with 4 lines\". This suggests small, targeted changes can have an impact.\n\nThe proposed changes are small and targeted at the core functions.\n\n```python\n# re-check balanced_packing optimization\n# Original:\n# available_mask = (current_pack_items < groups_per_pack)\n# available_pack_indices = torch.nonzero(available_mask).squeeze(-1)\n# weights_of_available_packs = current_pack_weights[available_pack_indices]\n# local_min_idx = torch.argmin(weights_of_available_packs).item()\n# pack = available_pack_indices[local_min_idx].item()\n\n# Proposed:\n# available_mask = (current_pack_items < groups_per_pack)\n# available_pack_indices = torch.where(available_mask)[0]\n# if available_pack_indices.numel() > 0:\n#     weights_of_available_packs = current_pack_weights[available_pack_indices]\n#     local_min_idx = torch.argmin(weights_of_available_packs)\n#     pack = available_pack_indices[local_min_idx].item()\n# else:\n#     raise RuntimeError(...)\n\n# The difference is:\n# 1. `torch.nonzero(available_mask).squeeze(-1)` vs `torch.where(available_mask)[0]`\n#    `torch.where` is generally preferred for getting indices and is often more efficient.\n# 2. `.item()` is called after `torch.argmin` in the original. In the proposed, it's called after getting the pack index. This is a minor difference.\n# 3. The proposed version includes an explicit check for `available_pack_indices.numel() > 0` and a raise, which is good for robustness.\n\n# re-check replicate_experts optimization\n# Original:\n# redundant_indices = (weight / logcnt).max(dim=-1).indices\n# phy2log[:, i] = redundant_indices\n# rank[:, i] = logcnt[arangen, redundant_indices]\n# logcnt[arangen, redundant_indices] += 1\n\n# Proposed:\n# _, redundant_indices = torch.max(weight / logcnt, dim=-1)\n# phy2log[:, i] = redundant_indices\n# rank[:, i] = logcnt[arangen, redundant_indices]\n# logcnt[arangen, redundant_indices] += 1\n\n# The proposed version uses `torch.max` which returns both values and indices,\n# and then explicitly takes the indices. This is functionally identical to `.max(dim=-1).indices`\n# but might be slightly more explicit. The core computation `weight / logcnt` is still within the loop.\n# The main performance bottleneck is likely the loop itself if `num_redundant_experts` is large.\n# However, without a more fundamental algorithmic change (which is hard with SEARCH/REPLACE),\n# these are the best incremental improvements.\n```\nThe changes are subtle but aim to leverage more efficient PyTorch operations where possible.\n\nFinal decision: Proceed with the proposed changes for `balanced_packing` and `replicate_experts`.\n\nOne more thought: the problem statement mentions \"Consider reverting or revising recent changes in these areas.\" This implies that the current implementation might be a result of changes that *reduced* speed. The proposed changes are more about *improving* efficiency of the existing algorithms, assuming the algorithms themselves are sound.\n\nThe `rebalance_experts_hierarchical` function is complex. If the speed regression is due to its internal logic, it's harder to pinpoint with simple SEARCH/REPLACE. However, the most computationally intensive parts are the calls to `balanced_packing` and `replicate_experts`.\n\nLet's ensure the `balanced_packing` optimization is correctly formatted.\nThe original code has an `assert` inside the loop. The proposed change replaces this with a conditional check and a `RuntimeError`. This is a functional change that improves robustness.\n\n```python\n# The original `balanced_packing` has this structure for finding the pack:\n# ...\n# available_mask = (current_pack_items < groups_per_pack)\n# available_pack_indices = torch.nonzero(available_mask).squeeze(-1)\n# assert available_pack_indices.numel() > 0, \"No available packs found, but expected to find one.\"\n# weights_of_available_packs = current_pack_weights[available_pack_indices]\n# local_min_idx = torch.argmin(weights_of_available_packs).item()\n# pack = available_pack_indices[local_min_idx].item()\n# ...\n\n# My proposed change:\n# ...\n# available_mask = (current_pack_items < groups_per_pack)\n# available_pack_indices = torch.where(available_mask)[0]\n# if available_pack_indices.numel() > 0:\n#     weights_of_available_packs = current_pack_weights[available_pack_indices]\n#     local_min_idx = torch.argmin(weights_of_available_packs)\n#     pack = available_pack_indices[local_min_idx].item()\n# else:\n#     raise RuntimeError(\"No available packs found, but expected to find one.\")\n# ...\n\n# This change is correct. The `assert` is replaced by a more explicit check and error.\n# The `torch.where` is generally preferred over `torch.nonzero` for getting indices.\n# The `.item()` call is correctly placed after getting the final `pack` index.\n```\n\nThe `replicate_experts` change is also fine. It's a minor stylistic/efficiency improvement.\n\n\n```python\n# The original `replicate_experts` has this structure:\n# for i in range(num_log, num_phy):\n#     redundant_indices = (weight / logcnt).max(dim=-1).indices\n#     phy2log[:, i] = redundant_indices\n#     rank[:, i] = logcnt[arangen, redundant_indices]\n#     logcnt[arangen, redundant_indices] += 1\n\n# My proposed change:\n# for i in range(num_log, num_phy):\n#     _, redundant_indices = torch.max(weight / logcnt, dim=-1)\n#     phy2log[:, i] = redundant_indices\n#     rank[:, i] = logcnt[arangen, redundant_indices]\n#     logcnt[arangen, redundant_indices] += 1\n\n# This is a correct replacement. It uses `torch.max` which returns both values and indices,\n# and then we explicitly take the indices. This is functionally equivalent to `.max(dim=-1).indices`\n# but might be slightly more efficient depending on PyTorch's internal optimizations.\n```\nThese changes are ready.\n\n```python\n# Consider the \"Areas identified for improvement\":\n# - Consider simplifying the code to improve readability and maintainability\n# - Metrics showing regression: speed_score, combined_score. Consider reverting or revising recent changes in these areas.\n\n# The proposed changes are minor optimizations targeting the core functions,\n# which might indirectly improve readability by being more direct.\n# They don't fundamentally alter the algorithms, so they are unlikely to\n# *cause* regression if the original algorithms were sound.\n# If the regression was due to specific changes not in the provided code,\n# these suggestions won't fix that, but they are the best possible\n# improvements given the current context.\n```\nThe goal is to improve the *current* program. The proposed changes are improvements to the *current* code, aiming for better speed and maintaining or slightly improving balancedness.\n\nThe problem states \"Your goal will be two-fold: 1. Improve the algorithm to achieve better load balancing; while 2. Improve the algorithm to be more efficient, i.e. reduce the execution time of the algorithm itself\".\n\nThe current balancedness score is 0.2983. The historical data shows this score has been stable across attempts. It's likely that the current algorithm already achieves a good balance. The main issue is the speed. Therefore, the focus should be on efficiency. The proposed changes are efficiency-focused.\n\nThe prompt also asks to \"Consider simplifying the code\". The proposed changes are minor and don't significantly simplify the overall structure, but they aim to make the core operations within the functions more direct.\n\n\nFinal check of diff format and exact matching:\nThe SEARCH blocks must be exact. The REPLACE blocks are the new code.\nThe format `<<<<<<< SEARCH \\n # ... \\n ======= \\n # ... \\n >>>>>>> REPLACE` must be used.\n\nI will provide two distinct diff blocks, one for each function.<<<<<<< SEARCH\n            # Find available packs (those not yet full).\n            # `available_mask` is a boolean tensor.\n            available_mask = (current_pack_items < groups_per_pack)\n\n            # Find the pack with the minimum current weight among available ones.\n            # This approach avoids creating a large temporary tensor with float('inf').\n            available_pack_indices = torch.nonzero(available_mask).squeeze(-1)\n            \n            # If no available packs, something is wrong with the logic or inputs.\n            # This assert ensures correctness under expected conditions.\n            assert available_pack_indices.numel() > 0, \"No available packs found, but expected to find one.\"\n\n            # Get weights of only the available packs\n            weights_of_available_packs = current_pack_weights[available_pack_indices]\n            \n            # Find the index within the `available_pack_indices` list that has the minimum weight.\n            local_min_idx = torch.argmin(weights_of_available_packs).item()\n            \n            # Map this local index back to the global pack index.\n            pack = available_pack_indices[local_min_idx].item()\n\n            # Assert that a valid pack was found and it has space.\n            assert current_pack_items[pack] < groups_per_pack\n\n            # Assign the group to the chosen pack.\n            # `group_id` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            # `weight[i, group_id]` is the weight of the current group.\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n=======\n            # Find available packs (those not yet full).\n            # `available_mask` is a boolean tensor.\n            available_mask = (current_pack_items < groups_per_pack)\n            \n            # Get the indices of available packs. torch.where is generally more efficient than torch.nonzero.\n            available_pack_indices = torch.where(available_mask)[0]\n\n            # If there are available packs, find the one with the minimum weight.\n            if available_pack_indices.numel() > 0:\n                # Get weights of only the available packs\n                weights_of_available_packs = current_pack_weights[available_pack_indices]\n                \n                # Find the index within `available_pack_indices` that has the minimum weight.\n                local_min_idx = torch.argmin(weights_of_available_packs)\n                \n                # Map this local index back to the global pack index.\n                pack = available_pack_indices[local_min_idx].item()\n            else:\n                # This case should ideally not be reached if inputs are valid and logic is correct.\n                # Raising a RuntimeError provides a clearer error message than an assert failing.\n                raise RuntimeError(\"No available packs found, but expected to find one.\")\n\n            # Assign the group to the chosen pack.\n            # `group_id` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            # `weight[i, group_id]` is the weight of the current group.\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n    for i in range(num_log, num_phy):\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n=======\n    # Iterate to add replicas until num_phy experts are reached.\n    for i in range(num_log, num_phy):\n        # Calculate the ratio of weight to current replica count for each logical expert.\n        # This ratio helps identify which logical expert is \"most loaded\" relative to its replication.\n        # Using torch.max directly returns both values and indices, which is generally efficient.\n        # The division by logcnt is safe because logcnt starts at 1.\n        _, redundant_indices = torch.max(weight / logcnt, dim=-1)\n        \n        # Assign the chosen logical expert to the current physical expert slot.\n        phy2log[:, i] = redundant_indices\n        \n        # Record the replica rank for this physical expert.\n        # This uses the current replica count of the chosen logical expert.\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        \n        # Increment the replica count for the chosen logical experts for the next iteration.\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n>>>>>>> REPLACE"]}}}