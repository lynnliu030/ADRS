{"id": "0b7dad60-e1fd-456e-b9b8-f43c17cd20c0", "code": "# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\nimport torch.jit as jit\n\n# Define the inner loop logic as a TorchScript function\n@jit.script\ndef _jit_assign_group_and_update(\n    group_id_tensor: torch.Tensor, # Group ID for the current item being placed (0-dim tensor)\n    pack_weights_layer_i: torch.Tensor, # 1D tensor representing weights of packs for current layer\n    pack_items_layer_i: torch.Tensor,   # 1D tensor representing item counts of packs for current layer\n    groups_per_pack: int, # Max items per pack\n    inf_val: torch.Tensor, # Pre-created infinity tensor\n    group_weight: torch.Tensor # Weight of the current group (0-dim tensor)\n) -> tuple[torch.Tensor, torch.Tensor]: # Returns chosen pack index and rank within that pack (0-dim tensors)\n    \"\"\"\n    JIT-compiled function for the core greedy assignment logic within balanced_packing.\n    This function modifies pack_weights_layer_i and pack_items_layer_i in-place.\n    \"\"\"\n    available_mask = (pack_items_layer_i < groups_per_pack)\n    candidate_weights = torch.where(available_mask, pack_weights_layer_i, inf_val)\n    \n    # Find the pack with the minimum current weight among available ones.\n    # Keep it as a 0-dim tensor for indexing in JIT-compiled code.\n    pack_idx_tensor = torch.argmin(candidate_weights)\n    \n    # Get rank before update. Keep as 0-dim tensor.\n    rank_in_pack_tensor = pack_items_layer_i[pack_idx_tensor]\n\n    # Update pack weights and item counts for the chosen pack in-place\n    pack_weights_layer_i[pack_idx_tensor] += group_weight\n    pack_items_layer_i[pack_idx_tensor] += 1\n    \n    return pack_idx_tensor, rank_in_pack_tensor\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # `weight` is already float from `rebalance_experts`\n    indices = weight.sort(-1, descending=True).indices  # Keep on original device\n    pack_index = torch.full_like(weight,\n                                 fill_value=-1,\n                                 dtype=torch.int64,\n                                 device=weight.device)\n    rank_in_pack = torch.full_like(pack_index,\n                                   fill_value=-1,\n                                   device=weight.device)\n\n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These are on the same device as the input 'weight'.\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Pre-create a tensor for infinity value to avoid repeated creation in loop\n    inf_tensor = torch.tensor(float('inf'), device=weight.device, dtype=weight.dtype)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # Get 1D slices for the current layer's pack states.\n        # These slices will be passed to the JIT-compiled function and modified in-place.\n        current_pack_weights_slice = pack_weights_per_layer[i]\n        current_pack_items_slice = pack_items_per_layer[i]\n\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id_tensor in indices[i]:\n            # Call the JIT-compiled function for the assignment logic\n            # Pass group_id_tensor directly as a 0-dim tensor\n            chosen_pack_idx_tensor, chosen_rank_in_pack_tensor = _jit_assign_group_and_update(\n                group_id_tensor,\n                current_pack_weights_slice,\n                current_pack_items_slice,\n                groups_per_pack,\n                inf_tensor,\n                weight[i, group_id_tensor] # Pass the weight of the current group as a 0-dim tensor\n            )\n\n            # Assign the group to the chosen pack based on results from JIT function.\n            # `group_id_tensor` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id_tensor] = chosen_pack_idx_tensor\n            rank_in_pack[i, group_id_tensor] = chosen_rank_in_pack_tensor\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    # Pre-calculate initial average loads for all logical experts\n    # Use float for division and average load calculations\n    avg_loads = weight.float() / logcnt.float() # Optimization: Pre-calculate average loads\n\n    for i in range(num_log, num_phy):\n        # Find the logical expert with the maximum current average load.\n        # This determines which logical expert gets the next replica.\n        _, redundant_indices = torch.max(avg_loads, dim=-1) # Optimization: Use pre-calculated avg_loads\n\n        # Assign the new physical expert to this logical expert\n        phy2log[:, i] = redundant_indices\n        \n        # Determine the replica rank for this new physical expert\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        \n        # Increment the replica count for the chosen logical expert\n        logcnt[arangen, redundant_indices] += 1\n        \n        # Update the average load for the chosen logical expert.\n        # This is crucial for the next iteration's decision.\n        # The average load is the total weight of the logical expert divided by its current replica count.\n        avg_loads[arangen, redundant_indices] = weight[arangen, redundant_indices].float() / logcnt[arangen, redundant_indices].float() # Optimization: Update only the relevant average load\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float()  # Ensure float, but keep on original device (e.g., GPU)\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    # Ensure log2phy is created on the same device as other tensors\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=weight.device,  # Use weight's device\n    )\n    # The scatter_ operation and arange should also use the correct device\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=weight.device).expand(num_layers, -1),  # Use weight's device\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n", "language": "python", "parent_id": "7fa921c3-3bc5-4167-917d-cd5304e5bdaf", "generation": 8, "timestamp": 1751056843.2718954, "iteration_found": 114, "metrics": {"balancedness_score": 0.29830835153138974, "speed_score": 0.031281843494993126, "combined_score": 0.16479509751319144}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 8 lines with 22 lines", "parent_metrics": {"balancedness_score": 0.29830835153138974, "speed_score": 0.031314921723297834, "combined_score": 0.16481163662734377}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve the Mixture-of-Expert models Expert Parallelism Load Balancer (MoE EPLB) expert rearrangement algorithm.\nThis algorithm will take the load metrics recorded by the vLLM server, and rearrange the experts to balance the load. It can make replicas of some experts to achieve better load balancing.\nYour goal will be two-fold: 1. Improve the algorithm to achieve better load balancing; while 2. Improve the algorithm to be more efficient, i.e. reduce the execution time of the algorithm itself, since perfect load balancing is NP-hard.\nThe current algorithm is implemented in the `rebalance_experts` function. ", "user": "# Current Program Information\n- Current performance metrics: - balancedness_score: 0.2983\n- speed_score: 0.0313\n- combined_score: 0.1648\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: speed_score, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0830, combined_score: 0.1907\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0833, combined_score: 0.1908\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0835, combined_score: 0.1909\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.1909)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0835), Performs well on combined_score (0.1909)\n\n\n### Program 2 (Score: 0.1908)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0833), Performs well on combined_score (0.1908)\n\n\n### Program 3 (Score: 0.1907)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0830), Performs well on combined_score (0.1907)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.1900)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n\n### Program D2 (Score: 0.1872)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.1909, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 3 lines with 4 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 2 (Score: 0.0939, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 23 lines with 236 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 3 (Score: 0.0232, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 45 lines with 44 lines\nChange 2: Replace 14 lines with 21 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 4 (Score: 0.0000, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 5 lines with 20 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 5 (Score: 0.0171, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 43 lines with 41 lines\nChange 2: Replace 12 lines with 13 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n\n# Current Program\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\nimport torch.jit as jit\n\n# Define the inner loop logic as a TorchScript function\n@jit.script\ndef _jit_assign_group_and_update(\n    group_id_tensor: torch.Tensor, # Group ID for the current item being placed (0-dim tensor)\n    pack_weights_layer_i: torch.Tensor, # 1D tensor representing weights of packs for current layer\n    pack_items_layer_i: torch.Tensor,   # 1D tensor representing item counts of packs for current layer\n    groups_per_pack: int, # Max items per pack\n    inf_val: torch.Tensor, # Pre-created infinity tensor\n    group_weight: torch.Tensor # Weight of the current group (0-dim tensor)\n) -> tuple[torch.Tensor, torch.Tensor]: # Returns chosen pack index and rank within that pack (0-dim tensors)\n    \"\"\"\n    JIT-compiled function for the core greedy assignment logic within balanced_packing.\n    This function modifies pack_weights_layer_i and pack_items_layer_i in-place.\n    \"\"\"\n    available_mask = (pack_items_layer_i < groups_per_pack)\n    candidate_weights = torch.where(available_mask, pack_weights_layer_i, inf_val)\n    \n    # Find the pack with the minimum current weight among available ones.\n    # Keep it as a 0-dim tensor for indexing in JIT-compiled code.\n    pack_idx_tensor = torch.argmin(candidate_weights)\n    \n    # Get rank before update. Keep as 0-dim tensor.\n    rank_in_pack_tensor = pack_items_layer_i[pack_idx_tensor]\n\n    # Update pack weights and item counts for the chosen pack in-place\n    pack_weights_layer_i[pack_idx_tensor] += group_weight\n    pack_items_layer_i[pack_idx_tensor] += 1\n    \n    return pack_idx_tensor, rank_in_pack_tensor\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # `weight` is already float from `rebalance_experts`\n    indices = weight.sort(-1, descending=True).indices  # Keep on original device\n    pack_index = torch.full_like(weight,\n                                 fill_value=-1,\n                                 dtype=torch.int64,\n                                 device=weight.device)\n    rank_in_pack = torch.full_like(pack_index,\n                                   fill_value=-1,\n                                   device=weight.device)\n\n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These are on the same device as the input 'weight'.\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Pre-create a tensor for infinity value to avoid repeated creation in loop\n    inf_tensor = torch.tensor(float('inf'), device=weight.device, dtype=weight.dtype)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # Get 1D slices for the current layer's pack states.\n        # These slices will be passed to the JIT-compiled function and modified in-place.\n        current_pack_weights_slice = pack_weights_per_layer[i]\n        current_pack_items_slice = pack_items_per_layer[i]\n\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id_tensor in indices[i]:\n            # Call the JIT-compiled function for the assignment logic\n            # Pass group_id_tensor directly as a 0-dim tensor\n            chosen_pack_idx_tensor, chosen_rank_in_pack_tensor = _jit_assign_group_and_update(\n                group_id_tensor,\n                current_pack_weights_slice,\n                current_pack_items_slice,\n                groups_per_pack,\n                inf_tensor,\n                weight[i, group_id_tensor] # Pass the weight of the current group as a 0-dim tensor\n            )\n\n            # Assign the group to the chosen pack based on results from JIT function.\n            # `group_id_tensor` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id_tensor] = chosen_pack_idx_tensor\n            rank_in_pack[i, group_id_tensor] = chosen_rank_in_pack_tensor\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        # To minimize the maximum load, we should add replicas to the\n        # logical experts with the maximum current average load.\n        # This strategy aims to reduce the load of the currently most loaded experts.\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float()  # Ensure float, but keep on original device (e.g., GPU)\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    # Ensure log2phy is created on the same device as other tensors\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=weight.device,  # Use weight's device\n    )\n    # The scatter_ operation and arange should also use the correct device\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=weight.device).expand(num_layers, -1),  # Use weight's device\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["The current program has a `balancedness_score` of 0.2983 and a `speed_score` of 0.0313, resulting in a `combined_score` of 0.1648. The goal is to improve both `balancedness_score` and `speed_score`. The history shows that previous attempts (Attempts 1-3) improved all metrics, but the current version has regressed in `speed_score` and `combined_score`.\n\nThe primary area for improvement seems to be in the `rebalance_experts_hierarchical` function, particularly the `balanced_packing` and `replicate_experts` sub-functions. These functions are responsible for distributing the load (expert weights) across available resources (packs, physical experts, GPUs) to minimize imbalance.\n\nHere are a few suggestions:\n\n### Improvement 1: Optimize `balanced_packing` for better speed\n\nThe `balanced_packing` function iterates through each layer and then through each group within that layer. For each group, it calls a JIT-compiled function `_jit_assign_group_and_update`. While JIT compilation helps, the nested loops can still be a bottleneck.\n\nOne potential optimization is to process all groups for all layers in a more vectorized manner if possible, reducing Python loop overhead. However, given the greedy nature of the `balanced_packing` which depends on the state of `pack_weights_layer_i` and `pack_items_layer_i` which are updated within the inner loop, a full vectorization might be complex.\n\nA more practical approach to improve speed without drastically changing the logic is to ensure that tensor operations within the loop are as efficient as possible. The current implementation seems reasonable, but let's consider if any small adjustments can be made. The `torch.where` operation to create `candidate_weights` is a good way to handle unavailable packs. The `torch.argmin` is also efficient.\n\nLet's re-examine the `balanced_packing` function for potential micro-optimizations or clarity improvements that might indirectly help speed.\n\nThe current `balanced_packing` function sorts the groups by weight and then greedily assigns them. This is a standard approach. The JIT compiled function `_jit_assign_group_and_update` is already designed to be efficient.\n\nConsider the `indices = weight.sort(-1, descending=True).indices` line. This sorts the groups for each layer. If `num_layers` is large, this sorting can contribute to the overall time. However, it's crucial for the greedy algorithm.\n\nLet's focus on the loop structure and how data is accessed.\n\nThe current loop structure is:\n```python\nfor i in range(num_layers):\n    # ...\n    for group_id_tensor in indices[i]:\n        # ... _jit_assign_group_and_update(...)\n```\n\nThe `_jit_assign_group_and_update` function takes `group_id_tensor` as a 0-dim tensor. This is correct for JIT.\n\nA subtle point: the `weight[i, group_id_tensor]` is passed as the `group_weight`. This is a single element tensor. The `_jit_assign_group_and_update` function adds this to `pack_weights_layer_i`.\n\nOne potential area for speedup, though it might slightly impact balancedness, is if we can reduce the number of times `_jit_assign_group_and_update` is called. However, this function encapsulates the core greedy logic, so reducing calls would mean fundamentally changing the algorithm.\n\nLet's look at the `replicate_experts` function. It iterates `num_redundant` times. Inside the loop, it calculates `redundant_indices = (weight / logcnt).max(dim=-1).indices`. This calculation involves division and then `max(dim=-1)`. This is a key operation for deciding which logical expert gets a new replica.\n\nThe current approach is to greedily add replicas to the logical expert with the highest current average load. This is a good heuristic.\n\nPerhaps the issue is not in the core logic but in how tensors are handled or if there are redundant computations.\n\nLet's consider the `replicate_experts` function again:\n```python\n    for i in range(num_log, num_phy):\n        # To minimize the maximum load, we should add replicas to the\n        # logical experts with the maximum current average load.\n        # This strategy aims to reduce the load of the currently most loaded experts.\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n```\nThe calculation `(weight / logcnt).max(dim=-1)` is performed in each iteration. `weight` is `[n, num_log]` and `logcnt` is `[n, num_log]`. The division and max are element-wise operations across the `num_log` dimension.\n\nIf `num_log` is very large, `max(dim=-1)` can be costly. However, this is fundamental to the greedy strategy.\n\nLet's consider the context of the current performance metrics:\n- `balancedness_score`: 0.2983 (This is the primary metric for load balancing)\n- `speed_score`: 0.0313 (This is the primary metric for execution speed)\n- `combined_score`: 0.1648\n\nThe regression in `speed_score` and `combined_score` suggests that recent changes might have introduced overhead or made the algorithm slower without a proportional increase in load balancing.\n\nOne observation from the \"Inspiration Programs\" is that some \"Exploratory\" programs have significantly different \"Unique approach\" descriptions, like \"Modification: Change 1: Replace 3 lines with 4 lines\" or \"Modification: Change 1: Replace 23 lines with 236 lines\". This suggests that perhaps the current implementation of `rebalance_experts` or its sub-functions might be overly complex or not the most efficient way to achieve the desired outcome.\n\nGiven that the current `speed_score` is low (0.0313), let's focus on making the existing logic more efficient.\n\n**Suggestion 1: Pre-allocate and reuse tensors where possible to minimize device allocations within loops.**\n\nIn `rebalance_experts_hierarchical`, the `inverse` function is called multiple times. The `inverse` function creates new tensors. It might be more efficient to pre-allocate these if the shapes are known and reuse them. However, the `inverse` function's output shape depends on the input `perm` shape, which can vary slightly.\n\nLet's look at `replicate_experts` again.\n```python\n    for i in range(num_log, num_phy):\n        # ...\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n```\nThe `logcnt[arangen, redundant_indices] += 1` is an in-place update. This is good. The issue might be the repeated calculation of `(weight / logcnt).max(dim=-1)`.\n\nCould we optimize the calculation of `redundant_indices`?\nThe goal is to find the logical expert that, if given another replica, would result in the smallest increase in its average load. This is equivalent to picking the logical expert with the currently highest average load.\n\nLet's consider the calculation: `(weight / logcnt)`. This is `[n, num_log]`. Then `.max(dim=-1)`. This finds the maximum value and its index along the `num_log` dimension for each of the `n` rows.\n\nIf `n` (number of layers) is large, this repeated calculation across all layers can be time-consuming.\n\nPerhaps instead of recalculating `weight / logcnt` every time, we can maintain a \"current average load\" tensor and update it.\n\nLet's try to refine the `replicate_experts` function.\n\n**Refinement for `replicate_experts`:**\n\nInstead of repeatedly calculating `weight / logcnt`, let's maintain the average loads and update them.\n\n```python\ndef replicate_experts(\n    weight: torch.Tensor,\n    num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    \n    # Initialize output tensors\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n\n    # Pre-calculate initial average loads\n    # Avoid division by zero if logcnt is 1 (initial state)\n    avg_loads = weight.float() / logcnt.float()\n\n    for i in range(num_log, num_phy):\n        # Find the logical expert with the maximum current average load.\n        # Use the pre-calculated avg_loads.\n        _, redundant_indices = torch.max(avg_loads, dim=-1)\n\n        # Assign the new physical expert to this logical expert\n        phy2log[:, i] = redundant_indices\n        \n        # Get the rank for this new replica\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        \n        # Update the count for the chosen logical expert\n        logcnt[arangen, redundant_indices] += 1\n        \n        # Update the average load for the chosen logical expert\n        # The new average load is (current_total_load + new_item_weight) / new_count\n        # We can calculate this directly without re-dividing the whole tensor.\n        # Let W_l be the total weight of logical expert l, and C_l be its current count.\n        # Average load A_l = W_l / C_l.\n        # If we add a new item of weight w_new, the new total weight is W_l + w_new.\n        # The new count is C_l + 1.\n        # The new average load A'_l = (W_l + w_new) / (C_l + 1).\n        # We know W_l = A_l * C_l.\n        # So, A'_l = (A_l * C_l + w_new) / (C_l + 1).\n        \n        # Get the weight of the *next* item to be added to this logical expert.\n        # This is tricky because the `weight` tensor represents the *initial* load,\n        # not the load of individual items. The problem statement implies `weight`\n        # is a statistic for the logical expert.\n        # The original code `(weight / logcnt)` implies we are looking at the\n        # average load of the *existing* replicas.\n        \n        # Let's re-read the original comment:\n        # \"To minimize the maximum load, we should add replicas to the\n        # logical experts with the maximum current average load.\"\n        # The average load is calculated as `weight / logcnt`.\n        # So, `weight` is the total load for a logical expert.\n        # `logcnt` is the current number of replicas.\n        # `weight / logcnt` is the average load per replica for that logical expert.\n        \n        # When `logcnt[arangen, redundant_indices]` is incremented, the average load\n        # for that specific logical expert needs to be recomputed for the next iteration.\n        # The `weight` tensor remains the same (total initial load).\n        \n        # The most direct way to update `avg_loads` for the chosen `redundant_indices`\n        # is to recompute it using the updated `logcnt`:\n        avg_loads[arangen, redundant_indices] = weight[arangen, redundant_indices] / logcnt[arangen, redundant_indices].float()\n\n    return phy2log, rank, logcnt\n\n```\nThis revised `replicate_experts` function pre-calculates the average loads and updates only the relevant average load after assigning a replica. This should reduce redundant computations, especially if `num_log` is large and `num_phy - num_log` is also significant. The division `weight[arangen, redundant_indices] / logcnt[arangen, redundant_indices].float()` is now done only for the selected logical expert, rather than for all logical experts in every iteration.\n\nLet's consider another area: `rebalance_experts_hierarchical`.\nThe `balanced_packing` function is called twice:\n1. `tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)`\n   This calculates tokens per group. `weight` is `[num_layers, num_logical_experts]`.\n   `unflatten` to `[num_layers, num_groups, group_size]`.\n   `sum(-1)` to `[num_layers, num_groups]`.\n   This is then passed to `balanced_packing(tokens_per_group, num_nodes)`.\n\n2. `tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)`\n   `tokens_per_mlog` is `[num_layers * num_nodes, num_logical_experts // num_nodes]`.\n   `mlogcnt` is `[num_layers * num_nodes, num_logical_experts // num_nodes]`.\n   `phy2mlog` is `[num_layers * num_nodes, num_physical_experts // num_nodes]`.\n   This calculates tokens per physical expert *within* a node.\n   Then `balanced_packing(tokens_per_phy, num_gpus // num_nodes)` is called.\n\nThe `balanced_packing` function itself has a time complexity related to `num_groups * log(num_groups)` for sorting and then `num_groups * num_items_per_pack` for the greedy assignment.\n\nGiven the regression in `speed_score`, the overhead of these packing operations could be significant.\n\n**Suggestion 2: Simplify `rebalance_experts_hierarchical` logic if possible, or optimize data preparation.**\n\nThe current `rebalance_experts_hierarchical` function is quite complex, with several reshaping and gathering operations. Let's scrutinize the data preparation steps.\n\nConsider the first `balanced_packing` call:\n`tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)`\nThis sums up the weights of experts within a group. This seems correct.\n\nThen, `group_pack_index, group_rank_in_pack = balanced_packing(tokens_per_group, num_nodes)` packs these groups onto nodes.\n\nThe subsequent steps involve `mlog2log`, `phy2mlog`, `pphy2mlog`, etc. These are essentially remapping and gathering operations to align the nested structure (layers -> nodes -> GPUs -> physical experts).\n\nIf the `speed_score` has regressed, it's possible that some of these tensor manipulations are inefficient.\n\nLet's look at the `inverse` function. It uses `scatter_`.\n```python\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n```\nThis is a standard way to compute an inverse permutation. It's generally efficient.\n\nThe use of `.view()` is also usually efficient as it doesn't copy data.\n\nLet's consider the possibility of reducing the number of `balanced_packing` calls. The current logic is hierarchical: first pack groups to nodes, then pack within nodes to GPUs. This seems sound for capturing intra-node network advantages.\n\nHowever, if the `speed_score` is the main concern, and `balancedness_score` is acceptable at 0.2983, we might consider a simpler, albeit potentially less optimal, global packing strategy. The code already has a fallback:\n```python\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n```\nThe \"global load-balance policy\" by setting `num_groups=1` means `rebalance_experts_hierarchical` will be called with `num_groups=1`. In this case, `group_size` becomes `num_logical_experts`, and `groups_per_node` becomes `1`. The logic effectively flattens the `num_groups` dimension and treats all experts as a single group. This simplifies the hierarchical structure.\n\nIf the `speed_score` is problematic, and the hierarchical approach is complex, forcing the global approach might be faster, even if it slightly degrades `balancedness_score`.\n\nHowever, the prompt asks to *improve* the algorithm. Simply switching to a less optimal policy might not be the intended solution.\n\nLet's reconsider the `replicate_experts` function optimization. The proposed change to pre-calculate `avg_loads` seems like a good candidate for improving `speed_score` without significantly impacting `balancedness_score`. It addresses a potential performance bottleneck in the replication logic.\n\nLet's apply this change.\n\n```python\n# EVOLVE-BLOCK-START\n\nimport torch\nimport torch.jit as jit\n\n# Define the inner loop logic as a TorchScript function\n@jit.script\ndef _jit_assign_group_and_update(\n    group_id_tensor: torch.Tensor, # Group ID for the current item being placed (0-dim tensor)\n    pack_weights_layer_i: torch.Tensor, # 1D tensor representing weights of packs for current layer\n    pack_items_layer_i: torch.Tensor,   # 1D tensor representing item counts of packs for current layer\n    groups_per_pack: int, # Max items per pack\n    inf_val: torch.Tensor, # Pre-created infinity tensor\n    group_weight: torch.Tensor # Weight of the current group (0-dim tensor)\n) -> tuple[torch.Tensor, torch.Tensor]: # Returns chosen pack index and rank within that pack (0-dim tensors)\n    \"\"\"\n    JIT-compiled function for the core greedy assignment logic within balanced_packing.\n    This function modifies pack_weights_layer_i and pack_items_layer_i in-place.\n    \"\"\"\n    available_mask = (pack_items_layer_i < groups_per_pack)\n    candidate_weights = torch.where(available_mask, pack_weights_layer_i, inf_val)\n    \n    # Find the pack with the minimum current weight among available ones.\n    # Keep it as a 0-dim tensor for indexing in JIT-compiled code.\n    pack_idx_tensor = torch.argmin(candidate_weights)\n    \n    # Get rank before update. Keep as 0-dim tensor.\n    rank_in_pack_tensor = pack_items_layer_i[pack_idx_tensor]\n\n    # Update pack weights and item counts for the chosen pack in-place\n    pack_weights_layer_i[pack_idx_tensor] += group_weight\n    pack_items_layer_i[pack_idx_tensor] += 1\n    \n    return pack_idx_tensor, rank_in_pack_tensor\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # `weight` is already float from `rebalance_experts`\n    indices = weight.sort(-1, descending=True).indices  # Keep on original device\n    pack_index = torch.full_like(weight,\n                                 fill_value=-1,\n                                 dtype=torch.int64,\n                                 device=weight.device)\n    rank_in_pack = torch.full_like(pack_index,\n                                   fill_value=-1,\n                                   device=weight.device)\n\n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These are on the same device as the input 'weight'.\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Pre-create a tensor for infinity value to avoid repeated creation in loop\n    inf_tensor = torch.tensor(float('inf'), device=weight.device, dtype=weight.dtype)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # Get 1D slices for the current layer's pack states.\n        # These slices will be passed to the JIT-compiled function and modified in-place.\n        current_pack_weights_slice = pack_weights_per_layer[i]\n        current_pack_items_slice = pack_items_per_layer[i]\n\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id_tensor in indices[i]:\n            # Call the JIT-compiled function for the assignment logic\n            # Pass group_id_tensor directly as a 0-dim tensor\n            chosen_pack_idx_tensor, chosen_rank_in_pack_tensor = _jit_assign_group_and_update(\n                group_id_tensor,\n                current_pack_weights_slice,\n                current_pack_items_slice,\n                groups_per_pack,\n                inf_tensor,\n                weight[i, group_id_tensor] # Pass the weight of the current group as a 0-dim tensor\n            )\n\n            # Assign the group to the chosen pack based on results from JIT function.\n            # `group_id_tensor` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id_tensor] = chosen_pack_idx_tensor\n            rank_in_pack[i, group_id_tensor] = chosen_rank_in_pack_tensor\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n    weight: torch.Tensor,\n    num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    \n    # Initialize output tensors\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n\n    # Pre-calculate initial average loads for all logical experts\n    # Use float for division and average load calculations\n    avg_loads = weight.float() / logcnt.float()\n\n    for i in range(num_log, num_phy):\n        # Find the logical expert with the maximum current average load.\n        # This determines which logical expert gets the next replica.\n        _, redundant_indices = torch.max(avg_loads, dim=-1)\n\n        # Assign the new physical expert to this logical expert\n        phy2log[:, i] = redundant_indices\n        \n        # Determine the replica rank for this new physical expert\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        \n        # Increment the replica count for the chosen logical expert\n        logcnt[arangen, redundant_indices] += 1\n        \n        # Update the average load for the chosen logical expert.\n        # This is crucial for the next iteration's decision.\n        # The average load is the total weight of the logical expert divided by its current replica count.\n        avg_loads[arangen, redundant_indices] = weight[arangen, redundant_indices].float() / logcnt[arangen, redundant_indices].float()\n\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float()  # Ensure float, but keep on original device (e.g., GPU)\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        # The current implementation calls rebalance_experts_hierarchical with num_groups=1\n        # This effectively flattens the problem into a single group, simplifying the hierarchy.\n        # If the speed score is the primary concern and the hierarchical approach is too slow,\n        # this global approach might be preferred, although it could slightly impact balancedness.\n        # Given the regression in speed_score, we will stick with the hierarchical call,\n        # but the previous change in replicate_experts should help speed.\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    # Ensure log2phy is created on the same device as other tensors\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=weight.device,  # Use weight's device\n    )\n    # The scatter_ operation and arange should also use the correct device\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=weight.device).expand(num_layers, -1),  # Use weight's device\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n```\n\n**Reasoning for the change:**\n\nThe primary goal is to improve the `speed_score` while maintaining or improving `balancedness_score`. The regression in `speed_score` suggests that the current implementation might have an inefficient step.\n\nThe `replicate_experts` function is responsible for deciding which logical experts receive additional replicas to balance the load. Its core loop iterates `num_redundant` times, and in each iteration, it calculates `(weight / logcnt).max(dim=-1).indices`. This means for every new replica being assigned, the average load for *all* logical experts is recalculated and the maximum is found. If `num_log` (number of logical experts) is large, this `max(dim=-1)` operation across all logical experts in every iteration can be computationally expensive.\n\nThe proposed change optimizes this by:\n1.  **Pre-calculating `avg_loads`**: Before the loop starts, the average load for all logical experts (`weight / logcnt`) is calculated once and stored in `avg_loads`.\n2.  **Efficiently finding the maximum**: Inside the loop, `torch.max(avg_loads, dim=-1)` is used to find the logical expert with the highest average load. This is still necessary but leverages the pre-calculated `avg_loads`.\n3.  **Updating only the affected average load**: After a logical expert (`redundant_indices`) is chosen and its `logcnt` is incremented, its average load (`avg_loads[arangen, redundant_indices]`) is updated directly. This avoids recomputing the average load for all logical experts in every iteration. Instead, only the average load of the chosen expert is recomputed using its new `logcnt` and the original `weight`.\n\nThis optimization should significantly reduce the computational overhead in `replicate_experts`, thereby improving the `speed_score`. The logic for selecting the expert with the maximum average load remains the same, so the impact on `balancedness_score` should be minimal, and potentially even positive if the previous calculation was slightly inaccurate due to floating-point issues or if the update strategy is more precise.\n\nThis change directly targets a potential performance bottleneck in the expert replication process, which is a core part of the load balancing algorithm.\n\nFinal check on the code and diff format:\n- The change is within the `replicate_experts` function.\n- It modifies the loop by pre-calculating `avg_loads` and updating it selectively.\n- The diff format is correct (`<<<<<<< SEARCH`, `=======`, `>>>>>>> REPLACE`).\n- The reasoning is provided.\n\nOne minor thought: the `weight.float()` conversion is done in `rebalance_experts`. The `replicate_experts` function receives `weight` which is already float. So, `weight.float()` in `replicate_experts` is redundant. However, given the current code, it's not causing harm and the provided diff focuses on the optimization. Let's keep it as is to minimize changes outside the core optimization.\n\nThe provided solution focuses on optimizing the `replicate_experts` function, which is a significant part of the load balancing logic. The change aims to reduce redundant computations and improve the `speed_score`.\n\n```python\n# EVOLVE-BLOCK-START\n\nimport torch\nimport torch.jit as jit\n\n# Define the inner loop logic as a TorchScript function\n@jit.script\ndef _jit_assign_group_and_update(\n    group_id_tensor: torch.Tensor, # Group ID for the current item being placed (0-dim tensor)\n    pack_weights_layer_i: torch.Tensor, # 1D tensor representing weights of packs for current layer\n    pack_items_layer_i: torch.Tensor,   # 1D tensor representing item counts of packs for current layer\n    groups_per_pack: int, # Max items per pack\n    inf_val: torch.Tensor, # Pre-created infinity tensor\n    group_weight: torch.Tensor # Weight of the current group (0-dim tensor)\n) -> tuple[torch.Tensor, torch.Tensor]: # Returns chosen pack index and rank within that pack (0-dim tensors)\n    \"\"\"\n    JIT-compiled function for the core greedy assignment logic within balanced_packing.\n    This function modifies pack_weights_layer_i and pack_items_layer_i in-place.\n    \"\"\"\n    available_mask = (pack_items_layer_i < groups_per_pack)\n    candidate_weights = torch.where(available_mask, pack_weights_layer_i, inf_val)\n    \n    # Find the pack with the minimum current weight among available ones.\n    # Keep it as a 0-dim tensor for indexing in JIT-compiled code.\n    pack_idx_tensor = torch.argmin(candidate_weights)\n    \n    # Get rank before update. Keep as 0-dim tensor.\n    rank_in_pack_tensor = pack_items_layer_i[pack_idx_tensor]\n\n    # Update pack weights and item counts for the chosen pack in-place\n    pack_weights_layer_i[pack_idx_tensor] += group_weight\n    pack_items_layer_i[pack_idx_tensor] += 1\n    \n    return pack_idx_tensor, rank_in_pack_tensor\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # `weight` is already float from `rebalance_experts`\n    indices = weight.sort(-1, descending=True).indices  # Keep on original device\n    pack_index = torch.full_like(weight,\n                                 fill_value=-1,\n                                 dtype=torch.int64,\n                                 device=weight.device)\n    rank_in_pack = torch.full_like(pack_index,\n                                   fill_value=-1,\n                                   device=weight.device)\n\n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These are on the same device as the input 'weight'.\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Pre-create a tensor for infinity value to avoid repeated creation in loop\n    inf_tensor = torch.tensor(float('inf'), device=weight.device, dtype=weight.dtype)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # Get 1D slices for the current layer's pack states.\n        # These slices will be passed to the JIT-compiled function and modified in-place.\n        current_pack_weights_slice = pack_weights_per_layer[i]\n        current_pack_items_slice = pack_items_per_layer[i]\n\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id_tensor in indices[i]:\n            # Call the JIT-compiled function for the assignment logic\n            # Pass group_id_tensor directly as a 0-dim tensor\n            chosen_pack_idx_tensor, chosen_rank_in_pack_tensor = _jit_assign_group_and_update(\n                group_id_tensor,\n                current_pack_weights_slice,\n                current_pack_items_slice,\n                groups_per_pack,\n                inf_tensor,\n                weight[i, group_id_tensor] # Pass the weight of the current group as a 0-dim tensor\n            )\n\n            # Assign the group to the chosen pack based on results from JIT function.\n            # `group_id_tensor` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id_tensor] = chosen_pack_idx_tensor\n            rank_in_pack[i, group_id_tensor] = chosen_rank_in_pack_tensor\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n    weight: torch.Tensor,\n    num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    \n    # Initialize output tensors\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n\n    # Pre-calculate initial average loads for all logical experts\n    # Use float for division and average load calculations\n    avg_loads = weight.float() / logcnt.float() # Optimization: Pre-calculate average loads\n\n    for i in range(num_log, num_phy):\n        # Find the logical expert with the maximum current average load.\n        # This determines which logical expert gets the next replica.\n        _, redundant_indices = torch.max(avg_loads, dim=-1) # Optimization: Use pre-calculated avg_loads\n\n        # Assign the new physical expert to this logical expert\n        phy2log[:, i] = redundant_indices\n        \n        # Determine the replica rank for this new physical expert\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        \n        # Increment the replica count for the chosen logical expert\n        logcnt[arangen, redundant_indices] += 1\n        \n        # Update the average load for the chosen logical expert.\n        # This is crucial for the next iteration's decision.\n        # The average load is the total weight of the logical expert divided by its current replica count.\n        avg_loads[arangen, redundant_indices] = weight[arangen, redundant_indices].float() / logcnt[arangen, redundant_indices].float() # Optimization: Update only the relevant average load\n\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float()  # Ensure float, but keep on original device (e.g., GPU)\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    # Ensure log2phy is created on the same device as other tensors\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=weight.device,  # Use weight's device\n    )\n    # The scatter_ operation and arange should also use the correct device\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=weight.device).expand(num_layers, -1),  # Use weight's device\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n```\n\n<<<<<<< SEARCH\n    for i in range(num_log, num_phy):\n        # To minimize the maximum load, we should add replicas to the\n        # logical experts with the maximum current average load.\n        # This strategy aims to reduce the load of the currently most loaded experts.\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n=======\n    # Pre-calculate initial average loads for all logical experts\n    # Use float for division and average load calculations\n    avg_loads = weight.float() / logcnt.float() # Optimization: Pre-calculate average loads\n\n    for i in range(num_log, num_phy):\n        # Find the logical expert with the maximum current average load.\n        # This determines which logical expert gets the next replica.\n        _, redundant_indices = torch.max(avg_loads, dim=-1) # Optimization: Use pre-calculated avg_loads\n\n        # Assign the new physical expert to this logical expert\n        phy2log[:, i] = redundant_indices\n        \n        # Determine the replica rank for this new physical expert\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        \n        # Increment the replica count for the chosen logical expert\n        logcnt[arangen, redundant_indices] += 1\n        \n        # Update the average load for the chosen logical expert.\n        # This is crucial for the next iteration's decision.\n        # The average load is the total weight of the logical expert divided by its current replica count.\n        avg_loads[arangen, redundant_indices] = weight[arangen, redundant_indices].float() / logcnt[arangen, redundant_indices].float() # Optimization: Update only the relevant average load\n>>>>>>> REPLACE"]}}}