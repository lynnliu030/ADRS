{"id": "f0dc6d71-be98-4477-b1d2-e2f6ca724659", "code": "# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort groups by weight in descending order.\n    # 'weight' is already float and on CPU from rebalance_experts,\n    # so no need for .float().cpu() here.\n    _, sorted_indices = weight.sort(-1, descending=True)\n\n    # Initialize pack_index and rank_in_pack.\n    pack_index = torch.empty_like(weight, dtype=torch.int64)\n    rank_in_pack = torch.empty_like(pack_index)\n    \n    # Generate the assignment pattern for items in sorted order (k = 0 to num_groups-1)\n    k_indices = torch.arange(num_groups, dtype=torch.int64, device=weight.device)\n\n    # Calculate rank_in_pack for the sorted items.\n    # The rank is based on how many items have already been assigned to a pack.\n    # Since each pack receives `groups_per_pack` items, and we are assigning\n    # items in sorted order, the `k`-th sorted item will get rank `k // num_packs`.\n    ranks_for_sorted_items = k_indices // num_packs\n\n    # Calculate pack_id for the sorted items using the \"snake\" pattern.\n    # This ensures that the heaviest items are distributed among packs,\n    # and subsequent items fill them in a way that keeps the sums balanced.\n    idx_in_block = k_indices % num_packs\n    block_num = k_indices // num_packs\n    is_even_block = (block_num % 2 == 0)\n    \n    packs_for_sorted_items = torch.where(\n        is_even_block,\n        idx_in_block,\n        num_packs - 1 - idx_in_block\n    )\n\n    # Expand the assignments to all layers\n    expanded_packs = packs_for_sorted_items.unsqueeze(0).expand(num_layers, -1)\n    expanded_ranks = ranks_for_sorted_items.unsqueeze(0).expand(num_layers, -1)\n\n    # Use scatter_ to populate the output tensors based on original group IDs.\n    # For each layer, `sorted_indices[i, k]` gives the original group ID\n    # for the item at sorted position `k`.\n    # We want to assign `expanded_packs[i, k]` to `pack_index[i, original_group_id]`\n    # And `expanded_ranks[i, k]` to `rank_in_pack[i, original_group_id]`\n    pack_index.scatter_(1, sorted_indices, expanded_packs)\n    rank_in_pack.scatter_(1, sorted_indices, expanded_ranks)\n\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized. Optimized using a vectorized approach.\n\n    Parameters:\n        weight: [X, num_log]  (e.g., [num_layers * num_nodes, num_logical_experts_per_node])\n        num_phy: total number of experts after replication (e.g., num_physical_experts_per_node)\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank (0-indexed)\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    # num_redundant = num_phy - num_log # No longer explicitly used in vectorized approach\n    # assert num_redundant >= 0 # Assumed num_phy >= num_log for replication to make sense\n    device = weight.device\n\n    # Initialize output tensors\n    # phy2log: maps physical expert index to logical expert index\n    # rank: maps physical expert index to its replica rank (0-indexed)\n    phy2log = torch.empty((n, num_phy), dtype=torch.int64, device=device)\n    rank = torch.empty((n, num_phy), dtype=torch.int64, device=device)\n    \n    # current_logcnt[i, j] stores the number of replicas already assigned to logical expert j in row i.\n    # This will be the final `logcnt` output.\n    current_logcnt = torch.zeros(n, num_log, dtype=torch.int64, device=device)\n    \n    # current_expert_avg_load[i, j] stores the average load if logical expert j in row i\n    # were to receive its (current_logcnt[i,j] + 1)-th replica.\n    # Initialize with the load for the first replica (weight / 1).\n    current_expert_avg_load = weight.clone() \n    \n    # Iterate num_phy times, assigning one physical expert in each iteration.\n    for k in range(num_phy): # k is the index of the physical expert being assigned (0 to num_phy-1)\n        # Find the (row, logical_expert_idx) with the minimum current average load.\n        # torch.argmin returns a flat index.\n        min_load_idx_flat = torch.argmin(current_expert_avg_load)\n        \n        # Convert flat index back to (row_idx, log_expert_idx)\n        # Use .item() to convert 0-dim tensors to scalars for indexing.\n        row_idx = min_load_idx_flat // num_log\n        log_expert_idx = min_load_idx_flat % num_log\n\n        # Assign this physical expert (k) to the chosen logical expert.\n        phy2log[row_idx.item(), k] = log_expert_idx.item()\n        rank[row_idx.item(), k] = current_logcnt[row_idx.item(), log_expert_idx.item()] \n\n        # Increment the replica count for the chosen logical expert.\n        current_logcnt[row_idx.item(), log_expert_idx.item()] += 1\n        \n        # Update the average load for this logical expert.\n        # If this expert has now received `num_phy` replicas, set its average load to infinity\n        # so it's not chosen again.\n        if current_logcnt[row_idx.item(), log_expert_idx.item()] < num_phy:\n            current_expert_avg_load[row_idx.item(), log_expert_idx.item()] = \\\n                weight[row_idx.item(), log_expert_idx.item()] / (current_logcnt[row_idx.item(), log_expert_idx.item()] + 1.0)\n        else:\n            # This expert has reached its maximum replicas, set its load to infinity.\n            current_expert_avg_load[row_idx.item(), log_expert_idx.item()] = float('inf')\n\n    return phy2log, rank, current_logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        # More concise way to get values 0 to perm.size(1)-1 for scattering\n        values = torch.arange(perm.size(1), dtype=torch.int64, device=perm.device)\n        inv.scatter_(\n            1,\n            perm,\n            values.unsqueeze(0).expand_as(perm), # Expand values to match perm's shape\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float().cpu()\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    # Fix: The `maxlogcnt` for `log2phy` must accommodate the maximum possible\n    # replica rank, which can be `num_replicas - 1`.\n    # Therefore, the third dimension of `log2phy` should be `num_replicas`.\n    max_log_replicas = num_replicas \n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, max_log_replicas),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    # The scatter_ operation needs a flat index.\n    # The index is logical_expert_id * max_log_replicas + phyrank.\n    # The maximum index will be (num_logical_experts - 1) * max_log_replicas + (max_log_replicas - 1)\n    # which equals num_logical_experts * max_log_replicas - 1.\n    # So the flattened size of log2phy must be num_logical_experts * max_log_replicas.\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * max_log_replicas + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n", "language": "python", "parent_id": "f16843cb-4cab-4a64-a22d-9334c5c8c22b", "generation": 9, "timestamp": 1751060353.2547455, "iteration_found": 165, "metrics": {"balancedness_score": 0.0, "speed_score": 0.0, "combined_score": 0.0, "error": "index 139694213957696 is out of bounds for dimension 1 with size 64"}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 74 lines with 65 lines", "parent_metrics": {"balancedness_score": 0.02438604597509766, "speed_score": 0.03763638404527457, "combined_score": 0.031011215010186118}, "island": 3}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve the Mixture-of-Expert models Expert Parallelism Load Balancer (MoE EPLB) expert rearrangement algorithm.\nThis algorithm will take the load metrics recorded by the vLLM server, and rearrange the experts to balance the load. It can make replicas of some experts to achieve better load balancing.\nYour goal will be two-fold: 1. Improve the algorithm to achieve better load balancing; while 2. Improve the algorithm to be more efficient, i.e. reduce the execution time of the algorithm itself, since perfect load balancing is NP-hard.\nThe current algorithm is implemented in the `rebalance_experts` function. ", "user": "# Current Program Information\n- Current performance metrics: - balancedness_score: 0.0244\n- speed_score: 0.0376\n- combined_score: 0.0310\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: balancedness_score, speed_score, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0835, combined_score: 0.1909\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0845, combined_score: 0.1914\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0857, combined_score: 0.1920\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.1920)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0857), Performs well on combined_score (0.1920)\n\n\n### Program 2 (Score: 0.1914)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0845), Performs well on combined_score (0.1914)\n\n\n### Program 3 (Score: 0.1909)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0835), Performs well on combined_score (0.1909)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.1907)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n\n### Program D2 (Score: 0.1908)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.1920, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 2 (Score: 0.0939, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 23 lines with 236 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 3 (Score: 0.1626, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 6 lines with 22 lines\nChange 2: Replace 9 lines with 7 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 4 (Score: 0.0000, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 5 (Score: 0.1627, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n\n# Current Program\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort groups by weight in descending order.\n    # 'weight' is already float and on CPU from rebalance_experts,\n    # so no need for .float().cpu() here.\n    _, sorted_indices = weight.sort(-1, descending=True)\n\n    # Initialize pack_index and rank_in_pack.\n    pack_index = torch.empty_like(weight, dtype=torch.int64)\n    rank_in_pack = torch.empty_like(pack_index)\n    \n    # Generate the assignment pattern for items in sorted order (k = 0 to num_groups-1)\n    k_indices = torch.arange(num_groups, dtype=torch.int64, device=weight.device)\n\n    # Calculate rank_in_pack for the sorted items.\n    # The rank is based on how many items have already been assigned to a pack.\n    # Since each pack receives `groups_per_pack` items, and we are assigning\n    # items in sorted order, the `k`-th sorted item will get rank `k // num_packs`.\n    ranks_for_sorted_items = k_indices // num_packs\n\n    # Calculate pack_id for the sorted items using the \"snake\" pattern.\n    # This ensures that the heaviest items are distributed among packs,\n    # and subsequent items fill them in a way that keeps the sums balanced.\n    idx_in_block = k_indices % num_packs\n    block_num = k_indices // num_packs\n    is_even_block = (block_num % 2 == 0)\n    \n    packs_for_sorted_items = torch.where(\n        is_even_block,\n        idx_in_block,\n        num_packs - 1 - idx_in_block\n    )\n\n    # Expand the assignments to all layers\n    expanded_packs = packs_for_sorted_items.unsqueeze(0).expand(num_layers, -1)\n    expanded_ranks = ranks_for_sorted_items.unsqueeze(0).expand(num_layers, -1)\n\n    # Use scatter_ to populate the output tensors based on original group IDs.\n    # For each layer, `sorted_indices[i, k]` gives the original group ID\n    # for the item at sorted position `k`.\n    # We want to assign `expanded_packs[i, k]` to `pack_index[i, original_group_id]`\n    # And `expanded_ranks[i, k]` to `rank_in_pack[i, original_group_id]`\n    pack_index.scatter_(1, sorted_indices, expanded_packs)\n    rank_in_pack.scatter_(1, sorted_indices, expanded_ranks)\n\n    return pack_index, rank_in_pack\n\n\nimport heapq\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized. Optimized using a min-heap.\n\n    Parameters:\n        weight: [X, num_log]  (e.g., [num_layers * num_nodes, num_logical_experts_per_node])\n        num_phy: total number of experts after replication (e.g., num_physical_experts_per_node)\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank (0-indexed)\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n\n    # Initialize output tensors\n    # phy2log: maps physical expert index to logical expert index\n    # rank: maps physical expert index to its replica rank (0-indexed)\n    phy2log = torch.empty((n, num_phy), dtype=torch.int64, device=device)\n    rank = torch.empty((n, num_phy), dtype=torch.int64, device=device)\n    \n    # Keep track of the current number of replicas for each logical expert\n    # This is effectively the 'logcnt' output, but used internally for selection\n    current_logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    \n    # Iterate over each of the 'n' independent scenarios/rows\n    for i in range(n):\n        # Initialize a min-heap for the current row.\n        # Heap elements are tuples: (-score_scalar, logical_expert_index, replica_rank)\n        min_heap_row = []\n        \n        # Populate the heap with the first replica's score for each logical expert in this row.\n        # The score represents the average load if this expert gets its k-th replica.\n        for j in range(num_log):\n            # Calculate score for the first replica (k=1) for the current row.\n            # Ensure division is float division and convert to scalar for heap.\n            score = weight[i, j] / 1.0 \n            heapq.heappush(min_heap_row, (-score.item(), j, 1)) # Push scalar score\n\n        # Fill the physical experts for the current row by greedily picking the best available replica.\n        for k in range(num_phy): # Iterate from 0 to num_phy-1 for physical expert index\n            if not min_heap_row: # Should not happen if num_phy >= num_log initially\n                break\n\n            # Get the logical expert with the highest score (most negative in heap) for the current row.\n            neg_score_scalar, best_log_idx, rank_val = heapq.heappop(min_heap_row)\n            \n            # Assign this physical expert to the chosen logical expert for the current row.\n            phy2log[i, k] = best_log_idx\n            # The rank is 0-indexed\n            rank[i, k] = rank_val - 1 \n\n            # Update the count of replicas for this logical expert in the current row.\n            current_logcnt[i, best_log_idx] = rank_val\n            \n            # If this logical expert can still receive more replicas,\n            # push its next replica's score to the heap for the current row.\n            # The maximum number of replicas for any logical expert is implicitly num_phy.\n            if rank_val < num_phy: # Check if there's room for another replica\n                # Ensure float division for the next score calculation and convert to scalar.\n                next_score = weight[i, best_log_idx] / (rank_val + 1.0)\n                heapq.heappush(min_heap_row, (-next_score.item(), best_log_idx, rank_val + 1))\n\n    # The `current_logcnt` tensor now holds the number of replicas for each logical expert.\n    # This is the `logcnt` output.\n    return phy2log, rank, current_logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        # More concise way to get values 0 to perm.size(1)-1 for scattering\n        values = torch.arange(perm.size(1), dtype=torch.int64, device=perm.device)\n        inv.scatter_(\n            1,\n            perm,\n            values.unsqueeze(0).expand_as(perm), # Expand values to match perm's shape\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float().cpu()\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    # Fix: The `maxlogcnt` for `log2phy` must accommodate the maximum possible\n    # replica rank, which can be `num_replicas - 1`.\n    # Therefore, the third dimension of `log2phy` should be `num_replicas`.\n    max_log_replicas = num_replicas \n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, max_log_replicas),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    # The scatter_ operation needs a flat index.\n    # The index is logical_expert_id * max_log_replicas + phyrank.\n    # The maximum index will be (num_logical_experts - 1) * max_log_replicas + (max_log_replicas - 1)\n    # which equals num_logical_experts * max_log_replicas - 1.\n    # So the flattened size of log2phy must be num_logical_experts * max_log_replicas.\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * max_log_replicas + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["The current performance metrics show a significant regression compared to previous attempts, especially in `speed_score` and `balancedness_score`. The `replicate_experts` function, which uses a Python `for` loop and `heapq` for each independent scenario (row), is a likely candidate for the performance bottleneck due to Python's interpreter overhead compared to vectorized PyTorch operations.\n\nMy proposed improvement focuses on vectorizing the `replicate_experts` function. This change will implement the exact same greedy load balancing logic but using highly optimized PyTorch tensor operations, which should drastically improve the `speed_score`. While this specific change doesn't alter the core balancing heuristic, a faster execution time for the algorithm allows for more frequent rebalancing in a dynamic system, which can indirectly lead to better overall load balancing over time.\n\n**Reasoning for the change:**\n\n1.  **Efficiency (Speed Score):** The original `replicate_experts` iterates `n` times (where `n` is `num_layers * num_nodes`), and inside this loop, it performs `num_phy` heap operations. Python's `heapq` module, while efficient for individual heap operations, introduces significant overhead when called repeatedly within a Python loop, especially when `n` or `num_phy` are large. By replacing this with a vectorized approach using `torch.argmin` and direct tensor updates, we leverage PyTorch's optimized C++/CUDA backend, leading to substantial speed improvements. The complexity shifts from `O(n * num_phy * log(num_log))` (with Python overhead) to `O(num_phy * n * num_log)` (pure PyTorch operations), where the constant factors for PyTorch are much smaller, making the vectorized approach faster in practice for typical tensor sizes.\n\n2.  **Balancedness Score:** The vectorized `replicate_experts` implements the *exact same greedy algorithm* as the `heapq` version. It repeatedly finds the logical expert with the minimum average load (if it were to receive its next replica) and assigns a physical replica to it. Therefore, this change is expected to maintain or slightly improve the `balancedness_score` (if previous regressions were due to performance throttling preventing the algorithm from running optimally) while significantly boosting `speed_score`.\n\n3.  **Readability/Maintainability:** The vectorized code, while potentially more dense, removes the explicit Python loop over `n` rows, making the flow more aligned with typical PyTorch tensor programming paradigms. It also removes the dependency on the `heapq` standard library, simplifying the module's dependencies.\n\n**Detailed plan:**\n\n1.  Remove the `import heapq` statement, as it will no longer be used.\n2.  Rewrite the `replicate_experts` function to use vectorized PyTorch operations instead of the row-wise Python loop with `heapq`.\n\n```diff\n<<<<<<< SEARCH\nimport heapq\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized. Optimized using a min-heap.\n\n    Parameters:\n        weight: [X, num_log]  (e.g., [num_layers * num_nodes, num_logical_experts_per_node])\n        num_phy: total number of experts after replication (e.g., num_physical_experts_per_node)\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank (0-indexed)\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n\n    # Initialize output tensors\n    # phy2log: maps physical expert index to logical expert index\n    # rank: maps physical expert index to its replica rank (0-indexed)\n    phy2log = torch.empty((n, num_phy), dtype=torch.int64, device=device)\n    rank = torch.empty((n, num_phy), dtype=torch.int64, device=device)\n    \n    # Keep track of the current number of replicas for each logical expert\n    # This is effectively the 'logcnt' output, but used internally for selection\n    current_logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    \n    # Iterate over each of the 'n' independent scenarios/rows\n    for i in range(n):\n        # Initialize a min-heap for the current row.\n        # Heap elements are tuples: (-score_scalar, logical_expert_index, replica_rank)\n        min_heap_row = []\n        \n        # Populate the heap with the first replica's score for each logical expert in this row.\n        # The score represents the average load if this expert gets its k-th replica.\n        for j in range(num_log):\n            # Calculate score for the first replica (k=1) for the current row.\n            # Ensure division is float division and convert to scalar for heap.\n            score = weight[i, j] / 1.0 \n            heapq.heappush(min_heap_row, (-score.item(), j, 1)) # Push scalar score\n\n        # Fill the physical experts for the current row by greedily picking the best available replica.\n        for k in range(num_phy): # Iterate from 0 to num_phy-1 for physical expert index\n            if not min_heap_row: # Should not happen if num_phy >= num_log initially\n                break\n\n            # Get the logical expert with the highest score (most negative in heap) for the current row.\n            neg_score_scalar, best_log_idx, rank_val = heapq.heappop(min_heap_row)\n            \n            # Assign this physical expert to the chosen logical expert for the current row.\n            phy2log[i, k] = best_log_idx\n            # The rank is 0-indexed\n            rank[i, k] = rank_val - 1 \n\n            # Update the count of replicas for this logical expert in the current row.\n            current_logcnt[i, best_log_idx] = rank_val\n            \n            # If this logical expert can still receive more replicas,\n            # push its next replica's score to the heap for the current row.\n            # The maximum number of replicas for any logical expert is implicitly num_phy.\n            if rank_val < num_phy: # Check if there's room for another replica\n                # Ensure float division for the next score calculation and convert to scalar.\n                next_score = weight[i, best_log_idx] / (rank_val + 1.0)\n                heapq.heappush(min_heap_row, (-next_score.item(), best_log_idx, rank_val + 1))\n\n    # The `current_logcnt` tensor now holds the number of replicas for each logical expert.\n    # This is the `logcnt` output.\n    return phy2log, rank, current_logcnt\n=======\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized. Optimized using a vectorized approach.\n\n    Parameters:\n        weight: [X, num_log]  (e.g., [num_layers * num_nodes, num_logical_experts_per_node])\n        num_phy: total number of experts after replication (e.g., num_physical_experts_per_node)\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank (0-indexed)\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    # num_redundant = num_phy - num_log # No longer explicitly used in vectorized approach\n    # assert num_redundant >= 0 # Assumed num_phy >= num_log for replication to make sense\n    device = weight.device\n\n    # Initialize output tensors\n    # phy2log: maps physical expert index to logical expert index\n    # rank: maps physical expert index to its replica rank (0-indexed)\n    phy2log = torch.empty((n, num_phy), dtype=torch.int64, device=device)\n    rank = torch.empty((n, num_phy), dtype=torch.int64, device=device)\n    \n    # current_logcnt[i, j] stores the number of replicas already assigned to logical expert j in row i.\n    # This will be the final `logcnt` output.\n    current_logcnt = torch.zeros(n, num_log, dtype=torch.int64, device=device)\n    \n    # current_expert_avg_load[i, j] stores the average load if logical expert j in row i\n    # were to receive its (current_logcnt[i,j] + 1)-th replica.\n    # Initialize with the load for the first replica (weight / 1).\n    current_expert_avg_load = weight.clone() \n    \n    # Iterate num_phy times, assigning one physical expert in each iteration.\n    for k in range(num_phy): # k is the index of the physical expert being assigned (0 to num_phy-1)\n        # Find the (row, logical_expert_idx) with the minimum current average load.\n        # torch.argmin returns a flat index.\n        min_load_idx_flat = torch.argmin(current_expert_avg_load)\n        \n        # Convert flat index back to (row_idx, log_expert_idx)\n        # Use .item() to convert 0-dim tensors to scalars for indexing.\n        row_idx = min_load_idx_flat // num_log\n        log_expert_idx = min_load_idx_flat % num_log\n\n        # Assign this physical expert (k) to the chosen logical expert.\n        phy2log[row_idx.item(), k] = log_expert_idx.item()\n        rank[row_idx.item(), k] = current_logcnt[row_idx.item(), log_expert_idx.item()] \n\n        # Increment the replica count for the chosen logical expert.\n        current_logcnt[row_idx.item(), log_expert_idx.item()] += 1\n        \n        # Update the average load for this logical expert.\n        # If this expert has now received `num_phy` replicas, set its average load to infinity\n        # so it's not chosen again.\n        if current_logcnt[row_idx.item(), log_expert_idx.item()] < num_phy:\n            current_expert_avg_load[row_idx.item(), log_expert_idx.item()] = \\\n                weight[row_idx.item(), log_expert_idx.item()] / (current_logcnt[row_idx.item(), log_expert_idx.item()] + 1.0)\n        else:\n            # This expert has reached its maximum replicas, set its load to infinity.\n            current_expert_avg_load[row_idx.item(), log_expert_idx.item()] = float('inf')\n\n    return phy2log, rank, current_logcnt\n>>>>>>> REPLACE\n```"]}}}