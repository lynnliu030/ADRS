{"id": "b0db63f4-9b7a-43d9-9045-9876a0a53117", "code": "# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\nimport torch.jit as jit\n\n# Define the inner loop logic as a TorchScript function\n@jit.script\ndef _jit_assign_group_and_update(\n    group_id_tensor: torch.Tensor, # Group ID for the current item being placed (0-dim tensor)\n    pack_weights_layer_i: torch.Tensor, # 1D tensor representing weights of packs for current layer\n    pack_items_layer_i: torch.Tensor,   # 1D tensor representing item counts of packs for current layer\n    groups_per_pack: int, # Max items per pack\n    inf_val: torch.Tensor, # Pre-created infinity tensor\n    group_weight: torch.Tensor # Weight of the current group (0-dim tensor)\n) -> tuple[torch.Tensor, torch.Tensor]: # Returns chosen pack index and rank within that pack (0-dim tensors)\n    \"\"\"\n    JIT-compiled function for the core greedy assignment logic within balanced_packing.\n    This function modifies pack_weights_layer_i and pack_items_layer_i in-place.\n    \"\"\"\n    available_mask = (pack_items_layer_i < groups_per_pack)\n    candidate_weights = torch.where(available_mask, pack_weights_layer_i, inf_val)\n    \n    # Find the pack with the minimum current weight among available ones.\n    # If there are ties, prefer the pack with fewer items to encourage more even distribution.\n    min_weight_mask = (candidate_weights == torch.min(candidate_weights))\n    available_items = pack_items_layer_i[min_weight_mask]\n    \n    # Select the pack index from the masked candidates.\n    # If multiple packs have the same minimum weight, choose the one with the fewest items.\n    # If there's still a tie in item count, argmin will pick the first one.\n    chosen_pack_idx_tensor = torch.where(min_weight_mask, torch.arange(candidate_weights.numel(), device=candidate_weights.device), torch.tensor(float('inf'), device=candidate_weights.device)).min()\n    \n    # Correctly get the index within the original pack_weights_layer_i\n    pack_idx_tensor = torch.argmin(torch.where(min_weight_mask, pack_items_layer_i, torch.tensor(float('inf'), device=pack_items_layer_i.device)))\n    \n    # Get rank before update. Keep as 0-dim tensor.\n    rank_in_pack_tensor = pack_items_layer_i[pack_idx_tensor]\n\n    # Update pack weights and item counts for the chosen pack in-place\n    pack_weights_layer_i[pack_idx_tensor] += group_weight\n    pack_items_layer_i[pack_idx_tensor] += 1\n    \n    return pack_idx_tensor, rank_in_pack_tensor\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # `weight` is already float from `rebalance_experts`\n    indices = weight.sort(-1, descending=True).indices  # Keep on original device\n    pack_index = torch.full_like(weight,\n                                 fill_value=-1,\n                                 dtype=torch.int64,\n                                 device=weight.device)\n    rank_in_pack = torch.full_like(pack_index,\n                                   fill_value=-1,\n                                   device=weight.device)\n\n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These are on the same device as the input 'weight'.\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Pre-create a tensor for infinity value to avoid repeated creation in loop\n    inf_tensor = torch.tensor(float('inf'), device=weight.device, dtype=weight.dtype)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # Get 1D slices for the current layer's pack states.\n        # These slices will be passed to the JIT-compiled function and modified in-place.\n        current_pack_weights_slice = pack_weights_per_layer[i]\n        current_pack_items_slice = pack_items_per_layer[i]\n\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id_tensor in indices[i]:\n            # Call the JIT-compiled function for the assignment logic\n            # Pass group_id_tensor directly as a 0-dim tensor\n            chosen_pack_idx_tensor, chosen_rank_in_pack_tensor = _jit_assign_group_and_update(\n                group_id_tensor,\n                current_pack_weights_slice,\n                current_pack_items_slice,\n                groups_per_pack,\n                inf_tensor,\n                weight[i, group_id_tensor] # Pass the weight of the current group as a 0-dim tensor\n            )\n\n            # Assign the group to the chosen pack based on results from JIT function.\n            # `group_id_tensor` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id_tensor] = chosen_pack_idx_tensor\n            rank_in_pack[i, group_id_tensor] = chosen_rank_in_pack_tensor\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    # Pre-calculate initial loads to optimize the loop in replicate_experts.\n    # This avoids repeated division in each iteration.\n    current_loads = weight / logcnt\n\n    for i in range(num_log, num_phy):\n        # Find the logical expert with the maximum current average load.\n        # Use argmax for efficiency on pre-calculated loads.\n        redundant_indices = torch.argmax(current_loads, dim=-1)\n\n        # Assign the current physical expert to the chosen logical expert.\n        phy2log[:, i] = redundant_indices\n\n        # Record the rank of this replica within its logical expert.\n        rank[:, i] = logcnt[arangen, redundant_indices]\n\n        # Update the count of replicas for the chosen logical expert.\n        chosen_expert_idx = redundant_indices\n        logcnt[arangen, chosen_expert_idx] += 1\n\n        # Update the load for the chosen expert in the current_loads tensor.\n        # This avoids recomputing the division for all experts in the next iteration.\n        current_loads[arangen, chosen_expert_idx] = weight[arangen, chosen_expert_idx] / logcnt[arangen, chosen_expert_idx]\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float()  # Ensure float, but keep on original device (e.g., GPU)\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    # Ensure log2phy is created on the same device as other tensors\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=weight.device,  # Use weight's device\n    )\n    # The scatter_ operation and arange should also use the correct device\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=weight.device).expand(num_layers, -1),  # Use weight's device\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n", "language": "python", "parent_id": "7fa921c3-3bc5-4167-917d-cd5304e5bdaf", "generation": 8, "timestamp": 1751062993.1660755, "iteration_found": 209, "metrics": {"balancedness_score": 0.2982487625377158, "speed_score": 0.02804052274942313, "combined_score": 0.16314464264356948}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 8 lines with 22 lines\nChange 2: Replace 3 lines with 12 lines\nChange 3: Replace 3 lines with 13 lines\nChange 4: Replace 8 lines with 22 lines\nChange 5: Replace 3 lines with 13 lines", "parent_metrics": {"balancedness_score": 0.29830835153138974, "speed_score": 0.031314921723297834, "combined_score": 0.16481163662734377}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve the Mixture-of-Expert models Expert Parallelism Load Balancer (MoE EPLB) expert rearrangement algorithm.\nThis algorithm will take the load metrics recorded by the vLLM server, and rearrange the experts to balance the load. It can make replicas of some experts to achieve better load balancing.\nYour goal will be two-fold: 1. Improve the algorithm to achieve better load balancing; while 2. Improve the algorithm to be more efficient, i.e. reduce the execution time of the algorithm itself, since perfect load balancing is NP-hard.\nThe current algorithm is implemented in the `rebalance_experts` function. ", "user": "# Current Program Information\n- Current performance metrics: - balancedness_score: 0.2983\n- speed_score: 0.0313\n- combined_score: 0.1648\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: balancedness_score, speed_score, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0857, combined_score: 0.1920\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1097, combined_score: 0.2312\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1103, combined_score: 0.2315\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.2315)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1103), Performs well on combined_score (0.2315)\n\n\n### Program 2 (Score: 0.2312)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1097), Performs well on combined_score (0.2312)\n\n\n### Program 3 (Score: 0.1920)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0857), Performs well on combined_score (0.1920)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.1914)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n\n### Program D2 (Score: 0.1909)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.2315, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 32 lines with 32 lines, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 2 (Score: 0.1920, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 3 (Score: 0.1920, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 4 (Score: 0.1920, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 5 (Score: 0.0000, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 8 lines with 16 lines\nChange 2: Replace 21 lines with 26 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n\n# Current Program\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\nimport torch.jit as jit\n\n# Define the inner loop logic as a TorchScript function\n@jit.script\ndef _jit_assign_group_and_update(\n    group_id_tensor: torch.Tensor, # Group ID for the current item being placed (0-dim tensor)\n    pack_weights_layer_i: torch.Tensor, # 1D tensor representing weights of packs for current layer\n    pack_items_layer_i: torch.Tensor,   # 1D tensor representing item counts of packs for current layer\n    groups_per_pack: int, # Max items per pack\n    inf_val: torch.Tensor, # Pre-created infinity tensor\n    group_weight: torch.Tensor # Weight of the current group (0-dim tensor)\n) -> tuple[torch.Tensor, torch.Tensor]: # Returns chosen pack index and rank within that pack (0-dim tensors)\n    \"\"\"\n    JIT-compiled function for the core greedy assignment logic within balanced_packing.\n    This function modifies pack_weights_layer_i and pack_items_layer_i in-place.\n    \"\"\"\n    available_mask = (pack_items_layer_i < groups_per_pack)\n    candidate_weights = torch.where(available_mask, pack_weights_layer_i, inf_val)\n    \n    # Find the pack with the minimum current weight among available ones.\n    # Keep it as a 0-dim tensor for indexing in JIT-compiled code.\n    pack_idx_tensor = torch.argmin(candidate_weights)\n    \n    # Get rank before update. Keep as 0-dim tensor.\n    rank_in_pack_tensor = pack_items_layer_i[pack_idx_tensor]\n\n    # Update pack weights and item counts for the chosen pack in-place\n    pack_weights_layer_i[pack_idx_tensor] += group_weight\n    pack_items_layer_i[pack_idx_tensor] += 1\n    \n    return pack_idx_tensor, rank_in_pack_tensor\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # `weight` is already float from `rebalance_experts`\n    indices = weight.sort(-1, descending=True).indices  # Keep on original device\n    pack_index = torch.full_like(weight,\n                                 fill_value=-1,\n                                 dtype=torch.int64,\n                                 device=weight.device)\n    rank_in_pack = torch.full_like(pack_index,\n                                   fill_value=-1,\n                                   device=weight.device)\n\n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These are on the same device as the input 'weight'.\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Pre-create a tensor for infinity value to avoid repeated creation in loop\n    inf_tensor = torch.tensor(float('inf'), device=weight.device, dtype=weight.dtype)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # Get 1D slices for the current layer's pack states.\n        # These slices will be passed to the JIT-compiled function and modified in-place.\n        current_pack_weights_slice = pack_weights_per_layer[i]\n        current_pack_items_slice = pack_items_per_layer[i]\n\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id_tensor in indices[i]:\n            # Call the JIT-compiled function for the assignment logic\n            # Pass group_id_tensor directly as a 0-dim tensor\n            chosen_pack_idx_tensor, chosen_rank_in_pack_tensor = _jit_assign_group_and_update(\n                group_id_tensor,\n                current_pack_weights_slice,\n                current_pack_items_slice,\n                groups_per_pack,\n                inf_tensor,\n                weight[i, group_id_tensor] # Pass the weight of the current group as a 0-dim tensor\n            )\n\n            # Assign the group to the chosen pack based on results from JIT function.\n            # `group_id_tensor` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id_tensor] = chosen_pack_idx_tensor\n            rank_in_pack[i, group_id_tensor] = chosen_rank_in_pack_tensor\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        # To minimize the maximum load, we should add replicas to the\n        # logical experts with the maximum current average load.\n        # This strategy aims to reduce the load of the currently most loaded experts.\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float()  # Ensure float, but keep on original device (e.g., GPU)\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    # Ensure log2phy is created on the same device as other tensors\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=weight.device,  # Use weight's device\n    )\n    # The scatter_ operation and arange should also use the correct device\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=weight.device).expand(num_layers, -1),  # Use weight's device\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["The current program exhibits a decline in all performance metrics compared to previous versions, particularly \"Program 1\" which achieved a combined score of 0.2315. The goal is to improve both balancedness and speed.\n\nThe `rebalance_experts` function currently calls `rebalance_experts_hierarchical` if `num_groups % num_nodes == 0`, otherwise it calls it with `num_groups=1, num_nodes=1`. This suggests that the hierarchical approach is more effective. However, the global fallback might be suboptimal.\n\nLet's analyze potential areas for improvement:\n\n1.  **`balanced_packing` efficiency:** The current `balanced_packing` uses a greedy approach with a JIT-compiled inner loop. While JIT compilation helps, the overall complexity might still be a bottleneck. The sorting of weights within each layer contributes to the time complexity.\n\n2.  **`replicate_experts` strategy:** The `replicate_experts` function uses a greedy strategy of adding replicas to the logical experts with the maximum current average load. This is a reasonable heuristic, but might not be optimal.\n\n3.  **Hierarchical vs. Global:** The current logic to switch between hierarchical and global balancing is simplistic. A more nuanced approach to determine when to use which strategy, or even a hybrid approach, could be beneficial.\n\n4.  **Redundant Expert Mapping:** The creation of `log2phy` using `scatter_` might be a point of optimization.\n\nGiven the regression in metrics, it's important to consider changes that might have caused this. The prompt states \"Metrics showing regression: balancedness_score, speed_score, combined_score. Consider reverting or revising recent changes in these areas.\" Since we don't have the specific changes, we will focus on general optimization strategies that are known to improve load balancing and performance in similar contexts.\n\nHere are a few targeted improvements:\n\n### Improvement 1: Optimize `balanced_packing` by reducing redundant sorting.\n\nThe `balanced_packing` function sorts weights for each layer independently. If the relative order of weights across layers is similar, sorting each layer from scratch might be inefficient. However, without more information about the typical input `weight` tensor, it's hard to guarantee this. A more direct approach to improve `balanced_packing`'s speed while maintaining its greedy logic would be to ensure the JIT compilation is as effective as possible and that tensor operations are streamlined.\n\nThe current `balanced_packing` sorts `weight` within each layer using `indices = weight.sort(-1, descending=True).indices`. This is done for each layer `i`. The JIT function `_jit_assign_group_and_update` handles the core assignment. The issue might be in how `weight[i, group_id_tensor]` is passed to the JIT function. It's passed as a 0-dim tensor.\n\nLet's focus on making the existing `balanced_packing` as efficient as possible by ensuring the JIT function is well-utilized and that data is passed efficiently.\n\n### Improvement 2: Refine `replicate_experts` for better load balancing.\n\nThe current `replicate_experts` greedily assigns replicas to the most loaded experts. This is a common heuristic, but it can sometimes lead to suboptimal solutions. An alternative would be to use a more sophisticated assignment strategy, perhaps inspired by bin-packing algorithms or flow-based methods, but these can be computationally expensive.\n\nA simpler improvement could be to ensure the calculation of `weight / logcnt` is efficient and that the `max(dim=-1).indices` operation is optimized.\n\n### Improvement 3: Revisit the hierarchical vs. global strategy.\n\nThe current logic to switch to global balancing (`num_groups=1, num_nodes=1`) when `num_groups % num_nodes != 0` is a simplification. It might be more beneficial to always use a hierarchical approach if possible, or at least a more intelligent fallback.\n\nHowever, the prompt suggests focusing on improvements to the *current* algorithm. The current implementation of `rebalance_experts` *does* call `rebalance_experts_hierarchical` in both cases, but with different parameters for the global case. This implies the hierarchical structure is always preferred.\n\nLet's focus on optimizing the core `rebalance_experts_hierarchical` and its dependencies.\n\n### Improvement 4: Optimize `log2phy` creation.\n\nThe creation of `log2phy` involves `scatter_`. This operation can be costly. We can explore if there's a more direct way to construct this tensor, potentially by pre-allocating and filling it.\n\nConsidering the regression, the most likely culprit is that the previous top-performing programs (Program 1 and 2) might have had subtle optimizations or different algorithmic choices that were lost. The prompt also highlights \"Consider simplifying the code to improve readability and maintainability\" and \"Metrics showing regression\". This implies that recent changes might have over-complicated the logic or introduced inefficiencies.\n\nLet's try to bring back some efficiency and potentially improve the balancedness score.\n\nThe `balanced_packing` function has a key parameter `groups_per_pack`. The logic `assert num_groups % num_packs == 0` and `groups_per_pack = num_groups // num_packs` suggests that `num_packs` is intended to be the number of bins for packing. In `rebalance_experts_hierarchical`, `balanced_packing` is called with `tokens_per_group` and `num_nodes`. Here, `num_nodes` is the number of packs. So, `groups_per_pack` becomes `num_groups // num_nodes`.\n\nThe issue might be in the way `group_id_tensor` is passed to `_jit_assign_group_and_update`. It's a 0-dim tensor, and `weight[i, group_id_tensor]` is also a 0-dim tensor. This is correct for JIT.\n\nLet's focus on the `replicate_experts` function. The line `redundant_indices = (weight / logcnt).max(dim=-1).indices` is the core of its greedy strategy.\n\nLet's consider a change that might improve balancedness by modifying how experts are replicated or packed.\n\nThe current `rebalance_experts_hierarchical` function uses `balanced_packing` twice and `replicate_experts` once.\nThe first `balanced_packing` is on `tokens_per_group` with `num_nodes`. This packs groups into nodes.\nThe second `balanced_packing` is on `tokens_per_phy` with `num_gpus // num_nodes`. This packs physical experts (within a node's logical experts) onto GPUs.\n\nThe `replicate_experts` is applied to `tokens_per_mlog` which is `weight.gather(-1, mlog2log).view(-1, num_logical_experts // num_nodes)`. This is replicating the *logical experts that are already packed onto nodes* to match the required number of physical experts per node.\n\nIf the regression is due to speed, perhaps the JIT compilation is not as effective as expected, or there are too many tensor operations.\n\nLet's consider a change that could improve the speed score by simplifying the logic in `replicate_experts` and potentially the mapping.\n\nOne potential issue is that `replicate_experts` is called with `weight / logcnt`. This division can be costly. Also, the `max(dim=-1).indices` operation is repeated for each new replica.\n\nLet's try to optimize the `replicate_experts` function to reduce its computational cost. The current approach is to repeatedly find the expert with the highest load and add a replica. This can be slow if `num_phy` is much larger than `num_log`.\n\nA more efficient way to achieve a similar greedy goal might be to pre-calculate the load increases for adding a replica to each expert and then select the best ones. However, this adds complexity.\n\nLet's focus on a simpler optimization that targets the repetitive nature of the loop in `replicate_experts`.\n\nThe current `replicate_experts` function iterates `num_log` to `num_phy` times. Inside the loop, it calculates `weight / logcnt` and then finds the `max(dim=-1).indices`. This recalculation of `weight / logcnt` in each iteration is a potential performance bottleneck.\n\nWhat if we maintain the current loads and greedily pick the expert with the highest load to add a replica to?\n\nLet's try to optimize the `replicate_experts` function by avoiding repeated division and sorting. We can maintain the current loads and use a priority queue or a similar data structure. However, since we are restricted to modifying the existing code structure and using Python/PyTorch, a direct optimization of the loop is more feasible.\n\nConsider the impact of `weight.float()`. This ensures consistency but might incur a copy if the input is not already float. It's generally good practice to keep tensors on their original device.\n\nLet's revisit the `balanced_packing` function. The JIT part looks reasonable. The main cost is the `sort` operation.\n\nThe problem statement mentions \"Consider simplifying the code to improve readability and maintainability\" and \"Metrics showing regression\". This suggests that recent changes might have made the code more complex without a proportional gain in performance, or even caused a performance drop.\n\nLet's look at `replicate_experts` again. The calculation `(weight / logcnt)` is done repeatedly. If `logcnt` increases, the denominator increases, potentially reducing the load. The `max(dim=-1).indices` finds the expert with the highest *current* average load.\n\nWhat if we try to optimize the `replicate_experts` function by pre-sorting the experts based on their current load and then iteratively picking from the sorted list? This would still involve re-sorting or updating the sorted list, which can be complex.\n\nA simpler optimization for `replicate_experts` that might improve speed without drastically altering the balancing strategy is to optimize the calculation of `weight / logcnt`.\n\nLet's consider a change that could improve the *balancedness_score* by modifying the replication strategy. The current strategy is \"add to the most loaded\". What if we consider the *marginal gain* of adding a replica?\n\nHowever, without knowing the exact changes that led to the regression, it's hard to pinpoint the exact cause. The prompt also asks for *efficiency*.\n\nLet's focus on optimizing the `replicate_experts` function's performance. The repeated division `weight / logcnt` is a prime candidate.\n\nInstead of recalculating `weight / logcnt` every time, we can maintain the current loads and update them.\n\nConsider the `replicate_experts` function:\n```python\n    for i in range(num_log, num_phy):\n        # To minimize the maximum load, we should add replicas to the\n        # logical experts with the maximum current average load.\n        # This strategy aims to reduce the load of the currently most loaded experts.\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n```\nThe division `weight / logcnt` is performed inside the loop. `logcnt` is updated in each iteration.\n\nLet's try to optimize this by avoiding the repeated division. We can compute the current loads once and then update them.\n\nOne way to improve the speed of `replicate_experts` is to avoid recomputing `weight / logcnt` from scratch in each iteration. We can maintain a tensor of current loads and update it.\n\nLet's propose a change to `replicate_experts` that optimizes the load calculation.\n\nInstead of `(weight / logcnt)`, we can calculate the loads more efficiently.\n\nConsider the case where `num_phy` is significantly larger than `num_log`. The loop runs `num_phy - num_log` times. Inside the loop, `weight / logcnt` is computed, which is `O(n * num_log)`. Then `max(dim=-1)` is `O(num_log)`. Total complexity for this part is roughly `O((num_phy - num_log) * n * num_log)`.\n\nWe can maintain the current loads and update them.\n\n```python\n    # Inside replicate_experts\n    current_loads = weight / logcnt # Initial calculation\n    for i in range(num_log, num_phy):\n        # Find the expert with the maximum current load\n        redundant_indices = torch.argmax(current_loads, dim=-1)\n\n        # Update assignments\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n\n        # Update logcnt and current_loads for the chosen expert\n        chosen_expert_idx = redundant_indices\n        logcnt[arangen, chosen_expert_idx] += 1\n        # Update the load for the chosen expert. The load is weight / new_logcnt.\n        # We can directly update the current_loads tensor for the chosen experts.\n        current_loads[arangen, chosen_expert_idx] = weight[arangen, chosen_expert_idx] / logcnt[arangen, chosen_expert_idx]\n```\nThis change avoids recomputing `weight / logcnt` for all experts in each iteration. Instead, it computes it once initially and then only updates the loads for the chosen experts. This should improve the speed score.\n\nLet's check the impact on balancedness. The strategy remains the same: add replicas to the expert with the highest current average load. This should maintain or improve balancedness.\n\nThe indices `arangen` are used to update `logcnt` and `current_loads` efficiently across all layers.\n\nNow let's look at the `rebalance_experts_hierarchical` function.\nThe mapping `mlog2log = inverse(log2mlog)` is used to get the mapping from \"middle logical experts\" (those packed onto nodes) to original logical experts.\nThen `phy2mlog = ...` gets the mapping from physical experts to these \"middle logical experts\".\nThen `pphy2mlog = phy2mlog.gather(-1, pphy2phy)` gathers the \"middle logical expert\" for each physical expert on a GPU.\nThe line `(pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(0, num_logical_experts, num_logical_experts // num_nodes, device=group_pack_index.device).view(1, -1, 1)).flatten(-2)` is complex. It seems to be mapping the \"middle logical experts\" back to the original logical experts across all nodes.\n\nThe inverse operation `inverse(perm)` is called twice. Let's ensure it's efficient. The current implementation of `inverse` seems standard.\n\nGiven the regression, simplifying the code is also a goal. The current `rebalance_experts_hierarchical` is quite complex.\n\nLet's focus on the `replicate_experts` optimization first, as it's a clear candidate for speed improvement.\n\n```python\n<<<<<<< SEARCH\n    for i in range(num_log, num_phy):\n        # To minimize the maximum load, we should add replicas to the\n        # logical experts with the maximum current average load.\n        # This strategy aims to reduce the load of the currently most loaded experts.\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n=======\n    # Pre-calculate initial loads to optimize the loop in replicate_experts.\n    # This avoids repeated division in each iteration.\n    current_loads = weight / logcnt\n\n    for i in range(num_log, num_phy):\n        # Find the logical expert with the maximum current average load.\n        # Use argmax for efficiency on pre-calculated loads.\n        redundant_indices = torch.argmax(current_loads, dim=-1)\n\n        # Assign the current physical expert to the chosen logical expert.\n        phy2log[:, i] = redundant_indices\n\n        # Record the rank of this replica within its logical expert.\n        rank[:, i] = logcnt[arangen, redundant_indices]\n\n        # Update the count of replicas for the chosen logical expert.\n        chosen_expert_idx = redundant_indices\n        logcnt[arangen, chosen_expert_idx] += 1\n\n        # Update the load for the chosen expert in the current_loads tensor.\n        # This avoids recomputing the division for all experts in the next iteration.\n        current_loads[arangen, chosen_expert_idx] = weight[arangen, chosen_expert_idx] / logcnt[arangen, chosen_expert_idx]\n>>>>>>> REPLACE\n```\n\nThis change directly addresses the potential performance bottleneck in `replicate_experts` by optimizing the load calculation. It should improve the `speed_score` and ideally not degrade the `balancedness_score`.\n\nLet's consider another aspect: the `balanced_packing` function. The JIT function `_jit_assign_group_and_update` takes `group_id_tensor` as input. This is a 0-dim tensor. The `pack_weights_layer_i` and `pack_items_layer_i` are 1D tensors representing the state of packs for a specific layer.\n\nThe line `pack_index[i, group_id_tensor] = chosen_pack_idx_tensor` uses `group_id_tensor` to index into `pack_index`. This is correct because `group_id_tensor` is a 0-dim tensor representing a single group's ID.\n\nThe issue might be in the overall strategy of `balanced_packing`. The problem states \"improve the algorithm to achieve better load balancing\". The current `balanced_packing` is a greedy approach. While it's efficient, it's not guaranteed to be optimal.\n\nThe prompt also mentions \"Consider simplifying the code to improve readability and maintainability\". The `rebalance_experts_hierarchical` function is quite dense with tensor manipulations.\n\nLet's look at the `rebalance_experts` function itself. It has a simple `if/else` that calls `rebalance_experts_hierarchical`. The `else` block calls `rebalance_experts_hierarchical` with `num_groups=1, num_nodes=1`. This effectively makes it a global load balancing strategy.\n\nIf the regression is due to the global fallback being less effective, we might want to ensure that the hierarchical approach is always used, or that the global fallback is implemented more robustly. However, the prompt asks to improve the *current* algorithm.\n\nLet's consider a subtle point in `replicate_experts`. The greedy strategy of picking the highest load might be sensitive to small differences.\n\nThe prompt asks to improve *both* balancedness and speed. The `replicate_experts` optimization focuses on speed. For balancedness, we need to look at the packing and replication strategy.\n\nThe `balanced_packing` function aims to balance weights by packing items greedily. The goal is to have `groups_per_pack` items in each pack.\n\nLet's consider the possibility that the JIT compilation is not fully effective due to how tensors are passed or modified. However, the current usage seems standard for JIT.\n\nWhat if we modify the `balanced_packing` strategy slightly to improve balancedness? The current greedy strategy picks the pack with the minimum current weight among available ones.\n\nThe prompt mentions \"Alternative approach to balancedness_score, Alternative approach to speed_score\" in \"Diverse Programs\" and \"Inspiration Programs\". This suggests exploring different strategies.\n\nHowever, without specific code changes to analyze, it's hard to know what was tried.\n\nLet's stick to optimizing the existing logic. The `replicate_experts` optimization seems solid for speed. For balancedness, we might need to rethink the greedy choices.\n\nConsider the `balanced_packing` function. The `groups_per_pack` parameter is crucial.\nThe current logic in `rebalance_experts_hierarchical`:\n1. Packs groups to nodes: `balanced_packing(tokens_per_group, num_nodes)`\n2. Replicates logical experts within nodes: `replicate_experts(...)`\n3. Packs physical experts to GPUs: `balanced_packing(tokens_per_phy, num_gpus // num_nodes)`\n\nThe balancedness of the final assignment depends on all these steps.\n\nIf we want to improve balancedness, we might need to adjust the `balanced_packing` strategy. For example, instead of purely greedy, consider a look-ahead or a more global view of pack loads. However, this would increase complexity and likely decrease speed.\n\nLet's reconsider the `replicate_experts` function. The strategy is to add replicas to the logical experts with the highest current average load. This aims to reduce the maximum load.\n\nWhat if we modify the way `current_loads` is calculated or used?\n\nThe prompt mentions \"Consider reverting or revising recent changes\". This implies that the current version might have introduced a regression. If the `replicate_experts` optimization we proposed makes the code *more* complex and doesn't significantly improve balancedness, it might be counterproductive.\n\nLet's assume the regression is due to a performance bottleneck that was introduced. The repeated division in `replicate_experts` is a strong candidate.\n\nLet's also consider the `balanced_packing` function. The sorting operation `weight.sort(-1, descending=True).indices` is performed for each layer. If `num_layers` is large, this can be costly.\n\nIf we can avoid sorting within `balanced_packing`, it would be a significant speedup. However, the greedy algorithm relies on processing elements in sorted order.\n\nPerhaps the issue is with the JIT compilation itself, or how it interacts with the data.\n\nLet's focus on a change that is likely to improve speed without sacrificing balancedness significantly. The `replicate_experts` optimization is a good candidate.\n\nLet's consider an additional change that might improve balancedness, perhaps by modifying how `balanced_packing` works. However, modifying `balanced_packing` might impact its speed.\n\nThe prompt asks for *both* balancedness and speed. The `replicate_experts` optimization primarily targets speed.\n\nWhat if we simplify the logic around `num_groups % num_nodes`? The current `else` block calls `rebalance_experts_hierarchical` with `num_groups=1, num_nodes=1`. This effectively makes it a global load balancer.\n\nIf the regression is in `balancedness_score`, it might be that the global fallback is not good enough.\n\nLet's consider a change that could improve the `balancedness_score` by ensuring a more consistent application of the hierarchical strategy, or by improving the `balanced_packing` itself.\n\nThe `balanced_packing` function aims to pack `n/m` items per pack. The current greedy approach is a standard heuristic.\n\nLet's consider a change to `balanced_packing` that might improve its balancedness.\n\nThe current `balanced_packing` uses `torch.where(available_mask, pack_weights_layer_i, inf_val)` to select available packs. Then `torch.argmin` selects the pack with the minimum weight.\n\nIf we want to improve balancedness, we might consider a different selection criterion. For example, instead of picking the absolute minimum, we could pick from a small set of minimums, or consider the *variance* of pack weights. However, this adds complexity.\n\nLet's re-evaluate the `replicate_experts` optimization. It's a solid speed improvement. Now, let's think about balancedness.\n\nThe `rebalance_experts_hierarchical` function has several steps. The `tokens_per_group` calculation is `weight.unflatten(-1, (num_groups, group_size)).sum(-1)`. This is essentially summing up weights within each group.\n\nThe `log2mlog` calculation is `(((group_pack_index * groups_per_node + group_rank_in_pack) * group_size).unsqueeze(-1) + torch.arange(group_size, dtype=torch.int64, device=group_pack_index.device)).flatten(-2)`. This seems to map the packed groups (on nodes) back to the original logical experts.\n\nThe `inverse` function is used to get the reverse mapping.\n\nThe issue might be in the interaction between `balanced_packing` and `replicate_experts`.\n\nLet's consider the possibility that the original `balanced_packing` was more effective, and the JIT compilation might not be fully leveraging it.\n\nHowever, the prompt asks for *improvements*. The `replicate_experts` optimization is a good candidate for speed.\n\nFor balancedness, let's consider a change within `balanced_packing` that might improve the distribution. The current strategy is to pick the pack with the minimum current weight. What if we pick the pack that results in the minimum *variance* of pack weights after adding the item? This is more complex.\n\nA simpler approach for balancedness might be to ensure that the `balanced_packing` algorithm itself is as effective as possible. The current greedy approach is standard.\n\nLet's focus on the `replicate_experts` optimization for speed, and consider a small adjustment to `balanced_packing` for balancedness, if possible without major complexity.\n\nThe `balanced_packing` function is called with `tokens_per_group` and `num_nodes`. This packs groups into nodes. Then, `tokens_per_phy` is calculated based on `replicate_experts`, and `balanced_packing` is called again to pack physical experts onto GPUs.\n\nThe regression in `balancedness_score` suggests that the packing or replication might be less effective.\n\nLet's consider a change to `balanced_packing` that might improve balancedness. The current greedy approach is to pick the pack with the minimum current weight.\n\nPerhaps the issue lies in the `inf_val` usage.\n\nLet's propose a change to `balanced_packing` to potentially improve balancedness by making a slightly more informed choice.\n\nInstead of just picking the minimum weight pack, we could consider the pack that, after adding the item, would have the smallest *increase* in its maximum load relative to the average. This is getting complicated.\n\nLet's stick to the `replicate_experts` optimization for speed. For balancedness, let's consider a change that might improve the packing quality.\n\nThe `balanced_packing` function's core is `_jit_assign_group_and_update`.\nInside this JIT function:\n`available_mask = (pack_items_layer_i < groups_per_pack)`\n`candidate_weights = torch.where(available_mask, pack_weights_layer_i, inf_val)`\n`pack_idx_tensor = torch.argmin(candidate_weights)`\n\nThis correctly finds an available pack with the minimum current weight.\n\nWhat if we consider the *next* best available pack as well? This might help break ties or find a slightly better distribution. However, this would increase complexity.\n\nLet's focus on the `replicate_experts` optimization for speed. For balancedness, we might need to reconsider the overall strategy if this doesn't yield enough improvement.\n\nThe prompt mentions \"Consider simplifying the code to improve readability and maintainability\". The `replicate_experts` optimization as proposed does add a few lines, but it's a direct optimization of an existing loop.\n\nLet's consider one more potential improvement for `balanced_packing`. The `groups_per_pack` constraint is strict. If `groups_per_pack` is small, the packing might be suboptimal.\n\nThe current `balanced_packing` is a greedy algorithm. For better balancedness, one could consider a more global optimization, but that would likely hurt speed.\n\nLet's focus on the `replicate_experts` optimization as the primary change for speed. For balancedness, let's consider if there's a simple tweak to `balanced_packing`.\n\nThe current `balanced_packing` implementation is a standard greedy algorithm (First Fit Decreasing variant if items were sorted). The key is that it tries to fill packs with `groups_per_pack` items.\n\nIf the regression is in balancedness, it might be that the greedy choice of minimum weight pack is not always optimal.\n\nLet's look at the `rebalance_experts_hierarchical` function again.\nThe calculation `tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)` calculates the load of each physical expert. This is then used in the second `balanced_packing`.\n\nWhat if we ensure that the `balanced_packing` function is more robust to edge cases or slightly better at balancing?\n\nThe prompt mentions \"Alternative approach to balancedness_score\".\nOne simple modification to the greedy `balanced_packing` could be to introduce a small perturbation or a tie-breaking mechanism that favors packs with lower current variance, or to consider the impact on the overall distribution.\n\nHowever, without specific insights into the regression, it's hard to guess the exact cause. The `replicate_experts` optimization is a strong candidate for speed.\n\nLet's consider a change that might improve balancedness by slightly altering the greedy choice in `balanced_packing`.\n\nThe current `balanced_packing` picks the pack with the minimum weight. If there are multiple packs with the same minimum weight, `torch.argmin` picks the first one it encounters.\n\nWhat if we try to break ties by picking the pack with fewer items? This might lead to a more even distribution of items per pack.\n\n```python\n<<<<<<< SEARCH\n    # Find the pack with the minimum current weight among available ones.\n    # Keep it as a 0-dim tensor for indexing in JIT-compiled code.\n    pack_idx_tensor = torch.argmin(candidate_weights)\n=======\n    # Find the pack with the minimum current weight among available ones.\n    # If there are ties, prefer the pack with fewer items to encourage more even distribution.\n    min_weight_mask = (candidate_weights == torch.min(candidate_weights))\n    available_items = pack_items_layer_i[min_weight_mask]\n    \n    # Select the pack index from the masked candidates.\n    # If multiple packs have the same minimum weight, choose the one with the fewest items.\n    # If there's still a tie in item count, argmin will pick the first one.\n    chosen_pack_idx_tensor = torch.where(min_weight_mask, torch.arange(candidate_weights.numel(), device=candidate_weights.device), torch.tensor(float('inf'), device=candidate_weights.device)).min()\n    \n    # Correctly get the index within the original pack_weights_layer_i\n    pack_idx_tensor = torch.argmin(torch.where(min_weight_mask, pack_items_layer_i, torch.tensor(float('inf'), device=pack_items_layer_i.device)))\n>>>>>>> REPLACE\n```\nThe above change to `balanced_packing` attempts to improve balancedness by breaking ties in minimum weight by preferring packs with fewer items. This is a heuristic that aims for a more even distribution.\n\nLet's refine the tie-breaking logic for `balanced_packing`. The goal is to select the pack with the minimum weight, and among those, the one with the minimum number of items.\n\n```python\n<<<<<<< SEARCH\n    # Find the pack with the minimum current weight among available ones.\n    # Keep it as a 0-dim tensor for indexing in JIT-compiled code.\n    pack_idx_tensor = torch.argmin(candidate_weights)\n=======\n    # Find the pack with the minimum current weight among available ones.\n    # If there are ties in minimum weight, break ties by choosing the pack with fewer items.\n    min_weight = torch.min(candidate_weights)\n    # Create a mask for packs with the minimum weight.\n    min_weight_mask = (candidate_weights == min_weight)\n    \n    # Consider only the packs that have the minimum weight.\n    # Among these, find the one with the minimum number of items.\n    # Use torch.full_like with infinity to ensure packs not meeting the criteria are not chosen.\n    items_for_min_weight_packs = torch.where(min_weight_mask, pack_items_layer_i, torch.full_like(pack_items_layer_i, float('inf')))\n    \n    # Find the index of the pack with the minimum number of items among those with minimum weight.\n    pack_idx_tensor = torch.argmin(items_for_min_weight_packs)\n>>>>>>> REPLACE\n```\nThis refined tie-breaking logic in `balanced_packing` should improve the balancedness score by making a more informed greedy choice. Combined with the `replicate_experts` optimization for speed, this should address both aspects of the task.\n\nLet's finalize the proposed changes. We will provide two changes: one for `replicate_experts` focusing on speed, and one for `balanced_packing` focusing on balancedness.\n\n**Change 1 (Speed): Optimize `replicate_experts`**\nThis change avoids repeated division by maintaining `current_loads` and updating it incrementally.\n\n**Change 2 (Balancedness): Improve tie-breaking in `balanced_packing`**\nThis change makes the greedy selection in `balanced_packing` slightly smarter by preferring packs with fewer items when weights are tied, aiming for a more even distribution.\n\nThese two changes target different aspects of the performance metrics.\n\n```python\n<<<<<<< SEARCH\n    for i in range(num_log, num_phy):\n        # To minimize the maximum load, we should add replicas to the\n        # logical experts with the maximum current average load.\n        # This strategy aims to reduce the load of the currently most loaded experts.\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n=======\n    # Pre-calculate initial loads to optimize the loop in replicate_experts.\n    # This avoids repeated division in each iteration.\n    current_loads = weight / logcnt\n\n    for i in range(num_log, num_phy):\n        # Find the logical expert with the maximum current average load.\n        # Use argmax for efficiency on pre-calculated loads.\n        redundant_indices = torch.argmax(current_loads, dim=-1)\n\n        # Assign the current physical expert to the chosen logical expert.\n        phy2log[:, i] = redundant_indices\n\n        # Record the rank of this replica within its logical expert.\n        rank[:, i] = logcnt[arangen, redundant_indices]\n\n        # Update the count of replicas for the chosen logical expert.\n        chosen_expert_idx = redundant_indices\n        logcnt[arangen, chosen_expert_idx] += 1\n\n        # Update the load for the chosen expert in the current_loads tensor.\n        # This avoids recomputing the division for all experts in the next iteration.\n        current_loads[arangen, chosen_expert_idx] = weight[arangen, chosen_expert_idx] / logcnt[arangen, chosen_expert_idx]\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n    # Find the pack with the minimum current weight among available ones.\n    # Keep it as a 0-dim tensor for indexing in JIT-compiled code.\n    pack_idx_tensor = torch.argmin(candidate_weights)\n=======\n    # Find the pack with the minimum current weight among available ones.\n    # If there are ties in minimum weight, break ties by choosing the pack with fewer items.\n    min_weight = torch.min(candidate_weights)\n    # Create a mask for packs with the minimum weight.\n    min_weight_mask = (candidate_weights == min_weight)\n    \n    # Consider only the packs that have the minimum weight.\n    # Among these, find the one with the minimum number of items.\n    # Use torch.full_like with infinity to ensure packs not meeting the criteria are not chosen.\n    items_for_min_weight_packs = torch.where(min_weight_mask, pack_items_layer_i, torch.full_like(pack_items_layer_i, float('inf')))\n    \n    # Find the index of the pack with the minimum number of items among those with minimum weight.\n    pack_idx_tensor = torch.argmin(items_for_min_weight_packs)\n>>>>>>> REPLACE\n```"]}}}