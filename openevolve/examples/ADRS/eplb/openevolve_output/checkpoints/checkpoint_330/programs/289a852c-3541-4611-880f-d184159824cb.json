{"id": "289a852c-3541-4611-880f-d184159824cb", "code": "# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort groups by weight in descending order.\n    # 'weight' is already float and on CPU from rebalance_experts,\n    # so no need for .float().cpu() here.\n    indices = weight.sort(-1, descending=True).indices\n\n    # Initialize pack_index and rank_in_pack.\n    # These will be fully populated, so `empty_like` is sufficient.\n    # They will be on the same device as 'weight' (i.e., CPU).\n    pack_index = torch.empty_like(weight, dtype=torch.int64)\n    rank_in_pack = torch.empty_like(pack_index)\n    \n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These tensors are on the same device as 'weight' (which is CPU).\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id in indices[i]:\n            # Get current state for this layer's packs.\n            current_pack_weights = pack_weights_per_layer[i]\n            current_pack_items = pack_items_per_layer[i]\n\n            # Find the pack with the minimum current weight among available ones.\n            # Set weights of full packs to infinity to exclude them from argmin.\n            temp_pack_weights = current_pack_weights.clone()\n            full_mask = (current_pack_items == groups_per_pack)\n            temp_pack_weights[full_mask] = float('inf')\n            \n            # Find the index of the pack with the minimum weight.\n            # This will always find an available pack as long as there are items left to assign.\n            pack = torch.argmin(temp_pack_weights).item()\n\n            # Assert that a valid pack was found and it has space.\n            assert current_pack_items[pack] < groups_per_pack\n\n            # Assign the group to the chosen pack.\n            # `group_id` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            # `weight[i, group_id]` is the weight of the current group.\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n\n\nimport heapq\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized. Optimized using a min-heap.\n\n    Parameters:\n        weight: [X, num_log]  (e.g., [num_layers * num_nodes, num_logical_experts_per_node])\n        num_phy: total number of experts after replication (e.g., num_physical_experts_per_node)\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank (0-indexed)\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n\n    # Initialize output tensors\n    # phy2log: maps physical expert index to logical expert index\n    # rank: maps physical expert index to its replica rank (0-indexed)\n    phy2log = torch.empty((n, num_phy), dtype=torch.int64, device=device)\n    rank = torch.empty((n, num_phy), dtype=torch.int64, device=device)\n    \n    # Keep track of the current number of replicas for each logical expert\n    # This is effectively the 'logcnt' output, but used internally for selection\n    current_logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    \n    # Initialize output tensors for all N rows\n    phy2log_out = torch.empty((n, num_phy), dtype=torch.int64, device=device)\n    rank_out = torch.empty((n, num_phy), dtype=torch.int64, device=device)\n    logcnt_out = torch.ones(n, num_log, dtype=torch.int64, device=device)\n\n    # Process each row (e.g., each combination of layer and node) independently.\n    # The original implementation attempted to process all rows at once using a single heap\n    # with tensor scores, which led to the \"Boolean value of Tensor with more than one value is ambiguous\" error.\n    # This loop fixes that by handling each row's replication problem separately.\n    for i in range(n):\n        current_weight_row = weight[i] # Shape [num_log] for the current row\n        \n        # Min-heap for the current row's replication problem.\n        # Heap elements are tuples: (negative_score_scalar, logical_expert_index, replica_rank)\n        min_heap = []\n        \n        # Populate the heap with the first replica's score for each logical expert in the current row.\n        for j in range(num_log):\n            score = current_weight_row[j] / 1.0 # Ensure float division\n            # Convert 0-dim tensor to scalar with .item() before pushing to heap\n            heapq.heappush(min_heap, (-score.item(), j, 1))\n\n        # Fill the `num_phy` physical experts for this specific row by greedily\n        # picking the logical expert with the highest average load (lowest negative score).\n        for k in range(num_phy): # k is the physical expert index within this row\n            if not min_heap:\n                # This indicates an issue, e.g., num_phy < num_log, or an empty heap\n                # after processing all experts. This assert should catch it if it happens.\n                # In normal operation, this break should not be hit if num_phy >= num_log\n                # and experts are correctly added to the heap.\n                break \n\n            neg_score_item, best_log_idx, rank_val = heapq.heappop(min_heap)\n            \n            # Assign this physical expert to the chosen logical expert for the current row\n            phy2log_out[i, k] = best_log_idx\n            rank_out[i, k] = rank_val - 1 # 0-indexed rank\n\n            # Update the count of replicas for this logical expert in the current row\n            logcnt_out[i, best_log_idx] = rank_val\n            \n            # If this logical expert can still receive more replicas,\n            # calculate the score for its next potential replica and push to heap.\n            if rank_val < num_phy: \n                next_score = current_weight_row[best_log_idx] / (rank_val + 1.0)\n                heapq.heappush(min_heap, (-next_score.item(), best_log_idx, rank_val + 1))\n    \n    # Return the aggregated results for all rows\n    return phy2log_out, rank_out, logcnt_out\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float().cpu()\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n", "language": "python", "parent_id": "a8a0a538-943f-4f53-abe2-75c533612f17", "generation": 5, "timestamp": 1751053967.5214963, "iteration_found": 67, "metrics": {"balancedness_score": 0.0, "speed_score": 0.0, "combined_score": 0.0, "error": "index 8460 is out of bounds for dimension 1 with size 8448"}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 47 lines with 49 lines", "parent_metrics": {"balancedness_score": 0.0, "speed_score": 0.0, "combined_score": 0.0, "error": "Boolean value of Tensor with more than one value is ambiguous"}, "island": 3}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve the Mixture-of-Expert models Expert Parallelism Load Balancer (MoE EPLB) expert rearrangement algorithm.\nThis algorithm will take the load metrics recorded by the vLLM server, and rearrange the experts to balance the load. It can make replicas of some experts to achieve better load balancing.\nYour goal will be two-fold: 1. Improve the algorithm to achieve better load balancing; while 2. Improve the algorithm to be more efficient, i.e. reduce the execution time of the algorithm itself, since perfect load balancing is NP-hard.\nThe current algorithm is implemented in the `rebalance_experts` function. ", "user": "# Current Program Information\n- Current performance metrics: - balancedness_score: 0.0000\n- speed_score: 0.0000\n- combined_score: 0.0000\n- error: Boolean value of Tensor with more than one value is ambiguous\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: balancedness_score, speed_score, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0405, combined_score: 0.1694\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0722, combined_score: 0.1852\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0761, combined_score: 0.1872\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.1872)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0761), Performs well on combined_score (0.1872)\n\n\n### Program 2 (Score: 0.1852)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0722), Performs well on combined_score (0.1852)\n\n\n### Program 3 (Score: 0.1694)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0405), Performs well on combined_score (0.1694)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.1661)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n\n### Program D2 (Score: 0.1667)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.1872, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 2 (Score: 0.0939, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 23 lines with 236 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 3 (Score: 0.0000, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 30 lines with 27 lines\nChange 2: Replace 211 lines with 26 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 4 (Score: 0.1612, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 11 lines with 10 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 5 (Score: 0.0000, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n\n# Current Program\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort groups by weight in descending order.\n    # 'weight' is already float and on CPU from rebalance_experts,\n    # so no need for .float().cpu() here.\n    indices = weight.sort(-1, descending=True).indices\n\n    # Initialize pack_index and rank_in_pack.\n    # These will be fully populated, so `empty_like` is sufficient.\n    # They will be on the same device as 'weight' (i.e., CPU).\n    pack_index = torch.empty_like(weight, dtype=torch.int64)\n    rank_in_pack = torch.empty_like(pack_index)\n    \n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These tensors are on the same device as 'weight' (which is CPU).\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id in indices[i]:\n            # Get current state for this layer's packs.\n            current_pack_weights = pack_weights_per_layer[i]\n            current_pack_items = pack_items_per_layer[i]\n\n            # Find the pack with the minimum current weight among available ones.\n            # Set weights of full packs to infinity to exclude them from argmin.\n            temp_pack_weights = current_pack_weights.clone()\n            full_mask = (current_pack_items == groups_per_pack)\n            temp_pack_weights[full_mask] = float('inf')\n            \n            # Find the index of the pack with the minimum weight.\n            # This will always find an available pack as long as there are items left to assign.\n            pack = torch.argmin(temp_pack_weights).item()\n\n            # Assert that a valid pack was found and it has space.\n            assert current_pack_items[pack] < groups_per_pack\n\n            # Assign the group to the chosen pack.\n            # `group_id` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            # `weight[i, group_id]` is the weight of the current group.\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n\n\nimport heapq\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized. Optimized using a min-heap.\n\n    Parameters:\n        weight: [X, num_log]  (e.g., [num_layers * num_nodes, num_logical_experts_per_node])\n        num_phy: total number of experts after replication (e.g., num_physical_experts_per_node)\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank (0-indexed)\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n\n    # Initialize output tensors\n    # phy2log: maps physical expert index to logical expert index\n    # rank: maps physical expert index to its replica rank (0-indexed)\n    phy2log = torch.empty((n, num_phy), dtype=torch.int64, device=device)\n    rank = torch.empty((n, num_phy), dtype=torch.int64, device=device)\n    \n    # Keep track of the current number of replicas for each logical expert\n    # This is effectively the 'logcnt' output, but used internally for selection\n    current_logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    \n    # Use arangen for efficient indexing into current_logcnt\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n\n    # Min-heap to store the best available replica score for each logical expert.\n    # Heap elements are tuples: (-score_tensor, logical_expert_index, replica_rank)\n    # We use negative score to simulate a max-heap.\n    min_heap = []\n    \n    # Populate the heap with the first replica's score for each logical expert.\n    # The score represents the average load if this expert gets its k-th replica.\n    for j in range(num_log):\n        # Calculate score for the first replica (k=1)\n        # Ensure division is float division\n        score = weight[:, j] / 1.0 \n        heapq.heappush(min_heap, (-score, j, 1)) # (negative_score, logical_idx, rank)\n\n    # Fill the remaining physical experts by greedily picking the best available replica.\n    # We need to assign `num_phy` physical experts in total.\n    # We iterate `num_phy` times to assign all physical experts.\n    for i in range(num_phy): # Iterate from 0 to num_phy-1 for physical expert index\n        if not min_heap: # Should not happen if num_phy >= num_log\n            break\n\n        # Get the logical expert with the highest score (most negative in heap)\n        neg_score, best_log_idx, rank_val = heapq.heappop(min_heap)\n        \n        # Assign this physical expert to the chosen logical expert\n        phy2log[:, i] = best_log_idx\n        # The rank is 0-indexed\n        rank[:, i] = rank_val - 1 \n\n        # Update the count of replicas for this logical expert\n        # This `current_logcnt` will be the final `logcnt` output.\n        current_logcnt[arangen, best_log_idx] = rank_val\n        \n        # If this logical expert can still receive more replicas,\n        # push its next replica's score to the heap.\n        # The maximum number of replicas for any logical expert is implicitly num_phy.\n        # We add the next replica only if it's potentially useful.\n        if rank_val < num_phy: # Check if there's room for another replica\n            # Ensure float division for the next score calculation\n            next_score = weight[:, best_log_idx] / (rank_val + 1.0)\n            heapq.heappush(min_heap, (-next_score, best_log_idx, rank_val + 1))\n\n    # The `current_logcnt` tensor now holds the number of replicas for each logical expert.\n    # This is the `logcnt` output.\n    return phy2log, rank, current_logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float().cpu()\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["The current program is encountering a critical error: \"Boolean value of Tensor with more than one value is ambiguous\". This error typically arises when a PyTorch tensor with multiple elements is used in a context that expects a single boolean value (e.g., in Python's `if` statements or comparisons by `heapq`).\n\nUpon inspection, the `replicate_experts` function is the likely culprit. Specifically, the line `heapq.heappush(min_heap, (-score, j, 1))` attempts to push a tuple where `score` is a `torch.Tensor` (of shape `[n]`, where `n` is `num_layers * num_nodes`). The `heapq` module, being a Python standard library, performs comparisons using Python's built-in comparison operators. When it tries to compare two tuples, it compares their first elements. If these first elements are `torch.Tensor` objects with more than one value, the comparison `tensor1 < tensor2` results in a boolean tensor, which cannot be implicitly converted to a single boolean for the heap's internal sorting logic.\n\nThe conceptual issue is that `replicate_experts` is designed to solve `n` independent replication problems (one for each `layer * node` combination), but the original code attempts to solve them all simultaneously using a single heap, treating a tensor of `n` scores as a single comparable item. This is incorrect.\n\n**Proposed Improvement:**\n\n1.  **Fix the `replicate_experts` bug:** The primary improvement is to refactor `replicate_experts` to correctly handle the `n` independent replication problems. This involves iterating through each of the `n` rows of the `weight` tensor and running the min-heap logic for each row individually. This ensures that scalar scores are pushed to the heap, resolving the \"ambiguous boolean\" error. This will significantly improve the `balancedness_score` as the algorithm will now function correctly.\n\n2.  **Code clarity:** While fixing the bug, I will also make minor adjustments to variable names within the `replicate_experts` function (e.g., `phy2log_out`, `rank_out`, `logcnt_out`) to clearly distinguish the outputs from temporary internal variables. The `arangen` variable becomes redundant and will be removed.\n\n**Impact on Metrics:**\n\n*   **Balancedness Score:** Expected to improve significantly from 0.0000, as the algorithm will now execute correctly and produce meaningful expert assignments.\n*   **Speed Score:** The execution time of the `replicate_experts` function will increase. The original faulty implementation was attempting a vectorization that was not valid for `heapq`, leading to an error. The corrected version explicitly loops `n` times, where `n = num_layers * num_nodes`. Each iteration involves heap operations, leading to a complexity of `O(n * num_phy * log(num_log))` for this part. While this is computationally more intensive than the *attempted* vectorization, it is necessary for correctness. Given the initial 0.0000 scores, correctness is the priority. The overall speed might still be acceptable for typical `num_layers` and `num_nodes` values.\n*   **Combined Score:** Expected to improve due to the massive gain in `balancedness_score` outweighing a potential decrease in `speed_score`.\n\n**Detailed Diff:**\n\n<<<<<<< SEARCH\n    # Use arangen for efficient indexing into current_logcnt\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n\n    # Min-heap to store the best available replica score for each logical expert.\n    # Heap elements are tuples: (-score_tensor, logical_expert_index, replica_rank)\n    # We use negative score to simulate a max-heap.\n    min_heap = []\n    \n    # Populate the heap with the first replica's score for each logical expert.\n    # The score represents the average load if this expert gets its k-th replica.\n    for j in range(num_log):\n        # Calculate score for the first replica (k=1)\n        # Ensure division is float division\n        score = weight[:, j] / 1.0 \n        heapq.heappush(min_heap, (-score, j, 1)) # (negative_score, logical_idx, rank)\n\n    # Fill the remaining physical experts by greedily picking the best available replica.\n    # We need to assign `num_phy` physical experts in total.\n    # We iterate `num_phy` times to assign all physical experts.\n    for i in range(num_phy): # Iterate from 0 to num_phy-1 for physical expert index\n        if not min_heap: # Should not happen if num_phy >= num_log\n            break\n\n        # Get the logical expert with the highest score (most negative in heap)\n        neg_score, best_log_idx, rank_val = heapq.heappop(min_heap)\n        \n        # Assign this physical expert to the chosen logical expert\n        phy2log[:, i] = best_log_idx\n        # The rank is 0-indexed\n        rank[:, i] = rank_val - 1 \n\n        # Update the count of replicas for this logical expert\n        # This `current_logcnt` will be the final `logcnt` output.\n        current_logcnt[arangen, best_log_idx] = rank_val\n        \n        # If this logical expert can still receive more replicas,\n        # push its next replica's score to the heap.\n        # The maximum number of replicas for any logical expert is implicitly num_phy.\n        # We add the next replica only if it's potentially useful.\n        if rank_val < num_phy: # Check if there's room for another replica\n            # Ensure float division for the next score calculation\n            next_score = weight[:, best_log_idx] / (rank_val + 1.0)\n            heapq.heappush(min_heap, (-next_score, best_log_idx, rank_val + 1))\n\n    # The `current_logcnt` tensor now holds the number of replicas for each logical expert.\n    # This is the `logcnt` output.\n    return phy2log, rank, current_logcnt\n=======\n    # Initialize output tensors for all N rows\n    phy2log_out = torch.empty((n, num_phy), dtype=torch.int64, device=device)\n    rank_out = torch.empty((n, num_phy), dtype=torch.int64, device=device)\n    logcnt_out = torch.ones(n, num_log, dtype=torch.int64, device=device)\n\n    # Process each row (e.g., each combination of layer and node) independently.\n    # The original implementation attempted to process all rows at once using a single heap\n    # with tensor scores, which led to the \"Boolean value of Tensor with more than one value is ambiguous\" error.\n    # This loop fixes that by handling each row's replication problem separately.\n    for i in range(n):\n        current_weight_row = weight[i] # Shape [num_log] for the current row\n        \n        # Min-heap for the current row's replication problem.\n        # Heap elements are tuples: (negative_score_scalar, logical_expert_index, replica_rank)\n        min_heap = []\n        \n        # Populate the heap with the first replica's score for each logical expert in the current row.\n        for j in range(num_log):\n            score = current_weight_row[j] / 1.0 # Ensure float division\n            # Convert 0-dim tensor to scalar with .item() before pushing to heap\n            heapq.heappush(min_heap, (-score.item(), j, 1))\n\n        # Fill the `num_phy` physical experts for this specific row by greedily\n        # picking the logical expert with the highest average load (lowest negative score).\n        for k in range(num_phy): # k is the physical expert index within this row\n            if not min_heap:\n                # This indicates an issue, e.g., num_phy < num_log, or an empty heap\n                # after processing all experts. This assert should catch it if it happens.\n                # In normal operation, this break should not be hit if num_phy >= num_log\n                # and experts are correctly added to the heap.\n                break \n\n            neg_score_item, best_log_idx, rank_val = heapq.heappop(min_heap)\n            \n            # Assign this physical expert to the chosen logical expert for the current row\n            phy2log_out[i, k] = best_log_idx\n            rank_out[i, k] = rank_val - 1 # 0-indexed rank\n\n            # Update the count of replicas for this logical expert in the current row\n            logcnt_out[i, best_log_idx] = rank_val\n            \n            # If this logical expert can still receive more replicas,\n            # calculate the score for its next potential replica and push to heap.\n            if rank_val < num_phy: \n                next_score = current_weight_row[best_log_idx] / (rank_val + 1.0)\n                heapq.heappush(min_heap, (-next_score.item(), best_log_idx, rank_val + 1))\n    \n    # Return the aggregated results for all rows\n    return phy2log_out, rank_out, logcnt_out\n>>>>>>> REPLACE"]}}}