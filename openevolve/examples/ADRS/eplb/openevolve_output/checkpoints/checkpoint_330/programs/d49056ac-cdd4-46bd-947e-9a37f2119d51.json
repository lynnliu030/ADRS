{"id": "d49056ac-cdd4-46bd-947e-9a37f2119d51", "code": "# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    This implementation vectorizes the greedy assignment across layers for improved efficiency.\n\n    Parameters:\n        weight: [X, n], the weight of each item (X is num_layers or num_layers * num_nodes)\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        # If each group is a pack, assignment is straightforward.\n        pack_index = torch.arange(num_groups,\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(num_layers, -1)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort groups by weight in descending order for all layers.\n    # 'weight' is already float and on CPU from rebalance_experts.\n    indices = weight.sort(-1, descending=True).indices  # [num_layers, num_groups]\n\n    # Initialize output tensors.\n    pack_index = torch.empty_like(weight, dtype=torch.int64)\n    rank_in_pack = torch.empty_like(pack_index)\n    \n    # Initialize tensors to hold current pack weights and item counts for all layers.\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Precompute layer indices for vectorized operations.\n    layer_indices = torch.arange(num_layers, device=weight.device)\n\n    # Iterate over sorted group ranks (k) for all layers simultaneously.\n    # In each iteration, we assign the k-th heaviest group (for each layer)\n    # to the least loaded pack (for that layer).\n    for k in range(num_groups):\n        # group_ids_to_assign: [num_layers] - the actual group_id for each layer at rank k\n        group_ids_to_assign = indices[:, k] \n        \n        # weights_to_assign: [num_layers] - the weight of these groups\n        weights_to_assign = weight[layer_indices, group_ids_to_assign]\n\n        # Find the pack with the minimum current weight for each layer.\n        # Mask full packs by setting their weights to infinity.\n        current_pack_weights_masked = torch.where(\n            pack_items_per_layer == groups_per_pack,\n            float('inf'),  # Set full packs to infinity\n            pack_weights_per_layer\n        )\n        \n        # chosen_packs: [num_layers] - the chosen pack index for each layer\n        chosen_packs = torch.argmin(current_pack_weights_masked, dim=1)\n\n        # Assign the group to the chosen pack for each layer.\n        pack_index[layer_indices, group_ids_to_assign] = chosen_packs\n        rank_in_pack[layer_indices, group_ids_to_assign] = \\\n            pack_items_per_layer[layer_indices, chosen_packs]\n\n        # Update pack weights and item counts for the chosen packs across layers.\n        pack_weights_per_layer[layer_indices, chosen_packs] += weights_to_assign\n        pack_items_per_layer[layer_indices, chosen_packs] += 1\n    \n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\n# Helper function for global rebalancing\ndef rebalance_experts_global(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate experts to minimize maximum load across all GPUs globally.\n\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: total number of physical experts after replication\n        num_gpus: total number of GPUs\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        rank: [num_moe_layers, num_physical_experts]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_physical_experts >= num_logical_experts\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    # Step 1: Use replicate_experts to create num_physical_experts across all layers.\n    # This function balances the load distribution of logical experts into physical slots.\n    phy2log_all, rank_all, logcnt_all = replicate_experts(weight, num_physical_experts)\n    # phy2log_all: [num_layers, num_physical_experts] - logical expert ID for each physical expert.\n    # rank_all: [num_layers, num_physical_experts] - replica rank of each physical expert.\n    # logcnt_all: [num_layers, num_logical_experts] - number of replicas for each logical expert.\n\n    # Step 2: Pack these physical experts into num_gpus.\n    # We need a weight tensor for balanced_packing that represents the load of each physical expert.\n    # The load of a physical expert is related to the load of its logical expert.\n    # `replicate_experts` already balanced the load across `num_physical_experts`.\n    # The `weight` passed to `balanced_packing` should reflect the load of the logical expert\n    # that each physical expert represents.\n    physical_expert_load = torch.gather(weight, 1, phy2log_all) # [num_layers, num_physical_experts]\n\n    # Use balanced_packing to assign these physical experts to num_gpus.\n    # `balanced_packing` will pack `num_physical_experts` items (per layer) into `num_gpus` packs.\n    pack_index_gpu, rank_in_gpu_pack = balanced_packing(\n        physical_expert_load, num_gpus\n    ) # pack_index_gpu: [num_layers, num_physical_experts]\n\n    # Step 3: Reorder the physical experts based on the GPU assignment to create the final mapping.\n    # Create a combined sort key to order physical experts first by GPU, then by rank within GPU.\n    combined_sort_key = pack_index_gpu * phy_experts_per_gpu + rank_in_gpu_pack\n    sorted_indices = torch.argsort(combined_sort_key, dim=1)\n\n    # Reorder phy2log_all and rank_all using the sorted indices.\n    final_phy2log = torch.gather(phy2log_all, 1, sorted_indices)\n    final_rank = torch.gather(rank_all, 1, sorted_indices)\n\n    # The logical_count (logcnt_all) is already determined by replicate_experts and is correct.\n    return final_phy2log, final_rank, logcnt_all\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    # Ensure weight is float. Keep on original device (CPU or GPU).\n    # Moving data to CPU can be a significant bottleneck in GPU environments.\n    if weight.dtype != torch.float:\n        weight = weight.float()\n    \n    # Use hierarchical load-balance policy if num_groups > 1 and is divisible by num_nodes.\n    # Otherwise, use the dedicated global load-balance policy.\n    if num_groups > 1 and num_groups % num_nodes == 0:\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # Use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_global(\n            weight, num_replicas, num_gpus)\n            \n    num_redundant_experts = num_replicas - num_logical_experts\n    # Ensure maxlogcnt is at least 1, even if num_redundant_experts is negative (should not happen).\n    maxlogcnt = max(1, num_redundant_experts + 1)\n    \n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    \n    # The scatter operation maps physical experts to their logical expert and rank slots.\n    # The indices into the flattened log2phy are derived from phy2log and phyrank.\n    # The values being scattered are the physical expert indices (0 to num_replicas-1).\n    scatter_indices = phy2log * maxlogcnt + phyrank\n    \n    # Ensure scatter_indices are within bounds.\n    # max_idx = num_logical_experts * maxlogcnt\n    # scatter_indices = torch.remainder(scatter_indices, max_idx) # This might not be correct.\n    # The logic relies on the structure of phy2log and phyrank correctly mapping.\n    # Let's re-check the original scatter logic.\n\n    # The original scatter:\n    # log2phy.view(num_layers, -1).scatter_(-1, phy2log * maxlogcnt + phyrank, torch.arange(num_replicas, dtype=torch.int64, device=log2phy.device).expand(num_layers, -1))\n    # This scatter targets indices based on `phy2log * maxlogcnt + phyrank`.\n    # The number of elements in `phy2log` is `num_layers * num_replicas`.\n    # The number of elements in `log2phy.view(num_layers, -1)` is `num_layers * num_logical_experts * maxlogcnt`.\n    # The shape of `phy2log * maxlogcnt + phyrank` is `[num_layers, num_replicas]`.\n    # The shape of `torch.arange(num_replicas).expand(num_layers, -1)` is `[num_layers, num_replicas]`.\n\n    # The scatter operation is `scatter_(dim, index, src)`.\n    # `scatter_(dim=-1, index=[num_layers, num_replicas], src=[num_layers, num_replicas])`\n    # This means for each `l` from `0` to `num_layers-1`:\n    # For each `p` from `0` to `num_replicas-1`:\n    #   `target_flat_idx = (phy2log[l, p] * maxlogcnt + phyrank[l, p])`\n    #   `value_to_scatter = p`\n    #   `log2phy.view(num_layers, -1)[l, target_flat_idx] = value_to_scatter`\n    # This correctly places the physical expert index `p` into the slot corresponding to its logical expert `phy2log[l, p]` and rank `phyrank[l, p]`.\n\n    # The scatter logic remains the same, only the inputs `phy2log` and `phyrank` might change\n    # based on the chosen rebalancing strategy.\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n", "language": "python", "parent_id": "7c8c1c3f-2625-41bf-b61e-f519387af7a4", "generation": 8, "timestamp": 1751055808.5938632, "iteration_found": 98, "metrics": {"balancedness_score": 0.29830835153138974, "speed_score": 0.08301414105783737, "combined_score": 0.19066124629461356}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 57 lines with 150 lines", "parent_metrics": {"balancedness_score": 0.29830835153138974, "speed_score": 0.0834894710061783, "combined_score": 0.19089891126878403}, "island": 4}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve the Mixture-of-Expert models Expert Parallelism Load Balancer (MoE EPLB) expert rearrangement algorithm.\nThis algorithm will take the load metrics recorded by the vLLM server, and rearrange the experts to balance the load. It can make replicas of some experts to achieve better load balancing.\nYour goal will be two-fold: 1. Improve the algorithm to achieve better load balancing; while 2. Improve the algorithm to be more efficient, i.e. reduce the execution time of the algorithm itself, since perfect load balancing is NP-hard.\nThe current algorithm is implemented in the `rebalance_experts` function. ", "user": "# Current Program Information\n- Current performance metrics: - balancedness_score: 0.2983\n- speed_score: 0.0835\n- combined_score: 0.1909\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing improvement: speed_score, combined_score. Consider continuing with similar changes.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0761, combined_score: 0.1872\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0818, combined_score: 0.1900\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0835, combined_score: 0.1909\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.1909)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0835), Performs well on combined_score (0.1909)\n\n\n### Program 2 (Score: 0.1900)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0818), Performs well on combined_score (0.1900)\n\n\n### Program 3 (Score: 0.1872)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0761), Performs well on combined_score (0.1872)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.1852)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n\n### Program D2 (Score: 0.1694)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.0939, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 23 lines with 236 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 2 (Score: 0.1634, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 31 lines with 65 lines\nChange 2: Replace 16 lines with 19 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 3 (Score: 0.1645, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 33 lines with 31 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 4 (Score: 0.0000, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 5 lines with 20 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 5 (Score: 0.0000, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n\n# Current Program\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    This implementation vectorizes the greedy assignment across layers for improved efficiency.\n\n    Parameters:\n        weight: [X, n], the weight of each item (X is num_layers or num_layers * num_nodes)\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        # If each group is a pack, assignment is straightforward.\n        pack_index = torch.arange(num_groups,\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(num_layers, -1)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort groups by weight in descending order for all layers.\n    # 'weight' is already float and on CPU from rebalance_experts.\n    indices = weight.sort(-1, descending=True).indices  # [num_layers, num_groups]\n\n    # Initialize output tensors.\n    pack_index = torch.empty_like(weight, dtype=torch.int64)\n    rank_in_pack = torch.empty_like(pack_index)\n    \n    # Initialize tensors to hold current pack weights and item counts for all layers.\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Precompute layer indices for vectorized operations.\n    layer_indices = torch.arange(num_layers, device=weight.device)\n\n    # Iterate over sorted group ranks (k) for all layers simultaneously.\n    # In each iteration, we assign the k-th heaviest group (for each layer)\n    # to the least loaded pack (for that layer).\n    for k in range(num_groups):\n        # group_ids_to_assign: [num_layers] - the actual group_id for each layer at rank k\n        group_ids_to_assign = indices[:, k] \n        \n        # weights_to_assign: [num_layers] - the weight of these groups\n        weights_to_assign = weight[layer_indices, group_ids_to_assign]\n\n        # Find the pack with the minimum current weight for each layer.\n        # Mask full packs by setting their weights to infinity.\n        current_pack_weights_masked = torch.where(\n            pack_items_per_layer == groups_per_pack,\n            float('inf'),  # Set full packs to infinity\n            pack_weights_per_layer\n        )\n        \n        # chosen_packs: [num_layers] - the chosen pack index for each layer\n        chosen_packs = torch.argmin(current_pack_weights_masked, dim=1)\n\n        # Assign the group to the chosen pack for each layer.\n        pack_index[layer_indices, group_ids_to_assign] = chosen_packs\n        rank_in_pack[layer_indices, group_ids_to_assign] = \\\n            pack_items_per_layer[layer_indices, chosen_packs]\n\n        # Update pack weights and item counts for the chosen packs across layers.\n        pack_weights_per_layer[layer_indices, chosen_packs] += weights_to_assign\n        pack_items_per_layer[layer_indices, chosen_packs] += 1\n    \n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    # Ensure weight is float. Keep on original device (CPU or GPU).\n    # Moving data to CPU can be a significant bottleneck in GPU environments.\n    if weight.dtype != torch.float:\n        weight = weight.float()\n    \n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["The current `rebalance_experts` function relies on `rebalance_experts_hierarchical` for both hierarchical and global load balancing. The global load balancing case essentially treats the entire system as a single node, which might not be optimal.\n\nTo improve both balancedness and efficiency, we can introduce a dedicated global load balancing strategy that directly addresses the problem of distributing experts across all available GPUs without the intermediate grouping by nodes.\n\nHere's a proposed change:\n\n1.  **Introduce a dedicated `rebalance_experts_global` function:** This function will implement a more direct approach to global load balancing.\n2.  **Modify `rebalance_experts` to call `rebalance_experts_global` when `num_groups == 1`:** This ensures that the global balancing logic is used when intended.\n\n**Reasoning for Improvement:**\n\n*   **Better Balancedness:** The current hierarchical approach, when forced into a global scenario (by setting `num_nodes=1`), might not be as effective as an algorithm designed specifically for global distribution. A dedicated global balancing function can explore a wider range of expert assignments across all GPUs to minimize the maximum load.\n*   **Improved Efficiency:** By removing the intermediate node-level packing and replication steps for the global case, we can potentially reduce computation and data movement, leading to a faster execution time.\n\nLet's implement `rebalance_experts_global` which will be similar to `rebalance_experts_hierarchical` but will directly target `num_gpus` as the number of packs.\n\n```python\n# Helper function for global rebalancing\ndef rebalance_experts_global(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate experts to minimize maximum load across all GPUs globally.\n\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: total number of physical experts after replication\n        num_gpus: total number of GPUs\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        rank: [num_moe_layers, num_physical_experts]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_physical_experts >= num_logical_experts\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    # Step 1: Distribute logical experts to physical slots across all GPUs.\n    # This is a form of multi-way number partitioning or multi-processor scheduling problem.\n    # We'll use a greedy approach similar to balanced_packing but directly for GPUs.\n\n    # Flatten the weight to treat all experts across all layers as items to be packed.\n    # However, we need to maintain layer information. A better approach is to pack per layer.\n    # Let's adapt the balanced_packing logic for the global case.\n\n    # We want to pack `num_logical_experts` into `num_gpus` bins (GPUs).\n    # Each GPU should ideally have `phy_experts_per_gpu` physical experts.\n    # The `balanced_packing` function is designed to pack `num_groups` into `num_packs`.\n    # Here, we are packing `num_logical_experts` into `num_gpus`.\n\n    # The `balanced_packing` function assumes that the number of items to pack\n    # is a multiple of the number of packs. Here, `num_logical_experts` might not\n    # be a multiple of `num_gpus`.\n\n    # A direct application of balanced_packing would be to pack `num_logical_experts`\n    # into `num_gpus` groups. Each group would then be assigned `phy_experts_per_gpu`\n    # physical experts.\n\n    # Let's refine the `replicate_experts` logic for a global context.\n    # `replicate_experts` takes `weight` [n, num_log] and replicates `num_phy` total.\n    # Here, `n` is `num_layers`, `num_log` is `num_logical_experts`, and `num_phy` is `num_physical_experts`.\n\n    # The current `replicate_experts` function is designed to distribute replicas\n    # of logical experts to achieve balanced load. It iteratively assigns the\n    # next replica of the currently least-replicated expert to the least loaded\n    # physical slot.\n\n    # Let's adapt `replicate_experts` to work directly with `num_gpus` as the target.\n    # The `replicate_experts` function implicitly handles the \"packing\" into a fixed number of slots.\n    # The `num_phy` argument in `replicate_experts` is the total number of physical experts.\n    # We need to map these `num_physical_experts` to `num_gpus`.\n\n    # The `replicate_experts` function already produces `phy2log`, `rank`, and `logcnt`.\n    # The key is how `replicate_experts` is called.\n    # In `rebalance_experts_hierarchical`, `replicate_experts` is called with:\n    # `weight.gather(-1, mlog2log).view(-1, num_logical_experts // num_nodes)`\n    # and `num_physical_experts // num_nodes`.\n\n    # For global rebalancing, we want to distribute `num_physical_experts` across `num_gpus`.\n    # This means each GPU will have `num_physical_experts // num_gpus` physical experts.\n\n    # The `replicate_experts` function itself can be viewed as solving the problem\n    # of assigning `num_physical_experts` slots to `num_logical_experts` logical experts\n    # to minimize the maximum load on any logical expert (by distributing replicas).\n    # The `num_phy` argument in `replicate_experts` is the *total* number of physical slots.\n\n    # The current implementation of `replicate_experts` assigns the `num_phy` physical experts.\n    # The question is how these `num_phy` physical experts are then mapped to `num_gpus`.\n    # In `rebalance_experts_hierarchical`, this mapping is done by `balanced_packing(tokens_per_phy, num_gpus // num_nodes)`.\n\n    # To do global balancing, we need to assign `num_physical_experts` physical experts to `num_gpus`.\n    # This means each GPU will host `num_physical_experts / num_gpus` physical experts.\n    # The `replicate_experts` function creates `num_physical_experts` physical experts.\n    # We need to group these `num_physical_experts` into `num_gpus` groups.\n\n    # Let's rethink the structure. The `replicate_experts` function generates `phy2log`, `rank`, `logcnt`\n    # for a given `num_phy`. The problem is how to assign these `num_phy` physical experts to `num_gpus`.\n\n    # The `balanced_packing` function is key here. It packs `num_groups` items into `num_packs`.\n    # In the hierarchical case:\n    # 1. Pack `num_groups` (groups of logical experts) into `num_nodes` (nodes).\n    # 2. Replicate logical experts within each node group to `num_physical_experts // num_nodes` per node.\n    # 3. Pack these node-local physical experts into `num_gpus // num_nodes` (GPUs per node).\n\n    # For global balancing, we want to pack `num_logical_experts` into `num_gpus` directly.\n    # The total number of physical experts is `num_physical_experts`.\n    # The number of physical experts per GPU is `phy_experts_per_gpu = num_physical_experts // num_gpus`.\n\n    # Let's try to use `replicate_experts` to create `num_physical_experts` physical experts and then\n    # pack these into `num_gpus`.\n\n    # Step 1: Create all physical experts using `replicate_experts`.\n    # The `weight` here should represent the load of assigning *one* physical expert.\n    # `replicate_experts` takes `weight` [n, num_log] and `num_phy`.\n    # The `weight` passed to `replicate_experts` in the hierarchical case is `tokens_per_mlog`.\n    # This `tokens_per_mlog` is the load of the logical experts *within a node group*.\n    # So, `replicate_experts` is distributing the replication load for logical experts *within a node group*.\n\n    # For global balancing, we need to consider the load of all logical experts.\n    # The `weight` parameter in `rebalance_experts` is `[num_layers, num_logical_experts]`.\n    # This `weight` represents the load of each logical expert.\n\n    # Let's directly use `replicate_experts` on the original `weight` to create `num_physical_experts`\n    # physical experts and then pack these into `num_gpus`.\n\n    # `replicate_experts` takes `weight: torch.Tensor` [n, num_log] and `num_phy`.\n    # Here, n = num_layers, num_log = num_logical_experts, num_phy = num_physical_experts.\n    phy2log_all, rank_all, logcnt_all = replicate_experts(weight, num_physical_experts)\n    # `phy2log_all`: [num_layers, num_physical_experts] - logical expert ID for each physical expert.\n    # `rank_all`: [num_layers, num_physical_experts] - replica rank of each physical expert.\n    # `logcnt_all`: [num_layers, num_logical_experts] - number of replicas for each logical expert.\n\n    # Now we have `num_physical_experts` physical experts, distributed across `num_layers`.\n    # We need to assign these `num_physical_experts` to `num_gpus`.\n    # Each GPU should have `phy_experts_per_gpu` physical experts.\n\n    # We can use `balanced_packing` to assign these `num_physical_experts` to `num_gpus`.\n    # The \"items\" to pack are the `num_physical_experts` (across all layers).\n    # The \"packs\" are the `num_gpus`.\n\n    # The `balanced_packing` function expects `weight` [X, n] and `num_packs`.\n    # Here, X is `num_layers`, and `n` is `num_physical_experts`.\n    # `num_packs` is `num_gpus`.\n    # The `weight` for `balanced_packing` should represent the load of each physical expert.\n    # The load of a physical expert is related to the load of its logical expert,\n    # adjusted by its replica rank.\n\n    # The `replicate_experts` function already calculates the load distribution in `rank_all`.\n    # A simpler way to think about it: we have `num_layers * num_physical_experts`\n    # physical expert slots. We need to assign these to `num_gpus`.\n    # The `balanced_packing` function can be used to assign `num_physical_experts`\n    # \"slots\" (for each layer) to `num_gpus`.\n\n    # Let's consider the load distribution for each physical expert slot.\n    # The `replicate_experts` function's `rank_all` indicates how many replicas of a logical expert\n    # are assigned to a physical expert.\n    # The `phy2log_all` gives the logical expert ID.\n\n    # We need a \"weight\" tensor for `balanced_packing` that represents the load\n    # of each physical expert *slot*.\n    # The `tokens_per_phy` in `rebalance_experts_hierarchical` is calculated as\n    # `(tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)`.\n    # This represents the load assigned to a physical expert within a node group.\n\n    # For global balancing, we can use the original `weight` and distribute it.\n    # The `balanced_packing` function is designed to pack `n` items into `m` packs where\n    # `n` is divisible by `m`. Here, `num_physical_experts` is divisible by `num_gpus`.\n    # So, we can use `balanced_packing` to assign `num_physical_experts` logical experts\n    # (considering their weights) to `num_gpus`.\n\n    # The `weight` tensor for `balanced_packing` should be `[num_layers, num_physical_experts]`.\n    # What should be the values in this tensor?\n    # The `replicate_experts` function implicitly distributes the `weight` to create `num_phy` physical experts.\n    # The `weight` argument to `replicate_experts` is `[n, num_log]`.\n    # The output `phy2log` and `rank` are based on this input weight.\n\n    # Let's try to use `balanced_packing` on the original `weight` to assign `num_logical_experts`\n    # to `num_gpus`. This is not quite right, as we need to assign `num_physical_experts`.\n\n    # The core problem is assigning `num_physical_experts` total physical experts to `num_gpus`.\n    # Each GPU gets `num_physical_experts // num_gpus` physical experts.\n\n    # The `replicate_experts` function creates `num_phy` physical experts.\n    # The `weight` passed to `replicate_experts` is `[n, num_log]`.\n    # The `num_phy` is the total number of physical experts.\n    # The `replicate_experts` function itself performs a greedy assignment to minimize load variance.\n\n    # The `rebalance_experts_hierarchical` function's logic for packing physical experts to GPUs:\n    # `tokens_per_phy`: [num_layers * num_nodes, num_logical_experts // num_nodes]\n    # `pack_index, rank_in_pack = balanced_packing(tokens_per_phy, num_gpus // num_nodes)`\n    # This assigns `num_logical_experts // num_nodes` \"logical expert groups per node\"\n    # to `num_gpus // num_nodes` \"GPUs per node\".\n\n    # For global balancing, we want to assign `num_physical_experts` to `num_gpus`.\n    # The `balanced_packing` function can be used directly to assign `num_physical_experts`\n    # (for each layer) to `num_gpus`.\n    # The weight tensor for `balanced_packing` should be `[num_layers, num_physical_experts]`.\n\n    # Let's reconstruct the `weight` tensor for `balanced_packing` that represents the load\n    # of each physical expert slot.\n    # The `replicate_experts` function's output `phy2log` and `rank` imply a distribution.\n    # The load of a physical expert can be thought of as the load of its logical expert\n    # divided by the number of replicas of that logical expert.\n    # `load_per_physical_expert = weight / logcnt` where `logcnt` is the number of replicas\n    # for the corresponding logical expert.\n\n    # The `replicate_experts` function is designed to distribute `num_phy` slots.\n    # The `weight` passed to it is `[n, num_log]`.\n    # The `num_phy` is the total number of physical experts.\n    # The `replicate_experts` function itself aims to balance the load across `num_phy` slots.\n    # So, `phy2log` and `rank` already represent a balanced distribution of `num_phy` slots.\n\n    # The problem then is to assign these `num_phy` slots to `num_gpus`.\n    # We can use `balanced_packing` for this.\n    # `balanced_packing(weight_for_packing, num_packs)`\n    # Here, `weight_for_packing` should be `[num_layers, num_physical_experts]`\n    # and `num_packs` should be `num_gpus`.\n\n    # What is the `weight_for_packing`?\n    # It should represent the load contribution of each of the `num_physical_experts`\n    # to the `num_gpus`.\n\n    # Consider the output of `replicate_experts`.\n    # `phy2log_all`: [num_layers, num_physical_experts]\n    # `rank_all`: [num_layers, num_physical_experts]\n\n    # We need to pack `num_physical_experts` items (per layer) into `num_gpus` bins.\n    # The `balanced_packing` function takes `weight: [X, n]` and `num_packs`.\n    # Here `X = num_layers`, `n = num_physical_experts`, `num_packs = num_gpus`.\n    # The `weight` for this call should be derived from the original `weight` and `rank_all`.\n\n    # The load of a physical expert is roughly `weight[layer, logical_expert] / num_replicas_for_logical_expert`.\n    # The `rank_all` tells us the replica count for each physical expert's logical counterpart.\n    # So, a proxy for the load of a physical expert `(l, p)` is:\n    # `weight[l, phy2log_all[l, p]] / logcnt_all[l, phy2log_all[l, p]]`\n    # This seems overly complex.\n\n    # Let's leverage the `replicate_experts` directly. It already balances the load\n    # across `num_phy` physical experts.\n    # The `num_phy` physical experts are effectively distributed.\n    # We just need to group these `num_phy` experts into `num_gpus` buckets.\n\n    # The `balanced_packing` function is designed to assign `n` items to `m` packs.\n    # Let's use it to assign `num_physical_experts` (for each layer) to `num_gpus`.\n    # The input weight to `balanced_packing` should reflect the load of each physical expert.\n    # The `replicate_experts` function implicitly creates this load distribution.\n\n    # Let's consider the `replicate_experts` output `rank_all` and `phy2log_all`.\n    # The `rank_all` indicates how \"early\" a physical expert is in its logical expert's replica sequence.\n    # A lower rank means it's an earlier replica, which might be assigned more load in some schemes.\n    # The `weight` tensor is the load of the logical expert.\n\n    # Let's create a weight tensor for `balanced_packing` that represents the \"base load\"\n    # that needs to be distributed.\n    # The `replicate_experts` function already ensures that the total load is distributed\n    # as evenly as possible across `num_physical_experts` slots.\n    # The `rank_all` tells us which replica each physical expert is.\n\n    # We want to pack `num_physical_experts` (per layer) into `num_gpus`.\n    # The `balanced_packing` function takes `weight` [X, n] and `num_packs`.\n    # Here, X = num_layers, n = num_physical_experts, num_packs = num_gpus.\n    # The `weight` parameter for `balanced_packing` should represent the load\n    # of each of the `num_physical_experts`.\n\n    # The `replicate_experts` function distributes the load of `num_logical_experts`\n    # into `num_phy` physical experts. The `rank_all` indicates how the load is distributed.\n    # A physical expert `p` for layer `l` is assigned a rank `rank_all[l, p]`.\n    # The logical expert it maps to is `phy2log_all[l, p]`.\n    # The load of the logical expert is `weight[l, phy2log_all[l, p]]`.\n\n    # Let's define a \"physical expert load\" tensor:\n    # `physical_expert_load[l, p] = weight[l, phy2log_all[l, p]]`\n    # This is the load of the logical expert that physical expert `p` in layer `l` represents.\n    # The `replicate_experts` function already balanced this load across `num_physical_experts`.\n\n    physical_expert_load = torch.gather(\n        weight, 1, phy2log_all\n    ) # [num_layers, num_physical_experts]\n\n    # Now, we use `balanced_packing` to pack these `num_physical_experts` (per layer)\n    # into `num_gpus` (packs).\n    pack_index_gpu, rank_in_gpu_pack = balanced_packing(\n        physical_expert_load, num_gpus\n    ) # pack_index_gpu: [num_layers, num_physical_experts]\n\n    # `pack_index_gpu` tells us which GPU each physical expert is assigned to.\n    # `rank_in_gpu_pack` tells us the order of physical experts within a GPU.\n\n    # We need to map the original `num_physical_experts` to the new physical indices\n    # that account for GPU assignment.\n    # The total number of physical experts is `num_physical_experts`.\n    # Each GPU gets `phy_experts_per_gpu`.\n\n    # The final `phy2log` should be `[num_layers, num_physical_experts]`.\n    # The original `phy2log_all` already provides the logical expert mapping.\n    # We need to reorder `phy2log_all` based on the `pack_index_gpu` and `rank_in_gpu_pack`.\n\n    # Let's create a permutation that orders physical experts by GPU and then by rank within GPU.\n    # `gpu_assignment_order`: [num_layers, num_physical_experts]\n    # For each layer `l`, we want to arrange `num_physical_experts` indices such that\n    # experts assigned to GPU 0 come first (in their `rank_in_gpu_pack` order),\n    # then experts for GPU 1, and so on.\n\n    # We can use `torch.argsort` on `pack_index_gpu` and `rank_in_gpu_pack` to get this ordering.\n    # For each layer `l`:\n    # `indices_for_layer_l = torch.arange(num_physical_experts, device=weight.device)`\n    # `combined_sort_key_l = pack_index_gpu[l] * phy_experts_per_gpu + rank_in_gpu_pack[l]`\n    # `sorted_indices_l = torch.argsort(combined_sort_key_l)`\n\n    # Or more efficiently, across all layers:\n    # `combined_sort_key = pack_index_gpu * phy_experts_per_gpu + rank_in_gpu_pack`\n    # `sorted_indices = torch.argsort(combined_sort_key, dim=1)`\n\n    combined_sort_key = pack_index_gpu * phy_experts_per_gpu + rank_in_gpu_pack\n    sorted_indices = torch.argsort(combined_sort_key, dim=1)\n\n    # Now, reorder `phy2log_all` and `rank_all` using `sorted_indices`.\n    final_phy2log = torch.gather(phy2log_all, 1, sorted_indices)\n    final_rank = torch.gather(rank_all, 1, sorted_indices)\n\n    # The `logcnt` remains the same as `logcnt_all` because `replicate_experts`\n    # already determined the optimal number of replicas for each logical expert\n    # to satisfy the `num_physical_experts` constraint.\n\n    # We need to ensure the `logcnt` is correctly mapped if `replicate_experts`\n    # was called with a different `num_log`. Here, `num_log` is `num_logical_experts`.\n    # The `logcnt_all` is `[num_layers, num_logical_experts]`. This is correct.\n\n    # The output `logcnt` should be `[num_layers, num_logical_experts]`.\n    # `logcnt_all` is already in this shape.\n\n    return final_phy2log, final_rank, logcnt_all\n\n\n# Modify the rebalance_experts function to include the new global rebalancing logic.\ndef rebalance_experts_modified(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    # Ensure weight is float. Keep on original device (CPU or GPU).\n    # Moving data to CPU can be a significant bottleneck in GPU environments.\n    if weight.dtype != torch.float:\n        weight = weight.float()\n    \n    if num_groups > 1 and num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        # Note: The original code uses `rebalance_experts_hierarchical` for both cases.\n        # We will keep the hierarchical call for the hierarchical case.\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        # This is where we introduce the new global rebalancing function.\n        # The condition `num_groups % num_nodes == 0` implies that if `num_groups` is not\n        # divisible by `num_nodes`, it defaults to the \"global\" case.\n        # Also, if `num_groups == 1`, it's a global case.\n        # So, the `else` block correctly captures the global scenario.\n        phy2log, phyrank, logcnt = rebalance_experts_global(\n            weight, num_replicas, num_gpus)\n            \n    num_redundant_experts = num_replicas - num_logical_experts\n    # Ensure maxlogcnt is at least 1, even if num_redundant_experts is negative (should not happen).\n    maxlogcnt = max(1, num_redundant_experts + 1)\n    \n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    \n    # The scatter operation needs to be careful about the number of replicas.\n    # The `phy2log` and `phyrank` are indexed by `num_replicas`.\n    # `logcnt` has shape `[num_layers, num_logical_experts]`.\n    # `phy2log` has shape `[num_layers, num_replicas]`.\n    # `phyrank` has shape `[num_layers, num_replicas]`.\n\n    # The scatter logic:\n    # For each logical expert `le` in layer `l`, it has `logcnt[l, le]` physical replicas.\n    # These `logcnt[l, le]` replicas are assigned ranks `0, 1, ..., logcnt[l, le] - 1`.\n    # The `phy2log` and `phyrank` tell us which logical expert and rank each physical expert is.\n    # We want to fill `log2phy[l, le, rank]` with the physical expert index.\n\n    # The current scatter logic:\n    # `log2phy.view(num_layers, -1).scatter_(-1, phy2log * maxlogcnt + phyrank, torch.arange(num_replicas, dtype=torch.int64, device=log2phy.device).expand(num_layers, -1))`\n    # This scatter is filling a flattened `log2phy` array.\n    # The `phy2log * maxlogcnt + phyrank` creates a unique index for each physical expert slot.\n    # `phy2log` is `[num_layers, num_replicas]`.\n    # `phyrank` is `[num_layers, num_replicas]`.\n    # `maxlogcnt` is a scalar.\n    # `phy2log * maxlogcnt + phyrank` gives `[num_layers, num_replicas]`.\n    # This is used as the index into the flattened `log2phy`.\n    # The `torch.arange(num_replicas)` is the value being scattered.\n\n    # This scatter logic maps a physical expert index `p` (from `0` to `num_replicas - 1`)\n    # to its corresponding slot in `log2phy`.\n    # The target index in `log2phy` is `phy2log[l, p] * maxlogcnt + phyrank[l, p]`.\n    # The value being placed is `p`.\n    # This assumes that `log2phy` is structured such that the first `logcnt[l, le]` slots\n    # for each `le` are the correct ones.\n\n    # Let's trace the shapes and indices carefully.\n    # `log2phy` shape: `[num_layers, num_logical_experts, maxlogcnt]`\n    # `phy2log` shape: `[num_layers, num_replicas]`\n    # `phyrank` shape: `[num_layers, num_replicas]`\n    # `logcnt` shape: `[num_layers, num_logical_experts]`\n\n    # We are trying to fill `log2phy[l, logical_expert, rank]` with the physical expert index.\n    # For each physical expert `p` (from `0` to `num_replicas - 1`), we know:\n    # - Its layer `l` (implicitly, if we iterate or use broadcasting).\n    # - Its logical expert `le = phy2log[l, p]`.\n    # - Its rank `r = phyrank[l, p]`.\n    # We want to set `log2phy[l, le, r] = p`.\n\n    # The current scatter uses `phy2log * maxlogcnt + phyrank`.\n    # Let's consider `num_layers = 1`, `num_logical_experts = 2`, `num_replicas = 4`.\n    # `maxlogcnt = 3` (if `num_redundant = 2`).\n    # `log2phy` shape: `[1, 2, 3]`.\n    # `phy2log`: `[1, 4]`. e.g., `[[0, 1, 0, 0]]`\n    # `phyrank`: `[1, 4]`. e.g., `[[0, 0, 1, 2]]`\n    # `logcnt`: `[1, 2]`. e.g., `[[3, 1]]`\n\n    # `phy2log * maxlogcnt + phyrank`:\n    # `[[0, 1, 0, 0]] * 3 + [[0, 0, 1, 2]] = [[0, 3, 1, 2]]`\n\n    # Scatter target indices (flattened `log2phy`):\n    # `log2phy.view(1, -1)` has shape `[1, 6]`. Indices are `0, 1, 2, 3, 4, 5`.\n    # The indices `0, 3, 1, 2` are used.\n    # `scatter_(-1, [[0, 3, 1, 2]], torch.arange(4).expand(1, -1))`\n    # `scatter_(-1, [[0, 3, 1, 2]], [[0, 1, 2, 3]])`\n    # This means:\n    # `log2phy.view(1, -1)[0, 0] = 0` (for physical expert 0, logical 0, rank 0)\n    # `log2phy.view(1, -1)[0, 3] = 1` (for physical expert 1, logical 1, rank 0)\n    # `log2phy.view(1, -1)[0, 1] = 2` (for physical expert 2, logical 0, rank 1)\n    # `log2phy.view(1, -1)[0, 2] = 3` (for physical expert 3, logical 0, rank 2)\n\n    # Resulting `log2phy.view(1, -1)`: `[0, 2, 3, 1]`\n    # Reshaped `log2phy`: `[[[0, 2, 3]]]`\n    # This correctly fills `log2phy[0, 0, :]` with `[0, 2, 3]` and `log2phy[0, 1, :]` with `[-1, 1, -1]`.\n    # The scatter logic seems correct for mapping physical experts to their logical expert and rank slots.\n\n    # The main change is replacing the call to `rebalance_experts_hierarchical` with\n    # `rebalance_experts_global` when `num_groups <= 1` or `num_groups % num_nodes != 0`.\n    # The original code had:\n    # `if num_groups % num_nodes == 0:`\n    #   `phy2log, phyrank, logcnt = rebalance_experts_hierarchical(...)`\n    # `else:`\n    #   `phy2log, phyrank, logcnt = rebalance_experts_hierarchical(weight, num_replicas, 1, 1, num_gpus)`\n\n    # My proposed change is:\n    # `if num_groups > 1 and num_groups % num_nodes == 0:`\n    #   `phy2log, phyrank, logcnt = rebalance_experts_hierarchical(...)`\n    # `else:`\n    #   `phy2log, phyrank, logcnt = rebalance_experts_global(...)`\n\n    # This correctly separates the hierarchical and global strategies.\n\n    return phy2log, log2phy, logcnt\n\n\n# The actual replacement will be in the rebalance_experts function.\n# We need to define rebalance_experts_global and then modify rebalance_experts.\n\n# The provided code block is the `rebalance_experts` function.\n# We will replace the entire `rebalance_experts` function with the modified version\n# that includes the call to `rebalance_experts_global`.\n# We also need to define `rebalance_experts_global` and potentially reuse `replicate_experts` and `balanced_packing`.\n\n# The current `rebalance_experts` function calls `rebalance_experts_hierarchical`\n# for both hierarchical and the \"global\" case (when `num_groups % num_nodes != 0`).\n# My proposed change is to use a dedicated `rebalance_experts_global` for the global case.\n\n# Let's put the `rebalance_experts_global` definition before `rebalance_experts`.\n# The `rebalance_experts` function itself will be modified to call the new global function.\n\n# The `rebalance_experts_hierarchical` function is already present.\n# We will keep it and modify `rebalance_experts` to call either it or `rebalance_experts_global`.\n\n# The following diff will replace the original `rebalance_experts` with the modified one.\n# It assumes `rebalance_experts_global` is defined elsewhere or will be added.\n# For the purpose of this diff, I will include the `rebalance_experts_global` definition\n# and then show the diff for `rebalance_experts`.\n\n# --- Start of new code to be inserted ---\n# Helper function for global rebalancing (as derived above)\ndef rebalance_experts_global(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate experts to minimize maximum load across all GPUs globally.\n\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: total number of physical experts after replication\n        num_gpus: total number of GPUs\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        rank: [num_moe_layers, num_physical_experts]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_physical_experts >= num_logical_experts\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    # Step 1: Use replicate_experts to create num_physical_experts across all layers.\n    # This function balances the load distribution of logical experts into physical slots.\n    phy2log_all, rank_all, logcnt_all = replicate_experts(weight, num_physical_experts)\n    # phy2log_all: [num_layers, num_physical_experts] - logical expert ID for each physical expert.\n    # rank_all: [num_layers, num_physical_experts] - replica rank of each physical expert.\n    # logcnt_all: [num_layers, num_logical_experts] - number of replicas for each logical expert.\n\n    # Step 2: Pack these physical experts into num_gpus.\n    # We need a weight tensor for balanced_packing that represents the load of each physical expert.\n    # The load of a physical expert is related to the load of its logical expert.\n    # `replicate_experts` already balanced the load across `num_physical_experts`.\n    # The `weight` passed to `balanced_packing` should reflect the load of the logical expert\n    # that each physical expert represents.\n    physical_expert_load = torch.gather(weight, 1, phy2log_all) # [num_layers, num_physical_experts]\n\n    # Use balanced_packing to assign these physical experts to num_gpus.\n    # `balanced_packing` will pack `num_physical_experts` items (per layer) into `num_gpus` packs.\n    pack_index_gpu, rank_in_gpu_pack = balanced_packing(\n        physical_expert_load, num_gpus\n    ) # pack_index_gpu: [num_layers, num_physical_experts]\n\n    # Step 3: Reorder the physical experts based on the GPU assignment to create the final mapping.\n    # Create a combined sort key to order physical experts first by GPU, then by rank within GPU.\n    combined_sort_key = pack_index_gpu * phy_experts_per_gpu + rank_in_gpu_pack\n    sorted_indices = torch.argsort(combined_sort_key, dim=1)\n\n    # Reorder phy2log_all and rank_all using the sorted indices.\n    final_phy2log = torch.gather(phy2log_all, 1, sorted_indices)\n    final_rank = torch.gather(rank_all, 1, sorted_indices)\n\n    # The logical_count (logcnt_all) is already determined by replicate_experts and is correct.\n    return final_phy2log, final_rank, logcnt_all\n# --- End of new code to be inserted ---\n\n# Now, the diff for `rebalance_experts` itself.\n\n```\n\n<<<<<<< SEARCH\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    # Ensure weight is float. Keep on original device (CPU or GPU).\n    # Moving data to CPU can be a significant bottleneck in GPU environments.\n    if weight.dtype != torch.float:\n        weight = weight.float()\n    \n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n=======\n# Helper function for global rebalancing\ndef rebalance_experts_global(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate experts to minimize maximum load across all GPUs globally.\n\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: total number of physical experts after replication\n        num_gpus: total number of GPUs\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        rank: [num_moe_layers, num_physical_experts]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_physical_experts >= num_logical_experts\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    # Step 1: Use replicate_experts to create num_physical_experts across all layers.\n    # This function balances the load distribution of logical experts into physical slots.\n    phy2log_all, rank_all, logcnt_all = replicate_experts(weight, num_physical_experts)\n    # phy2log_all: [num_layers, num_physical_experts] - logical expert ID for each physical expert.\n    # rank_all: [num_layers, num_physical_experts] - replica rank of each physical expert.\n    # logcnt_all: [num_layers, num_logical_experts] - number of replicas for each logical expert.\n\n    # Step 2: Pack these physical experts into num_gpus.\n    # We need a weight tensor for balanced_packing that represents the load of each physical expert.\n    # The load of a physical expert is related to the load of its logical expert.\n    # `replicate_experts` already balanced the load across `num_physical_experts`.\n    # The `weight` passed to `balanced_packing` should reflect the load of the logical expert\n    # that each physical expert represents.\n    physical_expert_load = torch.gather(weight, 1, phy2log_all) # [num_layers, num_physical_experts]\n\n    # Use balanced_packing to assign these physical experts to num_gpus.\n    # `balanced_packing` will pack `num_physical_experts` items (per layer) into `num_gpus` packs.\n    pack_index_gpu, rank_in_gpu_pack = balanced_packing(\n        physical_expert_load, num_gpus\n    ) # pack_index_gpu: [num_layers, num_physical_experts]\n\n    # Step 3: Reorder the physical experts based on the GPU assignment to create the final mapping.\n    # Create a combined sort key to order physical experts first by GPU, then by rank within GPU.\n    combined_sort_key = pack_index_gpu * phy_experts_per_gpu + rank_in_gpu_pack\n    sorted_indices = torch.argsort(combined_sort_key, dim=1)\n\n    # Reorder phy2log_all and rank_all using the sorted indices.\n    final_phy2log = torch.gather(phy2log_all, 1, sorted_indices)\n    final_rank = torch.gather(rank_all, 1, sorted_indices)\n\n    # The logical_count (logcnt_all) is already determined by replicate_experts and is correct.\n    return final_phy2log, final_rank, logcnt_all\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    # Ensure weight is float. Keep on original device (CPU or GPU).\n    # Moving data to CPU can be a significant bottleneck in GPU environments.\n    if weight.dtype != torch.float:\n        weight = weight.float()\n    \n    # Use hierarchical load-balance policy if num_groups > 1 and is divisible by num_nodes.\n    # Otherwise, use the dedicated global load-balance policy.\n    if num_groups > 1 and num_groups % num_nodes == 0:\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # Use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_global(\n            weight, num_replicas, num_gpus)\n            \n    num_redundant_experts = num_replicas - num_logical_experts\n    # Ensure maxlogcnt is at least 1, even if num_redundant_experts is negative (should not happen).\n    maxlogcnt = max(1, num_redundant_experts + 1)\n    \n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    \n    # The scatter operation maps physical experts to their logical expert and rank slots.\n    # The indices into the flattened log2phy are derived from phy2log and phyrank.\n    # The values being scattered are the physical expert indices (0 to num_replicas-1).\n    scatter_indices = phy2log * maxlogcnt + phyrank\n    \n    # Ensure scatter_indices are within bounds.\n    # max_idx = num_logical_experts * maxlogcnt\n    # scatter_indices = torch.remainder(scatter_indices, max_idx) # This might not be correct.\n    # The logic relies on the structure of phy2log and phyrank correctly mapping.\n    # Let's re-check the original scatter logic.\n\n    # The original scatter:\n    # log2phy.view(num_layers, -1).scatter_(-1, phy2log * maxlogcnt + phyrank, torch.arange(num_replicas, dtype=torch.int64, device=log2phy.device).expand(num_layers, -1))\n    # This scatter targets indices based on `phy2log * maxlogcnt + phyrank`.\n    # The number of elements in `phy2log` is `num_layers * num_replicas`.\n    # The number of elements in `log2phy.view(num_layers, -1)` is `num_layers * num_logical_experts * maxlogcnt`.\n    # The shape of `phy2log * maxlogcnt + phyrank` is `[num_layers, num_replicas]`.\n    # The shape of `torch.arange(num_replicas).expand(num_layers, -1)` is `[num_layers, num_replicas]`.\n\n    # The scatter operation is `scatter_(dim, index, src)`.\n    # `scatter_(dim=-1, index=[num_layers, num_replicas], src=[num_layers, num_replicas])`\n    # This means for each `l` from `0` to `num_layers-1`:\n    # For each `p` from `0` to `num_replicas-1`:\n    #   `target_flat_idx = (phy2log[l, p] * maxlogcnt + phyrank[l, p])`\n    #   `value_to_scatter = p`\n    #   `log2phy.view(num_layers, -1)[l, target_flat_idx] = value_to_scatter`\n    # This correctly places the physical expert index `p` into the slot corresponding to its logical expert `phy2log[l, p]` and rank `phyrank[l, p]`.\n\n    # The scatter logic remains the same, only the inputs `phy2log` and `phyrank` might change\n    # based on the chosen rebalancing strategy.\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n>>>>>>> REPLACE"]}}}