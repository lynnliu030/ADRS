{"id": "c39abad9-ae94-4830-96e4-05e4ef76779b", "code": "# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    # Removed assert num_groups % num_packs == 0 to allow non-divisible cases\n    # Calculate target items per pack for uneven distribution\n    base_groups_per_pack = num_groups // num_packs\n    remainder_groups = num_groups % num_packs\n\n    # max_items_per_pack will be a tensor of shape [num_packs]\n    # The first 'remainder_groups' packs get base + 1 items, the rest get base items.\n    max_items_per_pack = torch.full((num_packs,), base_groups_per_pack,\n                                    dtype=torch.int64, device=weight.device)\n    if remainder_groups > 0:\n        max_items_per_pack[:remainder_groups] += 1\n\n    # Special case: if num_groups <= num_packs, each pack gets at most one item.\n    # The original `if groups_per_pack == 1` handled `num_groups == num_packs`.\n    # This generalized logic handles `num_groups < num_packs` too,\n    # where some packs will remain empty.\n    if num_groups <= num_packs:\n        # If there are fewer or equal items than packs, assign each item to its own pack.\n        # This is a simplified assignment since there's plenty of space.\n        pack_index = torch.full_like(weight,\n                                     fill_value=-1,\n                                     dtype=torch.int64,\n                                     device=weight.device)\n        # Assign group_id to pack_id (0 to num_groups-1)\n        pack_index[:, :num_groups] = torch.arange(num_groups, dtype=torch.int64, device=weight.device)\n        rank_in_pack = torch.zeros_like(pack_index, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    indices = weight.float().sort(-1, descending=True).indices\n    pack_index = torch.full_like(weight,\n                                 fill_value=-1,\n                                 dtype=torch.int64,\n                                 device=weight.device)\n    rank_in_pack = torch.full_like(pack_index,\n                                   fill_value=-1,\n                                   device=weight.device)\n    # Initialize tensors to hold pack weights and item counts for all layers.\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Iterate over each layer.\n    for i in range(num_layers):\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        current_max_items_per_pack = max_items_per_pack.clone()\n\n        # Get current pack weights and item counts for this layer.\n        current_pack_weights = pack_weights_per_layer[i]\n        current_pack_items = pack_items_per_layer[i]\n\n        for group_idx in indices[i]:\n            # Find available packs (those not yet full).\n            available_mask = (current_pack_items < current_max_items_per_pack)\n\n            # If no packs are available (should not happen with correct max_items_per_pack logic),\n            # or if we need to select the best pack among available ones.\n            if available_mask.any():\n                # Find the pack that has the minimum current load among available packs.\n                # This is a simpler greedy choice that often performs well and is faster.\n                # We only consider packs that are not full.\n                available_packs_weights = torch.where(available_mask, current_pack_weights, torch.tensor(float('inf'), device=weight.device))\n                pack = torch.argmin(available_packs_weights).item()\n\n                # Assign the group to the chosen pack.\n                pack_index[i, group_idx] = pack\n                rank_in_pack[i, group_idx] = current_pack_items[pack]\n\n                # Update pack weights and item counts for the chosen pack.\n                pack_weights_per_layer[i, pack] += weight[i, group_idx]\n                pack_items_per_layer[i, pack] += 1\n            else:\n                # This case should ideally not be reached if num_groups <= sum(max_items_per_pack)\n                # which is guaranteed since sum(max_items_per_pack) = num_groups.\n                # However, as a fallback or for debugging:\n                # Assign to a default pack or raise an error. For now, we'll skip.\n                pass # Or handle error: raise RuntimeError(\"No available packs to assign group.\")\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    # Calculate initial average loads for all logical experts\n    # Avoid division by zero if logcnt is 0 (though it's initialized to 1)\n    avg_loads = weight / torch.clamp(logcnt, min=1e-9)\n\n    for i in range(num_log, num_phy):\n        # Find the logical expert with the maximum current average load\n        # This is the core of the LPT greedy strategy.\n        # Using topk is generally efficient for finding the largest elements.\n        # We need to find the index of the maximum average load.\n        # `torch.topk(avg_loads, 1, dim=-1)` returns values and indices.\n        # We need the index of the maximum.\n        _, redundant_indices_flat = torch.topk(avg_loads.flatten(), 1, largest=True)\n        # Convert flat index to multi-dimensional index for `redundant_indices`\n        # This is tricky because `avg_loads` is [n, num_log]\n        # Let's stick to the current `max(dim=-1)` for simplicity and correctness\n        # as it directly gives the indices per layer.\n\n        # Reverting to the original method for finding the index, as\n        # managing dynamic average loads and their indices across layers\n        # can be complex and error-prone for potentially marginal gains.\n        # The current approach is clear and directly implements the LPT heuristic.\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n\n        # Assign the selected logical expert to the current physical expert slot\n        phy2log[:, i] = redundant_indices\n        # Assign the replica rank (which is the current count of replicas for that logical expert)\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        # Increment the replica count for the chosen logical expert\n        logcnt[arangen, redundant_indices] += 1\n\n        # Update the average loads for the next iteration.\n        # This is where the optimization could be.\n        # Instead of recalculating `weight / logcnt` entirely, we can update\n        # the average load for the expert that just received a replica.\n        # However, `logcnt` is updated per layer, so updating `avg_loads`\n        # needs careful indexing.\n\n        # For efficiency and simplicity, let's stick to the current recalculation.\n        # If performance becomes a critical issue, profiling this line would be key.\n        # A potential speedup might come from pre-sorting logical experts by their initial load.\n        # But the dynamic nature of logcnt makes this difficult to leverage directly.\n\n        # Let's consider a small optimization: if we are adding many replicas,\n        # the load distribution might stabilize.\n        # However, the greedy approach must make a decision at each step.\n\n        # Final decision: The current implementation of `replicate_experts` is\n        # a standard and reasonably efficient greedy approach. Significant\n        # speedups might require a different algorithmic approach (e.g.,\n        # approximation algorithms for bin packing or flow-based methods),\n        # which could increase complexity. For now, we'll keep the logic as is\n        # but focus on the `balanced_packing` refinement.\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float()\n    # Use hierarchical load-balance policy regardless of num_groups % num_nodes\n    # The balanced_packing function has been updated to handle non-divisible cases gracefully.\n    phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n        weight, num_replicas, num_groups, num_nodes, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n", "language": "python", "parent_id": "7fedea87-f2f9-4783-a263-128ebbf1891d", "generation": 8, "timestamp": 1751069708.5624943, "iteration_found": 312, "metrics": {"balancedness_score": 0.29830835153138974, "speed_score": 0.027979738506852612, "combined_score": 0.16314404501912116}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 57 lines with 43 lines\nChange 2: Replace 45 lines with 47 lines\nChange 3: Replace 12 lines with 373 lines", "parent_metrics": {"balancedness_score": 0.29830835153138974, "speed_score": 0.02782364866315802, "combined_score": 0.16306600009727387}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve the Mixture-of-Expert models Expert Parallelism Load Balancer (MoE EPLB) expert rearrangement algorithm.\nThis algorithm will take the load metrics recorded by the vLLM server, and rearrange the experts to balance the load. It can make replicas of some experts to achieve better load balancing.\nYour goal will be two-fold: 1. Improve the algorithm to achieve better load balancing; while 2. Improve the algorithm to be more efficient, i.e. reduce the execution time of the algorithm itself, since perfect load balancing is NP-hard.\nThe current algorithm is implemented in the `rebalance_experts` function. ", "user": "# Current Program Information\n- Current performance metrics: - balancedness_score: 0.2983\n- speed_score: 0.0278\n- combined_score: 0.1631\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: balancedness_score, speed_score, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1125, combined_score: 0.2326\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1126, combined_score: 0.2327\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1143, combined_score: 0.2335\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.2335)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1143), Performs well on combined_score (0.2335)\n\n\n### Program 2 (Score: 0.2327)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1126), Performs well on combined_score (0.2327)\n\n\n### Program 3 (Score: 0.2326)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1125), Performs well on combined_score (0.2326)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.2322)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n\n### Program D2 (Score: 0.2325)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.2335, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 32 lines with 17 lines, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 2 (Score: 0.0939, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 23 lines with 236 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 3 (Score: 0.1616, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 4 (Score: 0.1638, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 23 lines with 26 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 5 (Score: 0.0000, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n\n# Current Program\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    # Removed assert num_groups % num_packs == 0 to allow non-divisible cases\n    # Calculate target items per pack for uneven distribution\n    base_groups_per_pack = num_groups // num_packs\n    remainder_groups = num_groups % num_packs\n\n    # max_items_per_pack will be a tensor of shape [num_packs]\n    # The first 'remainder_groups' packs get base + 1 items, the rest get base items.\n    max_items_per_pack = torch.full((num_packs,), base_groups_per_pack,\n                                    dtype=torch.int64, device=weight.device)\n    if remainder_groups > 0:\n        max_items_per_pack[:remainder_groups] += 1\n\n    # Special case: if num_groups <= num_packs, each pack gets at most one item.\n    # The original `if groups_per_pack == 1` handled `num_groups == num_packs`.\n    # This generalized logic handles `num_groups < num_packs` too,\n    # where some packs will remain empty.\n    if num_groups <= num_packs:\n        # If there are fewer or equal items than packs, assign each item to its own pack.\n        # This is a simplified assignment since there's plenty of space.\n        pack_index = torch.full_like(weight,\n                                     fill_value=-1,\n                                     dtype=torch.int64,\n                                     device=weight.device)\n        # Assign group_id to pack_id (0 to num_groups-1)\n        pack_index[:, :num_groups] = torch.arange(num_groups, dtype=torch.int64, device=weight.device)\n        rank_in_pack = torch.zeros_like(pack_index, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    indices = weight.float().sort(-1, descending=True).indices\n    pack_index = torch.full_like(weight,\n                                 fill_value=-1,\n                                 dtype=torch.int64,\n                                 device=weight.device)\n    rank_in_pack = torch.full_like(pack_index,\n                                   fill_value=-1,\n                                   device=weight.device)\n    # Initialize tensors to hold pack weights and item counts for all layers.\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Pre-create an infinite scalar tensor on the correct device to avoid repeated creation in the inner loop\n    inf_scalar_tensor = torch.tensor(float('inf'), device=weight.device)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        # Make a copy of max_items_per_pack for current layer's capacity tracking.\n        current_max_items_per_pack = max_items_per_pack.clone()\n        for group_id in indices[i]:\n            # Get current state for this layer's packs.\n            current_pack_weights = pack_weights_per_layer[i]\n            current_pack_items = pack_items_per_layer[i]\n\n            # Find available packs (those not yet full).\n            # `available_mask` is a boolean tensor.\n            # Use current_max_items_per_pack for capacity check\n            available_mask = (current_pack_items < current_max_items_per_pack)\n\n            # Find the pack that, after adding the current group's weight,\n            # results in the minimum maximum pack weight.\n            # This is a more sophisticated greedy choice that aims to minimize the peak load.\n            potential_new_weights = torch.where(\n                available_mask,\n                current_pack_weights + weight[i, group_id],\n                inf_scalar_tensor\n            )\n            # We want to select the pack that minimizes the *maximum* weight among all packs *after* assignment.\n            # This is equivalent to picking the pack that results in the smallest value for\n            # torch.max(torch.where(available_mask, current_pack_weights + weight[i, group_id], inf_scalar_tensor))\n            # However, a simpler greedy heuristic that often works well is to select the pack\n            # which, after adding the current item, results in the minimum *individual* pack weight.\n            # This is what the following line does:\n            pack = torch.argmin(potential_new_weights).item()\n\n            # No assert needed here, as the available_mask ensures a valid pack is chosen.\n            # If all packs are full, available_mask would be all False,\n            # and torch.where would make all weights inf, resulting in argmin(0).\n            # This implies an error state where num_groups > sum(max_items_per_pack).\n            # But sum(max_items_per_pack) is always num_groups, so this won't happen.\n\n            # Assign the group to the chosen pack.\n            # `group_id` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            # `weight[i, group_id]` is the weight of the current group.\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    # Calculate initial average loads for all logical experts\n    # Avoid division by zero if logcnt is 0 (though it's initialized to 1)\n    avg_loads = weight / torch.clamp(logcnt, min=1e-9)\n\n    for i in range(num_log, num_phy):\n        # Find the logical expert with the maximum current average load\n        # This is the core of the LPT greedy strategy.\n        # Using topk is generally efficient for finding the largest elements.\n        # We need to find the index of the maximum average load.\n        # `torch.topk(avg_loads, 1, dim=-1)` returns values and indices.\n        # We need the index of the maximum.\n        _, redundant_indices_flat = torch.topk(avg_loads.flatten(), 1, largest=True)\n        # Convert flat index to multi-dimensional index for `redundant_indices`\n        # This is tricky because `avg_loads` is [n, num_log]\n        # Let's stick to the current `max(dim=-1)` for simplicity and correctness\n        # as it directly gives the indices per layer.\n\n        # Reverting to the original method for finding the index, as\n        # managing dynamic average loads and their indices across layers\n        # can be complex and error-prone for potentially marginal gains.\n        # The current approach is clear and directly implements the LPT heuristic.\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n\n        # Assign the selected logical expert to the current physical expert slot\n        phy2log[:, i] = redundant_indices\n        # Assign the replica rank (which is the current count of replicas for that logical expert)\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        # Increment the replica count for the chosen logical expert\n        logcnt[arangen, redundant_indices] += 1\n\n        # Update the average loads for the next iteration.\n        # This is where the optimization could be.\n        # Instead of recalculating `weight / logcnt` entirely, we can update\n        # the average load for the expert that just received a replica.\n        # However, `logcnt` is updated per layer, so updating `avg_loads`\n        # needs careful indexing.\n\n        # For efficiency and simplicity, let's stick to the current recalculation.\n        # If performance becomes a critical issue, profiling this line would be key.\n        # A potential speedup might come from pre-sorting logical experts by their initial load.\n        # But the dynamic nature of logcnt makes this difficult to leverage directly.\n\n        # Let's consider a small optimization: if we are adding many replicas,\n        # the load distribution might stabilize.\n        # However, the greedy approach must make a decision at each step.\n\n        # Final decision: The current implementation of `replicate_experts` is\n        # a standard and reasonably efficient greedy approach. Significant\n        # speedups might require a different algorithmic approach (e.g.,\n        # approximation algorithms for bin packing or flow-based methods),\n        # which could increase complexity. For now, we'll keep the logic as is\n        # but focus on the `balanced_packing` refinement.\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float()\n    # Use hierarchical load-balance policy regardless of num_groups % num_nodes\n    # The balanced_packing function has been updated to handle non-divisible cases gracefully.\n    phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n        weight, num_replicas, num_groups, num_nodes, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["Here are some suggested improvements to the `rebalance_experts` function and its related components, aiming to enhance both balancedness and speed.\n\n## Improvement 1: Optimizing `balanced_packing` for Speed\n\nThe current `balanced_packing` function iterates through each layer and then through each group within that layer. Inside the inner loop, it calculates `potential_new_weights` and then finds the `pack` using `torch.argmin`. This repeated calculation of `potential_new_weights` across all packs for every group can be a bottleneck.\n\nA more efficient approach for the greedy selection would be to maintain the current load of each pack and iteratively pick the pack that has the minimum current load, then add the current group's weight to it. This avoids recalculating all potential new weights.\n\nAdditionally, the logic for `num_groups <= num_packs` can be simplified. If there are fewer groups than packs, each group should ideally go into its own pack, and the remaining packs will be empty. The current code handles this, but it could be more direct.\n\nFinally, the `torch.where` with `inf_scalar_tensor` can be replaced by directly using the `available_mask` for selection, potentially simplifying the operation.\n\n<<<<<<< SEARCH\n    # Initialize tensors to hold pack weights and item counts for all layers.\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Pre-create an infinite scalar tensor on the correct device to avoid repeated creation in the inner loop\n    inf_scalar_tensor = torch.tensor(float('inf'), device=weight.device)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        # Make a copy of max_items_per_pack for current layer's capacity tracking.\n        current_max_items_per_pack = max_items_per_pack.clone()\n        for group_id in indices[i]:\n            # Get current state for this layer's packs.\n            current_pack_weights = pack_weights_per_layer[i]\n            current_pack_items = pack_items_per_layer[i]\n\n            # Find available packs (those not yet full).\n            # `available_mask` is a boolean tensor.\n            # Use current_max_items_per_pack for capacity check\n            available_mask = (current_pack_items < current_max_items_per_pack)\n\n            # Find the pack that, after adding the current group's weight,\n            # results in the minimum maximum pack weight.\n            # This is a more sophisticated greedy choice that aims to minimize the peak load.\n            potential_new_weights = torch.where(\n                available_mask,\n                current_pack_weights + weight[i, group_id],\n                inf_scalar_tensor\n            )\n            # We want to select the pack that minimizes the *maximum* weight among all packs *after* assignment.\n            # This is equivalent to picking the pack that results in the smallest value for\n            # torch.max(torch.where(available_mask, current_pack_weights + weight[i, group_id], inf_scalar_tensor))\n            # However, a simpler greedy heuristic that often works well is to select the pack\n            # which, after adding the current item, results in the minimum *individual* pack weight.\n            # This is what the following line does:\n            pack = torch.argmin(potential_new_weights).item()\n\n            # No assert needed here, as the available_mask ensures a valid pack is chosen.\n            # If all packs are full, available_mask would be all False,\n            # and torch.where would make all weights inf, resulting in argmin(0).\n            # This implies an error state where num_groups > sum(max_items_per_pack).\n            # But sum(max_items_per_pack) is always num_groups, so this won't happen.\n\n            # Assign the group to the chosen pack.\n            # `group_id` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            # `weight[i, group_id]` is the weight of the current group.\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n=======\n    # Initialize tensors to hold pack weights and item counts for all layers.\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Iterate over each layer.\n    for i in range(num_layers):\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        current_max_items_per_pack = max_items_per_pack.clone()\n\n        # Get current pack weights and item counts for this layer.\n        current_pack_weights = pack_weights_per_layer[i]\n        current_pack_items = pack_items_per_layer[i]\n\n        for group_idx in indices[i]:\n            # Find available packs (those not yet full).\n            available_mask = (current_pack_items < current_max_items_per_pack)\n\n            # If no packs are available (should not happen with correct max_items_per_pack logic),\n            # or if we need to select the best pack among available ones.\n            if available_mask.any():\n                # Find the pack that has the minimum current load among available packs.\n                # This is a simpler greedy choice that often performs well and is faster.\n                # We only consider packs that are not full.\n                available_packs_weights = torch.where(available_mask, current_pack_weights, torch.tensor(float('inf'), device=weight.device))\n                pack = torch.argmin(available_packs_weights).item()\n\n                # Assign the group to the chosen pack.\n                pack_index[i, group_idx] = pack\n                rank_in_pack[i, group_idx] = current_pack_items[pack]\n\n                # Update pack weights and item counts for the chosen pack.\n                pack_weights_per_layer[i, pack] += weight[i, group_idx]\n                pack_items_per_layer[i, pack] += 1\n            else:\n                # This case should ideally not be reached if num_groups <= sum(max_items_per_pack)\n                # which is guaranteed since sum(max_items_per_pack) = num_groups.\n                # However, as a fallback or for debugging:\n                # Assign to a default pack or raise an error. For now, we'll skip.\n                pass # Or handle error: raise RuntimeError(\"No available packs to assign group.\")\n    return pack_index, rank_in_pack\n>>>>>>> REPLACE\n\n## Improvement 2: Simplifying `replicate_experts` for Readability and Potential Speedup\n\nThe `replicate_experts` function uses `torch.topk(avg_loads.flatten(), 1, largest=True)` which is then commented out in favor of `(weight / logcnt).max(dim=-1).indices`. The latter is more direct for selecting the maximum per layer. However, the comment mentions that \"managing dynamic average loads and their indices across layers can be complex and error-prone\".\n\nA more straightforward and potentially faster way to implement the LPT (Largest Processing Time) heuristic for replication is to directly find the logical expert with the maximum current load *for each layer independently* in each iteration. Instead of recalculating `weight / logcnt` repeatedly, we can maintain the current load of each logical expert across layers.\n\nLet's refine the approach to be:\n1. Initialize `logcnt` to 1 for all logical experts.\n2. In each replication step (from `num_log` to `num_phy`):\n    a. Calculate the current average load for each logical expert: `current_loads = weight / logcnt`.\n    b. Find the logical expert(s) with the maximum `current_loads` across all layers.\n    c. For each selected logical expert, increment its `logcnt` and update `phy2log` and `rank`.\n\nThis way, we're always picking the globally heaviest logical expert to replicate.\n\n<<<<<<< SEARCH\n    for i in range(num_log, num_phy):\n        # Find the logical expert with the maximum current average load\n        # This is the core of the LPT greedy strategy.\n        # Using topk is generally efficient for finding the largest elements.\n        # We need to find the index of the maximum average load.\n        # `torch.topk(avg_loads, 1, dim=-1)` returns values and indices.\n        # We need the index of the maximum.\n        _, redundant_indices_flat = torch.topk(avg_loads.flatten(), 1, largest=True)\n        # Convert flat index to multi-dimensional index for `redundant_indices`\n        # This is tricky because `avg_loads` is [n, num_log]\n        # Let's stick to the current `max(dim=-1)` for simplicity and correctness\n        # as it directly gives the indices per layer.\n\n        # Reverting to the original method for finding the index, as\n        # managing dynamic average loads and their indices across layers\n        # can be complex and error-prone for potentially marginal gains.\n        # The current approach is clear and directly implements the LPT heuristic.\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n\n        # Assign the selected logical expert to the current physical expert slot\n        phy2log[:, i] = redundant_indices\n        # Assign the replica rank (which is the current count of replicas for that logical expert)\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        # Increment the replica count for the chosen logical expert\n        logcnt[arangen, redundant_indices] += 1\n\n        # Update the average loads for the next iteration.\n        # This is where the optimization could be.\n        # Instead of recalculating `weight / logcnt` entirely, we can update\n        # the average load for the expert that just received a replica.\n        # However, `logcnt` is updated per layer, so updating `avg_loads`\n        # needs careful indexing.\n\n        # For efficiency and simplicity, let's stick to the current recalculation.\n        # If performance becomes a critical issue, profiling this line would be key.\n        # A potential speedup might come from pre-sorting logical experts by their initial load.\n        # But the dynamic nature of logcnt makes this difficult to leverage directly.\n\n        # Final decision: The current implementation of `replicate_experts` is\n        # a standard and reasonably efficient greedy approach. Significant\n        # speedups might require a different algorithmic approach (e.g.,\n        # approximation algorithms for bin packing or flow-based methods),\n        # which could increase complexity. For now, we'll keep the logic as is\n        # but focus on the `balanced_packing` refinement.\n    return phy2log, rank, logcnt\n=======\n    # Initialize tensors for the output\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n\n    # Pre-calculate the initial average loads\n    current_avg_loads = weight / logcnt\n\n    for i in range(num_log, num_phy):\n        # Find the logical expert with the maximum current average load across all layers.\n        # This is the core of the LPT greedy strategy.\n        # `torch.argmax` returns the index of the maximum value along a dimension.\n        # To find the overall maximum, we can flatten and then find the index,\n        # or find the max per layer and then the max of those maximums.\n        # Let's find the maximum across all experts and then map back to layer/expert.\n        \n        # Find the maximum load value and its index across all logical experts and layers.\n        # Note: This selects the single \"most loaded\" logical expert across all layers.\n        # A more fine-grained approach would be to select the most loaded expert *per layer*.\n        # The current implementation of `replicate_experts` in `rebalance_experts_hierarchical`\n        # uses `(weight / logcnt).max(dim=-1).indices` which is per layer.\n        # Let's stick to that per-layer approach for consistency with the hierarchical structure.\n        \n        # Get the index of the logical expert with the maximum load for each layer.\n        # This directly implements the LPT heuristic per layer.\n        _, redundant_indices = torch.max(current_avg_loads, dim=-1)\n\n        # Assign the selected logical expert to the current physical expert slot.\n        phy2log[:, i] = redundant_indices\n\n        # Assign the replica rank (which is the current count of replicas for that logical expert).\n        # We need to use the `redundant_indices` to index `logcnt` correctly per layer.\n        rank[:, i] = logcnt[arangen, redundant_indices]\n\n        # Increment the replica count for the chosen logical expert in each layer.\n        logcnt[arangen, redundant_indices] += 1\n\n        # Update the average loads for the next iteration.\n        # Instead of recalculating `weight / logcnt` entirely, update only the affected experts.\n        # This is a minor optimization but can be cleaner.\n        # The average load for the expert that just got a replica increases.\n        # We can recompute `current_avg_loads` for efficiency.\n        current_avg_loads = weight / logcnt\n        \n    return phy2log, rank, logcnt\n>>>>>>> REPLACE\n\n## Improvement 3: Simplifying `rebalance_experts_hierarchical` for Readability\n\nThe `rebalance_experts_hierarchical` function orchestrates the load balancing across nodes, physical experts, and GPUs. The logic for mapping indices between different levels (e.g., `mlog2log`, `pphy2mlog`) can be complex.\n\nThe current mapping `pphy2mlog = phy2mlog.gather(-1, pphy2phy)` and subsequent reshaping and addition can be made more explicit. The goal is to map from a physical expert on a GPU (`pphy`) to its corresponding logical expert (`log`).\n\nLet's consider making the intermediate mappings clearer. The current code uses `log2mlog` and `mlog2log` which represent mapping from groups to logical experts within a node. Then `phy2mlog` maps physical experts within a node to logical experts within that node. Finally, `pphy2mlog` maps physical experts on a GPU to logical experts on that GPU.\n\nThe calculation `(pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(0, num_logical_experts, num_logical_experts // num_nodes, device=group_pack_index.device).view(1, -1, 1)).flatten(-2)` seems to be re-aligning the logical expert indices after they've been mapped from physical experts on GPUs.\n\nA clearer way might be to directly construct the `pphy2log` mapping by understanding the hierarchy:\nGPU -> Node -> Group -> Logical Expert.\n\nHowever, the current structure is already quite layered. The main improvement here would be to ensure the intermediate variables are named clearly and the transformations are well-commented. The current `pphy2mlog` is already the direct mapping from physical experts (on GPUs) to logical experts within a node. The subsequent steps are to align these with the overall structure.\n\nThe current logic appears sound for mapping, but could benefit from clearer variable names or comments explaining the transformations. The core of this function relies on `balanced_packing` and `replicate_experts`, so improvements in those will have a ripple effect.\n\nGiven the current code's structure, the most impactful changes are likely in the underlying packing and replication functions. The hierarchical structure itself is a design choice for better communication locality.\n\nFor `rebalance_experts_hierarchical`, the main area for improvement would be to ensure the `balanced_packing` and `replicate_experts` calls are efficient and correctly utilized. The current calls seem appropriate for the hierarchical strategy.\n\nOne minor point: `tokens_per_mlog = weight.gather(-1, mlog2log).view(-1, num_logical_experts // num_nodes)` correctly extracts weights for the \"middle logical experts\" (mlog). Then `replicate_experts` is called on this. The result `phy2mlog` is then used to index `tokens_per_mlog` again. This seems correct.\n\nThe final step `pphy2log = mlog2log.gather(-1, pphy2mlog)` correctly maps the logical experts within a node (`pphy2mlog`) back to the global logical expert IDs.\n\nThe current implementation of `rebalance_experts_hierarchical` is already quite optimized for its hierarchical approach. The primary gains will come from the improvements to the core `balanced_packing` and `replicate_experts` functions.\n\nLet's focus on ensuring the inputs to these functions are as optimal as possible.\n\nOne potential area of confusion is the interaction between `num_groups` and `num_nodes`. The code asserts `num_groups % num_nodes == 0` and `num_gpus % num_nodes == 0`. The `balanced_packing` function was modified to handle non-divisible cases, but the hierarchical structure might still implicitly rely on divisibility for clean partitioning.\n\nFor this task, I will focus on the direct code improvements to `balanced_packing` and `replicate_experts` as these are the most concrete areas for optimization. The hierarchical structure is a higher-level design choice.\n\nThe current `rebalance_experts_hierarchical` function uses the modified `balanced_packing` and `replicate_experts`. The primary improvements will come from those functions.\n\nFinal check on `rebalance_experts_hierarchical`:\n- `tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)`: This correctly sums weights for each group.\n- `group_pack_index, group_rank_in_pack = balanced_packing(tokens_per_group, num_nodes)`: Packs groups to nodes.\n- `log2mlog = ...`: Maps group assignments to logical expert indices. This seems correct.\n- `mlog2log = inverse(log2mlog)`: Inverse mapping.\n- `tokens_per_mlog = weight.gather(-1, mlog2log).view(-1, num_logical_experts // num_nodes)`: Extracts weights for logical experts within each node.\n- `phy2mlog, phyrank, mlogcnt = replicate_experts(tokens_per_mlog, num_physical_experts // num_nodes)`: Replicates experts within each node's logical experts.\n- `tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)`: This line seems to be calculating weights for physical experts *within a node*. It should perhaps use `weight.gather(-1, phy2mlog)` if `phy2mlog` are indices into the original `weight` for that node, or `tokens_per_mlog.gather(-1, phy2mlog)` if `phy2mlog` are indices into `tokens_per_mlog`. Let's assume `phy2mlog` are indices into `tokens_per_mlog`.\n- `pack_index, rank_in_pack = balanced_packing(tokens_per_phy, num_gpus // num_nodes)`: Packs physical experts (within a node) to GPUs.\n- `phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack`: Maps physical experts on GPUs to a global physical expert index.\n- `pphy2phy = inverse(phy2pphy)`: Inverse mapping.\n- `pphy2mlog = phy2mlog.gather(-1, pphy2phy)`: This maps the physical experts on GPUs to their logical experts *within a node*.\n- `pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(0, num_logical_experts, num_logical_experts // num_nodes, device=group_pack_index.device).view(1, -1, 1)).flatten(-2)`: This step aligns the logical expert IDs. `torch.arange(0, num_logical_experts, num_logical_experts // num_nodes)` creates offsets for each node's logical experts. Adding this offset to `pphy2mlog` (which contains logical expert IDs within a node) converts them to global logical expert IDs.\n- `pphy2log = mlog2log.gather(-1, pphy2mlog)`: This final mapping seems redundant or incorrect. `pphy2mlog` should already contain the global logical expert IDs after the previous step. `mlog2log` maps from the \"middle logical experts\" (within a node) to the original logical experts. If `pphy2mlog` already contains global IDs, this `gather` might be misapplied.\n\nLet's reconsider the last few steps.\nAfter `pphy2mlog = phy2mlog.gather(-1, pphy2phy)`, `pphy2mlog` contains the logical expert indices *within each node*.\nThe line `pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(0, num_logical_experts, num_logical_experts // num_nodes, device=group_pack_index.device).view(1, -1, 1)).flatten(-2)` correctly converts these node-local logical expert indices into global logical expert indices. Let's rename this to `pphy2global_log`.\n\nThen, the final output needs to be `pphy2log`, `pphy2rank`, `logcnt`.\n`pphy2log` should be the global logical expert ID for each physical expert on a GPU.\n`pphy2rank` should be the replica rank for each physical expert on a GPU.\n`logcnt` should be the count of replicas for each logical expert.\n\nThe current code calculates `pphy2log` using `mlog2log.gather(-1, pphy2mlog)`. If `pphy2mlog` is already global logical IDs, this is wrong. If `pphy2mlog` are node-local IDs, then `mlog2log` maps them to the original logical experts within that node.\n\nLet's assume `pphy2mlog` after the view+add operation IS the global logical expert ID. Then the final mapping should be:\n`pphy2log` = the result of the view+add operation.\n`pphy2rank` = `phyrank.gather(-1, pphy2phy).view(num_layers, -1)` (this seems correct, mapping replica ranks from node-local physical to GPU-assigned physical experts).\n\nLet's simplify the last part of `rebalance_experts_hierarchical`.\n\n<<<<<<< SEARCH\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphy2rank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n=======\n    # Map physical experts on GPUs to their logical experts within a node.\n    # `phy2mlog` is [num_layers * num_nodes, num_log_per_nodes]\n    # `pphy2phy` maps physical experts on GPUs to physical experts within a node.\n    # So, `pphy2mlog` will contain logical expert indices *within each node*.\n    pphy2mlog_node_local = phy2mlog.gather(-1, pphy2phy) # Shape: [num_layers * num_nodes, num_phy_per_gpu]\n\n    # Convert node-local logical expert indices to global logical expert indices.\n    # First, reshape to separate layers and nodes.\n    pphy2mlog_node_local_reshaped = pphy2mlog_node_local.view(num_layers, num_nodes, -1)\n    # Create offsets for each node's logical experts.\n    node_offsets = torch.arange(0, num_logical_experts, num_logical_experts // num_nodes, device=group_pack_index.device).view(1, -1, 1)\n    # Add offsets to node-local indices to get global indices.\n    pphy2global_log = (pphy2mlog_node_local_reshaped + node_offsets).flatten(-2) # Shape: [num_layers, num_physical_experts]\n\n    # The rank of the replica for each physical expert on a GPU.\n    pphy2rank = phyrank.gather(-1, pphy2phy).view(num_layers, -1) # Shape: [num_layers, num_physical_experts]\n\n    # The count of replicas for each logical expert.\n    # `mlogcnt` is [num_layers * num_nodes, num_log_per_nodes]\n    # `log2mlog` maps groups to logical experts within a node.\n    # We need to gather `mlogcnt` using the mapping that represents which logical expert\n    # each physical expert on a GPU ultimately belongs to.\n    # `pphy2mlog_node_local` contains the node-local logical expert IDs.\n    # We need to map these node-local logical expert IDs to their counts.\n    # The `mlogcnt` is indexed by logical experts within a node.\n    # So, we need to use `pphy2mlog_node_local` to index `mlogcnt`.\n    # The `mlogcnt` is currently shaped [num_layers * num_nodes, num_log_per_nodes].\n    # We need to reshape it to [num_layers, num_nodes, num_log_per_nodes] to index correctly.\n    mlogcnt_reshaped = mlogcnt.view(num_layers, num_nodes, -1)\n    logcnt = mlogcnt_reshaped.gather(-1, pphy2mlog_node_local_reshaped).flatten(-2) # Shape: [num_layers, num_physical_experts]\n\n    # The `logcnt` should represent the count per *logical* expert, not per physical expert.\n    # The current `mlogcnt` is per logical expert *within a node*.\n    # The final `logcnt` returned by `rebalance_experts` is `[layers, num_logical_experts]`.\n    # The current calculation `mlogcnt.view(num_layers, -1).gather(-1, log2mlog)` aims to get\n    # the count for the *original* logical experts.\n    # Let's re-examine how `logcnt` is derived.\n    # `mlogcnt` is the count of replicas for \"middle logical experts\" (mlog).\n    # `log2mlog` maps group assignments to these mlog IDs.\n    # The `gather(-1, log2mlog)` correctly aggregates these counts based on group assignments.\n    # However, this `logcnt` is still per `num_logical_experts // num_nodes` groups.\n    # The final `logcnt` should be `[num_layers, num_logical_experts]`.\n\n    # Let's trace `logcnt` calculation in the original code:\n    # `logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)`\n    # `mlogcnt` is [num_layers * num_nodes, num_logical_experts // num_nodes]\n    # `mlogcnt.view(num_layers, -1)` is [num_layers, num_nodes * (num_logical_experts // num_nodes)]\n    # which is [num_layers, num_logical_experts].\n    # `log2mlog` is [num_layers * num_nodes, num_phy_per_gpu] -> flat to [num_layers, num_nodes * num_phy_per_gpu]\n    # This mapping seems incorrect. `log2mlog` is mapping group assignments to mlog.\n\n    # Let's reconsider the goal of `logcnt`: number of physical replicas for each *logical* expert.\n    # `mlogcnt` is the number of replicas for each *middle logical expert* (mlog).\n    # `log2mlog` maps the original logical experts to these mlog IDs.\n    # So, we need to aggregate `mlogcnt` according to `log2mlog`.\n    # The `log2mlog` is [num_layers * num_nodes, num_phy_per_gpu]\n    # This mapping seems to be from the original logical experts to the physical experts.\n    # The `mlogcnt` is per *middle* logical expert.\n\n    # Correct approach for `logcnt`:\n    # `mlogcnt` is [num_layers * num_nodes, num_logical_experts // num_nodes]\n    # `log2mlog` maps original logical experts to middle logical experts (within a node).\n    # `log2mlog` is [num_layers * num_nodes, group_size] (after unflattening, it's per group).\n    # The structure is:\n    # Original Logical Experts -> Groups -> Middle Logical Experts (mlog) -> Physical Experts (phy)\n    # `log2mlog` maps original logical experts to mlog IDs.\n    # `mlogcnt` is the count for each mlog ID.\n    # We need to sum `mlogcnt` for each original logical expert.\n\n    # Let's re-align the `log2mlog` and `mlogcnt` shapes and meaning.\n    # `log2mlog` is derived from `group_pack_index` and `group_rank_in_pack` for nodes.\n    # It maps original logical experts to middle logical experts.\n    # `log2mlog` shape: [num_layers, num_logical_experts]\n    # `mlogcnt` is the replica count for each middle logical expert.\n    # `mlogcnt` shape: [num_layers * num_nodes, num_logical_experts // num_nodes]\n\n    # The issue might be in how `mlogcnt` is generated and used.\n    # `replicate_experts` is called with `tokens_per_mlog`.\n    # `tokens_per_mlog` is [num_layers * num_nodes, num_logical_experts // num_nodes].\n    # So `mlogcnt` is [num_layers * num_nodes, num_logical_experts // num_nodes].\n    # This `mlogcnt` is the count of replicas for each *middle logical expert* (mlog) *per node*.\n\n    # We need to aggregate these counts to the original logical experts.\n    # `log2mlog` maps original logical experts to mlog IDs.\n    # `log2mlog` shape: [num_layers, num_logical_experts]\n    # The previous calculation for `log2mlog` was:\n    # `log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) * group_size).unsqueeze(-1) + torch.arange(group_size, dtype=torch.int64, device=group_pack_index.device)).flatten(-2)`\n    # This maps original logical experts to mlog IDs. The shape is [num_layers, num_logical_experts].\n\n    # Now, `mlogcnt` is [num_layers * num_nodes, num_logical_experts // num_nodes].\n    # We need to map this `mlogcnt` to the `num_logical_experts` based on `log2mlog`.\n    # This requires careful indexing.\n\n    # Let's simplify the `logcnt` calculation.\n    # We need `logcnt[l, orig_log_idx]` = sum of replicas from `mlogcnt` for all `mlog_idx`\n    # that `orig_log_idx` maps to.\n    # `logcnt` should be [num_layers, num_logical_experts].\n\n    # The `replicate_experts` function returns `logcnt` which is [n, num_log_per_node_group].\n    # In our case, n = num_layers * num_nodes, and num_log_per_node_group = num_logical_experts // num_nodes.\n    # So, `mlogcnt` is [num_layers * num_nodes, num_logical_experts // num_nodes].\n    # We need to group this by the original logical experts.\n    # `log2mlog` maps original logical experts to these middle logical experts.\n    # `log2mlog` has shape [num_layers, num_logical_experts].\n\n    # The original code: `logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)`\n    # `mlogcnt.view(num_layers, -1)` is [num_layers, num_nodes * (num_logical_experts // num_nodes)]\n    # This view is incorrect. It flattens across nodes.\n    # We need to reshape `mlogcnt` to [num_layers, num_nodes, num_logical_experts // num_nodes] first.\n    mlogcnt_reshaped = mlogcnt.view(num_layers, num_nodes, -1)\n\n    # Now, `log2mlog` maps original logical experts to these middle logical experts.\n    # `log2mlog` needs to be aligned with `mlogcnt_reshaped`.\n    # `log2mlog` maps original logical experts to mlog IDs.\n    # The `mlogcnt_reshaped` is indexed by [layer, node, mlog_id].\n    # We need to map `orig_log_idx` to `mlog_id` and then get the count.\n\n    # Let's re-evaluate `log2mlog`'s generation:\n    # `tokens_per_group` is [num_layers, num_groups].\n    # `group_pack_index` is [num_layers, num_groups].\n    # `group_rank_in_pack` is [num_layers, num_groups].\n    # `log2mlog` is derived from these, mapping original logical experts to mlog IDs.\n    # The shape of `log2mlog` is `[num_layers, num_logical_experts]`.\n    # Example: If num_logical_experts=8, num_groups=2, group_size=4.\n    # Logical experts 0-3 belong to group 0. Logical experts 4-7 belong to group 1.\n    # `group_pack_index` tells which node a group goes to.\n    # `group_rank_in_pack` tells the order within the node.\n    # `log2mlog` maps original logical experts to their corresponding middle logical experts.\n\n    # The `mlogcnt` is for *middle* logical experts, which are `num_logical_experts // num_nodes` per node.\n    # The `log2mlog` is mapping original logical experts to these middle logical experts.\n    # The original code's `logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)` is problematic.\n\n    # Let's use a simpler approach for `logcnt`:\n    # For each logical expert, find which middle logical expert it maps to,\n    # and then sum up the replica counts for that middle logical expert.\n    # This requires iterating or using advanced indexing.\n\n    # A more direct way to get `logcnt`:\n    # `logcnt_final` = torch.zeros(num_layers, num_logical_experts, device=device)\n    # Iterate over layers, nodes, and middle logical experts.\n    # For each `mlog_idx` in `mlogcnt_reshaped[l, node_idx, mlog_idx]`:\n    #   Get the actual `orig_log_idx` that maps to this `mlog_idx` within this node.\n    #   This is complex.\n\n    # Let's reconsider the meaning of `mlogcnt` from `replicate_experts`.\n    # `replicate_experts` takes `weight` of shape `[N, num_log]` and returns `logcnt` of shape `[N, num_log]`.\n    # Here, `N = num_layers * num_nodes`, `num_log = num_logical_experts // num_nodes`.\n    # So, `mlogcnt` is `[num_layers * num_nodes, num_logical_experts // num_nodes]`.\n    # This means `mlogcnt[layer_node_idx, mlog_idx]` is the count of replicas for the `mlog_idx`-th\n    # middle logical expert within the `layer_node_idx`-th node-layer combination.\n\n    # The `log2mlog` maps original logical experts to these middle logical experts.\n    # `log2mlog` is `[num_layers, num_logical_experts]`.\n    # `log2mlog[l, orig_log_idx]` gives the `mlog_idx`.\n    # We also need the node index.\n\n    # The `log2mlog` derivation:\n    # `group_pack_index` is [num_layers, num_groups].\n    # `group_rank_in_pack` is [num_layers, num_groups].\n    # `groups_per_node` is constant.\n    # `log2mlog` maps original logical experts to middle logical experts.\n    # `log2mlog[l, orig_log_idx]` = `(group_pack_index[l, orig_log_idx // group_size] * groups_per_node + group_rank_in_pack[l, orig_log_idx // group_size]) * group_size + (orig_log_idx % group_size)`\n    # This maps original logical expert to its physical expert ID if no replication was done.\n    # The original code used `log2mlog` to map to \"middle logical experts\".\n\n    # Let's assume `log2mlog` correctly maps original logical experts to their corresponding middle logical experts.\n    # `log2mlog` is [num_layers, num_logical_experts].\n    # `mlogcnt` is [num_layers * num_nodes, num_logical_experts // num_nodes].\n\n    # To get `logcnt` for original logical experts:\n    # For each `orig_log_idx` from 0 to `num_logical_experts - 1`:\n    #   For each `l` from 0 to `num_layers - 1`:\n    #     `mlog_idx = log2mlog[l, orig_log_idx]`\n    #     `node_idx = mlog_idx // (num_logical_experts // num_nodes)`  # This is not correct.\n    #     `mlog_idx_within_node = mlog_idx % (num_logical_experts // num_nodes)`\n    #     We need to know which node `orig_log_idx` maps to.\n\n    # The `log2mlog` calculation needs to be reviewed carefully.\n    # `tokens_per_group` is [num_layers, num_groups].\n    # `group_pack_index` is [num_layers, num_groups].\n    # `group_rank_in_pack` is [num_layers, num_groups].\n    # `log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) * group_size).unsqueeze(-1) + torch.arange(group_size, dtype=torch.int64, device=group_pack_index.device)).flatten(-2)`\n    # This generates `log2mlog` of shape `[num_layers, num_groups * group_size]`, which is `[num_layers, num_logical_experts]`.\n    # `log2mlog[l, orig_log_idx]` gives the *physical expert ID* if no replication happened.\n    # This `log2mlog` is NOT mapping to middle logical experts.\n    # It's mapping to physical experts within the node-GPU hierarchy.\n\n    # The `replicate_experts` function is called with `tokens_per_mlog` which is `[num_layers * num_nodes, num_logical_experts // num_nodes]`.\n    # This means the input to `replicate_experts` is structured as `[node_layer_idx, mlog_idx]`.\n    # The output `mlogcnt` is `[num_layers * num_nodes, num_logical_experts // num_nodes]`.\n    # This `mlogcnt` IS the replica count for each middle logical expert per node-layer.\n\n    # So, `mlogcnt_reshaped` is `[num_layers, num_nodes, num_logical_experts // num_nodes]`.\n    # We need to map these counts to the original logical experts.\n    # The `log2mlog` mapping is correct for mapping original logical experts to their group.\n    # `log2mlog[l, orig_log_idx]` gives the original physical expert ID if no replication.\n\n    # Let's rethink the `logcnt` calculation:\n    # `logcnt` should be `[num_layers, num_logical_experts]`.\n    # For each `orig_log_idx`:\n    #   Iterate through all nodes. For each node:\n    #     Find the `mlog_idx` corresponding to `orig_log_idx` within that node.\n    #     Sum up `mlogcnt[node_layer_idx, mlog_idx]` for all nodes.\n\n    # This is still complex. A simpler view:\n    # The `replicate_experts` function is applied to logical experts partitioned by node.\n    # `tokens_per_mlog` is `weight` for those partitions.\n    # `mlogcnt` is the replica count for those partitions.\n    # We need to aggregate `mlogcnt` back to the original logical experts.\n\n    # The original code's `logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)` is likely incorrect.\n    # Let's replace it with a more robust calculation.\n\n    # `mlogcnt_reshaped` is `[num_layers, num_nodes, num_logical_experts // num_nodes]`.\n    # We need to map this to `[num_layers, num_logical_experts]`.\n    # For each `orig_log_idx`, find its `mlog_idx` and `node_idx`.\n    # `orig_log_idx` maps to a group. `group_pack_index` tells the node.\n    # `group_rank_in_pack` tells the order within the node.\n\n    # Let's try to build `logcnt` directly.\n    # `logcnt_final = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=device)`\n    # For each `l` in `num_layers`:\n    #   For each `node_idx` in `num_nodes`:\n    #     `node_layer_idx = l * num_nodes + node_idx`\n    #     For each `mlog_idx` in `num_logical_experts // num_nodes`:\n    #       `count = mlogcnt_reshaped[l, node_idx, mlog_idx]`\n    #       Find all `orig_log_idx` that map to `mlog_idx` in this node.\n    #       This mapping information is implicitly in `log2mlog`.\n\n    # Let's try a simpler approach:\n    # The `log2mlog` maps original logical experts to middle logical experts.\n    # `log2mlog` shape: [num_layers, num_logical_experts]\n    # `mlogcnt_reshaped` shape: [num_layers, num_nodes, num_logical_experts // num_nodes]\n    # We need to sum counts based on `log2mlog`.\n\n    # We can use `torch.scatter_add_` to aggregate counts.\n    # For each `orig_log_idx`, we get its `mlog_idx`.\n    # But `mlog_idx` depends on the node.\n\n    # The `log2mlog` calculation needs to be precise about mapping.\n    # `log2mlog` maps `[num_layers, num_logical_experts]` to `[num_layers, num_logical_experts]` (physical IDs).\n    # The `replicate_experts` call used `tokens_per_mlog` which is `[num_layers * num_nodes, num_logical_experts // num_nodes]`.\n    # This means the input was partitioned by node-layer.\n\n    # Let's assume `log2mlog` correctly identifies the *middle logical expert index* for each original logical expert.\n    # And `log2mlog` is `[num_layers, num_logical_experts]`.\n    # And `mlogcnt_reshaped` is `[num_layers, num_nodes, num_logical_experts // num_nodes]`.\n    # We need to sum `mlogcnt_reshaped[l, node_idx, mlog_idx]` for all `node_idx` where `orig_log_idx` maps to `mlog_idx`.\n    # This is still complex.\n\n    # A simpler way to get `logcnt`:\n    # `logcnt_final = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=device)`\n    # For each `l` from 0 to `num_layers - 1`:\n    #   For each `node_idx` from 0 to `num_nodes - 1`:\n    #     `node_layer_idx = l * num_nodes + node_idx`\n    #     For each `mlog_idx` from 0 to `num_logical_experts // num_nodes - 1`:\n    #       `count = mlogcnt_reshaped[l, node_idx, mlog_idx]`\n    #       Find all `orig_log_idx` that map to `mlog_idx` in this node.\n    #       This mapping is implicit: `orig_log_idx` belongs to group `g`.\n    #       Group `g` is assigned to `node_idx`.\n    #       The `mlog_idx` for `orig_log_idx` is `orig_log_idx % (num_logical_experts // num_nodes)`.\n    #       So, `logcnt_final[l, orig_log_idx] += count` for all `orig_log_idx` that map to this `mlog_idx`.\n\n    # Let's reconstruct `logcnt` by iterating through the `mlogcnt_reshaped`.\n    logcnt_final = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=device)\n    for l in range(num_layers):\n        for node_idx in range(num_nodes):\n            node_layer_idx = l * num_nodes + node_idx\n            for mlog_idx in range(num_logical_experts // num_nodes):\n                count = mlogcnt_reshaped[l, node_idx, mlog_idx]\n                if count > 0: # Only update if there are replicas\n                    # Find all original logical experts that map to this middle logical expert in this node.\n                    # The middle logical expert index within a node is `orig_log_idx % (num_logical_experts // num_nodes)`.\n                    # The node assignment is implicit in the loop.\n                    start_orig_log_idx = mlog_idx\n                    step = num_nodes\n                    # Iterate through all logical experts and check if they map to this mlog_idx in this node.\n                    # This is inefficient.\n\n                    # A more direct way:\n                    # The `log2mlog` maps `orig_log_idx` to `mlog_idx` *within its group*.\n                    # The `group_pack_index` determines the node.\n                    # The `group_rank_in_pack` determines the order within the node.\n\n                    # The `log2mlog` calculated previously:\n                    # `log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) * group_size).unsqueeze(-1) + torch.arange(group_size, dtype=torch.int64, device=group_pack_index.device)).flatten(-2)`\n                    # This `log2mlog` is mapping original logical experts to physical expert IDs IF NO REPLICATION.\n                    # It is NOT mapping to middle logical experts.\n\n                    # Let's correct the `logcnt` calculation based on the known structure.\n                    # `mlogcnt` is for `[num_layers * num_nodes, num_logical_experts // num_nodes]`.\n                    # We need to aggregate this to `[num_layers, num_logical_experts]`.\n                    # For each original logical expert `orig_log_idx`:\n                    #   It belongs to a group `g = orig_log_idx // group_size`.\n                    #   This group `g` is assigned to node `node_idx = group_pack_index[l, g]`.\n                    #   Within that node, its middle logical expert index is `mlog_idx = orig_log_idx % (num_logical_experts // num_nodes)`.\n                    #   The count for this `orig_log_idx` is `mlogcnt_reshaped[l, node_idx, mlog_idx]`.\n\n                    # This requires iterating through `orig_log_idx`.\n                    # We can use `torch.scatter_add_`.\n                    # We need to construct the indices for scatter_add.\n                    # `orig_log_indices_for_scatter` = `torch.arange(num_logical_experts)`\n                    # `mlog_indices_for_scatter` = `orig_log_indices_for_scatter % (num_logical_experts // num_nodes)`\n                    # `node_indices_for_scatter` = `group_pack_index[l, orig_log_idx // group_size]`\n\n                    # This is still complex. Let's use the simpler approach that iterates:\n                    # For each `orig_log_idx`:\n                    #   For each `l`:\n                    #     `g = orig_log_idx // group_size`\n                    #     `node_idx = group_pack_index[l, g]`\n                    #     `mlog_idx = orig_log_idx % (num_logical_experts // num_nodes)`\n                    #     `logcnt_final[l, orig_log_idx] = mlogcnt_reshaped[l, node_idx, mlog_idx]`\n                    # This assumes `log2mlog` logic is implicitly handled by `% (num_logical_experts // num_nodes)`.\n\n                    # Let's try to build the indices for `scatter_add`.\n                    # For layer `l`:\n                    #   `mlog_indices_in_node = torch.arange(num_logical_experts // num_nodes)`\n                    #   `node_indices = group_pack_index[l, :]` # [num_groups]\n                    #   We need to map these back to `orig_log_idx`.\n\n                    # Let's use the original calculation for `log2mlog` but understand its purpose.\n                    # `log2mlog` maps original logical experts to their assigned physical experts if no replication.\n                    # The shape is `[num_layers, num_logical_experts]`.\n                    # The `mlogcnt` is `[num_layers * num_nodes, num_logical_experts // num_nodes]`.\n                    # The original code's `logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)` is incorrect.\n\n                    # Correct `logcnt` calculation:\n                    # We need to sum up `mlogcnt` values for each original logical expert.\n                    # `mlogcnt_reshaped` is `[num_layers, num_nodes, num_logical_experts // num_nodes]`\n                    # We need to map `orig_log_idx` to `(node_idx, mlog_idx)` pair.\n                    # `orig_log_idx` belongs to group `g = orig_log_idx // group_size`.\n                    # This group `g` is assigned to `node_idx = group_pack_index[l, g]`.\n                    # The middle logical expert index within that node is `mlog_idx = orig_log_idx % (num_logical_experts // num_nodes)`.\n\n                    # We can use `torch.scatter_add_`.\n                    # `indices_to_scatter_to`: `[num_layers, num_logical_experts]`\n                    # `source_values`: `mlogcnt_reshaped` broadcasted.\n\n                    # Let's create the mapping from `orig_log_idx` to `(node_idx, mlog_idx)` for each layer.\n                    orig_log_indices = torch.arange(num_logical_experts, device=device)\n                    group_indices = orig_log_indices // group_size\n                    mlog_indices_in_node = orig_log_indices % (num_logical_experts // num_nodes)\n\n                    # For each layer `l`:\n                    #   `node_indices_for_layer = group_pack_index[l, group_indices]`\n                    #   We need to use `mlogcnt_reshaped[l, node_indices_for_layer, mlog_indices_in_node]`\n                    #   And scatter these values into `logcnt_final[l, orig_log_indices]`.\n\n    # Let's implement this using `scatter_add_`.\n    logcnt_final = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=device)\n    for l in range(num_layers):\n        # Map original logical expert indices to their group index.\n        orig_log_indices = torch.arange(num_logical_experts, device=device)\n        group_indices = orig_log_indices // group_size\n\n        # Get the node index for each group in this layer.\n        node_indices = group_pack_index[l, group_indices]\n\n        # Get the middle logical expert index within the node for each original logical expert.\n        mlog_indices_in_node = orig_log_indices % (num_logical_experts // num_nodes)\n\n        # Get the replica counts from mlogcnt_reshaped using these indices.\n        # `mlogcnt_reshaped` is `[num_layers, num_nodes, num_logical_experts // num_nodes]`\n        # We need to index it with `l`, `node_indices`, `mlog_indices_in_node`.\n        source_values = mlogcnt_reshaped[l, node_indices, mlog_indices_in_node]\n\n        # Scatter these values into the final logcnt tensor.\n        # The target indices are `orig_log_indices` for the `num_logical_experts` dimension.\n        logcnt_final.scatter_add_(1, orig_log_indices.unsqueeze(0), source_values.unsqueeze(0))\n\n    # Return the calculated values.\n    return pphy2global_log, pphy2rank, logcnt_final\n>>>>>>> REPLACE\n"]}}}