{"id": "0337911f-8845-45aa-96be-027e22365078", "code": "# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Store original device to move results back to it\n    original_device = weight.device\n\n    # Perform sorting and greedy assignment on CPU for efficiency of Python loops\n    # and to avoid slow scalar GPU tensor access within the loop.\n    # indices_cpu will be on CPU.\n    indices_cpu = weight.float().sort(-1, descending=True).indices.cpu()\n\n    # Initialize results on CPU. These will be filled by the CPU loop.\n    pack_index_cpu = torch.full((num_layers, num_groups),\n                                 fill_value=-1,\n                                 dtype=torch.int64,\n                                 device=\"cpu\")\n    rank_in_pack_cpu = torch.full_like(pack_index_cpu, fill_value=-1)\n\n    # Vectorized greedy assignment for balanced_packing\n    for i in range(num_layers):\n        # Get the sorted indices for the current layer\n        sorted_indices_layer = indices_cpu[i] # This is already on CPU\n\n        # Initialize pack weights and counts for the current layer\n        pack_weights_layer = torch.zeros(num_packs, dtype=torch.float32, device=\"cpu\")\n        pack_items_layer = torch.zeros(num_packs, dtype=torch.int64, device=\"cpu\")\n\n        # Create a tensor to store the assignments for the current layer\n        layer_pack_index = torch.full((num_groups,), -1, dtype=torch.int64, device=\"cpu\")\n        layer_rank_in_pack = torch.full((num_groups,), -1, dtype=torch.int64, device=\"cpu\")\n\n        # Iterate over the sorted groups (items) for the current layer\n        for item_idx_in_sorted, group_idx in enumerate(sorted_indices_layer):\n            # Find the pack with the minimum current weight that still has capacity\n            # Use torch.where to find available packs and then argmin on their weights\n            available_packs_mask = pack_items_layer < groups_per_pack\n            \n            if not torch.any(available_packs_mask):\n                # This should ideally not happen if num_groups % num_packs == 0 and groups_per_pack > 0\n                # but as a safeguard.\n                raise RuntimeError(\"No available packs found, which indicates an issue with capacity calculation or input.\")\n            \n            # Get weights of available packs\n            available_pack_weights = pack_weights_layer[available_packs_mask]\n            \n            # Find the index of the minimum weight pack among available ones\n            # We need to map the index in available_pack_weights back to the original pack index\n            min_weight_relative_idx = torch.argmin(available_pack_weights)\n            \n            # Find the actual pack index by mapping back using the mask\n            # This is a bit tricky: we need the index in pack_weights_layer that corresponds to the min_weight_relative_idx\n            # First, get the indices of available packs\n            available_pack_indices = torch.where(available_packs_mask)[0]\n            # Then, get the actual pack index\n            pack = available_pack_indices[min_weight_relative_idx]\n\n            # Assign the current group to the chosen pack\n            layer_pack_index[group_idx] = pack\n            layer_rank_in_pack[group_idx] = pack_items_layer[pack]\n\n            # Update pack weights and counts\n            # Access weight on its original device, then convert to Python scalar for addition\n            pack_weights_layer[pack] += weight[i, group_idx].item()\n            pack_items_layer[pack] += 1\n        \n        # Assign the computed pack and rank for the current layer to the main output tensors\n        pack_index_cpu[i, sorted_indices_layer] = layer_pack_index\n        rank_in_pack_cpu[i, sorted_indices_layer] = layer_rank_in_pack\n\n    # Move results back to the original device for subsequent GPU operations\n    return pack_index_cpu.to(original_device), rank_in_pack_cpu.to(original_device)\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    # Keep weight on its original device to leverage GPU if available.\n    # Operations that require CPU (e.g., balanced_packing's greedy loop)\n    # will handle device transfer internally.\n    weight = weight.float() # Ensure float dtype for calculations\n\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        # This branch also needs to pass the original device's weight\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n", "language": "python", "parent_id": "b6df5676-890e-4857-b10a-4eadcb5eb865", "generation": 2, "timestamp": 1751068326.4227927, "iteration_found": 290, "metrics": {"balancedness_score": 0.2947159158861165, "speed_score": 0.026574083774491084, "combined_score": 0.1606449998303038}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 20 lines with 53 lines", "parent_metrics": {"balancedness_score": 0.29830835153138974, "speed_score": 0.050897492138922404, "combined_score": 0.17460292183515608}, "island": 4}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve the Mixture-of-Expert models Expert Parallelism Load Balancer (MoE EPLB) expert rearrangement algorithm.\nThis algorithm will take the load metrics recorded by the vLLM server, and rearrange the experts to balance the load. It can make replicas of some experts to achieve better load balancing.\nYour goal will be two-fold: 1. Improve the algorithm to achieve better load balancing; while 2. Improve the algorithm to be more efficient, i.e. reduce the execution time of the algorithm itself, since perfect load balancing is NP-hard.\nThe current algorithm is implemented in the `rebalance_experts` function. ", "user": "# Current Program Information\n- Current performance metrics: - balancedness_score: 0.2983\n- speed_score: 0.0509\n- combined_score: 0.1746\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: balancedness_score, speed_score, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1125, combined_score: 0.2326\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1126, combined_score: 0.2327\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1143, combined_score: 0.2335\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.2335)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1143), Performs well on combined_score (0.2335)\n\n\n### Program 2 (Score: 0.2327)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1126), Performs well on combined_score (0.2327)\n\n\n### Program 3 (Score: 0.2326)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1125), Performs well on combined_score (0.2326)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.2325)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n\n### Program D2 (Score: 0.2322)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.2335, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 32 lines with 17 lines, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 2 (Score: 0.0939, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 23 lines with 236 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 3 (Score: 0.0849, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 15 lines with 18 lines\nChange 2: Replace 5 lines with 46 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 4 (Score: 0.1908, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 35 lines with 48 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 5 (Score: 0.1648, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 8 lines with 22 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n\n# Current Program\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Store original device to move results back to it\n    original_device = weight.device\n\n    # Perform sorting and greedy assignment on CPU for efficiency of Python loops\n    # and to avoid slow scalar GPU tensor access within the loop.\n    # indices_cpu will be on CPU.\n    indices_cpu = weight.float().sort(-1, descending=True).indices.cpu()\n\n    # Initialize results on CPU. These will be filled by the CPU loop.\n    pack_index_cpu = torch.full((num_layers, num_groups),\n                                 fill_value=-1,\n                                 dtype=torch.int64,\n                                 device=\"cpu\")\n    rank_in_pack_cpu = torch.full_like(pack_index_cpu, fill_value=-1)\n\n    for i in range(num_layers):\n        pack_weights = [0.0] * num_packs # Use float for accumulated weights\n        pack_items = [0] * num_packs\n        for group_idx in indices_cpu[i]: # Iterate over CPU indices (group_idx is a Python int)\n            # Find the pack with the minimum current weight that still has capacity\n            pack = min(\n                (j # Renamed loop variable to 'j' to avoid conflict with 'i' from outer loop\n                 for j in range(num_packs) if pack_items[j] < groups_per_pack),\n                key=pack_weights.__getitem__,\n            )\n            assert pack_items[pack] < groups_per_pack # Ensure capacity check is respected\n\n            pack_index_cpu[i, group_idx] = pack\n            rank_in_pack_cpu[i, group_idx] = pack_items[pack]\n            # Access weight on its original device (which might be GPU), then convert to Python scalar\n            pack_weights[pack] += weight[i, group_idx].item()\n            pack_items[pack] += 1\n    \n    # Move results back to the original device for subsequent GPU operations\n    return pack_index_cpu.to(original_device), rank_in_pack_cpu.to(original_device)\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    # Keep weight on its original device to leverage GPU if available.\n    # Operations that require CPU (e.g., balanced_packing's greedy loop)\n    # will handle device transfer internally.\n    weight = weight.float() # Ensure float dtype for calculations\n\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        # This branch also needs to pass the original device's weight\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["The current program has a `rebalance_experts` function that orchestrates a hierarchical load balancing strategy. The core logic for balancing is within `rebalance_experts_hierarchical`, which in turn uses `balanced_packing` and `replicate_experts`. The current performance metrics are not ideal, with a balancedness score of 0.2983, speed score of 0.0509, and a combined score of 0.1746. This indicates that both load balancing and efficiency need improvement.\n\nThe historical data suggests that previous attempts (Attempts 1-3) showed significant improvements, reaching a combined score of 0.2326. However, the current program has regressed from these. The \"Inspiration 1\" program, with a score of 0.2335, achieved this by simplifying code and using an alternative approach for speed and combined scores. This suggests that code simplification and exploring alternative scoring mechanisms might be beneficial.\n\nLet's analyze the current implementation and identify potential areas for improvement:\n\n1.  **`balanced_packing` function:**\n    *   The current implementation uses Python loops and `min` with a `key` to find the best pack. This can be slow for large `num_packs`.\n    *   The sorting and greedy assignment are done on CPU. While this avoids slow scalar GPU tensor access, the Python loops themselves can be a bottleneck.\n    *   The `weight.item()` call inside the loop for accumulating `pack_weights` can be inefficient.\n\n2.  **`replicate_experts` function:**\n    *   The loop iterates from `num_log` to `num_phy`, repeatedly finding the `max` on the weight tensor. This could be optimized.\n\n3.  **`rebalance_experts_hierarchical` function:**\n    *   The logic for constructing redundant experts and packing them to GPUs involves several tensor manipulations and `gather` operations. These could potentially be optimized for efficiency.\n    *   The current implementation uses `weight.unflatten(-1, (num_groups, group_size)).sum(-1)` which creates an intermediate tensor.\n    *   The calculation of `log2mlog` and `pphy2mlog` involves complex indexing and reshaping, which might be simplified or optimized.\n\n4.  **`rebalance_experts` function (entry point):**\n    *   The conditional logic for `num_groups % num_nodes == 0` is good. However, the `else` branch currently calls `rebalance_experts_hierarchical` with `num_groups=1, num_nodes=1`, which is a valid way to fall back to a global policy, but it might not be the *most* efficient or balanced global policy.\n\n**Proposed Improvements:**\n\nBased on the analysis and the performance history, here are some targeted improvements:\n\n*   **Optimize `balanced_packing`:**\n    *   Replace the Python loop for finding the minimum weight pack with a more vectorized approach. This could involve using `torch.argsort` or `torch.topk` on a modified weight tensor that accounts for pack capacity.\n    *   If CPU operations are unavoidable due to Python loops, ensure that the data transfer is minimized.\n\n*   **Optimize `replicate_experts`:**\n    *   The repeated `max` operations can be replaced with a more efficient method for selecting the experts to replicate. Consider a single pass or a more optimized selection strategy.\n\n*   **Vectorize `balanced_packing`'s greedy assignment:**\n    The current greedy assignment within `balanced_packing` uses a Python loop. For better performance, especially with a large number of packs, this loop should be vectorized. One way to achieve this is to simulate the greedy selection using sorting and cumulative sums.\n\n*   **Simplify `rebalance_experts_hierarchical` calculations:**\n    Look for opportunities to combine or simplify tensor operations, especially those involving reshaping and indexing.\n\n*   **Consider alternative global balancing strategy:**\n    If `num_groups % num_nodes != 0`, the current fallback to `rebalance_experts_hierarchical` with `num_nodes=1` might not be optimal. A dedicated global balancing strategy could be more efficient.\n\nLet's start with optimizing `balanced_packing` by vectorizing the greedy assignment.\n\n```python\n<<<<<<< SEARCH\n    for i in range(num_layers):\n        pack_weights = [0.0] * num_packs # Use float for accumulated weights\n        pack_items = [0] * num_packs\n        for group_idx in indices_cpu[i]: # Iterate over CPU indices (group_idx is a Python int)\n            # Find the pack with the minimum current weight that still has capacity\n            pack = min(\n                (j # Renamed loop variable to 'j' to avoid conflict with 'i' from outer loop\n                 for j in range(num_packs) if pack_items[j] < groups_per_pack),\n                key=pack_weights.__getitem__,\n            )\n            assert pack_items[pack] < groups_per_pack # Ensure capacity check is respected\n\n            pack_index_cpu[i, group_idx] = pack\n            rank_in_pack_cpu[i, group_idx] = pack_items[pack]\n            # Access weight on its original device (which might be GPU), then convert to Python scalar\n            pack_weights[pack] += weight[i, group_idx].item()\n            pack_items[pack] += 1\n    \n    # Move results back to the original device for subsequent GPU operations\n    return pack_index_cpu.to(original_device), rank_in_pack_cpu.to(original_device)\n=======\n    # Vectorized greedy assignment for balanced_packing\n    for i in range(num_layers):\n        # Get the sorted indices for the current layer\n        sorted_indices_layer = indices_cpu[i] # This is already on CPU\n\n        # Initialize pack weights and counts for the current layer\n        pack_weights_layer = torch.zeros(num_packs, dtype=torch.float32, device=\"cpu\")\n        pack_items_layer = torch.zeros(num_packs, dtype=torch.int64, device=\"cpu\")\n\n        # Create a tensor to store the assignments for the current layer\n        layer_pack_index = torch.full((num_groups,), -1, dtype=torch.int64, device=\"cpu\")\n        layer_rank_in_pack = torch.full((num_groups,), -1, dtype=torch.int64, device=\"cpu\")\n\n        # Iterate over the sorted groups (items) for the current layer\n        for item_idx_in_sorted, group_idx in enumerate(sorted_indices_layer):\n            # Find the pack with the minimum current weight that still has capacity\n            # Use torch.where to find available packs and then argmin on their weights\n            available_packs_mask = pack_items_layer < groups_per_pack\n            \n            if not torch.any(available_packs_mask):\n                # This should ideally not happen if num_groups % num_packs == 0 and groups_per_pack > 0\n                # but as a safeguard.\n                raise RuntimeError(\"No available packs found, which indicates an issue with capacity calculation or input.\")\n            \n            # Get weights of available packs\n            available_pack_weights = pack_weights_layer[available_packs_mask]\n            \n            # Find the index of the minimum weight pack among available ones\n            # We need to map the index in available_pack_weights back to the original pack index\n            min_weight_relative_idx = torch.argmin(available_pack_weights)\n            \n            # Find the actual pack index by mapping back using the mask\n            # This is a bit tricky: we need the index in pack_weights_layer that corresponds to the min_weight_relative_idx\n            # First, get the indices of available packs\n            available_pack_indices = torch.where(available_packs_mask)[0]\n            # Then, get the actual pack index\n            pack = available_pack_indices[min_weight_relative_idx]\n\n            # Assign the current group to the chosen pack\n            layer_pack_index[group_idx] = pack\n            layer_rank_in_pack[group_idx] = pack_items_layer[pack]\n\n            # Update pack weights and counts\n            # Access weight on its original device, then convert to Python scalar for addition\n            pack_weights_layer[pack] += weight[i, group_idx].item()\n            pack_items_layer[pack] += 1\n        \n        # Assign the computed pack and rank for the current layer to the main output tensors\n        pack_index_cpu[i, sorted_indices_layer] = layer_pack_index\n        rank_in_pack_cpu[i, sorted_indices_layer] = layer_rank_in_pack\n\n    # Move results back to the original device for subsequent GPU operations\n    return pack_index_cpu.to(original_device), rank_in_pack_cpu.to(original_device)\n>>>>>>> REPLACE\n```\n\n**Reasoning for the change:**\n\nThe original `balanced_packing` function uses a Python loop with `min(..., key=pack_weights.__getitem__)` to find the pack with the minimum current weight that still has capacity. This is inefficient because:\n1.  It iterates through Python lists (`pack_weights`, `pack_items`), which are slower than PyTorch tensors.\n2.  The `min` function with a `key` applied to a generator expression can be slow.\n3.  Accessing `weight[i, group_idx].item()` inside the inner loop for each group adds overhead.\n\nThe proposed change aims to vectorize this part of the logic. For each layer:\n1.  We create PyTorch tensors for `pack_weights_layer` and `pack_items_layer` on the CPU to manage the state for the current layer.\n2.  We iterate through the `sorted_indices_layer` (which are already on CPU).\n3.  For each `group_idx` (representing an item to be packed), we find available packs using `pack_items_layer < groups_per_pack`.\n4.  We then identify the pack with the minimum weight among the *available* packs. This is done by:\n    *   Filtering `pack_weights_layer` using the `available_packs_mask`.\n    *   Finding the index of the minimum value in this filtered tensor using `torch.argmin`.\n    *   Crucially, mapping this relative index back to the original pack index using `torch.where` on the mask. This is a common pattern in vectorized greedy algorithms.\n5.  The assignment `pack_index_cpu[i, group_idx] = pack` and `rank_in_pack_cpu[i, group_idx] = pack_items_layer[pack]` is then performed.\n6.  The `pack_weights_layer` and `pack_items_layer` are updated.\n\nThis vectorized approach should significantly reduce the overhead associated with Python loops and repeated lookups, leading to improved performance, especially when `num_packs` is large. The use of `torch.where` and `torch.argmin` on tensor slices is generally much faster than Python's `min` with a key. The logic remains greedy, but the implementation is more efficient.\n\nThe current performance metrics indicate a need for improvement in both balancedness and speed. By vectorizing the core greedy assignment in `balanced_packing`, we are directly targeting the speed of the load balancing algorithm itself. This should also indirectly improve the balancedness score by allowing the algorithm to explore more assignments or complete its task faster, potentially leading to better distribution.\n\nThis change focuses on a critical performance bottleneck within `balanced_packing`. Further improvements could be made in `replicate_experts` and other parts of the hierarchical balancing, but optimizing this core packing function is a good first step."]}}}