{"id": "3ce78409-bcfa-4f09-a1b4-5d2c686eac49", "code": "# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort groups by weight in descending order.\n    # 'weight' is already float and on CPU from rebalance_experts,\n    # so no need for .float().cpu() here.\n    indices = weight.sort(-1, descending=True).indices\n\n    # Initialize pack_index and rank_in_pack.\n    # These will be fully populated, so `empty_like` is sufficient.\n    # They will be on the same device as 'weight' (i.e., CPU).\n    pack_index = torch.empty_like(weight, dtype=torch.int64)\n    rank_in_pack = torch.empty_like(pack_index)\n    \n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These tensors are on the same device as 'weight' (which is CPU).\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # Get current state for this layer's packs.\n        # These are kept local to the loop for potential minor efficiency gains.\n        current_pack_weights = pack_weights_per_layer[i]\n        current_pack_items = pack_items_per_layer[i]\n        \n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id in indices[i]:\n            # Find the pack with the minimum current weight among available ones.\n            # Set weights of full packs to infinity to exclude them from argmin.\n            full_mask = (current_pack_items == groups_per_pack)\n            \n            # Find the pack with the minimum weight, ignoring full packs.\n            # `torch.argmin` on a masked tensor is efficient.\n            pack = torch.argmin(current_pack_weights.masked_fill(full_mask, float('inf'))).item()\n\n            # Assign the group to the chosen pack.\n            # `group_id` is the original index of the group within the layer.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n\n    # Initialize output tensors\n    phy2log = torch.empty(n, num_phy, dtype=torch.int64, device=device)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    \n    # Pre-calculate indices for each layer\n    layer_indices = torch.arange(n, dtype=torch.int64, device=device)\n\n    # Create a list of (layer_idx, logical_expert_idx, current_load_per_replica)\n    # This will be used to efficiently select the best logical expert to replicate.\n    # We'll use a heap or sorted list to manage this.\n    \n    # For simplicity and to avoid complex heap implementations within this diff,\n    # we'll sort all possible (layer, logical_expert) pairs by their current\n    # load-per-replica metric and greedily assign. This might be less efficient\n    # than a heap but is a step towards optimization.\n\n    # Calculate initial load-per-replica for all logical experts\n    load_per_replica = weight / logcnt\n    \n    # Create a flattened representation of (layer_idx, logical_expert_idx)\n    # and their corresponding load_per_replica.\n    flat_load_per_replica = load_per_replica.flatten()\n    \n    # Create indices for the flattened tensor\n    flat_indices = torch.arange(n * num_log, dtype=torch.int64, device=device)\n    \n    # Sort these indices based on the load_per_replica in descending order\n    # We want to pick the experts with the highest load_per_replica first to replicate\n    sorted_indices = torch.argsort(flat_load_per_replica, descending=True)\n\n    # Keep track of how many replicas have been assigned to each logical expert\n    assigned_replicas_count = torch.zeros(n, num_log, dtype=torch.int64, device=device)\n    \n    # Assign initial replicas (one for each logical expert)\n    current_phy_idx = 0\n    for i in range(n):\n        for j in range(num_log):\n            phy2log[i, current_phy_idx] = j\n            rank[i, current_phy_idx] = 0\n            assigned_replicas_count[i, j] += 1\n            current_phy_idx += 1\n\n    # Assign the remaining redundant experts\n    for i in range(num_log, num_phy):\n        # Find the logical expert with the highest load per replica\n        # This involves finding the maximum of (weight[l, j] / assigned_replicas_count[l, j])\n        # over all logical experts j for each layer l.\n        \n        # Recalculate load_per_replica for current counts\n        current_load_per_replica = weight / assigned_replicas_count\n        \n        # Find the (layer, logical_expert) pair that has the highest load_per_replica\n        # This is the bottleneck in the original code. We aim to optimize this.\n        # Instead of re-calculating max for all, we can use the sorted list.\n        \n        # Iterate through the sorted list of (layer, logical_expert) pairs\n        # and pick the first one that still needs more replicas.\n        best_layer_idx, best_logical_expert_idx = -1, -1\n        \n        # We need a way to efficiently find the next available logical expert\n        # from the sorted list. A simple linear scan through sorted_indices is\n        # still O(N*M*log(N*M)) for sorting and then potentially O(N*M) for scans.\n        # A more efficient approach would be to use a min-heap for each layer\n        # storing (load_per_replica, logical_expert_idx) and extract min.\n        # However, for simplicity and to stay within typical tensor operations:\n        \n        # Find the logical expert with the highest load_per_replica across all layers\n        # This is what the original code does: (weight / logcnt).max(dim=-1).indices\n        # Let's try to optimize this selection.\n        \n        # Calculate current load per replica for all logical experts\n        current_load_per_replica_all = weight / assigned_replicas_count\n\n        # Find the logical expert with the maximum load across all layers\n        # This is still the core of the greedy choice.\n        # The optimization might come from how `assigned_replicas_count` is updated.\n        \n        # Instead of finding the max each time, let's maintain a prioritized list.\n        # For each layer, we want to find the logical expert 'j' that maximizes\n        # weight[l, j] / assigned_replicas_count[l, j].\n        \n        # Let's re-implement the greedy choice more explicitly.\n        # For each layer, find the logical expert that has the highest load/replica.\n        # The challenge is that assigning a replica to one expert changes its load_per_replica,\n        # affecting future choices.\n        \n        # The original code's approach of finding the max over all logical experts\n        # for each layer and then picking the overall maximum seems to be a greedy strategy.\n        # Let's stick to that logic but ensure it's efficient.\n        \n        # Calculate ratios for all logical experts in all layers\n        ratios = weight / assigned_replicas_count\n        \n        # Find the maximum ratio and its index for each layer\n        max_ratios_per_layer, best_logical_expert_indices_per_layer = torch.max(ratios, dim=-1)\n        \n        # Find the layer with the overall maximum ratio\n        best_layer_idx = torch.argmax(max_ratios_per_layer).item()\n        best_logical_expert_idx = best_logical_expert_indices_per_layer[best_layer_idx].item()\n\n        # Assign the current physical expert to this logical expert\n        phy2log[best_layer_idx, i] = best_logical_expert_idx\n        rank[best_layer_idx, i] = assigned_replicas_count[best_layer_idx, best_logical_expert_idx]\n        \n        # Increment the replica count for the chosen logical expert\n        assigned_replicas_count[best_layer_idx, best_logical_expert_idx] += 1\n\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float().cpu()\n    # Utilize hierarchical balancing if provided with relevant parameters.\n    # This allows the algorithm to leverage network topology for potentially\n    # better load balancing.\n    phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n        weight, num_replicas, num_groups, num_nodes, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    # The scatter operation is used to reconstruct the logical_to_physical_map\n    # from the physical_to_logical_map and replica ranks.\n    # The mapping needs to account for the maximum possible replicas a logical expert can have.\n    # `phy2log * maxlogcnt + phyrank` creates a unique index for each physical expert\n    # within the context of its logical expert and replica rank.\n    # `torch.arange(num_replicas)` provides the physical expert IDs to be placed.\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n", "language": "python", "parent_id": "e772a930-ecc4-4994-958e-4c7bbf9f74fd", "generation": 5, "timestamp": 1751066520.4255683, "iteration_found": 263, "metrics": {"balancedness_score": 0.0, "speed_score": 0.0, "combined_score": 0.0, "error": "index 72 is out of bounds for dimension 1 with size 72"}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 21 lines with 26 lines\nChange 2: Replace 15 lines with 114 lines\nChange 3: Replace 36 lines with 34 lines", "parent_metrics": {"balancedness_score": 0.3307444807293194, "speed_score": 0.027791395167807244, "combined_score": 0.17926793794856333}, "island": 3}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve the Mixture-of-Expert models Expert Parallelism Load Balancer (MoE EPLB) expert rearrangement algorithm.\nThis algorithm will take the load metrics recorded by the vLLM server, and rearrange the experts to balance the load. It can make replicas of some experts to achieve better load balancing.\nYour goal will be two-fold: 1. Improve the algorithm to achieve better load balancing; while 2. Improve the algorithm to be more efficient, i.e. reduce the execution time of the algorithm itself, since perfect load balancing is NP-hard.\nThe current algorithm is implemented in the `rebalance_experts` function. ", "user": "# Current Program Information\n- Current performance metrics: - balancedness_score: 0.3307\n- speed_score: 0.0278\n- combined_score: 0.1793\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: balancedness_score, speed_score, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1103, combined_score: 0.2315\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1103, combined_score: 0.2315\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1103, combined_score: 0.2315\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.2315)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1103), Performs well on combined_score (0.2315)\n\n\n### Program 2 (Score: 0.2315)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1103), Performs well on combined_score (0.2315)\n\n\n### Program 3 (Score: 0.2315)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1103), Performs well on combined_score (0.2315)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.2315)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n\n### Program D2 (Score: 0.2315)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.2315, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 32 lines with 32 lines, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 2 (Score: 0.2315, Type: Migrant)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 32 lines with 32 lines, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 3 (Score: 0.1626, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 4 (Score: 0.1920, Type: Migrant)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 5 (Score: 0.0000, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 25 lines with 53 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n\n# Current Program\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort groups by weight in descending order.\n    # 'weight' is already float and on CPU from rebalance_experts,\n    # so no need for .float().cpu() here.\n    indices = weight.sort(-1, descending=True).indices\n\n    # Initialize pack_index and rank_in_pack.\n    # These will be fully populated, so `empty_like` is sufficient.\n    # They will be on the same device as 'weight' (i.e., CPU).\n    pack_index = torch.empty_like(weight, dtype=torch.int64)\n    rank_in_pack = torch.empty_like(pack_index)\n    \n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These tensors are on the same device as 'weight' (which is CPU).\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id in indices[i]:\n            # Get current state for this layer's packs.\n            current_pack_weights = pack_weights_per_layer[i]\n            current_pack_items = pack_items_per_layer[i]\n\n            # Find the pack with the minimum current weight among available ones.\n            # Set weights of full packs to infinity to exclude them from argmin.\n            full_mask = (current_pack_items == groups_per_pack)\n            # Create a temporary tensor for argmin that excludes full packs.\n            # Using masked_fill is often more efficient than clone() then assignment.\n            pack = torch.argmin(current_pack_weights.masked_fill(full_mask, float('inf'))).item()\n\n            # Assert that a valid pack was found and it has space.\n            assert current_pack_items[pack] < groups_per_pack\n\n            # Assign the group to the chosen pack.\n            # `group_id` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            # `weight[i, group_id]` is the weight of the current group.\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float().cpu()\n    # Always use the global load-balance policy by treating all experts as one group\n    # and all nodes as one conceptual node. This maximizes flexibility for balancing\n    # across all physical experts, potentially improving balancedness_score\n    # at the cost of ignoring network topology.\n    phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n        weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["Here are some suggestions for improving the `rebalance_experts` function and its supporting functions, focusing on both balancedness and efficiency.\n\nThe current implementation of `rebalance_experts` calls `rebalance_experts_hierarchical` with `num_groups=1`, `num_nodes=1`, and `num_gpus=num_replicas`. This effectively flattens the problem into a single-level balancing task. While this might be a good default, it doesn't leverage the hierarchical structure that the `rebalance_experts_hierarchical` function is designed for. This could be a source of suboptimal balancedness.\n\n### Improvement 1: Dynamic Hierarchical Balancing\n\n**Reasoning for Improvement:**\nThe current `rebalance_experts` function forces a flat balancing strategy by setting `num_groups=1`, `num_nodes=1`. This ignores the potential benefits of hierarchical balancing, which can be more effective when there's a significant difference in communication costs between intra-node (e.g., NVLink) and inter-node communication. By allowing the algorithm to utilize the `num_groups`, `num_nodes`, and `num_gpus` parameters, we can potentially achieve better load balancing by first balancing within groups and nodes, and then balancing across nodes. This could improve the `balancedness_score`.\n\nTo make this more dynamic and potentially more efficient, we can introduce a mechanism to decide whether to use hierarchical balancing or a simpler flat approach based on the input parameters. However, for this specific task, we will focus on enabling the hierarchical balancing by passing the provided `num_groups`, `num_nodes`, and `num_gpus` directly.\n\n**Code Change:**\n\n<<<<<<< SEARCH\n    # Always use the global load-balance policy by treating all experts as one group\n    # and all nodes as one conceptual node. This maximizes flexibility for balancing\n    # across all physical experts, potentially improving balancedness_score\n    # at the cost of ignoring network topology.\n    phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n        weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n=======\n    # Utilize hierarchical balancing if provided with relevant parameters.\n    # This allows the algorithm to leverage network topology for potentially\n    # better load balancing.\n    phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n        weight, num_replicas, num_groups, num_nodes, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    # The scatter operation is used to reconstruct the logical_to_physical_map\n    # from the physical_to_logical_map and replica ranks.\n    # The mapping needs to account for the maximum possible replicas a logical expert can have.\n    # `phy2log * maxlogcnt + phyrank` creates a unique index for each physical expert\n    # within the context of its logical expert and replica rank.\n    # `torch.arange(num_replicas)` provides the physical expert IDs to be placed.\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n>>>>>>> REPLACE\n\n### Improvement 2: Optimize `replicate_experts` for efficiency\n\n**Reasoning for Improvement:**\nThe `replicate_experts` function uses a loop that iterates `num_log` to `num_phy` times. Inside the loop, it performs several tensor operations. Specifically, `(weight / logcnt).max(dim=-1).indices` is computed in each iteration. This computation involves a division and a `max` operation across the `num_log` logical experts. If `num_phy` is much larger than `num_log`, this can be a bottleneck.\n\nWe can potentially optimize this by pre-calculating the \"best\" logical expert to replicate for each physical expert slot. This might involve a more direct assignment or a different greedy strategy that avoids recomputing the `max` over all experts repeatedly.\n\nA more direct approach would be to sort all logical experts based on their current load (weight / logcnt) and then greedily assign replicas.\n\nLet's try to optimize the greedy selection process. Instead of iterating and finding the max each time, we can sort the experts by their current load-to-replica count ratio and then pick from the top.\n\n**Code Change:**\n\n<<<<<<< SEARCH\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n=======\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n\n    # Initialize output tensors\n    phy2log = torch.empty(n, num_phy, dtype=torch.int64, device=device)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    \n    # Pre-calculate indices for each layer\n    layer_indices = torch.arange(n, dtype=torch.int64, device=device)\n\n    # Create a list of (layer_idx, logical_expert_idx, current_load_per_replica)\n    # This will be used to efficiently select the best logical expert to replicate.\n    # We'll use a heap or sorted list to manage this.\n    \n    # For simplicity and to avoid complex heap implementations within this diff,\n    # we'll sort all possible (layer, logical_expert) pairs by their current\n    # load-per-replica metric and greedily assign. This might be less efficient\n    # than a heap but is a step towards optimization.\n\n    # Calculate initial load-per-replica for all logical experts\n    load_per_replica = weight / logcnt\n    \n    # Create a flattened representation of (layer_idx, logical_expert_idx)\n    # and their corresponding load_per_replica.\n    flat_load_per_replica = load_per_replica.flatten()\n    \n    # Create indices for the flattened tensor\n    flat_indices = torch.arange(n * num_log, dtype=torch.int64, device=device)\n    \n    # Sort these indices based on the load_per_replica in descending order\n    # We want to pick the experts with the highest load_per_replica first to replicate\n    sorted_indices = torch.argsort(flat_load_per_replica, descending=True)\n\n    # Keep track of how many replicas have been assigned to each logical expert\n    assigned_replicas_count = torch.zeros(n, num_log, dtype=torch.int64, device=device)\n    \n    # Assign initial replicas (one for each logical expert)\n    current_phy_idx = 0\n    for i in range(n):\n        for j in range(num_log):\n            phy2log[i, current_phy_idx] = j\n            rank[i, current_phy_idx] = 0\n            assigned_replicas_count[i, j] += 1\n            current_phy_idx += 1\n\n    # Assign the remaining redundant experts\n    for i in range(num_log, num_phy):\n        # Find the logical expert with the highest load per replica\n        # This involves finding the maximum of (weight[l, j] / assigned_replicas_count[l, j])\n        # over all logical experts j for each layer l.\n        \n        # Recalculate load_per_replica for current counts\n        current_load_per_replica = weight / assigned_replicas_count\n        \n        # Find the (layer, logical_expert) pair that has the highest load_per_replica\n        # This is the bottleneck in the original code. We aim to optimize this.\n        # Instead of re-calculating max for all, we can use the sorted list.\n        \n        # Iterate through the sorted list of (layer, logical_expert) pairs\n        # and pick the first one that still needs more replicas.\n        best_layer_idx, best_logical_expert_idx = -1, -1\n        \n        # We need a way to efficiently find the next available logical expert\n        # from the sorted list. A simple linear scan through sorted_indices is\n        # still O(N*M*log(N*M)) for sorting and then potentially O(N*M) for scans.\n        # A more efficient approach would be to use a min-heap for each layer\n        # storing (load_per_replica, logical_expert_idx) and extract min.\n        # However, for simplicity and to stay within typical tensor operations:\n        \n        # Find the logical expert with the highest load_per_replica across all layers\n        # This is what the original code does: (weight / logcnt).max(dim=-1).indices\n        # Let's try to optimize this selection.\n        \n        # Calculate current load per replica for all logical experts\n        current_load_per_replica_all = weight / assigned_replicas_count\n\n        # Find the logical expert with the maximum load across all layers\n        # This is still the core of the greedy choice.\n        # The optimization might come from how `assigned_replicas_count` is updated.\n        \n        # Instead of finding the max each time, let's maintain a prioritized list.\n        # For each layer, we want to find the logical expert 'j' that maximizes\n        # weight[l, j] / assigned_replicas_count[l, j].\n        \n        # Let's re-implement the greedy choice more explicitly.\n        # For each layer, find the logical expert that has the highest load/replica.\n        # The challenge is that assigning a replica to one expert changes its load_per_replica,\n        # affecting future choices.\n        \n        # The original code's approach of finding the max over all logical experts\n        # for each layer and then picking the overall maximum seems to be a greedy strategy.\n        # Let's stick to that logic but ensure it's efficient.\n        \n        # Calculate ratios for all logical experts in all layers\n        ratios = weight / assigned_replicas_count\n        \n        # Find the maximum ratio and its index for each layer\n        max_ratios_per_layer, best_logical_expert_indices_per_layer = torch.max(ratios, dim=-1)\n        \n        # Find the layer with the overall maximum ratio\n        best_layer_idx = torch.argmax(max_ratios_per_layer).item()\n        best_logical_expert_idx = best_logical_expert_indices_per_layer[best_layer_idx].item()\n\n        # Assign the current physical expert to this logical expert\n        phy2log[best_layer_idx, i] = best_logical_expert_idx\n        rank[best_layer_idx, i] = assigned_replicas_count[best_layer_idx, best_logical_expert_idx]\n        \n        # Increment the replica count for the chosen logical expert\n        assigned_replicas_count[best_layer_idx, best_logical_expert_idx] += 1\n\n    return phy2log, rank, logcnt\n>>>>>>> REPLACE\n\n### Improvement 3: Simplify `balanced_packing` for readability and potential efficiency\n\n**Reasoning for Improvement:**\nThe `balanced_packing` function implements a greedy algorithm for bin packing. While functional, some parts could be made more readable and potentially slightly more efficient by simplifying tensor operations. Specifically, the way `pack_index` and `rank_in_pack` are populated and updated could be more direct.\n\nThe current loop iterates through `indices[i]` (sorted group IDs). Inside, it finds the `pack` with the minimum weight and updates.\nThe use of `masked_fill` is good. However, the repeated access to `pack_weights_per_layer[i]` and `pack_items_per_layer[i]` inside the inner loop can be slightly optimized by keeping these as local variables within the outer loop.\n\n**Code Change:**\n\n<<<<<<< SEARCH\n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These tensors are on the same device as 'weight' (which is CPU).\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id in indices[i]:\n            # Get current state for this layer's packs.\n            current_pack_weights = pack_weights_per_layer[i]\n            current_pack_items = pack_items_per_layer[i]\n\n            # Find the pack with the minimum current weight among available ones.\n            # Set weights of full packs to infinity to exclude them from argmin.\n            full_mask = (current_pack_items == groups_per_pack)\n            # Create a temporary tensor for argmin that excludes full packs.\n            # Using masked_fill is often more efficient than clone() then assignment.\n            pack = torch.argmin(current_pack_weights.masked_fill(full_mask, float('inf'))).item()\n\n            # Assert that a valid pack was found and it has space.\n            assert current_pack_items[pack] < groups_per_pack\n\n            # Assign the group to the chosen pack.\n            # `group_id` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            # `weight[i, group_id]` is the weight of the current group.\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n=======\n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These tensors are on the same device as 'weight' (which is CPU).\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # Get current state for this layer's packs.\n        # These are kept local to the loop for potential minor efficiency gains.\n        current_pack_weights = pack_weights_per_layer[i]\n        current_pack_items = pack_items_per_layer[i]\n        \n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id in indices[i]:\n            # Find the pack with the minimum current weight among available ones.\n            # Set weights of full packs to infinity to exclude them from argmin.\n            full_mask = (current_pack_items == groups_per_pack)\n            \n            # Find the pack with the minimum weight, ignoring full packs.\n            # `torch.argmin` on a masked tensor is efficient.\n            pack = torch.argmin(current_pack_weights.masked_fill(full_mask, float('inf'))).item()\n\n            # Assign the group to the chosen pack.\n            # `group_id` is the original index of the group within the layer.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n>>>>>>> REPLACE"]}}}