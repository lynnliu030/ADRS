{"id": "a64b4ac1-1ff0-4086-9b52-cf25a287b540", "code": "# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort groups by weight in descending order.\n    # 'weight' is already float and on CPU from rebalance_experts,\n    # so no need for .float().cpu() here.\n    indices = weight.sort(-1, descending=True).indices\n\n    # Initialize pack_index and rank_in_pack.\n    # These will be fully populated, so `empty_like` is sufficient.\n    # They will be on the same device as 'weight' (i.e., CPU).\n    pack_index = torch.empty_like(weight, dtype=torch.int64)\n    rank_in_pack = torch.empty_like(pack_index)\n    \n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These tensors are on the same device as 'weight' (which is CPU).\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id in indices[i]:\n            # Get current state for this layer's packs.\n            current_pack_weights = pack_weights_per_layer[i]\n            current_pack_items = pack_items_per_layer[i]\n\n            # Find the pack with the minimum current weight among available ones.\n            # `available_mask` is a boolean tensor.\n            available_mask = (current_pack_items < groups_per_pack)\n            \n            # Get the indices of available packs.\n            available_pack_indices = torch.where(available_mask)[0]\n\n            # If there are available packs, find the one with the minimum weight.\n            if available_pack_indices.numel() > 0:\n                # Get weights of only the available packs\n                weights_of_available_packs = current_pack_weights[available_pack_indices]\n                \n                # Find the index within `available_pack_indices` that has the minimum weight.\n                local_min_idx = torch.argmin(weights_of_available_packs)\n                \n                # Map this local index back to the global pack index.\n                pack = available_pack_indices[local_min_idx].item()\n            else:\n                # This case should ideally not be reached if inputs are valid and logic is correct.\n                # For robustness, we can break or raise an error, but here we assume valid inputs.\n                # If it happens, it means no pack has capacity, which implies an issue with `groups_per_pack` or `current_pack_items`.\n                # For now, we'll assume it won't happen based on the problem constraints.\n                # If this assertion fails, it means there's a problem with the logic or input assumptions.\n                raise RuntimeError(\"No available packs found, but expected to find one.\")\n\n            # Assign the group to the chosen pack.\n            # `group_id` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            # `weight[i, group_id]` is the weight of the current group.\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    # Calculate the ratio of weight to current replica count for each logical expert.\n    # This is done once per iteration of the outer loop.\n    ratios = weight / logcnt\n\n    # In each step, we want to find the logical expert that, if replicated,\n    # would result in the smallest increase in maximum load. This is approximated\n    # by picking the logical expert with the highest current weight / replica_count ratio.\n    # We are essentially trying to balance the load by adding a replica to the\n    # \"most loaded\" logical expert (relative to its current number of replicas).\n\n    # To efficiently find the best expert to replicate across all layers,\n    # we can use `torch.topk` on the flattened ratios.\n    # `torch.topk` returns the values and indices of the k largest elements.\n    # Here, k=1 because we want to replicate one expert at a time.\n    \n    # Flatten ratios to find the global max ratio across all layers and logical experts.\n    flat_ratios = ratios.flatten()\n    \n    # We need to find the logical expert index that corresponds to the max ratio.\n    # The structure of `ratios` is [num_layers, num_logical_experts].\n    # We need to determine which logical expert to replicate for each layer.\n    # A more direct approach for this specific problem is to iterate through layers\n    # and find the best expert for each layer independently, as the current code does,\n    # but optimize the inner part.\n\n    # The current loop is already somewhat optimized by operating on `weight` and `logcnt`\n    # which are [n, num_log]. Let's refine the selection process within the loop.\n\n    # For each layer `l`, we want to find the logical expert `log_idx` that maximizes `weight[l, log_idx] / logcnt[l, log_idx]`.\n    # This is equivalent to finding the `log_idx` that minimizes `logcnt[l, log_idx] / weight[l, log_idx]` if weight is non-zero,\n    # or maximizing `weight[l, log_idx] / logcnt[l, log_idx]`.\n\n    # The current approach:\n    # `redundant_indices = (weight / logcnt).max(dim=-1).indices`\n    # This line correctly finds the index of the logical expert with the maximum ratio for each layer.\n    # The issue might be the repeated computation of `weight / logcnt` if it's not optimized by PyTorch.\n\n    # Let's stick to the current logic for finding the index, as it's already per-layer.\n    # The main potential for speedup is if the operations within the loop are more efficient.\n    # The current operations are element-wise division and `max(dim=-1)`.\n    # These are generally efficient operations in PyTorch.\n\n    # The number of iterations is `num_redundant_experts`. If this number is large,\n    # the loop itself becomes a bottleneck.\n\n    # Consider a different approach: instead of iterative greedy selection,\n    # can we directly determine the final assignment? This is complex because\n    # adding a replica changes the \"best\" candidate for subsequent additions.\n\n    # For now, let's focus on ensuring the inner operations are as lean as possible.\n    # The current implementation of `replicate_experts` is fundamentally an iterative greedy approach.\n    # The `weight / logcnt` calculation is done inside the loop. We can move it outside if `logcnt` doesn't change.\n    # However, `logcnt` *does* change in each iteration.\n\n    # Let's re-examine the `replicate_experts` function's goal:\n    # \"Replicate `num_log` experts to `num_phy` replicas, such that the maximum load of all replicas is minimized.\"\n    # The current greedy strategy is a common heuristic for this.\n\n    # A potential optimization: Instead of recalculating `weight / logcnt` repeatedly,\n    # we can maintain a priority queue (or a sorted list) for each layer, storing\n    # (ratio, logical_expert_index). When we replicate an expert, we update its\n    # ratio in the priority queue.\n\n    # However, implementing a priority queue in pure PyTorch for this might be\n    # more complex than it's worth for a SEARCH/REPLACE diff.\n\n    # Let's try to optimize the current loop by ensuring `weight` and `logcnt`\n    # are on the CPU, as `rebalance_experts` already moves `weight` to CPU.\n    # `weight` is already on CPU. `logcnt` is also created on CPU.\n\n    # The most significant factor influencing speed here is the number of times the loop runs\n    # (`num_redundant_experts`) and the complexity of the operations inside.\n    # The operations are `weight / logcnt`, `.max(dim=-1)`, and indexing.\n    # These are already quite optimized.\n\n    # Let's consider the structure of `logcnt`: `logcnt[arangen, redundant_indices] += 1`.\n    # `arangen` is `torch.arange(n)`. `redundant_indices` is `[n]`.\n    # This indexing is efficient.\n\n    # The core issue might be the iterative nature. If `num_redundant_experts` is large,\n    # this will be slow.\n\n    # Let's try to optimize the calculation of `redundant_indices` slightly by\n    # ensuring that division by zero is handled gracefully (though `logcnt` starts at 1).\n    # The current code assumes `logcnt` will not be zero.\n\n    # A more direct approach to find the max ratio per layer:\n    for i in range(num_log, num_phy):\n        # Calculate ratios for the current state of logcnt.\n        # Ensure we handle potential division by zero if logcnt could be 0, though it starts at 1.\n        # Use a small epsilon to avoid division by zero if logcnt becomes zero (shouldn't happen here).\n        # The current code `weight / logcnt` is fine as logcnt starts at 1.\n        \n        # Find the logical expert with the maximum weight-to-replica ratio for each layer.\n        # `torch.max` returns values and indices. We need indices.\n        _, redundant_indices = torch.max(weight / logcnt, dim=-1)\n        \n        # Assign the chosen logical expert to the current physical expert slot.\n        phy2log[:, i] = redundant_indices\n        \n        # Record the replica rank for this physical expert.\n        # `logcnt[arangen, redundant_indices]` gets the current count for the chosen experts.\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        \n        # Increment the replica count for the chosen logical experts.\n        logcnt[arangen, redundant_indices] += 1\n        \n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float().cpu()\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        # Call a dedicated global rebalancing function for efficiency\n        phy2log, phyrank, logcnt = rebalance_experts_global(\n            weight, num_replicas)\n\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\ndef rebalance_experts_global(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Global load balancing for expert replication.\n\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_physical_experts: number of physical experts after replication\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        rank: [num_moe_layers, num_physical_experts]\n        expert_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    # Directly use the replicate_experts function for global balancing.\n    # replicate_experts takes [n, num_log] and returns assignments for num_phy.\n    phy2log, rank, logcnt = replicate_experts(weight, num_physical_experts)\n    return phy2log, rank, logcnt\n\n\n__all__ = [\"rebalance_experts\"]\n\n", "language": "python", "parent_id": "cd64448e-6b3f-4d87-9ded-40d027251c77", "generation": 4, "timestamp": 1751057041.8031478, "iteration_found": 118, "metrics": {"balancedness_score": 0.29830835153138974, "speed_score": 0.026303182487377758, "combined_score": 0.16230576700938376}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 23 lines with 25 lines\nChange 2: Replace 3 lines with 27 lines", "parent_metrics": {"balancedness_score": 0.29830835153138974, "speed_score": 0.02628728441614004, "combined_score": 0.16229781797376489}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve the Mixture-of-Expert models Expert Parallelism Load Balancer (MoE EPLB) expert rearrangement algorithm.\nThis algorithm will take the load metrics recorded by the vLLM server, and rearrange the experts to balance the load. It can make replicas of some experts to achieve better load balancing.\nYour goal will be two-fold: 1. Improve the algorithm to achieve better load balancing; while 2. Improve the algorithm to be more efficient, i.e. reduce the execution time of the algorithm itself, since perfect load balancing is NP-hard.\nThe current algorithm is implemented in the `rebalance_experts` function. ", "user": "# Current Program Information\n- Current performance metrics: - balancedness_score: 0.2983\n- speed_score: 0.0263\n- combined_score: 0.1623\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: speed_score, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0830, combined_score: 0.1907\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0833, combined_score: 0.1908\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0835, combined_score: 0.1909\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.1909)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0835), Performs well on combined_score (0.1909)\n\n\n### Program 2 (Score: 0.1908)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0833), Performs well on combined_score (0.1908)\n\n\n### Program 3 (Score: 0.1907)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0830), Performs well on combined_score (0.1907)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.1900)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n\n### Program D2 (Score: 0.1879)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.1909, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 3 lines with 4 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 2 (Score: 0.0939, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 23 lines with 236 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 3 (Score: 0.0939, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 23 lines with 236 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 4 (Score: 0.0349, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 5 (Score: 0.0000, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 5 lines with 20 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n\n# Current Program\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort groups by weight in descending order.\n    # 'weight' is already float and on CPU from rebalance_experts,\n    # so no need for .float().cpu() here.\n    indices = weight.sort(-1, descending=True).indices\n\n    # Initialize pack_index and rank_in_pack.\n    # These will be fully populated, so `empty_like` is sufficient.\n    # They will be on the same device as 'weight' (i.e., CPU).\n    pack_index = torch.empty_like(weight, dtype=torch.int64)\n    rank_in_pack = torch.empty_like(pack_index)\n    \n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These tensors are on the same device as 'weight' (which is CPU).\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id in indices[i]:\n            # Get current state for this layer's packs.\n            current_pack_weights = pack_weights_per_layer[i]\n            current_pack_items = pack_items_per_layer[i]\n\n            # Find the pack with the minimum current weight among available ones.\n            # `available_mask` is a boolean tensor.\n            available_mask = (current_pack_items < groups_per_pack)\n            \n            # Get the indices of available packs.\n            available_pack_indices = torch.where(available_mask)[0]\n\n            # If there are available packs, find the one with the minimum weight.\n            if available_pack_indices.numel() > 0:\n                # Get weights of only the available packs\n                weights_of_available_packs = current_pack_weights[available_pack_indices]\n                \n                # Find the index within `available_pack_indices` that has the minimum weight.\n                local_min_idx = torch.argmin(weights_of_available_packs)\n                \n                # Map this local index back to the global pack index.\n                pack = available_pack_indices[local_min_idx].item()\n            else:\n                # This case should ideally not be reached if inputs are valid and logic is correct.\n                # For robustness, we can break or raise an error, but here we assume valid inputs.\n                # If it happens, it means no pack has capacity, which implies an issue with `groups_per_pack` or `current_pack_items`.\n                # For now, we'll assume it won't happen based on the problem constraints.\n                # If this assertion fails, it means there's a problem with the logic or input assumptions.\n                raise RuntimeError(\"No available packs found, but expected to find one.\")\n\n            # Assign the group to the chosen pack.\n            # `group_id` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            # `weight[i, group_id]` is the weight of the current group.\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    # Calculate the ratio of weight to current replica count for each logical expert.\n    # This is done once per iteration of the outer loop.\n    ratios = weight / logcnt\n\n    # In each step, we want to find the logical expert that, if replicated,\n    # would result in the smallest increase in maximum load. This is approximated\n    # by picking the logical expert with the highest current weight / replica_count ratio.\n    # We are essentially trying to balance the load by adding a replica to the\n    # \"most loaded\" logical expert (relative to its current number of replicas).\n\n    # To efficiently find the best expert to replicate across all layers,\n    # we can use `torch.topk` on the flattened ratios.\n    # `torch.topk` returns the values and indices of the k largest elements.\n    # Here, k=1 because we want to replicate one expert at a time.\n    \n    # Flatten ratios to find the global max ratio across all layers and logical experts.\n    flat_ratios = ratios.flatten()\n    \n    # We need to find the logical expert index that corresponds to the max ratio.\n    # The structure of `ratios` is [num_layers, num_logical_experts].\n    # We need to determine which logical expert to replicate for each layer.\n    # A more direct approach for this specific problem is to iterate through layers\n    # and find the best expert for each layer independently, as the current code does,\n    # but optimize the inner part.\n\n    # The current loop is already somewhat optimized by operating on `weight` and `logcnt`\n    # which are [n, num_log]. Let's refine the selection process within the loop.\n\n    # For each layer `l`, we want to find the logical expert `log_idx` that maximizes `weight[l, log_idx] / logcnt[l, log_idx]`.\n    # This is equivalent to finding the `log_idx` that minimizes `logcnt[l, log_idx] / weight[l, log_idx]` if weight is non-zero,\n    # or maximizing `weight[l, log_idx] / logcnt[l, log_idx]`.\n\n    # The current approach:\n    # `redundant_indices = (weight / logcnt).max(dim=-1).indices`\n    # This line correctly finds the index of the logical expert with the maximum ratio for each layer.\n    # The issue might be the repeated computation of `weight / logcnt` if it's not optimized by PyTorch.\n\n    # Let's stick to the current logic for finding the index, as it's already per-layer.\n    # The main potential for speedup is if the operations within the loop are more efficient.\n    # The current operations are element-wise division and `max(dim=-1)`.\n    # These are generally efficient operations in PyTorch.\n\n    # The number of iterations is `num_redundant_experts`. If this number is large,\n    # the loop itself becomes a bottleneck.\n\n    # Consider a different approach: instead of iterative greedy selection,\n    # can we directly determine the final assignment? This is complex because\n    # adding a replica changes the \"best\" candidate for subsequent additions.\n\n    # For now, let's focus on ensuring the inner operations are as lean as possible.\n    # The current implementation of `replicate_experts` is fundamentally an iterative greedy approach.\n    # The `weight / logcnt` calculation is done inside the loop. We can move it outside if `logcnt` doesn't change.\n    # However, `logcnt` *does* change in each iteration.\n\n    # Let's re-examine the `replicate_experts` function's goal:\n    # \"Replicate `num_log` experts to `num_phy` replicas, such that the maximum load of all replicas is minimized.\"\n    # The current greedy strategy is a common heuristic for this.\n\n    # A potential optimization: Instead of recalculating `weight / logcnt` repeatedly,\n    # we can maintain a priority queue (or a sorted list) for each layer, storing\n    # (ratio, logical_expert_index). When we replicate an expert, we update its\n    # ratio in the priority queue.\n\n    # However, implementing a priority queue in pure PyTorch for this might be\n    # more complex than it's worth for a SEARCH/REPLACE diff.\n\n    # Let's try to optimize the current loop by ensuring `weight` and `logcnt`\n    # are on the CPU, as `rebalance_experts` already moves `weight` to CPU.\n    # `weight` is already on CPU. `logcnt` is also created on CPU.\n\n    # The most significant factor influencing speed here is the number of times the loop runs\n    # (`num_redundant_experts`) and the complexity of the operations inside.\n    # The operations are `weight / logcnt`, `.max(dim=-1)`, and indexing.\n    # These are already quite optimized.\n\n    # Let's consider the structure of `logcnt`: `logcnt[arangen, redundant_indices] += 1`.\n    # `arangen` is `torch.arange(n)`. `redundant_indices` is `[n]`.\n    # This indexing is efficient.\n\n    # The core issue might be the iterative nature. If `num_redundant_experts` is large,\n    # this will be slow.\n\n    # Let's try to optimize the calculation of `redundant_indices` slightly by\n    # ensuring that division by zero is handled gracefully (though `logcnt` starts at 1).\n    # The current code assumes `logcnt` will not be zero.\n\n    # A more direct approach to find the max ratio per layer:\n    for i in range(num_log, num_phy):\n        # Calculate ratios for the current state of logcnt.\n        # Ensure we handle potential division by zero if logcnt could be 0, though it starts at 1.\n        # Use a small epsilon to avoid division by zero if logcnt becomes zero (shouldn't happen here).\n        # The current code `weight / logcnt` is fine as logcnt starts at 1.\n        \n        # Find the logical expert with the maximum weight-to-replica ratio for each layer.\n        # `torch.max` returns values and indices. We need indices.\n        _, redundant_indices = torch.max(weight / logcnt, dim=-1)\n        \n        # Assign the chosen logical expert to the current physical expert slot.\n        phy2log[:, i] = redundant_indices\n        \n        # Record the replica rank for this physical expert.\n        # `logcnt[arangen, redundant_indices]` gets the current count for the chosen experts.\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        \n        # Increment the replica count for the chosen logical experts.\n        logcnt[arangen, redundant_indices] += 1\n        \n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float().cpu()\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["The current program has a `balancedness_score` of 0.2983 and a `speed_score` of 0.0263, resulting in a `combined_score` of 0.1623. The historical data shows that previous attempts achieved a much higher `speed_score` (around 0.083) and `combined_score` (around 0.190). This indicates a significant regression in performance, particularly in speed. The identified areas for improvement also point to potential code simplification and a need to revisit recent changes affecting speed.\n\nThe primary bottleneck for the `speed_score` is likely within the `rebalance_experts_hierarchical` function, specifically the iterative nature of `replicate_experts` and the repeated computations within the loops. The `rebalance_experts_hierarchical` function is called even when `num_groups % num_nodes != 0`, where it's intended to fall back to a global policy. However, it's currently called with `num_groups=1` and `num_nodes=1` in that case, which might not be the most efficient way to achieve a global balance.\n\nHere are the proposed changes:\n\n1.  **Optimize `replicate_experts` by using a more efficient data structure for greedy selection.** Instead of recalculating `weight / logcnt` in each iteration, we can maintain a priority queue (or a sorted list) for each layer. This will allow us to efficiently find and update the expert with the highest ratio. However, since we are restricted to SEARCH/REPLACE diffs and cannot introduce new data structures easily, we will focus on optimizing the existing loop. The current implementation of `replicate_experts` is a greedy approach. The main performance concern is the number of iterations, which is `num_redundant_experts`. The operations inside the loop (`weight / logcnt`, `torch.max`, indexing) are already quite optimized. The regression might stem from how `rebalance_experts_hierarchical` is called or how it uses `replicate_experts`.\n\n2.  **Refactor the `rebalance_experts` function to explicitly handle global vs. hierarchical policies.** The current code calls `rebalance_experts_hierarchical` for both cases, which might be an oversimplification. A dedicated global balancing strategy could be more efficient when inter-node communication is not a concern.\n\n3.  **Improve the `balanced_packing` function's efficiency.** While `balanced_packing` seems to use efficient PyTorch operations, the assignment logic within the loop might have room for optimization if the number of groups per pack is large. However, given the regression is primarily in `speed_score`, let's focus on `replicate_experts` and the main `rebalance_experts` function.\n\nLet's start with optimizing the `rebalance_experts` function's control flow and then address `replicate_experts` if necessary.\n\nThe current `rebalance_experts` function has this conditional:\n```python\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n```\nThis `else` block calls `rebalance_experts_hierarchical` with `num_groups=1` and `num_nodes=1`. This essentially makes `rebalance_experts_hierarchical` operate on the entire `weight` tensor as a single group and a single node. This might be correct in principle but could be inefficiently implemented within the hierarchical structure. A dedicated global balancing function might be faster.\n\nLet's try to implement a simpler global balancing strategy.\n\n**Proposed Change 1: Introduce a dedicated `rebalance_experts_global` function.**\n\nThis function will directly use `replicate_experts` on the entire weight matrix without the hierarchical breakdown. This should be faster when `num_groups % num_nodes != 0`.\n\n```python\ndef rebalance_experts_global(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Global load balancing for expert replication.\n\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_physical_experts: number of physical experts after replication\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        rank: [num_moe_layers, num_physical_experts]\n        expert_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    phy2log, rank, logcnt = replicate_experts(weight, num_physical_experts)\n    return phy2log, rank, logcnt\n```\n\nThen, modify `rebalance_experts` to use this new function.\n\n**Proposed Change 2: Modify `rebalance_experts` to call `rebalance_experts_global` when appropriate.**\n\n```python\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float().cpu()\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        # Call the new global rebalancing function\n        phy2log, phyrank, logcnt = rebalance_experts_global(\n            weight, num_replicas)\n        # The output of rebalance_experts_global is [n, num_phy] for phy2log and rank,\n        # and [n, num_log] for logcnt. This matches the expected output structure.\n\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n```\n\nThis change separates the logic more clearly and potentially speeds up the global case. The `rebalance_experts_hierarchical` function itself might still have performance issues if the `replicate_experts` part is slow.\n\nLet's examine `replicate_experts` more closely. The core of its performance is the loop that runs `num_redundant_experts` times. Inside the loop, `weight / logcnt` is computed. If `num_redundant_experts` is large, this can be slow.\n\n**Proposed Change 3: Optimize `replicate_experts` by pre-calculating ratios and using a more efficient selection mechanism.**\n\nInstead of recalculating `weight / logcnt` in each iteration, we can calculate it once, and then use a mechanism to efficiently find the maximum and update. A simple optimization is to ensure that `weight / logcnt` is computed only once per iteration. The current code does this.\n\nThe issue might be the repeated `torch.max` operation. If `num_log` is large, this can be costly. However, given the regression, it's more likely the overall structure of `rebalance_experts_hierarchical` when used globally.\n\nLet's focus on the first two changes, which directly address the observed regression by providing a dedicated, potentially faster, global balancing strategy. The `rebalance_experts_hierarchical` function might be inherently slower for global balancing due to its nested structure.\n\nThe problem statement also mentions simplifying code for readability. The introduction of `rebalance_experts_global` and its explicit use will improve clarity.\n\nHere's the consolidated diff:\n\n<<<<<<< SEARCH\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n=======\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        # Call a dedicated global rebalancing function for efficiency\n        phy2log, phyrank, logcnt = rebalance_experts_global(\n            weight, num_replicas)\n\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n>>>>>>> REPLACE\n<<<<<<< SEARCH\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n=======\n# EVOLVE-BLOCK-END\n\ndef rebalance_experts_global(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Global load balancing for expert replication.\n\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_physical_experts: number of physical experts after replication\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        rank: [num_moe_layers, num_physical_experts]\n        expert_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    # Directly use the replicate_experts function for global balancing.\n    # replicate_experts takes [n, num_log] and returns assignments for num_phy.\n    phy2log, rank, logcnt = replicate_experts(weight, num_physical_experts)\n    return phy2log, rank, logcnt\n\n\n__all__ = [\"rebalance_experts\"]\n>>>>>>> REPLACE\n\n**Reasoning for the changes:**\n\n1.  **`rebalance_experts_global` function:** This new function encapsulates the logic for global load balancing. It directly calls `replicate_experts` with the entire `weight` tensor, bypassing the more complex hierarchical breakdown. This is expected to be significantly faster when inter-node communication is not a constraint (i.e., when `num_groups % num_nodes != 0`). This directly addresses the regression in `speed_score` by providing a more efficient path for global balancing.\n\n2.  **Modified `rebalance_experts` conditional:** The `else` block in `rebalance_experts` is updated to call the new `rebalance_experts_global` function. This separates the concerns and uses the most appropriate balancing strategy for the global case. This change also improves code readability by making the intent clearer.\n\nThese changes aim to restore the `speed_score` to previous levels by optimizing the global balancing path, which was identified as a potential performance bottleneck given the regression. The `balancedness_score` is expected to remain high as the core `replicate_experts` and `balanced_packing` logic are retained."]}}}