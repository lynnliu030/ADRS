{"id": "0c988c19-a0e2-4170-8d68-9e7473c92866", "code": "# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\nimport torch.jit as jit\n\n# Define the inner loop logic as a TorchScript function\n@jit.script\ndef _jit_assign_group_and_update_batched(\n    group_ids: torch.Tensor,       # 1D tensor [num_layers], actual group IDs for current sorted position\n    group_weights: torch.Tensor,   # 1D tensor [num_layers], weights of these groups\n    pack_weights: torch.Tensor,    # 2D tensor [num_layers, num_packs], current weights of packs\n    pack_items: torch.Tensor,      # 2D tensor [num_layers, num_packs], current item counts of packs\n    groups_per_pack: int,          # Max items per pack\n    inf_val: torch.Tensor          # Pre-created infinity tensor (0-dim)\n) -> tuple[torch.Tensor, torch.Tensor]: # Returns chosen pack indices [num_layers] and ranks [num_layers]\n    \"\"\"\n    JIT-compiled function for the core greedy assignment logic within balanced_packing,\n    batched across layers. This function modifies pack_weights and pack_items in-place.\n    \"\"\"\n    available_mask = (pack_items < groups_per_pack)\n    candidate_weights = torch.where(available_mask, pack_weights, inf_val)\n    \n    # Find the pack with the minimum current weight among available ones for each layer.\n    pack_idx_tensor = torch.argmin(candidate_weights, dim=-1) # [num_layers]\n    \n    # Get rank before update for each layer's chosen pack.\n    num_layers = pack_items.size(0)\n    layer_indices = torch.arange(num_layers, device=pack_items.device)\n    rank_in_pack_tensor = pack_items[layer_indices, pack_idx_tensor] # [num_layers]\n\n    # Update pack weights and item counts for the chosen packs in-place\n    pack_weights[layer_indices, pack_idx_tensor] += group_weights\n    pack_items[layer_indices, pack_idx_tensor] += 1\n    \n    return pack_idx_tensor, rank_in_pack_tensor\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # `weight` is already float from `rebalance_experts`\n    indices = weight.sort(-1, descending=True).indices  # Keep on original device\n    pack_index = torch.full_like(weight,\n                                 fill_value=-1,\n                                 dtype=torch.int64,\n                                 device=weight.device)\n    rank_in_pack = torch.full_like(pack_index,\n                                   fill_value=-1,\n                                   device=weight.device)\n\n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These are on the same device as the input 'weight'.\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Pre-create a tensor for infinity value to avoid repeated creation in loop\n    inf_tensor = torch.tensor(float('inf'), device=weight.device, dtype=weight.dtype)\n\n    # Pre-create layer_indices for advanced indexing, outside the loop\n    layer_indices_all = torch.arange(num_layers, device=weight.device)\n\n    # Iterate over the groups in sorted order. This loop processes all layers at once,\n    # calling the JIT-compiled function once per sorted group position.\n    for group_idx_in_sorted_order in range(num_groups):\n        # Get the actual group_ids for all layers at this sorted position\n        current_group_ids = indices[:, group_idx_in_sorted_order]  # [num_layers]\n        # Get the weights for these groups from the original weight tensor\n        current_group_weights = weight[layer_indices_all, current_group_ids]  # [num_layers]\n\n        # Call the JIT-compiled function for the batched assignment logic\n        chosen_pack_indices, chosen_rank_in_pack = _jit_assign_group_and_update_batched(\n            current_group_ids,\n            current_group_weights,\n            pack_weights_per_layer,  # Passed by reference, modified in-place\n            pack_items_per_layer,    # Passed by reference, modified in-place\n            groups_per_pack,\n            inf_tensor\n        )\n\n        # Assign the groups to the chosen packs based on results from JIT function.\n        # Use advanced indexing to update pack_index and rank_in_pack for all layers simultaneously.\n        pack_index[layer_indices_all, current_group_ids] = chosen_pack_indices\n        rank_in_pack[layer_indices_all, current_group_ids] = chosen_rank_in_pack\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    # Pre-calculate initial average loads for all logical experts\n    # Use float for division and average load calculations\n    avg_loads = weight.float() / logcnt.float() # Optimization: Pre-calculate average loads\n\n    for i in range(num_log, num_phy):\n        # Find the logical expert with the maximum current average load.\n        # This determines which logical expert gets the next replica.\n        _, redundant_indices = torch.max(avg_loads, dim=-1) # Optimization: Use pre-calculated avg_loads\n\n        # Assign the new physical expert to this logical expert\n        phy2log[:, i] = redundant_indices\n        \n        # Determine the replica rank for this new physical expert\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        \n        # Increment the replica count for the chosen logical expert\n        logcnt[arangen, redundant_indices] += 1\n        \n        # Update the average load for the chosen logical expert.\n        # This is crucial for the next iteration's decision.\n        # The average load is the total weight of the logical expert divided by its current replica count.\n        avg_loads[arangen, redundant_indices] = weight[arangen, redundant_indices].float() / logcnt[arangen, redundant_indices].float() # Optimization: Update only the relevant average load\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float()  # Ensure float, but keep on original device (e.g., GPU)\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    # Ensure log2phy is created on the same device as other tensors\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=weight.device,  # Use weight's device\n    )\n    # The scatter_ operation and arange should also use the correct device\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=weight.device).expand(num_layers, -1),  # Use weight's device\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n", "language": "python", "parent_id": "0b7dad60-e1fd-456e-b9b8-f43c17cd20c0", "generation": 9, "timestamp": 1751069251.567414, "iteration_found": 304, "metrics": {"balancedness_score": 0.29830835153138974, "speed_score": 0.08308483803571805, "combined_score": 0.19069659478355389}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 28 lines with 29 lines\nChange 2: Replace 25 lines with 25 lines", "parent_metrics": {"balancedness_score": 0.29830835153138974, "speed_score": 0.031281843494993126, "combined_score": 0.16479509751319144}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve the Mixture-of-Expert models Expert Parallelism Load Balancer (MoE EPLB) expert rearrangement algorithm.\nThis algorithm will take the load metrics recorded by the vLLM server, and rearrange the experts to balance the load. It can make replicas of some experts to achieve better load balancing.\nYour goal will be two-fold: 1. Improve the algorithm to achieve better load balancing; while 2. Improve the algorithm to be more efficient, i.e. reduce the execution time of the algorithm itself, since perfect load balancing is NP-hard.\nThe current algorithm is implemented in the `rebalance_experts` function. ", "user": "# Current Program Information\n- Current performance metrics: - balancedness_score: 0.2983\n- speed_score: 0.0313\n- combined_score: 0.1648\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: balancedness_score, speed_score, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1125, combined_score: 0.2326\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1126, combined_score: 0.2327\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1143, combined_score: 0.2335\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.2335)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1143), Performs well on combined_score (0.2335)\n\n\n### Program 2 (Score: 0.2327)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1126), Performs well on combined_score (0.2327)\n\n\n### Program 3 (Score: 0.2326)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1125), Performs well on combined_score (0.2326)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.2322)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n\n### Program D2 (Score: 0.2325)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.2335, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 32 lines with 17 lines, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 2 (Score: 0.1920, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 3 (Score: 0.1920, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 4 (Score: 0.0171, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 43 lines with 41 lines\nChange 2: Replace 12 lines with 13 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 5 (Score: 0.0000, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 14 lines with 16 lines\nChange 2: Replace 11 lines with 13 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n\n# Current Program\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\nimport torch.jit as jit\n\n# Define the inner loop logic as a TorchScript function\n@jit.script\ndef _jit_assign_group_and_update(\n    group_id_tensor: torch.Tensor, # Group ID for the current item being placed (0-dim tensor)\n    pack_weights_layer_i: torch.Tensor, # 1D tensor representing weights of packs for current layer\n    pack_items_layer_i: torch.Tensor,   # 1D tensor representing item counts of packs for current layer\n    groups_per_pack: int, # Max items per pack\n    inf_val: torch.Tensor, # Pre-created infinity tensor\n    group_weight: torch.Tensor # Weight of the current group (0-dim tensor)\n) -> tuple[torch.Tensor, torch.Tensor]: # Returns chosen pack index and rank within that pack (0-dim tensors)\n    \"\"\"\n    JIT-compiled function for the core greedy assignment logic within balanced_packing.\n    This function modifies pack_weights_layer_i and pack_items_layer_i in-place.\n    \"\"\"\n    available_mask = (pack_items_layer_i < groups_per_pack)\n    candidate_weights = torch.where(available_mask, pack_weights_layer_i, inf_val)\n    \n    # Find the pack with the minimum current weight among available ones.\n    # Keep it as a 0-dim tensor for indexing in JIT-compiled code.\n    pack_idx_tensor = torch.argmin(candidate_weights)\n    \n    # Get rank before update. Keep as 0-dim tensor.\n    rank_in_pack_tensor = pack_items_layer_i[pack_idx_tensor]\n\n    # Update pack weights and item counts for the chosen pack in-place\n    pack_weights_layer_i[pack_idx_tensor] += group_weight\n    pack_items_layer_i[pack_idx_tensor] += 1\n    \n    return pack_idx_tensor, rank_in_pack_tensor\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # `weight` is already float from `rebalance_experts`\n    indices = weight.sort(-1, descending=True).indices  # Keep on original device\n    pack_index = torch.full_like(weight,\n                                 fill_value=-1,\n                                 dtype=torch.int64,\n                                 device=weight.device)\n    rank_in_pack = torch.full_like(pack_index,\n                                   fill_value=-1,\n                                   device=weight.device)\n\n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These are on the same device as the input 'weight'.\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Pre-create a tensor for infinity value to avoid repeated creation in loop\n    inf_tensor = torch.tensor(float('inf'), device=weight.device, dtype=weight.dtype)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # Get 1D slices for the current layer's pack states.\n        # These slices will be passed to the JIT-compiled function and modified in-place.\n        current_pack_weights_slice = pack_weights_per_layer[i]\n        current_pack_items_slice = pack_items_per_layer[i]\n\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id_tensor in indices[i]:\n            # Call the JIT-compiled function for the assignment logic\n            # Pass group_id_tensor directly as a 0-dim tensor\n            chosen_pack_idx_tensor, chosen_rank_in_pack_tensor = _jit_assign_group_and_update(\n                group_id_tensor,\n                current_pack_weights_slice,\n                current_pack_items_slice,\n                groups_per_pack,\n                inf_tensor,\n                weight[i, group_id_tensor] # Pass the weight of the current group as a 0-dim tensor\n            )\n\n            # Assign the group to the chosen pack based on results from JIT function.\n            # `group_id_tensor` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id_tensor] = chosen_pack_idx_tensor\n            rank_in_pack[i, group_id_tensor] = chosen_rank_in_pack_tensor\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    # Pre-calculate initial average loads for all logical experts\n    # Use float for division and average load calculations\n    avg_loads = weight.float() / logcnt.float() # Optimization: Pre-calculate average loads\n\n    for i in range(num_log, num_phy):\n        # Find the logical expert with the maximum current average load.\n        # This determines which logical expert gets the next replica.\n        _, redundant_indices = torch.max(avg_loads, dim=-1) # Optimization: Use pre-calculated avg_loads\n\n        # Assign the new physical expert to this logical expert\n        phy2log[:, i] = redundant_indices\n        \n        # Determine the replica rank for this new physical expert\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        \n        # Increment the replica count for the chosen logical expert\n        logcnt[arangen, redundant_indices] += 1\n        \n        # Update the average load for the chosen logical expert.\n        # This is crucial for the next iteration's decision.\n        # The average load is the total weight of the logical expert divided by its current replica count.\n        avg_loads[arangen, redundant_indices] = weight[arangen, redundant_indices].float() / logcnt[arangen, redundant_indices].float() # Optimization: Update only the relevant average load\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    weight = weight.float()  # Ensure float, but keep on original device (e.g., GPU)\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    # Ensure log2phy is created on the same device as other tensors\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=weight.device,  # Use weight's device\n    )\n    # The scatter_ operation and arange should also use the correct device\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=weight.device).expand(num_layers, -1),  # Use weight's device\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["The current program's performance, especially `speed_score`, has regressed significantly compared to previous successful attempts. The core of the problem lies in the `balanced_packing` function, specifically how the JIT-compiled helper function `_jit_assign_group_and_update` is called.\n\nThe original implementation of `balanced_packing` iterates through `num_layers` in a Python loop, and for each layer, it then iterates through `num_groups` (the sorted experts) in another Python loop. This means the JIT-compiled function `_jit_assign_group_and_update` is called `num_layers * num_groups` times. While the JIT function itself is fast, the overhead of repeatedly calling it from Python can become a significant bottleneck, especially when `num_layers` is large.\n\nThe key insight for improvement is that the greedy packing logic for each layer is independent of other layers. Therefore, we can process all layers simultaneously in a batched manner within the JIT-compiled function.\n\n**Proposed Changes:**\n\n1.  **Modify `_jit_assign_group_and_update` to `_jit_assign_group_and_update_batched`:**\n    *   Change its signature to accept 1D tensors for `group_ids` and `group_weights` (representing data for all layers at a specific sorted position) and 2D tensors for `pack_weights` and `pack_items` (representing pack states for all layers).\n    *   Adjust the internal tensor operations (e.g., `torch.argmin(..., dim=-1)`, advanced indexing) to work on these batched dimensions. This will allow the JIT function to perform the greedy assignment for all layers in parallel for a given sorted group.\n\n2.  **Refactor `balanced_packing` to use the batched JIT function:**\n    *   Remove the outer `for i in range(num_layers)` Python loop.\n    *   Keep the inner `for group_idx_in_sorted_order in range(num_groups)` loop.\n    *   Inside this loop, prepare 1D tensors (`current_group_ids`, `current_group_weights`) that represent the current group (in sorted order) for *all* layers.\n    *   Call the new `_jit_assign_group_and_update_batched` function once per iteration of this `num_groups` loop, passing the batched tensors.\n    *   Use advanced indexing to assign the results back to `pack_index` and `rank_in_pack` for all layers simultaneously.\n\n**Expected Impact:**\n\n*   **Speed Score (Efficiency):** This is the primary target for this optimization. By reducing the number of Python-to-JIT calls from `num_layers * num_groups` to just `num_groups`, we significantly reduce Python overhead and leverage PyTorch's vectorized operations more effectively on the GPU. This should lead to a substantial improvement in execution time.\n*   **Balancedness Score:** The underlying greedy algorithm for balanced packing remains identical. The order of processing experts (sorted by weight) and the criteria for assigning them to packs are preserved. Therefore, the `balancedness_score` should remain the same or negligibly change due to floating-point precision differences, indicating that the load balancing quality is maintained.\n*   **Code Readability/Maintainability:** While the JIT-compiled function becomes slightly more complex due to handling batched dimensions, the main `balanced_packing` function becomes simpler by removing an entire Python loop, which improves overall clarity of the high-level logic.\n\nThis change directly addresses the identified performance regression by reducing redundant Python loop iterations and maximizing GPU utilization through batching.\n\n```python\n<<<<<<< SEARCH\n@jit.script\ndef _jit_assign_group_and_update(\n    group_id_tensor: torch.Tensor, # Group ID for the current item being placed (0-dim tensor)\n    pack_weights_layer_i: torch.Tensor, # 1D tensor representing weights of packs for current layer\n    pack_items_layer_i: torch.Tensor,   # 1D tensor representing item counts of packs for current layer\n    groups_per_pack: int, # Max items per pack\n    inf_val: torch.Tensor, # Pre-created infinity tensor\n    group_weight: torch.Tensor # Weight of the current group (0-dim tensor)\n) -> tuple[torch.Tensor, torch.Tensor]: # Returns chosen pack index and rank within that pack (0-dim tensors)\n    \"\"\"\n    JIT-compiled function for the core greedy assignment logic within balanced_packing.\n    This function modifies pack_weights_layer_i and pack_items_layer_i in-place.\n    \"\"\"\n    available_mask = (pack_items_layer_i < groups_per_pack)\n    candidate_weights = torch.where(available_mask, pack_weights_layer_i, inf_val)\n    \n    # Find the pack with the minimum current weight among available ones.\n    # Keep it as a 0-dim tensor for indexing in JIT-compiled code.\n    pack_idx_tensor = torch.argmin(candidate_weights)\n    \n    # Get rank before update. Keep as 0-dim tensor.\n    rank_in_pack_tensor = pack_items_layer_i[pack_idx_tensor]\n\n    # Update pack weights and item counts for the chosen pack in-place\n    pack_weights_layer_i[pack_idx_tensor] += group_weight\n    pack_items_layer_i[pack_idx_tensor] += 1\n    \n    return pack_idx_tensor, rank_in_pack_tensor\n=======\n@jit.script\ndef _jit_assign_group_and_update_batched(\n    group_ids: torch.Tensor,       # 1D tensor [num_layers], actual group IDs for current sorted position\n    group_weights: torch.Tensor,   # 1D tensor [num_layers], weights of these groups\n    pack_weights: torch.Tensor,    # 2D tensor [num_layers, num_packs], current weights of packs\n    pack_items: torch.Tensor,      # 2D tensor [num_layers, num_packs], current item counts of packs\n    groups_per_pack: int,          # Max items per pack\n    inf_val: torch.Tensor          # Pre-created infinity tensor (0-dim)\n) -> tuple[torch.Tensor, torch.Tensor]: # Returns chosen pack indices [num_layers] and ranks [num_layers]\n    \"\"\"\n    JIT-compiled function for the core greedy assignment logic within balanced_packing,\n    batched across layers. This function modifies pack_weights and pack_items in-place.\n    \"\"\"\n    available_mask = (pack_items < groups_per_pack)\n    candidate_weights = torch.where(available_mask, pack_weights, inf_val)\n    \n    # Find the pack with the minimum current weight among available ones for each layer.\n    pack_idx_tensor = torch.argmin(candidate_weights, dim=-1) # [num_layers]\n    \n    # Get rank before update for each layer's chosen pack.\n    num_layers = pack_items.size(0)\n    layer_indices = torch.arange(num_layers, device=pack_items.device)\n    rank_in_pack_tensor = pack_items[layer_indices, pack_idx_tensor] # [num_layers]\n\n    # Update pack weights and item counts for the chosen packs in-place\n    pack_weights[layer_indices, pack_idx_tensor] += group_weights\n    pack_items[layer_indices, pack_idx_tensor] += 1\n    \n    return pack_idx_tensor, rank_in_pack_tensor\n>>>>>>> REPLACE\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # `weight` is already float from `rebalance_experts`\n    indices = weight.sort(-1, descending=True).indices  # Keep on original device\n    pack_index = torch.full_like(weight,\n                                 fill_value=-1,\n                                 dtype=torch.int64,\n                                 device=weight.device)\n    rank_in_pack = torch.full_like(pack_index,\n                                   fill_value=-1,\n                                   device=weight.device)\n\n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These are on the same device as the input 'weight'.\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Pre-create a tensor for infinity value to avoid repeated creation in loop\n    inf_tensor = torch.tensor(float('inf'), device=weight.device, dtype=weight.dtype)\n\n<<<<<<< SEARCH\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # Get 1D slices for the current layer's pack states.\n        # These slices will be passed to the JIT-compiled function and modified in-place.\n        current_pack_weights_slice = pack_weights_per_layer[i]\n        current_pack_items_slice = pack_items_per_layer[i]\n\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id_tensor in indices[i]:\n            # Call the JIT-compiled function for the assignment logic\n            # Pass group_id_tensor directly as a 0-dim tensor\n            chosen_pack_idx_tensor, chosen_rank_in_pack_tensor = _jit_assign_group_and_update(\n                group_id_tensor,\n                current_pack_weights_slice,\n                current_pack_items_slice,\n                groups_per_pack,\n                inf_tensor,\n                weight[i, group_id_tensor] # Pass the weight of the current group as a 0-dim tensor\n            )\n\n            # Assign the group to the chosen pack based on results from JIT function.\n            # `group_id_tensor` is a 0-dim tensor, which can be directly used for indexing.\n            pack_index[i, group_id_tensor] = chosen_pack_idx_tensor\n            rank_in_pack[i, group_id_tensor] = chosen_rank_in_pack_tensor\n=======\n    # Pre-create layer_indices for advanced indexing, outside the loop\n    layer_indices_all = torch.arange(num_layers, device=weight.device)\n\n    # Iterate over the groups in sorted order. This loop processes all layers at once,\n    # calling the JIT-compiled function once per sorted group position.\n    for group_idx_in_sorted_order in range(num_groups):\n        # Get the actual group_ids for all layers at this sorted position\n        current_group_ids = indices[:, group_idx_in_sorted_order]  # [num_layers]\n        # Get the weights for these groups from the original weight tensor\n        current_group_weights = weight[layer_indices_all, current_group_ids]  # [num_layers]\n\n        # Call the JIT-compiled function for the batched assignment logic\n        chosen_pack_indices, chosen_rank_in_pack = _jit_assign_group_and_update_batched(\n            current_group_ids,\n            current_group_weights,\n            pack_weights_per_layer,  # Passed by reference, modified in-place\n            pack_items_per_layer,    # Passed by reference, modified in-place\n            groups_per_pack,\n            inf_tensor\n        )\n\n        # Assign the groups to the chosen packs based on results from JIT function.\n        # Use advanced indexing to update pack_index and rank_in_pack for all layers simultaneously.\n        pack_index[layer_indices_all, current_group_ids] = chosen_pack_indices\n        rank_in_pack[layer_indices_all, current_group_ids] = chosen_rank_in_pack\n>>>>>>> REPLACE\n    return pack_index, rank_in_pack\n```"]}}}