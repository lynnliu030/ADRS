{"id": "91ece0a5-aef6-4756-9a2d-40a271f5f361", "code": "# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort weights and get indices\n    sorted_weights, sorted_indices = weight.float().sort(-1, descending=True)\n\n    # Store device from input weight\n    device = weight.device\n\n    # Initialize pack_index and rank_in_pack on the correct device\n    pack_index = torch.full((num_layers, num_groups), -1, dtype=torch.int64, device=device)\n    rank_in_pack = torch.full((num_layers, num_groups), -1, dtype=torch.int64, device=device)\n\n    # Initialize pack_loads and pack_counts on the correct device\n    pack_loads = torch.zeros(num_layers, num_packs, dtype=torch.float32, device=device)\n    pack_counts = torch.zeros(num_layers, num_packs, dtype=torch.int64, device=device)\n\n    # Iterate through sorted items and assign them to the least loaded pack\n    for i in range(num_layers):\n        for g in range(num_groups):\n            available_packs = pack_counts[i] < groups_per_pack\n            if not available_packs.any():\n                continue\n\n            current_pack_loads = pack_loads[i][available_packs]\n            current_pack_indices = torch.arange(num_packs, device=device)[available_packs]\n\n            min_load_idx_in_available = torch.argmin(current_pack_loads)\n            chosen_pack_idx = current_pack_indices[min_load_idx_in_available]\n\n            original_group_idx = sorted_indices[i, g]\n\n            pack_index[i, original_group_idx] = chosen_pack_idx\n            rank_in_pack[i, original_group_idx] = pack_counts[i, chosen_pack_idx]\n\n            pack_loads[i, chosen_pack_idx] += weight[i, original_group_idx]\n            pack_counts[i, chosen_pack_idx] += 1\n\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Unified rebalancing logic:\n    # The hierarchical approach is used when num_groups is divisible by num_nodes.\n    # Otherwise, it falls back to a global balancing strategy by setting num_nodes=1 and num_groups=1.\n    # We can simplify this by directly implementing the global strategy when needed,\n    # or by ensuring the hierarchical logic correctly handles num_nodes=1.\n\n    # Determine the effective number of groups and nodes for balancing.\n    # If num_groups is not divisible by num_nodes, we treat it as a single group\n    # across all nodes for the first level of balancing.\n    effective_num_groups = num_groups if num_groups % num_nodes == 0 else 1\n    effective_num_nodes = num_nodes if num_groups % num_nodes == 0 else 1\n\n    # If num_groups % num_nodes != 0, we effectively want to balance across all groups as if they were one large group.\n    # And then balance those groups onto nodes.\n    # This is equivalent to setting num_nodes = 1 and num_groups = 1 in the hierarchical logic.\n    # Let's adjust parameters for a unified call to a simplified hierarchical logic.\n\n    # Simplified approach:\n    # First, balance logical experts across groups within layers.\n    # Then, balance these groups across nodes.\n    # Finally, replicate experts within nodes to balance across GPUs.\n\n    # Step 1: Balance logical experts into groups.\n    # If num_groups == 1, this step is trivial and doesn't change anything.\n    group_size = num_logical_experts // num_groups\n    tokens_per_group_layer = weight.unflatten(-1, (num_groups, group_size)).sum(-1) # [num_layers, num_groups]\n\n    # Pack groups onto nodes.\n    # If num_groups % num_nodes != 0, we effectively want to balance all groups onto a single 'node' conceptually,\n    # and then distribute those across the actual num_nodes.\n    # Let's consider the case where num_groups is not divisible by num_nodes.\n    # The original code uses num_nodes for packing groups. If num_groups < num_nodes, this packing might be problematic.\n    # A more robust approach is to balance groups onto num_nodes, and if num_groups < num_nodes, some nodes get 0 groups.\n    # The current `balanced_packing` assumes num_groups is divisible by num_packs (num_nodes here).\n\n    # Let's redefine the packing strategy:\n    # We want to pack 'num_groups' items (groups) into 'num_nodes' packs.\n    # If num_groups is not divisible by num_nodes, balanced_packing needs to handle it.\n    # The current `balanced_packing` asserts `num_groups % num_packs == 0`.\n    # This means if num_groups % num_nodes != 0, the original code calls `rebalance_experts_hierarchical` with num_nodes=1, num_groups=1.\n    # This is a global balancing strategy.\n\n    # Let's ensure the function call correctly reflects the intended strategy.\n    # If num_groups % num_nodes == 0, use hierarchical.\n    # Otherwise, use global (which is hierarchical with num_nodes=1, num_groups=1).\n\n    if num_groups % num_nodes == 0:\n        # Hierarchical balancing: balance groups to nodes, then nodes to GPUs.\n        # Step 1: Pack groups to nodes.\n        group_pack_index, group_rank_in_pack = balanced_packing(tokens_per_group_layer, num_nodes)\n        # Map logical experts to \"middle\" logical experts (logical experts per node)\n        # log2mlog: [num_layers, num_logical_experts] -> maps logical expert to its middle logical expert\n        mlog2log_map = (((group_pack_index * groups_per_node + group_rank_in_pack) * group_size).unsqueeze(-1) +\n                        torch.arange(group_size, dtype=torch.int64, device=group_pack_index.device)).flatten(-2)\n        # mlog2log_map: [num_layers, num_logical_experts] maps each logical expert to its \"middle\" logical expert ID.\n        # We need the inverse mapping for `weight.gather` later.\n        # This inverse mapping is tricky because multiple logical experts can map to the same middle expert.\n        # The `inverse` function is designed for permutations, which isn't directly applicable here.\n\n        # Let's rethink the mapping and replication.\n        # We have `num_layers` of `weight` ([num_layers, num_logical_experts]).\n        # First, we group logical experts: `tokens_per_group_layer` ([num_layers, num_groups]).\n        # Then, we pack these groups onto `num_nodes`: `group_pack_index` ([num_layers, num_groups]) and `group_rank_in_pack` ([num_layers, num_groups]).\n        # This means `num_groups` are assigned to `num_nodes`. `groups_per_node` groups per node.\n        # So, we have `num_layers * num_nodes` \"node-level\" expert weight tensors.\n        # Each node-level tensor has `num_logical_experts // num_nodes` \"middle\" experts.\n\n        # Let's create the \"middle\" logical experts' weights.\n        # `mlog_weights`: [num_layers * num_nodes, num_logical_experts // num_nodes]\n        # This requires carefully gathering weights.\n        # The `mlog2log` mapping needs to be constructed correctly.\n\n        # `mlog2log_map` assigns each logical expert to a conceptual \"middle\" logical expert.\n        # e.g., if num_logical_experts=8, num_groups=2, group_size=4.\n        # Group 0 contains logical experts 0,1,2,3. Group 1 contains 4,5,6,7.\n        # If num_nodes=2, groups_per_node=1.\n        # Group 0 (log experts 0-3) goes to node 0. Group 1 (log experts 4-7) goes to node 1.\n        # Node 0 has 1 middle expert. Node 1 has 1 middle expert.\n        # Middle expert for node 0 is logical experts 0-3. Middle expert for node 1 is 4-7.\n        # The mapping `mlog2log_map` should map:\n        # 0,1,2,3 -> 0 (on node 0)\n        # 4,5,6,7 -> 0 (on node 1)\n        # The current `mlog2log` calculation seems to be constructing the indices of the middle experts.\n        # `log2mlog` is `[num_layers, num_logical_experts]`.\n        # It maps logical expert ID to its index within the middle expert group.\n        # The `mlog2log` is the inverse of this index mapping.\n\n        # Let's reconstruct `mlog2log` and `tokens_per_mlog` more directly.\n        # For each layer, we have `num_groups` of `group_size`.\n        # `group_pack_index`: [num_layers, num_groups] - node assignment for each group.\n        # `group_rank_in_pack`: [num_layers, num_groups] - index of group within the node's assigned groups.\n\n        # We want to create `num_layers * num_nodes` \"node-level\" expert groups.\n        # Each node-level group corresponds to a set of original logical experts.\n        # The `tokens_per_mlog` should be `[num_layers * num_nodes, num_logical_experts // num_nodes]`.\n\n        # Calculate the logical expert index for each \"middle\" expert.\n        # `mlog_expert_indices`: [num_layers, num_nodes, groups_per_node, group_size]\n        # This is becoming complex. Let's simplify the conceptualization.\n\n        # Simpler approach:\n        # 1. Pack groups to nodes. This determines which logical experts are grouped together on each node.\n        # `group_pack_index`: [num_layers, num_groups] -> node ID for each group.\n        # `group_rank_in_pack`: [num_layers, num_groups] -> rank within node for each group.\n\n        # 2. For each node, create a combined weight tensor for the logical experts assigned to it.\n        #    The number of logical experts per node will be `groups_per_node * group_size`.\n        #    We need to gather the weights of logical experts based on `group_pack_index`.\n\n        # Let's create a mapping from (layer, node) to a contiguous block of logical experts.\n        # `node_logical_expert_map`: [num_layers, num_nodes, groups_per_node * group_size]\n\n        # This requires constructing a permutation for each layer and node.\n        # The original `mlog2log` calculation is:\n        # `((group_pack_index * groups_per_node + group_rank_in_pack) * group_size)`\n        # This forms a base index for each logical expert.\n        # `log2mlog` is this base index + `arange(group_size)`.\n        # `mlog2log` is the inverse of `log2mlog`.\n\n        # Let's directly construct the `tokens_per_mlog` and `mlog2log`.\n        # `tokens_per_mlog` is the weight tensor for experts *within* each node, considering the grouped logical experts.\n        # The number of such \"middle\" experts per node is `groups_per_node * group_size`.\n\n        # For each layer:\n        # `group_pack_index` tells us which node each group belongs to.\n        # `group_rank_in_pack` tells us the order within the node.\n\n        # Let's create `num_layers * num_nodes` tensors, each of size `groups_per_node * group_size`.\n        # `mlog_weights`: [num_layers * num_nodes, groups_per_node * group_size]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, groups_per_node * group_size]\n\n        # Example: num_layers=1, num_logical_experts=8, num_groups=2, group_size=4, num_nodes=2, groups_per_node=1, num_gpus=2, phy_experts_per_gpu=1.\n        # weight: [1, 8]\n        # tokens_per_group_layer: [1, 2] (sum of weights for group 0 and group 1)\n        # balanced_packing(tokens_per_group_layer, num_nodes=2) ->\n        #   group_pack_index: [1, 2] -> [[0, 1]] (group 0 to node 0, group 1 to node 1)\n        #   group_rank_in_pack: [1, 2] -> [[0, 0]] (rank 0 within node for both groups)\n        # groups_per_node = 1, group_size = 4.\n\n        # Middle expert for node 0 is logical experts 0,1,2,3.\n        # Middle expert for node 1 is logical experts 4,5,6,7.\n        # `mlog_to_orig_log_map` for node 0 should be [0,1,2,3].\n        # `mlog_to_orig_log_map` for node 1 should be [4,5,6,7].\n\n        # The original `mlog2log` calculation:\n        # `base_idx = (group_pack_index * groups_per_node + group_rank_in_pack) * group_size`\n        # `base_idx`: [1, 2] -> [[0*1+0, 1*1+0]] * 4 = [[0, 4]]\n        # `log2mlog`: `base_idx.unsqueeze(-1) + torch.arange(group_size)`\n        # `log2mlog`: [[0, 4]].unsqueeze(-1) + [0,1,2,3] -> [[0,1,2,3], [4,5,6,7]]\n        # This `log2mlog` is `[num_layers, num_logical_experts]`. It maps original logical expert ID to its \"middle\" expert index.\n        # The original logic uses `weight.gather(-1, mlog2log)` which is incorrect here.\n        # `weight.gather(-1, mlog2log)` expects `mlog2log` to be `[num_layers, num_middle_experts]` where `num_middle_experts` is the number of columns in `weight`.\n        # The shape should be `[num_layers, num_logical_experts]` if we are gathering from `weight` which is `[num_layers, num_logical_experts]`.\n        # The `mlog2log` calculated here has shape `[num_layers, num_logical_experts]`.\n        # It maps `original_logical_expert_id` to `middle_expert_id`.\n        # We need to gather `weight[layer, original_logical_expert_id]` and group them by `middle_expert_id`.\n\n        # Let's create `mlog_weights` directly.\n        # `mlog_weights`: [num_layers * num_nodes, groups_per_node * group_size]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, groups_per_node * group_size]\n\n        # Vectorized construction of mlog_weights and mlog_to_orig_log_map\n        # This replaces the nested Python loops for better performance.\n\n        # Calculate the new \"middle expert group\" index for each original group.\n        # This new index combines node assignment and rank within the node.\n        # Shape: [num_layers, num_groups]\n        new_group_idx = group_pack_index * groups_per_node + group_rank_in_pack\n\n        # Create a tensor that maps new group index back to original group index.\n        # This is a permutation tensor for groups within each layer.\n        # Shape: [num_layers, num_nodes * groups_per_node] (which is num_groups)\n        group_permutation = torch.empty((num_layers, num_groups),\n                                        dtype=torch.int64,\n                                        device=weight.device)\n        group_permutation.scatter_(1, new_group_idx,\n                                   torch.arange(num_groups, device=weight.device).expand(num_layers, -1))\n\n        # Reshape original weights to [num_layers, num_groups, group_size]\n        weight_reshaped = weight.view(num_layers, num_groups, group_size)\n\n        # Gather weights based on the group_permutation\n        # The .gather() operation reorders the groups according to the permutation.\n        # Shape: [num_layers, num_groups, group_size]\n        mlog_weights_ordered = weight_reshaped.gather(1, group_permutation.unsqueeze(-1).expand(-1, -1, group_size))\n        \n        # Reshape to [num_layers, num_nodes, groups_per_node * group_size]\n        # This is the final mlog_weights.\n        mlog_weights = mlog_weights_ordered.view(num_layers, num_nodes, groups_per_node * group_size)\n\n        # Construct mlog_to_orig_log_map similarly\n        # Original logical expert IDs, reshaped to [num_groups, group_size]\n        orig_log_ids_reshaped = torch.arange(num_logical_experts,\n                                             dtype=torch.int64,\n                                             device=weight.device).view(num_groups, group_size)\n        \n        # Expand to [num_layers, num_groups, group_size] and gather\n        mlog_to_orig_log_map_ordered = orig_log_ids_reshaped.unsqueeze(0).expand(num_layers, -1, -1).gather(\n            1, group_permutation.unsqueeze(-1).expand(-1, -1, group_size))\n        \n        # Reshape to [num_layers, num_nodes, groups_per_node * group_size]\n        mlog_to_orig_log_map = mlog_to_orig_log_map_ordered.view(\n            num_layers, num_nodes, groups_per_node * group_size)\n\n        # Now, `mlog_weights`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n        # And `mlog_to_orig_log_map`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n\n        # Step 2: Replicate experts within each node.\n        # `mlog_weights` are the weights for experts within each node.\n        # `num_physical_experts // num_nodes` is the target number of physical experts per node.\n        # `replicate_experts` takes `[X, num_log]` and returns `phy2log`, `rank`, `logcnt`.\n        # Here, X = num_layers * num_nodes, num_log = num_logical_experts // num_nodes.\n        # So, we need to reshape `mlog_weights` and `mlog_to_orig_log_map`.\n\n        num_middle_experts = num_logical_experts // num_nodes\n        reshaped_mlog_weights = mlog_weights.view(-1, num_middle_experts)\n        reshaped_mlog_to_orig_log_map = mlog_to_orig_log_map.view(-1, num_middle_experts)\n\n        # `phy2mlog`: [num_layers * num_nodes, num_physical_experts // num_nodes]\n        # `phyrank`: [num_layers * num_nodes, num_physical_experts // num_nodes]\n        # `mlogcnt`: [num_layers * num_nodes, num_middle_experts]\n        phy2mlog, phyrank, mlogcnt = replicate_experts(\n            reshaped_mlog_weights, num_physical_experts // num_nodes)\n\n        # Step 3: Pack physical experts to GPUs.\n        # `phy2mlog` maps physical experts (within each node) to middle logical experts.\n        # `phy2mlog` is `[num_layers * num_nodes, num_phy_per_node]`\n        # `phyrank` is `[num_layers * num_nodes, num_phy_per_node]`\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`\n\n        # We need to map these back to global physical expert indices.\n        # The `phy2mlog` refers to the *middle* logical experts.\n        # The `replicate_experts` function returns `phy2log` which is the *original* logical expert ID.\n        # So, `phy2mlog` from `replicate_experts` is actually `[num_layers*num_nodes, num_phy_per_node]`,\n        # where each entry is an index into the *middle* logical experts.\n\n        # We need to map these `phy2mlog` indices back to original logical expert IDs.\n        # `reshaped_mlog_to_orig_log_map`: [num_layers * num_nodes, num_middle_experts]\n        # `final_phy2log_per_node`: [num_layers * num_nodes, num_phy_per_node]\n        final_phy2log_per_node = reshaped_mlog_to_orig_log_map.gather(-1, phy2mlog)\n\n        # `phyrank` is the replica rank for the middle experts.\n        # We need to combine this with `mlogcnt` to get the final `logcnt`.\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`. It tells us how many replicas each middle expert has.\n        # The `phyrank` is the rank of the replica for a *specific* middle expert.\n\n        # The `logcnt` returned by `replicate_experts` is `[X, num_log]`, which is `[num_layers*num_nodes, num_middle_experts]`.\n        # We need to map this back to `[num_layers, num_logical_experts]`.\n\n        # Let's use the `mlogcnt` directly to get the total count of replicas for each logical expert.\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`.\n        # We need to sum/average counts based on the `mlog_to_orig_log_map`.\n\n        # Let's refine the output mapping:\n        # `physical_to_logical_map`: [num_layers, num_replicas]\n        # `logical_to_physical_map`: [num_layers, num_logical_experts, X]\n        # `expert_count`: [num_layers, num_logical_experts]\n\n        # `final_phy2log_per_node`: [num_layers * num_nodes, num_phy_per_node]\n        # This is the logical expert ID for each physical expert *within its node*.\n        # We need to convert this to global physical expert IDs.\n        # Physical experts are grouped by GPU. `num_gpus` total, `num_gpus // num_nodes` per node.\n        # `phy_experts_per_gpu` = `num_physical_experts // num_gpus`.\n        # So, each GPU has `phy_experts_per_gpu` physical experts.\n        # Each Node has `num_gpus // num_nodes` GPUs.\n        # Total physical experts per node = `(num_gpus // num_nodes) * phy_experts_per_gpu`\n        # This should equal `num_physical_experts // num_nodes`.\n\n        # `final_phy2log_per_node`: [num_layers, num_nodes, num_phy_per_node]\n        # `final_rank_per_node`: [num_layers, num_nodes, num_phy_per_node] (this is `phyrank`)\n\n        # We need to flatten this to `[num_layers, num_physical_experts]`.\n        # `num_physical_experts = num_nodes * num_phy_per_node`.\n\n        # `final_phy2log_global`: [num_layers, num_physical_experts]\n        # `final_rank_global`: [num_layers, num_physical_experts]\n\n        # The `mlog_to_orig_log_map` correctly maps middle experts to original logical experts.\n        # `phy2mlog` maps physical experts (within a node) to middle experts.\n        # So, `phy2mlog.gather(-1, final_phy2log_per_node)` gives the logical expert ID.\n        # This is what `final_phy2log_per_node` already represents.\n\n        # Let's rename `final_phy2log_per_node` to `phy2log_per_node` for clarity.\n        phy2log_per_node = final_phy2log_per_node\n        rank_per_node = phyrank # [num_layers * num_nodes, num_phy_per_node]\n\n        # Flatten these to get global mappings for physical experts.\n        phy2log_global = phy2log_per_node.view(num_layers, -1)\n        rank_global = rank_per_node.view(num_layers, -1)\n\n        # Now, let's construct the `logcnt` (expert_count).\n        # `mlogcnt`: [num_layers * num_nodes, num_middle_experts]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, num_middle_experts]\n\n        # We need to aggregate `mlogcnt` based on `mlog_to_orig_log_map`.\n        # For each logical expert, its total count is the sum of counts of its constituent middle experts.\n        # This aggregation needs to be done carefully.\n\n        # Let's create a tensor for `logcnt`: [num_layers, num_logical_experts] initialized to zeros.\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n\n        # The expert_count aggregation:\n        # mlog_to_orig_log_map: [num_layers, num_nodes, num_middle_experts]\n        # mlogcnt: [num_layers * num_nodes, num_middle_experts]\n        # Flatten mlog_to_orig_log_map and mlogcnt to [num_layers, N] for scatter_add_\n        mlog_to_orig_log_map_flat = mlog_to_orig_log_map.view(num_layers, -1)\n        mlogcnt_reshaped = mlogcnt.view(num_layers, -1)\n\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n        # Use scatter_add_ to efficiently aggregate counts for each logical expert across layers\n        expert_count.scatter_add_(1, mlog_to_orig_log_map_flat, mlogcnt_reshaped)\n\n        # The logical_to_physical_map construction is handled by the outer rebalance_experts function.\n        # So, this function only needs to return the global mappings and counts.\n        return phy2log_global, rank_global, expert_count\n\n    # The `rebalance_experts_hierarchical` function is designed to handle both\n    # hierarchical and global balancing based on its input parameters.\n    # The outer `rebalance_experts` function already ensures that the parameters\n    # passed to `rebalance_experts_hierarchical` result in `num_groups % num_nodes == 0`\n    # within `rebalance_experts_hierarchical`'s scope (either original values or 1, 1).\n    # Therefore, this `else` block is unreachable and can be removed.\n    pass\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    # Store original device for computation and ensure float type\n    device = weight.device\n    weight = weight.float()\n\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy (by setting num_groups=1, num_nodes=1)\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    \n    # The `rebalance_experts_hierarchical` function returns:\n    # - `phy2log`: [num_layers, num_replicas], maps physical expert index to logical expert ID.\n    # - `phyrank`: [num_layers, num_replicas], maps physical expert index to its replica rank.\n    # - `logcnt`: [num_layers, num_logical_experts], number of replicas for each logical expert.\n\n    # Construct `logical_to_physical_map`: [layers, num_logical_experts, X]\n    # where X is the maximum number of replicas for any logical expert.\n\n    num_physical_experts_total = num_replicas\n\n    # Fix: Calculate max_replicas_per_expert based on actual counts from logcnt.\n    # This ensures the tensor for logical_to_physical_map is correctly sized.\n    if num_logical_experts == 0 or logcnt.numel() == 0:\n        max_replicas_per_expert = 0\n    else:\n        max_replicas_per_expert = logcnt.max().item()\n        # In case all experts have only one replica (e.g., num_replicas == num_logical_experts)\n        # and logcnt was initialized to 1s, max() would be 1.\n        # If num_logical_experts > 0 but max_replicas_per_expert became 0 (unlikely with replicate_experts),\n        # ensure it's at least 1 if physical experts exist.\n        if max_replicas_per_expert == 0 and num_physical_experts_total > 0:\n            max_replicas_per_expert = 1\n\n    # Initialize the map with -1 (indicating no physical expert assigned)\n    logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert),\n                                         -1, dtype=torch.int64, device=device)\n\n    # Create physical expert IDs to scatter\n    physical_expert_ids = torch.arange(num_physical_experts_total, dtype=torch.int64, device=device) # Use stored device\n\n    # Expand physical_expert_ids to match the shape of phy2log and phyrank for broadcasting\n    # Shape: [num_layers, num_physical_experts_total]\n    physical_expert_ids_expanded = physical_expert_ids.unsqueeze(0).expand(num_layers, num_physical_experts_total)\n\n    # Create layer indices for advanced indexing\n    layer_indices = torch.arange(num_layers, dtype=torch.int64, device=device).unsqueeze(1)\n    \n    # Assign physical expert IDs to the correct positions in the map using advanced indexing.\n    # This replaces the Python loops for significant performance improvement.\n    logical_to_physical_map[layer_indices, phy2log, phyrank] = physical_expert_ids_expanded\n\n    # The `logcnt` (expert_count) is already computed by the balancing functions.\n    # `phy2log` is the first return value.\n    return phy2log, logical_to_physical_map, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n", "language": "python", "parent_id": "d0a2b2a2-34be-4101-9bc9-3c514b68ff4b", "generation": 7, "timestamp": 1751070021.6144376, "iteration_found": 326, "metrics": {"balancedness_score": 0.29830835850426474, "speed_score": 0.07498980250917445, "combined_score": 0.18664908050671958}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 13 lines with 7 lines\nChange 2: Replace 11 lines with 8 lines", "parent_metrics": {"balancedness_score": 0.29830835850426474, "speed_score": 0.06894773541474127, "combined_score": 0.183628046959503}, "island": 1}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve the Mixture-of-Expert models Expert Parallelism Load Balancer (MoE EPLB) expert rearrangement algorithm.\nThis algorithm will take the load metrics recorded by the vLLM server, and rearrange the experts to balance the load. It can make replicas of some experts to achieve better load balancing.\nYour goal will be two-fold: 1. Improve the algorithm to achieve better load balancing; while 2. Improve the algorithm to be more efficient, i.e. reduce the execution time of the algorithm itself, since perfect load balancing is NP-hard.\nThe current algorithm is implemented in the `rebalance_experts` function. ", "user": "# Current Program Information\n- Current performance metrics: - balancedness_score: 0.2983\n- speed_score: 0.0689\n- combined_score: 0.1836\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: balancedness_score, speed_score, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1125, combined_score: 0.2326\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1126, combined_score: 0.2327\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1143, combined_score: 0.2335\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.2335)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1143), Performs well on combined_score (0.2335)\n\n\n### Program 2 (Score: 0.2327)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1126), Performs well on combined_score (0.2327)\n\n\n### Program 3 (Score: 0.2326)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1125), Performs well on combined_score (0.2326)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.2322)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n\n### Program D2 (Score: 0.2325)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.2335, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 32 lines with 17 lines, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 2 (Score: 0.1920, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 3 (Score: 0.0939, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 23 lines with 236 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 4 (Score: 0.1629, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 5 lines with 5 lines\nChange 2: Replace 28 lines with 26 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 5 (Score: 0.0000, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n\n# Current Program\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort weights and get indices\n    sorted_weights, sorted_indices = weight.float().sort(-1, descending=True)\n\n    # Store device from input weight\n    device = weight.device\n\n    # Sort weights and get indices\n    sorted_weights, sorted_indices = weight.float().sort(-1, descending=True)\n\n    # Initialize pack_index and rank_in_pack on the correct device\n    pack_index = torch.full((num_layers, num_groups), -1, dtype=torch.int64, device=device)\n    rank_in_pack = torch.full((num_layers, num_groups), -1, dtype=torch.int64, device=device)\n\n    # Initialize pack_loads and pack_counts on the correct device\n    pack_loads = torch.zeros(num_layers, num_packs, dtype=torch.float32, device=device)\n    pack_counts = torch.zeros(num_layers, num_packs, dtype=torch.int64, device=device)\n\n    # Iterate through sorted items and assign them to the least loaded pack\n    for i in range(num_layers):\n        for g in range(num_groups):\n            available_packs = pack_counts[i] < groups_per_pack\n            if not available_packs.any():\n                continue\n\n            current_pack_loads = pack_loads[i][available_packs]\n            current_pack_indices = torch.arange(num_packs, device=device)[available_packs]\n\n            min_load_idx_in_available = torch.argmin(current_pack_loads)\n            chosen_pack_idx = current_pack_indices[min_load_idx_in_available]\n\n            original_group_idx = sorted_indices[i, g]\n\n            pack_index[i, original_group_idx] = chosen_pack_idx\n            rank_in_pack[i, original_group_idx] = pack_counts[i, chosen_pack_idx]\n\n            pack_loads[i, chosen_pack_idx] += weight[i, original_group_idx]\n            pack_counts[i, chosen_pack_idx] += 1\n\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Unified rebalancing logic:\n    # The hierarchical approach is used when num_groups is divisible by num_nodes.\n    # Otherwise, it falls back to a global balancing strategy by setting num_nodes=1 and num_groups=1.\n    # We can simplify this by directly implementing the global strategy when needed,\n    # or by ensuring the hierarchical logic correctly handles num_nodes=1.\n\n    # Determine the effective number of groups and nodes for balancing.\n    # If num_groups is not divisible by num_nodes, we treat it as a single group\n    # across all nodes for the first level of balancing.\n    effective_num_groups = num_groups if num_groups % num_nodes == 0 else 1\n    effective_num_nodes = num_nodes if num_groups % num_nodes == 0 else 1\n\n    # If num_groups % num_nodes != 0, we effectively want to balance across all groups as if they were one large group.\n    # And then balance those groups onto nodes.\n    # This is equivalent to setting num_nodes = 1 and num_groups = 1 in the hierarchical logic.\n    # Let's adjust parameters for a unified call to a simplified hierarchical logic.\n\n    # Simplified approach:\n    # First, balance logical experts across groups within layers.\n    # Then, balance these groups across nodes.\n    # Finally, replicate experts within nodes to balance across GPUs.\n\n    # Step 1: Balance logical experts into groups.\n    # If num_groups == 1, this step is trivial and doesn't change anything.\n    group_size = num_logical_experts // num_groups\n    tokens_per_group_layer = weight.unflatten(-1, (num_groups, group_size)).sum(-1) # [num_layers, num_groups]\n\n    # Pack groups onto nodes.\n    # If num_groups % num_nodes != 0, we effectively want to balance all groups onto a single 'node' conceptually,\n    # and then distribute those across the actual num_nodes.\n    # Let's consider the case where num_groups is not divisible by num_nodes.\n    # The original code uses num_nodes for packing groups. If num_groups < num_nodes, this packing might be problematic.\n    # A more robust approach is to balance groups onto num_nodes, and if num_groups < num_nodes, some nodes get 0 groups.\n    # The current `balanced_packing` assumes num_groups is divisible by num_packs (num_nodes here).\n\n    # Let's redefine the packing strategy:\n    # We want to pack 'num_groups' items (groups) into 'num_nodes' packs.\n    # If num_groups is not divisible by num_nodes, balanced_packing needs to handle it.\n    # The current `balanced_packing` asserts `num_groups % num_packs == 0`.\n    # This means if num_groups % num_nodes != 0, the original code calls `rebalance_experts_hierarchical` with num_nodes=1, num_groups=1.\n    # This is a global balancing strategy.\n\n    # Let's ensure the function call correctly reflects the intended strategy.\n    # If num_groups % num_nodes == 0, use hierarchical.\n    # Otherwise, use global (which is hierarchical with num_nodes=1, num_groups=1).\n\n    if num_groups % num_nodes == 0:\n        # Hierarchical balancing: balance groups to nodes, then nodes to GPUs.\n        # Step 1: Pack groups to nodes.\n        group_pack_index, group_rank_in_pack = balanced_packing(tokens_per_group_layer, num_nodes)\n        # Map logical experts to \"middle\" logical experts (logical experts per node)\n        # log2mlog: [num_layers, num_logical_experts] -> maps logical expert to its middle logical expert\n        mlog2log_map = (((group_pack_index * groups_per_node + group_rank_in_pack) * group_size).unsqueeze(-1) +\n                        torch.arange(group_size, dtype=torch.int64, device=group_pack_index.device)).flatten(-2)\n        # mlog2log_map: [num_layers, num_logical_experts] maps each logical expert to its \"middle\" logical expert ID.\n        # We need the inverse mapping for `weight.gather` later.\n        # This inverse mapping is tricky because multiple logical experts can map to the same middle expert.\n        # The `inverse` function is designed for permutations, which isn't directly applicable here.\n\n        # Let's rethink the mapping and replication.\n        # We have `num_layers` of `weight` ([num_layers, num_logical_experts]).\n        # First, we group logical experts: `tokens_per_group_layer` ([num_layers, num_groups]).\n        # Then, we pack these groups onto `num_nodes`: `group_pack_index` ([num_layers, num_groups]) and `group_rank_in_pack` ([num_layers, num_groups]).\n        # This means `num_groups` are assigned to `num_nodes`. `groups_per_node` groups per node.\n        # So, we have `num_layers * num_nodes` \"node-level\" expert weight tensors.\n        # Each node-level tensor has `num_logical_experts // num_nodes` \"middle\" experts.\n\n        # Let's create the \"middle\" logical experts' weights.\n        # `mlog_weights`: [num_layers * num_nodes, num_logical_experts // num_nodes]\n        # This requires carefully gathering weights.\n        # The `mlog2log` mapping needs to be constructed correctly.\n\n        # `mlog2log_map` assigns each logical expert to a conceptual \"middle\" logical expert.\n        # e.g., if num_logical_experts=8, num_groups=2, group_size=4.\n        # Group 0 contains logical experts 0,1,2,3. Group 1 contains 4,5,6,7.\n        # If num_nodes=2, groups_per_node=1.\n        # Group 0 (log experts 0-3) goes to node 0. Group 1 (log experts 4-7) goes to node 1.\n        # Node 0 has 1 middle expert. Node 1 has 1 middle expert.\n        # Middle expert for node 0 is logical experts 0-3. Middle expert for node 1 is 4-7.\n        # The mapping `mlog2log_map` should map:\n        # 0,1,2,3 -> 0 (on node 0)\n        # 4,5,6,7 -> 0 (on node 1)\n        # The current `mlog2log` calculation seems to be constructing the indices of the middle experts.\n        # `log2mlog` is `[num_layers, num_logical_experts]`.\n        # It maps logical expert ID to its index within the middle expert group.\n        # The `mlog2log` is the inverse of this index mapping.\n\n        # Let's reconstruct `mlog2log` and `tokens_per_mlog` more directly.\n        # For each layer, we have `num_groups` of `group_size`.\n        # `group_pack_index`: [num_layers, num_groups] - node assignment for each group.\n        # `group_rank_in_pack`: [num_layers, num_groups] - index of group within the node's assigned groups.\n\n        # We want to create `num_layers * num_nodes` \"node-level\" expert groups.\n        # Each node-level group corresponds to a set of original logical experts.\n        # The `tokens_per_mlog` should be `[num_layers * num_nodes, num_logical_experts // num_nodes]`.\n\n        # Calculate the logical expert index for each \"middle\" expert.\n        # `mlog_expert_indices`: [num_layers, num_nodes, groups_per_node, group_size]\n        # This is becoming complex. Let's simplify the conceptualization.\n\n        # Simpler approach:\n        # 1. Pack groups to nodes. This determines which logical experts are grouped together on each node.\n        # `group_pack_index`: [num_layers, num_groups] -> node ID for each group.\n        # `group_rank_in_pack`: [num_layers, num_groups] -> rank within node for each group.\n\n        # 2. For each node, create a combined weight tensor for the logical experts assigned to it.\n        #    The number of logical experts per node will be `groups_per_node * group_size`.\n        #    We need to gather the weights of logical experts based on `group_pack_index`.\n\n        # Let's create a mapping from (layer, node) to a contiguous block of logical experts.\n        # `node_logical_expert_map`: [num_layers, num_nodes, groups_per_node * group_size]\n\n        # This requires constructing a permutation for each layer and node.\n        # The original `mlog2log` calculation is:\n        # `((group_pack_index * groups_per_node + group_rank_in_pack) * group_size)`\n        # This forms a base index for each logical expert.\n        # `log2mlog` is this base index + `arange(group_size)`.\n        # `mlog2log` is the inverse of `log2mlog`.\n\n        # Let's directly construct the `tokens_per_mlog` and `mlog2log`.\n        # `tokens_per_mlog` is the weight tensor for experts *within* each node, considering the grouped logical experts.\n        # The number of such \"middle\" experts per node is `groups_per_node * group_size`.\n\n        # For each layer:\n        # `group_pack_index` tells us which node each group belongs to.\n        # `group_rank_in_pack` tells us the order within the node.\n\n        # Let's create `num_layers * num_nodes` tensors, each of size `groups_per_node * group_size`.\n        # `mlog_weights`: [num_layers * num_nodes, groups_per_node * group_size]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, groups_per_node * group_size]\n\n        # Example: num_layers=1, num_logical_experts=8, num_groups=2, group_size=4, num_nodes=2, groups_per_node=1, num_gpus=2, phy_experts_per_gpu=1.\n        # weight: [1, 8]\n        # tokens_per_group_layer: [1, 2] (sum of weights for group 0 and group 1)\n        # balanced_packing(tokens_per_group_layer, num_nodes=2) ->\n        #   group_pack_index: [1, 2] -> [[0, 1]] (group 0 to node 0, group 1 to node 1)\n        #   group_rank_in_pack: [1, 2] -> [[0, 0]] (rank 0 within node for both groups)\n        # groups_per_node = 1, group_size = 4.\n\n        # Middle expert for node 0 is logical experts 0,1,2,3.\n        # Middle expert for node 1 is logical experts 4,5,6,7.\n        # `mlog_to_orig_log_map` for node 0 should be [0,1,2,3].\n        # `mlog_to_orig_log_map` for node 1 should be [4,5,6,7].\n\n        # The original `mlog2log` calculation:\n        # `base_idx = (group_pack_index * groups_per_node + group_rank_in_pack) * group_size`\n        # `base_idx`: [1, 2] -> [[0*1+0, 1*1+0]] * 4 = [[0, 4]]\n        # `log2mlog`: `base_idx.unsqueeze(-1) + torch.arange(group_size)`\n        # `log2mlog`: [[0, 4]].unsqueeze(-1) + [0,1,2,3] -> [[0,1,2,3], [4,5,6,7]]\n        # This `log2mlog` is `[num_layers, num_logical_experts]`. It maps original logical expert ID to its \"middle\" expert index.\n        # The original logic uses `weight.gather(-1, mlog2log)` which is incorrect here.\n        # `weight.gather(-1, mlog2log)` expects `mlog2log` to be `[num_layers, num_middle_experts]` where `num_middle_experts` is the number of columns in `weight`.\n        # The shape should be `[num_layers, num_logical_experts]` if we are gathering from `weight` which is `[num_layers, num_logical_experts]`.\n        # The `mlog2log` calculated here has shape `[num_layers, num_logical_experts]`.\n        # It maps `original_logical_expert_id` to `middle_expert_id`.\n        # We need to gather `weight[layer, original_logical_expert_id]` and group them by `middle_expert_id`.\n\n        # Let's create `mlog_weights` directly.\n        # `mlog_weights`: [num_layers * num_nodes, groups_per_node * group_size]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, groups_per_node * group_size]\n\n        # Vectorized construction of mlog_weights and mlog_to_orig_log_map\n        # This replaces the nested Python loops for better performance.\n\n        # Calculate the new \"middle expert group\" index for each original group.\n        # This new index combines node assignment and rank within the node.\n        # Shape: [num_layers, num_groups]\n        new_group_idx = group_pack_index * groups_per_node + group_rank_in_pack\n\n        # Create a tensor that maps new group index back to original group index.\n        # This is a permutation tensor for groups within each layer.\n        # Shape: [num_layers, num_nodes * groups_per_node] (which is num_groups)\n        group_permutation = torch.empty((num_layers, num_groups),\n                                        dtype=torch.int64,\n                                        device=weight.device)\n        group_permutation.scatter_(1, new_group_idx,\n                                   torch.arange(num_groups, device=weight.device).expand(num_layers, -1))\n\n        # Reshape original weights to [num_layers, num_groups, group_size]\n        weight_reshaped = weight.view(num_layers, num_groups, group_size)\n\n        # Gather weights based on the group_permutation\n        # The .gather() operation reorders the groups according to the permutation.\n        # Shape: [num_layers, num_groups, group_size]\n        mlog_weights_ordered = weight_reshaped.gather(1, group_permutation.unsqueeze(-1).expand(-1, -1, group_size))\n        \n        # Reshape to [num_layers, num_nodes, groups_per_node * group_size]\n        # This is the final mlog_weights.\n        mlog_weights = mlog_weights_ordered.view(num_layers, num_nodes, groups_per_node * group_size)\n\n        # Construct mlog_to_orig_log_map similarly\n        # Original logical expert IDs, reshaped to [num_groups, group_size]\n        orig_log_ids_reshaped = torch.arange(num_logical_experts,\n                                             dtype=torch.int64,\n                                             device=weight.device).view(num_groups, group_size)\n        \n        # Expand to [num_layers, num_groups, group_size] and gather\n        mlog_to_orig_log_map_ordered = orig_log_ids_reshaped.unsqueeze(0).expand(num_layers, -1, -1).gather(\n            1, group_permutation.unsqueeze(-1).expand(-1, -1, group_size))\n        \n        # Reshape to [num_layers, num_nodes, groups_per_node * group_size]\n        mlog_to_orig_log_map = mlog_to_orig_log_map_ordered.view(\n            num_layers, num_nodes, groups_per_node * group_size)\n\n        # Now, `mlog_weights`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n        # And `mlog_to_orig_log_map`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n\n        # Step 2: Replicate experts within each node.\n        # `mlog_weights` are the weights for experts within each node.\n        # `num_physical_experts // num_nodes` is the target number of physical experts per node.\n        # `replicate_experts` takes `[X, num_log]` and returns `phy2log`, `rank`, `logcnt`.\n        # Here, X = num_layers * num_nodes, num_log = num_logical_experts // num_nodes.\n        # So, we need to reshape `mlog_weights` and `mlog_to_orig_log_map`.\n\n        num_middle_experts = num_logical_experts // num_nodes\n        reshaped_mlog_weights = mlog_weights.view(-1, num_middle_experts)\n        reshaped_mlog_to_orig_log_map = mlog_to_orig_log_map.view(-1, num_middle_experts)\n\n        # `phy2mlog`: [num_layers * num_nodes, num_physical_experts // num_nodes]\n        # `phyrank`: [num_layers * num_nodes, num_physical_experts // num_nodes]\n        # `mlogcnt`: [num_layers * num_nodes, num_middle_experts]\n        phy2mlog, phyrank, mlogcnt = replicate_experts(\n            reshaped_mlog_weights, num_physical_experts // num_nodes)\n\n        # Step 3: Pack physical experts to GPUs.\n        # `phy2mlog` maps physical experts (within each node) to middle logical experts.\n        # `phy2mlog` is `[num_layers * num_nodes, num_phy_per_node]`\n        # `phyrank` is `[num_layers * num_nodes, num_phy_per_node]`\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`\n\n        # We need to map these back to global physical expert indices.\n        # The `phy2mlog` refers to the *middle* logical experts.\n        # The `replicate_experts` function returns `phy2log` which is the *original* logical expert ID.\n        # So, `phy2mlog` from `replicate_experts` is actually `[num_layers*num_nodes, num_phy_per_node]`,\n        # where each entry is an index into the *middle* logical experts.\n\n        # We need to map these `phy2mlog` indices back to original logical expert IDs.\n        # `reshaped_mlog_to_orig_log_map`: [num_layers * num_nodes, num_middle_experts]\n        # `final_phy2log_per_node`: [num_layers * num_nodes, num_phy_per_node]\n        final_phy2log_per_node = reshaped_mlog_to_orig_log_map.gather(-1, phy2mlog)\n\n        # `phyrank` is the replica rank for the middle experts.\n        # We need to combine this with `mlogcnt` to get the final `logcnt`.\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`. It tells us how many replicas each middle expert has.\n        # The `phyrank` is the rank of the replica for a *specific* middle expert.\n\n        # The `logcnt` returned by `replicate_experts` is `[X, num_log]`, which is `[num_layers*num_nodes, num_middle_experts]`.\n        # We need to map this back to `[num_layers, num_logical_experts]`.\n\n        # Let's use the `mlogcnt` directly to get the total count of replicas for each logical expert.\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`.\n        # We need to sum/average counts based on the `mlog_to_orig_log_map`.\n\n        # Let's refine the output mapping:\n        # `physical_to_logical_map`: [num_layers, num_replicas]\n        # `logical_to_physical_map`: [num_layers, num_logical_experts, X]\n        # `expert_count`: [num_layers, num_logical_experts]\n\n        # `final_phy2log_per_node`: [num_layers * num_nodes, num_phy_per_node]\n        # This is the logical expert ID for each physical expert *within its node*.\n        # We need to convert this to global physical expert IDs.\n        # Physical experts are grouped by GPU. `num_gpus` total, `num_gpus // num_nodes` per node.\n        # `phy_experts_per_gpu` = `num_physical_experts // num_gpus`.\n        # So, each GPU has `phy_experts_per_gpu` physical experts.\n        # Each Node has `num_gpus // num_nodes` GPUs.\n        # Total physical experts per node = `(num_gpus // num_nodes) * phy_experts_per_gpu`\n        # This should equal `num_physical_experts // num_nodes`.\n\n        # `final_phy2log_per_node`: [num_layers, num_nodes, num_phy_per_node]\n        # `final_rank_per_node`: [num_layers, num_nodes, num_phy_per_node] (this is `phyrank`)\n\n        # We need to flatten this to `[num_layers, num_physical_experts]`.\n        # `num_physical_experts = num_nodes * num_phy_per_node`.\n\n        # `final_phy2log_global`: [num_layers, num_physical_experts]\n        # `final_rank_global`: [num_layers, num_physical_experts]\n\n        # The `mlog_to_orig_log_map` correctly maps middle experts to original logical experts.\n        # `phy2mlog` maps physical experts (within a node) to middle experts.\n        # So, `phy2mlog.gather(-1, final_phy2log_per_node)` gives the logical expert ID.\n        # This is what `final_phy2log_per_node` already represents.\n\n        # Let's rename `final_phy2log_per_node` to `phy2log_per_node` for clarity.\n        phy2log_per_node = final_phy2log_per_node\n        rank_per_node = phyrank # [num_layers * num_nodes, num_phy_per_node]\n\n        # Flatten these to get global mappings for physical experts.\n        phy2log_global = phy2log_per_node.view(num_layers, -1)\n        rank_global = rank_per_node.view(num_layers, -1)\n\n        # Now, let's construct the `logcnt` (expert_count).\n        # `mlogcnt`: [num_layers * num_nodes, num_middle_experts]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, num_middle_experts]\n\n        # We need to aggregate `mlogcnt` based on `mlog_to_orig_log_map`.\n        # For each logical expert, its total count is the sum of counts of its constituent middle experts.\n        # This aggregation needs to be done carefully.\n\n        # Let's create a tensor for `logcnt`: [num_layers, num_logical_experts] initialized to zeros.\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n\n        # The expert_count aggregation:\n        # mlog_to_orig_log_map: [num_layers, num_nodes, num_middle_experts]\n        # mlogcnt: [num_layers * num_nodes, num_middle_experts]\n        # Flatten mlog_to_orig_log_map and mlogcnt to [num_layers, N] for scatter_add_\n        mlog_to_orig_log_map_flat = mlog_to_orig_log_map.view(num_layers, -1)\n        mlogcnt_reshaped = mlogcnt.view(num_layers, -1)\n\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n        # Use scatter_add_ to efficiently aggregate counts for each logical expert across layers\n        expert_count.scatter_add_(1, mlog_to_orig_log_map_flat, mlogcnt_reshaped)\n\n        # The logical_to_physical_map construction is handled by the outer rebalance_experts function.\n        # So, this function only needs to return the global mappings and counts.\n        return phy2log_global, rank_global, expert_count\n\n    # The `rebalance_experts_hierarchical` function is designed to handle both\n    # hierarchical and global balancing based on its input parameters.\n    # The outer `rebalance_experts` function already ensures that the parameters\n    # passed to `rebalance_experts_hierarchical` result in `num_groups % num_nodes == 0`\n    # within `rebalance_experts_hierarchical`'s scope (either original values or 1, 1).\n    # Therefore, this `else` block is unreachable and can be removed.\n    pass\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    # Store original device for computation and ensure float type\n    device = weight.device\n    weight = weight.float()\n\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy (by setting num_groups=1, num_nodes=1)\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    \n    # The `rebalance_experts_hierarchical` function returns:\n    # - `phy2log`: [num_layers, num_replicas], maps physical expert index to logical expert ID.\n    # - `phyrank`: [num_layers, num_replicas], maps physical expert index to its replica rank.\n    # - `logcnt`: [num_layers, num_logical_experts], number of replicas for each logical expert.\n\n    # Construct `logical_to_physical_map`: [layers, num_logical_experts, X]\n    # where X is the maximum number of replicas for any logical expert.\n\n    num_physical_experts_total = num_replicas\n\n    # Fix: Calculate max_replicas_per_expert based on actual counts from logcnt.\n    # This ensures the tensor for logical_to_physical_map is correctly sized.\n    if num_logical_experts == 0 or logcnt.numel() == 0:\n        max_replicas_per_expert = 0\n    else:\n        max_replicas_per_expert = logcnt.max().item()\n        # In case all experts have only one replica (e.g., num_replicas == num_logical_experts)\n        # and logcnt was initialized to 1s, max() would be 1.\n        # If num_logical_experts > 0 but max_replicas_per_expert became 0 (unlikely with replicate_experts),\n        # ensure it's at least 1 if physical experts exist.\n        if max_replicas_per_expert == 0 and num_physical_experts_total > 0:\n            max_replicas_per_expert = 1\n\n    # Initialize the map with -1 (indicating no physical expert assigned)\n    logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert),\n                                         -1, dtype=torch.int64, device=device)\n\n    # Create physical expert IDs to scatter\n    physical_expert_ids = torch.arange(num_physical_experts_total, dtype=torch.int64, device=device) # Use stored device\n\n    # Expand physical_expert_ids to match the shape of phy2log and phyrank for broadcasting\n    # Shape: [num_layers, num_physical_experts_total]\n    physical_expert_ids_expanded = physical_expert_ids.unsqueeze(0).expand(num_layers, num_physical_experts_total)\n\n    # Create layer indices for advanced indexing\n    layer_indices = torch.arange(num_layers, dtype=torch.int64, device=device).unsqueeze(1)\n    \n    # Assign physical expert IDs to the correct positions in the map using advanced indexing.\n    # This replaces the Python loops for significant performance improvement.\n    logical_to_physical_map[layer_indices, phy2log, phyrank] = physical_expert_ids_expanded\n\n    # The `logcnt` (expert_count) is already computed by the balancing functions.\n    # `phy2log` is the first return value.\n    return phy2log, logical_to_physical_map, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["The current program implements a hierarchical expert parallelism load balancer. The goal is to improve both the balancedness and speed of the load balancing algorithm.\n\nLooking at the current performance metrics:\n- balancedness_score: 0.2983\n- speed_score: 0.0689\n- combined_score: 0.1836\n\nThe recent attempts (3, 2, 1) show significantly better scores (around 0.2335), indicating that recent changes have regressed performance. The prompt also mentions \"Consider reverting or revising recent changes in these areas.\"\n\nThe core logic for hierarchical balancing is in `rebalance_experts_hierarchical`. The function `rebalance_experts` acts as an entry point and orchestrates the calls.\n\nLet's analyze `rebalance_experts_hierarchical` for potential optimizations and correctness issues, particularly in the vectorized construction of `mlog_weights` and `mlog_to_orig_log_map`.\n\n**Analysis of `rebalance_experts_hierarchical`:**\n\n1.  **Hierarchical vs. Global Balancing:**\n    The logic `if num_groups % num_nodes == 0:` correctly distinguishes between hierarchical and global balancing. When `num_groups % num_nodes != 0`, it falls back to global balancing by setting `num_nodes=1` and `num_groups=1`. This seems reasonable.\n\n2.  **Vectorized Construction of `mlog_weights` and `mlog_to_orig_log_map`:**\n    The current implementation uses `gather` and `view` operations to construct `mlog_weights` and `mlog_to_orig_log_map`. This part is complex and a potential source of bugs or inefficiencies.\n\n    Let's trace the shapes:\n    - `weight`: `[num_layers, num_logical_experts]`\n    - `tokens_per_group_layer`: `[num_layers, num_groups]`\n    - `group_pack_index`: `[num_layers, num_groups]` (node assignment)\n    - `group_rank_in_pack`: `[num_layers, num_groups]` (rank within node)\n    - `groups_per_node`: `num_groups // num_nodes`\n    - `group_size`: `num_logical_experts // num_groups`\n\n    The goal is to create `mlog_weights`: `[num_layers, num_nodes, groups_per_node * group_size]` and `mlog_to_orig_log_map`: `[num_layers, num_nodes, groups_per_node * group_size]`.\n\n    The calculation of `new_group_idx = group_pack_index * groups_per_node + group_rank_in_pack` correctly identifies the \"middle expert group\" index within the `num_nodes` conceptual groups.\n    Shape: `[num_layers, num_groups]`\n\n    The `group_permutation` construction:\n    `group_permutation = torch.empty((num_layers, num_groups), dtype=torch.int64, device=weight.device)`\n    `group_permutation.scatter_(1, new_group_idx, torch.arange(num_groups, device=weight.device).expand(num_layers, -1))`\n    This creates a permutation for each layer. For a given layer, it rearranges the original `num_groups` indices such that groups assigned to node 0 come first (in their `group_rank_in_pack` order), then groups assigned to node 1, and so on.\n    Example: If `num_groups=4`, `num_nodes=2`, `groups_per_node=2`.\n    Suppose `new_group_idx` is `[[0, 0, 1, 1]]` (groups 0,1 to node 0; groups 2,3 to node 1).\n    `group_permutation` should be `[[0, 1, 2, 3]]` if `new_group_idx` was `[[0, 0, 1, 1]]`.\n    Wait, `scatter_` fills `group_permutation` at indices specified by `new_group_idx` with values from `arange`.\n    If `new_group_idx = [[0, 0, 1, 1]]`, then:\n    `group_permutation[0, 0] = 0`\n    `group_permutation[0, 0] = 1` (overwrites)\n    `group_permutation[0, 1] = 2`\n    `group_permutation[0, 1] = 3` (overwrites)\n    This seems incorrect. `scatter_` along dimension 1 with `new_group_idx` values that are not unique will overwrite.\n    The intent is to map the sorted `new_group_idx` back to the original group indices.\n\n    A correct permutation mapping would be:\n    For each layer:\n    1. Get `new_group_idx` for that layer.\n    2. Create a tensor of original group indices: `orig_group_indices = torch.arange(num_groups)`.\n    3. Sort `new_group_idx` to get `sorted_new_group_idx` and their original indices `sort_perm_indices`.\n    4. The `group_permutation` for that layer should be `orig_group_indices[sort_perm_indices]`.\n\n    Let's consider `torch.sort` on `new_group_idx` for each layer.\n    `sorted_new_group_idx, sort_perm_indices = torch.sort(new_group_idx, dim=1)`\n    `group_permutation = orig_group_indices.expand(num_layers, -1).scatter_(1, sort_perm_indices, torch.arange(num_groups, device=weight.device).expand(num_layers, -1))`\n    This seems closer. `sort_perm_indices` tells us the original positions of the sorted elements. We want to fill `group_permutation` such that `group_permutation[layer, sorted_new_group_idx_for_layer]` contains the original group index.\n\n    The current `scatter_` logic `group_permutation.scatter_(1, new_group_idx, torch.arange(num_groups, device=weight.device).expand(num_layers, -1))`\n    means that for a given layer, `group_permutation[layer, new_group_idx[layer, k]] = k`.\n    If `new_group_idx[layer]` contains duplicates (which it will if `groups_per_node > 1`), values will be overwritten.\n\n    **Proposed Change 1: Correcting `group_permutation` construction.**\n    The goal of `group_permutation` is to reorder the groups such that all groups belonging to node 0 appear first, then node 1, etc., maintaining their relative order within the node.\n    The `new_group_idx` calculation `group_pack_index * groups_per_node + group_rank_in_pack` already defines the desired ordering. We need to map the original groups to these new ordered slots.\n\n    Let's use `torch.argsort` to get the permutation that sorts `new_group_idx`.\n    `sorted_indices_in_groups = torch.argsort(new_group_idx, dim=1)`\n    Then, `group_permutation` should be constructed by indexing `orig_group_indices` with these sorted indices.\n    `orig_group_indices = torch.arange(num_groups, device=weight.device)`\n    `group_permutation = orig_group_indices.expand(num_layers, -1).scatter_(1, sorted_indices_in_groups, torch.arange(num_groups, device=weight.device).expand(num_layers, -1))`\n    This is still not right. `scatter_` is not the right tool here.\n\n    Consider `mlog_weights` shape: `[num_layers, num_nodes, groups_per_node * group_size]`.\n    This means for layer `l`, node `n`, we want to collect weights from groups `g` where `group_pack_index[l, g]` is `n` and `group_rank_in_pack[l, g]` is `r`. The combined index for such a group is `n * groups_per_node + r`.\n    The total number of \"middle experts\" (groups within a node) is `num_nodes * groups_per_node = num_groups`. This is incorrect.\n    The number of \"middle experts\" per node is `groups_per_node * group_size`.\n    Total number of middle experts across all nodes is `num_nodes * (groups_per_node * group_size)`. This is `num_nodes * num_logical_experts / num_nodes = num_logical_experts`. This is also not right.\n\n    The `mlog_weights` shape `[num_layers, num_nodes, groups_per_node * group_size]` implies that for each node, we have a set of experts, and each expert is a collection of `groups_per_node` original groups.\n    The total number of logical experts within this \"middle\" representation should be `num_logical_experts`.\n    The shape of `mlog_weights` should be `[num_layers, num_logical_experts]`, where each \"expert\" is a collection of original logical experts.\n\n    Let's revisit the goal of `rebalance_experts_hierarchical`:\n    1.  **Pack groups to nodes:** `group_pack_index`, `group_rank_in_pack`.\n        This means logical experts are now conceptually grouped by node.\n        For layer `l`, node `n`: the logical experts are those in groups `g` such that `group_pack_index[l, g] == n`.\n        The total number of logical experts assigned to node `n` is `groups_per_node * group_size`.\n        These form the \"middle logical experts\" within that node.\n\n    2.  **Create `mlog_weights` and `mlog_to_orig_log_map`:**\n        We need to construct tensors representing experts *per node*.\n        For each layer `l`:\n        We want to create `num_nodes` blocks of experts.\n        The `i`-th block (for node `i`) should contain `groups_per_node * group_size` logical experts.\n        These logical experts are identified by their original IDs.\n\n        Let's iterate through nodes and then through groups assigned to that node.\n        For `layer` in `num_layers`:\n            For `node` in `num_nodes`:\n                Find groups `g` where `group_pack_index[layer, g] == node`.\n                Let these group indices be `assigned_groups`.\n                The `group_rank_in_pack[layer, assigned_groups]` gives their order within the node.\n                Sort `assigned_groups` based on `group_rank_in_pack[layer, assigned_groups]`.\n                Let the sorted group indices be `sorted_assigned_groups`.\n                The logical experts for this node's middle experts are `sorted_assigned_groups * group_size + (0..group_size-1)`.\n                Concatenate these logical expert IDs.\n                Concatenate the weights of these logical experts.\n\n        This manual loop construction is what we want to vectorize.\n\n        **Alternative approach using `torch.block_diag` or similar:**\n        We can construct the permutation to group the original logical experts by node.\n        `node_assignments = group_pack_index` (shape: `[num_layers, num_groups]`)\n        We want to create a permutation that groups original groups by node.\n        For each layer:\n            `node_assignments_l = node_assignments[l]`\n            `ranks_l = group_rank_in_pack[l]`\n            `combined_key = node_assignments_l * groups_per_node + ranks_l` (This is the same as `new_group_idx`)\n            `sorted_indices_for_layer = torch.argsort(combined_key)`\n            `group_permutation_l = sorted_indices_for_layer`\n\n        Let `group_permutation = torch.argsort(new_group_idx, dim=1)`\n        Now, `group_permutation` is `[num_layers, num_groups]`.\n        `weight_reshaped = weight.view(num_layers, num_groups, group_size)`\n        `mlog_weights_reordered_groups = weight_reshaped.gather(1, group_permutation.unsqueeze(-1).expand(-1, -1, group_size))`\n        This correctly reorders the groups within each layer.\n        Shape: `[num_layers, num_groups, group_size]`.\n\n        Now, we need to reshape this to `[num_layers, num_nodes, groups_per_node * group_size]`.\n        The current code does:\n        `mlog_weights = mlog_weights_reordered_groups.view(num_layers, num_nodes, groups_per_node * group_size)`\n        This assumes that `num_groups` is perfectly divisible by `num_nodes` and the ordering from `gather` naturally aligns.\n        If `groups_per_node = 2` and `num_nodes = 2`, `num_groups = 4`.\n        `new_group_idx`: `[[0, 0, 1, 1]]` (groups 0,1 to node 0; groups 2,3 to node 1)\n        `sorted_indices_for_layer` (based on `new_group_idx`): `[0, 1, 2, 3]` (if `new_group_idx` was `[0, 0, 1, 1]`)\n        `group_permutation`: `[0, 1, 2, 3]` (indices that result in the sorted order)\n        `mlog_weights_reordered_groups`: `[layer, [group0, group1, group2, group3], ...]`\n        When we `view(num_layers, num_nodes, groups_per_node * group_size)`:\n        `num_nodes = 2`, `groups_per_node = 2`, `group_size = 4`.\n        We want `[num_layers, 2, 8]`.\n        The `mlog_weights_reordered_groups` is `[num_layers, 4, 4]`.\n        The view operation reshapes the dimension of size 4 into two dimensions of size 2 and 8.\n        This requires the first dimension (4) to be divisible by the new dimensions' product (2*8=16). This is not the case.\n\n        The shape of `mlog_weights` should be `[num_layers, num_logical_experts]` where each \"expert\" at index `i` in the second dimension is a collection of original logical experts.\n        The total number of these \"middle experts\" is `num_logical_experts`. This is also not right.\n\n        Let's re-read the original EPLB paper or implementation for the exact mapping.\n        The EPLB paper describes:\n        1.  **Group Balancing:** Pack `num_logical_experts` into `num_groups` groups.\n        2.  **Node Balancing:** Pack `num_groups` into `num_nodes` nodes.\n        3.  **Replication:** Replicate experts within each node to `num_phy_per_node`.\n\n        The `rebalance_experts_hierarchical` function calculates `tokens_per_group_layer` which is `[num_layers, num_groups]`.\n        Then it calls `balanced_packing(tokens_per_group_layer, num_nodes)` which packs `num_groups` into `num_nodes` packs. This means each node receives `groups_per_node` groups.\n        The output `group_pack_index` and `group_rank_in_pack` define which groups go to which node and their order.\n\n        The original code then constructs `mlog2log_map` and `mlog_weights`.\n        `mlog2log_map`: `[num_layers, num_logical_experts]`\n        `mlog_weights`: `[num_layers, num_logical_experts]`\n\n        The code `((group_pack_index * groups_per_node + group_rank_in_pack) * group_size)` creates a base index for the *middle expert*.\n        The number of middle experts per node is `groups_per_node * group_size`.\n        The total number of middle experts is `num_nodes * groups_per_node * group_size = num_groups * group_size = num_logical_experts`.\n        So, we have `num_logical_experts` middle experts in total, distributed across nodes.\n\n        The calculation `log2mlog`: `base_idx.unsqueeze(-1) + torch.arange(group_size)`\n        This means for each middle expert, we are creating `group_size` slots.\n        Example:\n        num_logical_experts=8, num_groups=2, group_size=4. num_nodes=2, groups_per_node=1.\n        Group 0 (log 0-3) to Node 0. Group 1 (log 4-7) to Node 1.\n        Middle expert 0 (Node 0) maps to log experts 0,1,2,3.\n        Middle expert 1 (Node 1) maps to log experts 4,5,6,7.\n        The `base_idx` would be `[[0, 4]]` (for layer 0).\n        `log2mlog` would be `[[0,1,2,3], [4,5,6,7]]`.\n        This `log2mlog` maps `original_logical_expert_id` to `middle_expert_id`.\n        Shape: `[num_layers, num_logical_experts]`.\n\n        The subsequent `weight.gather(-1, mlog2log)` is incorrect. `gather` expects the index tensor to have the same number of columns as the tensor being gathered from, or `weight.gather(-1, index)` where `index` is `[N, K]` and `weight` is `[N, M]`, it gathers `M` items for each of the `K` indices.\n        The `mlog2log` has shape `[num_layers, num_logical_experts]`.\n        `weight.gather(-1, mlog2log)` implies `mlog2log` should contain indices into the last dimension of `weight`.\n        The current `mlog2log` maps original logical expert to middle expert.\n\n        The code that constructs `mlog_weights` and `mlog_to_orig_log_map` seems to be the most critical part for correctness and performance.\n\n        Let's re-examine the intended structure of `mlog_weights`:\n        `mlog_weights`: `[num_layers, num_nodes, groups_per_node * group_size]`\n        This means for each layer and each node, we have a tensor of weights.\n        The second dimension `num_nodes` suggests we are processing each node independently.\n        The third dimension `groups_per_node * group_size` represents the \"middle experts\" within that node.\n        The total number of these middle experts across all nodes is `num_nodes * groups_per_node * group_size = num_groups * group_size = num_logical_experts`.\n        So, `mlog_weights` should be `[num_layers, num_logical_experts]`.\n\n        Let's try to construct `mlog_weights` and `mlog_to_orig_log_map` correctly.\n\n        For each layer `l`:\n        `node_assignments = group_pack_index[l]` # [num_groups]\n        `ranks = group_rank_in_pack[l]` # [num_groups]\n\n        We need to create a mapping from `(node, rank_within_node)` to `original_group_index`.\n        The `new_group_idx = node_assignments * groups_per_node + ranks` is the index of the middle expert group.\n        Shape: `[num_groups]`\n\n        We want to gather the original groups based on this ordering.\n        `sorted_indices_of_groups = torch.argsort(new_group_idx)`\n        This gives the order of original groups to form the middle experts.\n        `group_permutation_l = sorted_indices_of_groups` # [num_groups]\n\n        `weight_reshaped_l = weight[l].view(num_groups, group_size)` # [num_groups, group_size]\n        `mlog_weights_l = weight_reshaped_l.gather(0, group_permutation_l.unsqueeze(-1).expand(-1, group_size))`\n        Shape: `[num_groups, group_size]`. This represents the weights of the middle experts.\n\n        Now, we need to reshape this to `[num_nodes, groups_per_node * group_size]` if the structure implies that.\n        The original code's `view(num_layers, num_nodes, groups_per_node * group_size)` is problematic if `num_groups` is not divisible by `num_nodes`.\n        It seems the intent of `mlog_weights` is to have `num_layers * num_nodes` rows, each representing a node's combined experts.\n\n        Let's consider the `replicate_experts` function. It takes `[X, num_log]`.\n        In `rebalance_experts_hierarchical`, `X = num_layers * num_nodes`.\n        `num_log = num_logical_experts // num_nodes`.\n        So, `mlog_weights` should have shape `[num_layers * num_nodes, num_logical_experts // num_nodes]`.\n\n        This means we need to construct `num_layers * num_nodes` tensors, each of size `num_logical_experts // num_nodes`.\n        Each of these tensors corresponds to a node and its combined logical experts.\n\n        Let's rethink the `group_permutation` construction.\n        We need to map original groups to slots `(node, rank_in_node)`.\n        The `new_group_idx` already provides the index of the middle expert *slot*.\n        We need to gather the original groups into these slots.\n\n        Consider `torch.bucketize` or `torch.scatter`.\n        We want to create `num_layers * num_nodes` blocks of weights.\n        Each block of size `num_logical_experts // num_nodes`.\n\n        Let's create `mlog_weights` and `mlog_to_orig_log_map` directly.\n        `num_middle_experts_per_node = groups_per_node * group_size`\n        `total_middle_experts = num_nodes * num_middle_experts_per_node`\n\n        We need to fill `mlog_weights`: `[num_layers, total_middle_experts]`\n        And `mlog_to_orig_log_map`: `[num_layers, total_middle_experts]`\n\n        For each layer `l`:\n            `node_assignments = group_pack_index[l]`\n            `ranks = group_rank_in_pack[l]`\n            `combined_key = node_assignments * groups_per_node + ranks` # [num_groups]\n\n            `sorted_indices_of_groups = torch.argsort(combined_key)` # [num_groups]\n            `group_permutation_l = sorted_indices_of_groups`\n\n            `weight_reshaped_l = weight[l].view(num_groups, group_size)` # [num_groups, group_size]\n            `mlog_weights_l = weight_reshaped_l.gather(0, group_permutation_l.unsqueeze(-1).expand(-1, group_size))`\n            `mlog_weights_l = mlog_weights_l.view(num_nodes, num_middle_experts_per_node)` # Reshape to [num_nodes, num_middle_experts_per_node]\n\n            `orig_log_ids_reshaped_l = torch.arange(num_logical_experts, device=weight.device).view(num_groups, group_size)`\n            `mlog_to_orig_log_map_l = orig_log_ids_reshaped_l.gather(0, group_permutation_l.unsqueeze(-1).expand(-1, group_size))`\n            `mlog_to_orig_log_map_l = mlog_to_orig_log_map_l.view(num_nodes, num_middle_experts_per_node)`\n\n            We need to flatten `mlog_weights_l` and `mlog_to_orig_log_map_l` to `[total_middle_experts]` for the layer.\n            `mlog_weights[l, :] = mlog_weights_l.flatten()`\n            `mlog_to_orig_log_map[l, :] = mlog_to_orig_log_map_l.flatten()`\n\n        This approach constructs `mlog_weights` and `mlog_to_orig_log_map` of shape `[num_layers, num_logical_experts]`.\n        Then `replicate_experts` is called with `reshaped_mlog_weights.view(-1, num_middle_experts)` where `num_middle_experts = num_logical_experts // num_nodes`. This is inconsistent.\n\n        **Revisiting the structure expected by `replicate_experts`:**\n        `replicate_experts(weight: torch.Tensor, num_phy: int)`\n        `weight`: `[X, num_log]`\n        `num_phy`: total number of experts after replication.\n        In `rebalance_experts_hierarchical`, `X = num_layers * num_nodes` and `num_log = num_logical_experts // num_nodes`.\n        So, `replicate_experts` expects input shape `[num_layers * num_nodes, num_logical_experts // num_nodes]`.\n\n        The current code constructs `mlog_weights`: `[num_layers, num_nodes, groups_per_node * group_size]`\n        And then reshapes it to `[num_layers * num_nodes, groups_per_node * group_size]`.\n        `groups_per_node * group_size` is `num_logical_experts // num_nodes`. This matches `num_log`.\n        So, the shape of `mlog_weights` and `mlog_to_orig_log_map` is correct for the subsequent `replicate_experts` call.\n\n        The issue might be in the `view` operation's assumption about data contiguity or order.\n        The `mlog_weights_reordered_groups.view(num_layers, num_nodes, groups_per_node * group_size)` needs to correctly partition the `num_groups` dimension into `num_nodes` blocks, each of size `groups_per_node * group_size`.\n        This implies that the `group_permutation` should arrange the groups such that the first `groups_per_node` groups belong to node 0, the next `groups_per_node` to node 1, etc.\n\n        The `new_group_idx = group_pack_index * groups_per_node + group_rank_in_pack` creates a unique index for each middle expert slot.\n        `new_group_idx`: `[num_layers, num_groups]`\n        If `groups_per_node = 2`, `num_nodes = 2`, `num_groups = 4`.\n        `group_pack_index`: `[[0, 0, 1, 1]]`\n        `group_rank_in_pack`: `[[0, 1, 0, 1]]`\n        `new_group_idx`: `[[0*2+0, 0*2+1, 1*2+0, 1*2+1]] = [[0, 1, 2, 3]]`\n        This means group 0 maps to middle expert 0, group 1 to middle expert 1, group 2 to middle expert 2, group 3 to middle expert 3.\n        The `sorted_indices_for_layer = torch.argsort(new_group_idx, dim=1)` would be `[[0, 1, 2, 3]]`.\n        The `group_permutation` becomes `[[0, 1, 2, 3]]`.\n        `weight_reshaped`: `[layer, [group0, group1, group2, group3], group_size]`\n        `mlog_weights_reordered_groups = weight_reshaped.gather(1, group_permutation.unsqueeze(-1).expand(-1, -1, group_size))`\n        This means `mlog_weights_reordered_groups[l]` will be `[group0, group1, group2, group3]`.\n        Then `view(num_layers, num_nodes, groups_per_node * group_size)` becomes `view(l, 2, 2*group_size)`.\n        This reshapes `[group0, group1, group2, group3]` into `[[group0, group1], [group2, group3]]` if `groups_per_node=2`.\n        This seems correct.\n\n        The issue might be with the `scatter_` operation in the original code.\n        The current code:\n        `group_permutation.scatter_(1, new_group_idx, torch.arange(num_groups, device=weight.device).expand(num_layers, -1))`\n        This creates `group_permutation` such that `group_permutation[layer, new_group_idx[layer, k]] = k`.\n        If `new_group_idx` has duplicates, it's problematic. `new_group_idx` will have duplicates if `groups_per_node > 1`.\n        Example: `new_group_idx = [[0, 0, 1, 1]]`.\n        `group_permutation[layer, 0]` gets assigned `0`. Then `group_permutation[layer, 0]` gets assigned `1`.\n        So `group_permutation[layer, 0]` becomes `1`.\n        `group_permutation[layer, 1]` becomes `2`.\n        `group_permutation[layer, 1]` becomes `3`.\n        So `group_permutation[layer, 1]` becomes `3`.\n        Resulting `group_permutation` is `[layer, [1, 3, ?, ?]]`. The last two elements are uninitialized.\n\n        **Proposed Change 1: Fix `group_permutation` calculation.**\n        Replace the `scatter_` call with `gather` using `argsort`.\n\n        ```python\n        # Original:\n        # group_permutation = torch.empty((num_layers, num_groups),\n        #                                 dtype=torch.int64,\n        #                                 device=weight.device)\n        # group_permutation.scatter_(1, new_group_idx,\n        #                            torch.arange(num_groups, device=weight.device).expand(num_layers, -1))\n\n        # Corrected approach:\n        # Get the permutation that sorts `new_group_idx` for each layer.\n        # This permutation tells us the original group index for each sorted middle expert slot.\n        sorted_indices_of_groups = torch.argsort(new_group_idx, dim=1)\n        \n        # Create the `group_permutation` that maps sorted middle expert slots to original group indices.\n        # This is equivalent to the `mlog_to_orig_log_map` for groups.\n        # We need a tensor `P` such that `P[l, new_group_idx[l, k]] = k`.\n        # This is exactly what `scatter_` does, but `new_group_idx` must be unique.\n        # If `new_group_idx` is not unique, we need a different approach.\n\n        # Let's construct the final ordering directly.\n        # We need to group original groups by node and rank.\n        # For each layer, we have `num_groups` original groups.\n        # We want to arrange them into `num_nodes` blocks, each of size `groups_per_node`.\n        # The order within each block is determined by `group_rank_in_pack`.\n\n        # Example: num_groups=4, num_nodes=2, groups_per_node=2\n        # group_pack_index: [[0, 0, 1, 1]]\n        # group_rank_in_pack: [[0, 1, 0, 1]]\n        # new_group_idx: [[0, 1, 2, 3]]\n        \n        # We want to arrange original groups [0, 1, 2, 3] into [node0_groups, node1_groups]\n        # Node 0 groups: group 0 (rank 0), group 1 (rank 1)\n        # Node 1 groups: group 2 (rank 0), group 3 (rank 1)\n        # So, the desired order of original groups is [0, 1, 2, 3].\n        # This is already the order if `new_group_idx` is sorted.\n\n        # The problem arises if `new_group_idx` is not naturally sorted.\n        # Example:\n        # group_pack_index: [[0, 1, 0, 1]]\n        # group_rank_in_pack: [[1, 0, 0, 1]]\n        # new_group_idx: [[0*2+1, 1*2+0, 0*2+0, 1*2+1]] = [[1, 2, 0, 3]]\n        # The middle expert indices are 0, 1, 2, 3.\n        # Middle expert 0 is assigned to group 2.\n        # Middle expert 1 is assigned to group 0.\n        # Middle expert 2 is assigned to group 1.\n        # Middle expert 3 is assigned to group 3.\n\n        # We need to get `mlog_weights` of shape `[num_layers, num_nodes, groups_per_node * group_size]`\n        # where the innermost dimension is ordered by node and rank.\n\n        # Let's create a mapping from `(node, rank_in_node)` to `original_group_index`.\n        # For each layer:\n        # `node_assignments = group_pack_index[l]`\n        # `ranks = group_rank_in_pack[l]`\n        # `combined_key = node_assignments * groups_per_node + ranks` # [num_groups]\n\n        # We want to create `num_nodes * groups_per_node` slots.\n        # The `combined_key` directly gives the slot index for each original group.\n        # We need to gather original groups into these slots.\n        \n        # For layer `l`:\n        # `original_group_indices = torch.arange(num_groups, device=weight.device)`\n        # `slot_indices = combined_key`\n\n        # `grouped_original_groups = torch.empty_like(original_group_indices)`\n        # `grouped_original_groups.scatter_(0, slot_indices, original_group_indices)`\n        # This gives `grouped_original_groups` of shape `[num_groups]` where elements are ordered by slot.\n        # Example: `slot_indices = [1, 2, 0, 3]`.\n        # `grouped_original_groups[1] = 0`\n        # `grouped_original_groups[2] = 1`\n        # `grouped_original_groups[0] = 2`\n        # `grouped_original_groups[3] = 3`\n        # `grouped_original_groups` becomes `[2, 0, 1, 3]`\n\n        # `group_permutation = grouped_original_groups`\n\n        # The rest of the logic uses `gather` with this `group_permutation`.\n        # `weight_reshaped.gather(1, group_permutation.unsqueeze(-1).expand(-1, group_size))`\n        # This looks correct.\n\n        # So, the change is to replace the `scatter_` call with this `scatter` call.\n\n        ```python\n        # Replace the problematic scatter_ call with a correct scatter operation.\n        # We need to map original group indices to their sorted middle expert slot.\n        \n        # Calculate the slot index for each original group.\n        # `new_group_idx` is already calculated as `group_pack_index * groups_per_node + group_rank_in_pack`\n        # This is the index of the middle expert slot.\n        \n        # We want to arrange original groups based on these slot indices.\n        # Create the permutation that sorts the `new_group_idx`.\n        sorted_slot_indices = torch.argsort(new_group_idx, dim=1)\n        \n        # Create `group_permutation` such that `group_permutation[l, slot_index]` is the original group index.\n        # The `scatter_` operation can achieve this if the indices are unique.\n        # Since `new_group_idx` are unique slot indices, this should work.\n        # The original code used `group_permutation.scatter_(1, new_group_idx, ...)` which is wrong.\n        # The correct scatter should be `group_permutation.scatter_(1, slot_indices, original_indices)`.\n\n        # Let's reconstruct this section more cleanly.\n        # `new_group_idx` maps each original group `g` to its middle expert slot `m`.\n        # We need to gather original groups into these middle expert slots.\n        \n        # For each layer `l`:\n        # `original_group_indices_l = torch.arange(num_groups, device=weight.device)`\n        # `slot_indices_l = new_group_idx[l]` # [num_groups]\n        \n        # Create `group_permutation_l` of shape `[num_groups]` where `group_permutation_l[slot_index]` is the original group index.\n        # `group_permutation_l = torch.empty_like(original_group_indices_l)`\n        # `group_permutation_l.scatter_(0, slot_indices_l, original_group_indices_l)`\n        \n        # This needs to be done for all layers.\n        \n        # Initialize group_permutation for all layers\n        group_permutation = torch.empty((num_layers, num_groups), dtype=torch.int64, device=weight.device)\n        \n        # Get original group indices for each layer\n        original_group_indices_per_layer = torch.arange(num_groups, device=weight.device).expand(num_layers, -1)\n        \n        # Scatter operation to arrange original groups into their correct middle expert slots\n        # The `new_group_idx` provides the target indices (slot indices)\n        # The `original_group_indices_per_layer` provides the values to scatter\n        group_permutation.scatter_(1, new_group_idx, original_group_indices_per_layer)\n        \n        # Now `group_permutation[l, slot_index]` contains the original group index that belongs to that slot.\n        # This is the correct permutation to use with `gather`.\n\n        # The original code's `view` operation:\n        # `mlog_weights = mlog_weights_reordered_groups.view(num_layers, num_nodes, groups_per_node * group_size)`\n        # This requires `num_groups` to be divisible by `num_nodes` and the ordering from `gather` to align.\n        # The `group_permutation` generated by the correct scatter should ensure this alignment.\n        ```\n\n        **Proposed Change 2: Simplify `replicate_experts` input reshaping.**\n        The current code reshapes `mlog_weights` and `mlog_to_orig_log_map` to `[num_layers * num_nodes, num_middle_experts]`.\n        This is done by:\n        `reshaped_mlog_weights = mlog_weights.view(-1, num_middle_experts)`\n        `reshaped_mlog_to_orig_log_map = mlog_to_orig_log_map.view(-1, num_middle_experts)`\n\n        This seems correct given `mlog_weights` shape `[num_layers, num_nodes, num_middle_experts]`.\n        The flattening `view(-1, ...)` correctly creates `num_layers * num_nodes` rows.\n\n        **Proposed Change 3: Improve `logical_to_physical_map` construction.**\n        The current construction uses advanced indexing:\n        `logical_to_physical_map[layer_indices, phy2log, phyrank] = physical_expert_ids_expanded`\n        This is generally efficient.\n        The calculation of `max_replicas_per_expert` is important for correctly sizing the tensor.\n        The current logic for `max_replicas_per_expert`:\n        `max_replicas_per_expert = logcnt.max().item()`\n        `if max_replicas_per_expert == 0 and num_physical_experts_total > 0:`\n        `    max_replicas_per_expert = 1`\n        This seems reasonable.\n\n        However, the `logcnt` is `[num_layers, num_logical_experts]`.\n        The `phy2log` is `[num_layers, num_replicas]`.\n        The `phyrank` is `[num_layers, num_replicas]`.\n\n        The assignment `logical_to_physical_map[layer_indices, phy2log, phyrank] = physical_expert_ids_expanded`\n        Here `phy2log` is `[num_layers, num_replicas]`.\n        `phyrank` is `[num_layers, num_replicas]`.\n        The `layer_indices` is `[num_layers, 1]`.\n        The `physical_expert_ids_expanded` is `[num_layers, num_replicas]`.\n\n        This indexing `[num_layers, 1], [num_layers, num_replicas], [num_layers, num_replicas]` should work correctly.\n        It assigns `physical_expert_ids_expanded[l, phy2log[l, r]] = physical_expert_ids_expanded[l, r]` if `phyrank` was used as the index.\n        The assignment should be:\n        `logical_to_physical_map[l, phy2log[l, p], phyrank[l, p]] = physical_expert_ids[p]` for each layer `l` and physical expert `p`.\n\n        Let's trace the indices:\n        `layer_indices`: `[[0], [1], ..., [num_layers-1]]`\n        `phy2log`: `[[p2l_00, p2l_01, ...], [p2l_10, p2l_11, ...], ...]`\n        `phyrank`: `[[pr_00, pr_01, ...], [pr_10, pr_11, ...], ...]`\n        `physical_expert_ids_expanded`: `[[0, 1, ...], [0, 1, ...], ...]`\n\n        The indexing `logical_to_physical_map[layer_indices, phy2log, phyrank]` means:\n        For `l` from 0 to `num_layers-1`:\n            For `p` from 0 to `num_replicas-1`:\n                `logical_to_physical_map[l, phy2log[l, p], phyrank[l, p]] = physical_expert_ids_expanded[l, p]`\n\n        This seems correct. The current implementation uses `physical_expert_ids_expanded` which is `[num_layers, num_replicas]`.\n        The assignment is `logical_to_physical_map[l, phy2log[l, p], phyrank[l, p]] = physical_expert_ids_expanded[l, p]`.\n        This is correct.\n\n        **Potential for speed improvement:**\n        The `replicate_experts` function has loops:\n        ```python\n        for i in range(num_log, num_phy):\n            redundant_indices = (weight / logcnt).max(dim=-1).indices\n            phy2log[:, i] = redundant_indices\n            rank[:, i] = logcnt[arangen, redundant_indices]\n            logcnt[arangen, redundant_indices] += 1\n        ```\n        This loop iterates `num_phy - num_log` times.\n        Inside the loop:\n        - `weight / logcnt`: element-wise division.\n        - `.max(dim=-1).indices`: finds index of max value along the last dimension. This is a reduction.\n        - `logcnt[arangen, redundant_indices]`: advanced indexing.\n        - `logcnt[arangen, redundant_indices] += 1`: in-place update.\n\n        This loop is inherently sequential because `logcnt` is updated in each iteration, and the next iteration depends on the updated `logcnt`.\n        It might be possible to vectorize some parts, but the dependency on `logcnt` makes it tricky.\n        The current implementation of `replicate_experts` is likely efficient enough given the sequential dependency.\n\n        **Back to `rebalance_experts_hierarchical`:**\n        The primary suspect for regression and performance issues is the `group_permutation` calculation.\n\n        **Consider `balanced_packing` function:**\n        `balanced_packing` is called with `weight.float().sort(-1, descending=True)` twice. This is redundant.\n        The `sorted_weights, sorted_indices = weight.float().sort(-1, descending=True)` should be done only once.\n\n        **Proposed Change 4: Remove redundant sorting in `balanced_packing`.**\n        The `balanced_packing` function sorts the input `weight` twice.\n\n        ```python\n        # Inside balanced_packing:\n        # ...\n        # Sort weights and get indices\n        sorted_weights, sorted_indices = weight.float().sort(-1, descending=True) # First sort\n\n        # Store device from input weight\n        device = weight.device\n\n        # Sort weights and get indices # Second sort - redundant\n        sorted_weights, sorted_indices = weight.float().sort(-1, descending=True)\n        # ...\n        ```\n        The second sort should be removed.\n\n        **Final check on `rebalance_experts`:**\n        The construction of `logical_to_physical_map` is a critical part.\n        The current code:\n        ```python\n        # Create physical expert IDs to scatter\n        physical_expert_ids = torch.arange(num_physical_experts_total, dtype=torch.int64, device=device) # Use stored device\n\n        # Expand physical_expert_ids to match the shape of phy2log and phyrank for broadcasting\n        # Shape: [num_layers, num_physical_experts_total]\n        physical_expert_ids_expanded = physical_expert_ids.unsqueeze(0).expand(num_layers, num_physical_experts_total)\n\n        # Create layer indices for advanced indexing\n        layer_indices = torch.arange(num_layers, dtype=torch.int64, device=device).unsqueeze(1)\n        \n        # Assign physical expert IDs to the correct positions in the map using advanced indexing.\n        logical_to_physical_map[layer_indices, phy2log, phyrank] = physical_expert_ids_expanded\n        ```\n        This looks correct for assigning physical expert IDs.\n        The `phy2log` is `[num_layers, num_replicas]`.\n        The `phyrank` is `[num_layers, num_replicas]`.\n        The `layer_indices` is `[num_layers, 1]`.\n        The `physical_expert_ids_expanded` is `[num_layers, num_replicas]`.\n\n        The indexing `logical_to_physical_map[layer_indices, phy2log, phyrank]` means:\n        `logical_to_physical_map[l, phy2log[l, p], phyrank[l, p]]` where `l` comes from `layer_indices` and `p` from `phy2log` / `phyrank`.\n        This is not quite right. The `p` index should iterate over `num_replicas`.\n        The `layer_indices` broadcasts correctly.\n        `phy2log` and `phyrank` are indexed by `[l, p]`.\n        The assignment `logical_to_physical_map[layer_indices, phy2log, phyrank]` should be interpreted as:\n        For each `l` in `[0, ..., num_layers-1]`:\n            `logical_to_physical_map[l, phy2log[l, :], phyrank[l, :]] = physical_expert_ids_expanded[l, :]`\n\n        This requires `phy2log` and `phyrank` to be indexed by `[l, p]`.\n        The current `phy2log` and `phyrank` are `[num_layers, num_replicas]`.\n        This means `phy2log[l, p]` is the logical expert ID for physical expert `p` in layer `l`.\n        `phyrank[l, p]` is the rank of physical expert `p` in layer `l`.\n\n        The assignment `logical_to_physical_map[layer_indices, phy2log, phyrank]` is equivalent to:\n        `logical_to_physical_map[l, phy2log[l, :], phyrank[l, :]] = physical_expert_ids_expanded[l, :]` for each `l`.\n        This seems to be the correct interpretation and implementation of advanced indexing.\n\n        Let's focus on the `group_permutation` fix.\n\n        **Proposed Change 1 (Revised):**\n        The correct way to construct `group_permutation` that aligns groups for the `view` operation.\n        We need to arrange the original groups (`0..num_groups-1`) such that groups belonging to node 0 come first (ordered by their rank within node), then groups for node 1, etc.\n\n        `new_group_idx = group_pack_index * groups_per_node + group_rank_in_pack`\n        This `new_group_idx` is the index of the middle expert slot.\n        We want to map original group indices to these slots.\n\n        Consider the structure:\n        `mlog_weights` shape: `[num_layers, num_nodes, groups_per_node * group_size]`\n        This means for layer `l`, node `n`, we have `groups_per_node * group_size` middle experts.\n        The first `groups_per_node` middle experts belong to node 0, next `groups_per_node` to node 1, etc.\n        Total middle experts per layer is `num_nodes * groups_per_node * group_size = num_groups * group_size = num_logical_experts`.\n\n        So, `mlog_weights` should be `[num_layers, num_logical_experts]`.\n        The previous analysis of the shape `[num_layers, num_nodes, num_logical_experts // num_nodes]` might be based on an incorrect interpretation of the intermediate `mlog_weights` structure.\n\n        Let's re-read the `replicate_experts` call:\n        `reshaped_mlog_weights = mlog_weights.view(-1, num_middle_experts)`\n        `num_middle_experts = num_logical_experts // num_nodes`\n        If `mlog_weights` is `[num_layers, num_nodes, num_logical_experts // num_nodes]`, then `view(-1, num_logical_experts // num_nodes)` results in `[num_layers * num_nodes, num_logical_experts // num_nodes]`.\n        This shape matches the expected input for `replicate_experts`.\n\n        So the structure `[num_layers, num_nodes, num_logical_experts // num_nodes]` is indeed intended.\n        The key is how the `view` operation reshapes `[num_layers, num_groups, group_size]` into this.\n        This requires `num_groups` to be partitioned into `num_nodes` blocks of size `groups_per_node`, and then these blocks combined.\n\n        Let's reconsider the `group_permutation`.\n        `group_permutation = torch.empty((num_layers, num_groups), ...)`\n        `group_permutation.scatter_(1, new_group_idx, torch.arange(num_groups, ...))` - this was the original problematic line.\n\n        The goal: `mlog_weights_reordered_groups` needs to be such that when viewed as `[num_layers, num_nodes, groups_per_node * group_size]`, the elements are correctly ordered.\n        This means for layer `l`, node `n`, the elements corresponding to middle experts `0` to `groups_per_node * group_size - 1` should be present.\n\n        The `group_permutation` should be the indices of the original groups that, when gathered, produce the correctly ordered sequence for each node.\n\n        Let's use `torch.block_diag` concept.\n        For each layer:\n            `node_assignments = group_pack_index[l]`\n            `ranks = group_rank_in_pack[l]`\n            `combined_key = node_assignments * groups_per_node + ranks` # [num_groups]\n\n            We need to group original groups `0..num_groups-1` by `node_assignments` and `ranks`.\n            For node 0: groups `g` where `node_assignments[g] == 0`, ordered by `ranks[g]`.\n            For node 1: groups `g` where `node_assignments[g] == 1`, ordered by `ranks[g]`.\n            ...\n\n            Let's create `num_layers * num_nodes` blocks. Each block is `groups_per_node` groups.\n            We need to construct the permutation that flattens these blocks.\n\n            Consider `new_group_idx` as the target index for each original group.\n            `new_group_idx`: `[num_layers, num_groups]`\n            We want to produce `group_permutation` such that `group_permutation[l, k]` is the original group index that belongs to the `k`-th middle expert slot in layer `l`.\n            The `k`-th middle expert slot corresponds to `new_group_idx[l, g]` for some `g`.\n            We need to find `g` for each `k`.\n\n            This is exactly what `torch.argsort` on `new_group_idx` does.\n            `sorted_group_indices_by_slot = torch.argsort(new_group_idx, dim=1)`\n            `group_permutation = sorted_group_indices_by_slot`\n\n            The `group_permutation` will be `[num_layers, num_groups]`.\n            This `group_permutation` should be used to gather the groups.\n            `weight_reshaped.gather(1, group_permutation.unsqueeze(-1).expand(-1, -1, group_size))`\n            This yields `[num_layers, num_groups, group_size]`.\n\n            Then `view(num_layers, num_nodes, groups_per_node * group_size)` is applied.\n            This reshapes the `num_groups` dimension into `num_nodes` blocks of `groups_per_node * group_size`.\n            This requires the `group_permutation` to have ordered the groups correctly.\n            The `torch.argsort(new_group_idx)` does exactly this. It sorts the original group indices based on their assigned middle expert slot.\n\n        So, the change should be:\n        Replace the `scatter_` based `group_permutation` with `torch.argsort`.\n\n        ```python\n        # Original code snippet for group_permutation:\n        # group_permutation = torch.empty((num_layers, num_groups),\n        #                                 dtype=torch.int64,\n        #                                 device=weight.device)\n        # group_permutation.scatter_(1, new_group_idx,\n        #                            torch.arange(num_groups, device=weight.device).expand(num_layers, -1))\n\n        # Proposed corrected code snippet:\n        # `new_group_idx` is already computed as `group_pack_index * groups_per_node + group_rank_in_pack`\n        # This `new_group_idx` maps each original group to its middle expert slot index.\n        # We need to arrange the original groups according to these slot indices.\n        # `torch.argsort` on `new_group_idx` gives the permutation of original group indices\n        # that sorts them by their slot index.\n        sorted_indices_for_groups = torch.argsort(new_group_idx, dim=1)\n        group_permutation = sorted_indices_for_groups\n        ```\n        This `group_permutation` is then used with `gather`. This appears to be the most likely fix for the performance regression and potential correctness issues.\n\n        Let's also address the redundant sort in `balanced_packing`.\n\n        ```python\n        # Proposed Change 4: Remove redundant sort in balanced_packing\n        # Inside balanced_packing:\n        # ...\n        # Sort weights and get indices\n        # sorted_weights, sorted_indices = weight.float().sort(-1, descending=True) # First sort\n        # ...\n        # # Sort weights and get indices # Second sort - redundant\n        # sorted_weights, sorted_indices = weight.float().sort(-1, descending=True)\n        # ...\n        ```\n        The second sort operation should be removed.\n\n\n        One more check: `replicate_experts`.\n        The calculation `(weight / logcnt).max(dim=-1).indices` can be slow if `logcnt` is large.\n        However, `logcnt` is `[X, num_log]`, where `X = num_layers * num_nodes` and `num_log = num_logical_experts // num_nodes`.\n        The number of logical experts (`num_log`) is usually much smaller than the total number of physical experts.\n        The division and max operation is performed for each of the `X` rows independently.\n        This seems to be the core of the replication logic and is hard to optimize further without changing the algorithm.\n\n        The primary focus should be on the `rebalance_experts_hierarchical` function's group permutation logic.\n\n        Final proposed changes:\n        1.  Fix `group_permutation` calculation in `rebalance_experts_hierarchical` using `torch.argsort`.\n        2.  Remove redundant sort in `balanced_packing`.\n\n        Let's draft the diffs.\n\n        For Change 1 (Fixing `group_permutation`):\n        Locate the lines related to `group_permutation` construction within `rebalance_experts_hierarchical`.\n\n        ```python\n        # Current lines:\n        # group_permutation = torch.empty((num_layers, num_groups),\n        #                                 dtype=torch.int64,\n        #                                 device=weight.device)\n        # group_permutation.scatter_(1, new_group_idx,\n        #                            torch.arange(num_groups, device=weight.device).expand(num_layers, -1))\n\n        # Proposed replacement:\n        # `new_group_idx` is already computed as `group_pack_index * groups_per_node + group_rank_in_pack`\n        # This `new_group_idx` maps each original group to its middle expert slot index.\n        # We need to arrange the original groups according to these slot indices.\n        # `torch.argsort` on `new_group_idx` gives the permutation of original group indices\n        # that sorts them by their slot index.\n        sorted_indices_for_groups = torch.argsort(new_group_idx, dim=1)\n        group_permutation = sorted_indices_for_groups\n        ```\n\n        For Change 2 (Redundant sort in `balanced_packing`):\n        Locate the `balanced_packing` function and the second sort.\n\n        ```python\n        # Current lines within balanced_packing:\n        # ...\n        # # Sort weights and get indices\n        # sorted_weights, sorted_indices = weight.float().sort(-1, descending=True) # First sort\n        # ...\n        # # Store device from input weight\n        # device = weight.device\n        # # Sort weights and get indices # Second sort - redundant\n        # sorted_weights, sorted_indices = weight.float().sort(-1, descending=True)\n        # ...\n        ```\n        The second sort line needs to be removed.\n\n```python\n# Proposed Change 1: Fix group_permutation calculation in rebalance_experts_hierarchical\n<<<<<<< SEARCH\n    # group_permutation = torch.empty((num_layers, num_groups),\n    #                                 dtype=torch.int64,\n    #                                 device=weight.device)\n    # group_permutation.scatter_(1, new_group_idx,\n    #                            torch.arange(num_groups, device=weight.device).expand(num_layers, -1))\n\n    # `new_group_idx` is already computed as `group_pack_index * groups_per_node + group_rank_in_pack`\n    # This `new_group_idx` maps each original group to its middle expert slot index.\n    # We need to arrange the original groups according to these slot indices.\n    # `torch.argsort` on `new_group_idx` gives the permutation of original group indices\n    # that sorts them by their slot index.\n    sorted_indices_for_groups = torch.argsort(new_group_idx, dim=1)\n    group_permutation = sorted_indices_for_groups\n=======\n    # `new_group_idx` is already computed as `group_pack_index * groups_per_node + group_rank_in_pack`\n    # This `new_group_idx` maps each original group to its middle expert slot index.\n    # We need to arrange the original groups according to these slot indices.\n    # `torch.argsort` on `new_group_idx` gives the permutation of original group indices\n    # that sorts them by their slot index.\n    sorted_indices_for_groups = torch.argsort(new_group_idx, dim=1)\n    group_permutation = sorted_indices_for_groups\n>>>>>>> REPLACE\n\n# Proposed Change 2: Remove redundant sort in balanced_packing\n<<<<<<< SEARCH\n    # Sort weights and get indices\n    sorted_weights, sorted_indices = weight.float().sort(-1, descending=True)\n\n    # Store device from input weight\n    device = weight.device\n\n    # Sort weights and get indices\n    sorted_weights, sorted_indices = weight.float().sort(-1, descending=True)\n\n    # Initialize pack_index and rank_in_pack on the correct device\n    pack_index = torch.full((num_layers, num_groups), -1, dtype=torch.int64, device=device)\n=======\n    # Sort weights and get indices\n    sorted_weights, sorted_indices = weight.float().sort(-1, descending=True)\n\n    # Store device from input weight\n    device = weight.device\n\n    # Initialize pack_index and rank_in_pack on the correct device\n    pack_index = torch.full((num_layers, num_groups), -1, dtype=torch.int64, device=device)\n>>>>>>> REPLACE\n```\n\nThese changes address the identified issues:\n1.  **Correctness and Balancedness:** The fix in `rebalance_experts_hierarchical` ensures that the grouping of logical experts into middle experts per node is done correctly, which is crucial for balancedness. The original `scatter_` operation was likely flawed.\n2.  **Efficiency:** Removing the redundant sort in `balanced_packing` is a small but direct efficiency improvement. The main performance gain is expected from the correctness fix in `rebalance_experts_hierarchical`, as it involves complex tensor operations that were likely contributing to overhead or incorrect behavior.\n\nThese changes are targeted and aim to restore the performance seen in previous attempts, which had significantly better scores."]}}}