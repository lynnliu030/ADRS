{"id": "6d8fbab6-fefd-4679-87a0-d1377bf3dd35", "code": "# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort groups by weight in descending order.\n    # 'weight' is already float and on CPU from rebalance_experts,\n    # so no need for .float().cpu() here.\n    indices = weight.sort(-1, descending=True).indices\n\n    # Initialize pack_index and rank_in_pack.\n    # These will be fully populated, so `empty_like` is sufficient.\n    # They will be on the same device as 'weight' (i.e., CPU).\n    pack_index = torch.empty_like(weight, dtype=torch.int64)\n    rank_in_pack = torch.empty_like(pack_index)\n    \n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These tensors are on the same device as 'weight' (which is CPU).\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id in indices[i]:\n            # Get current state for this layer's packs.\n            current_pack_weights = pack_weights_per_layer[i]\n            current_pack_items = pack_items_per_layer[i]\n\n            # Find the pack with the minimum current weight among available ones.\n            # Use torch.where to effectively mask out full packs with infinity,\n            # then find the argmin. This avoids an explicit clone and subsequent\n            # masked assignment, potentially being more efficient.\n            pack = torch.argmin(\n                torch.where(current_pack_items < groups_per_pack,\n                            current_pack_weights,\n                            torch.full_like(current_pack_weights, float('inf')))\n            )\n\n            # Assert that a valid pack was found and it has space.\n            # Using pack directly as a 0-dim tensor for indexing.\n            assert current_pack_items[pack] < groups_per_pack, \\\n                f\"No available packs found, but expected to find one. Pack {pack} is full. \" \\\n                f\"Current pack items: {current_pack_items}, groups_per_pack: {groups_per_pack}\"\n\n            # Assign the group to the chosen pack.\n            # group_id is a 0-dim tensor, so it works directly for indexing.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            # weight[i, group_id] is the weight of the current group (0-dim tensor).\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    # Cast weight to float for calculations if not already, and keep on original device.\n    # Moving to CPU adds overhead if data is already on GPU and might slow down ops.\n    weight = weight.float()\n    \n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    # Construct logical_to_physical_map (log2phy) more efficiently.\n    # The current method creates a large tensor and scatters into a view.\n    # This new method uses advanced indexing for scatter, which is generally faster.\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    \n    # Initialize log2phy with -1.\n    log2phy = torch.full((num_layers, num_logical_experts, maxlogcnt), -1,\n                         dtype=torch.int64, device=logcnt.device)\n\n    # Efficiently populate log2phy using advanced indexing for scatter.\n    # Iterate through each physical expert and assign it to its logical expert and rank.\n    # This avoids creating the intermediate flattened tensor and scatter indices.\n    for p in range(num_replicas):\n        # Get the logical expert ID and its rank for the current physical expert p.\n        log_idx_for_p = phy2log[:, p]\n        rank_for_p = phyrank[:, p]\n\n        # Create a tensor of layer indices to use with advanced indexing.\n        layer_indices = torch.arange(num_layers, device=logcnt.device)\n\n        # Filter out assignments where log_idx is -1 (not assigned or invalid).\n        valid_mask = (log_idx_for_p != -1) & (rank_for_p < maxlogcnt)\n\n        # Apply scatter using advanced indexing.\n        # We scatter the physical expert index 'p' into the correct slot\n        # log2phy[layer_indices[valid_mask], log_idx_for_p[valid_mask], rank_for_p[valid_mask]]\n        log2phy[layer_indices[valid_mask], log_idx_for_p[valid_mask], rank_for_p[valid_mask]] = p\n\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n", "language": "python", "parent_id": "5a42d900-c883-4117-958b-ad4263a1ac7a", "generation": 6, "timestamp": 1751059172.926539, "iteration_found": 145, "metrics": {"balancedness_score": 0.29830835153138974, "speed_score": 0.02477245974681649, "combined_score": 0.1615404056391031}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 15 lines with 30 lines", "parent_metrics": {"balancedness_score": 0.29830835153138974, "speed_score": 0.026640255592653206, "combined_score": 0.16247430356202147}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve the Mixture-of-Expert models Expert Parallelism Load Balancer (MoE EPLB) expert rearrangement algorithm.\nThis algorithm will take the load metrics recorded by the vLLM server, and rearrange the experts to balance the load. It can make replicas of some experts to achieve better load balancing.\nYour goal will be two-fold: 1. Improve the algorithm to achieve better load balancing; while 2. Improve the algorithm to be more efficient, i.e. reduce the execution time of the algorithm itself, since perfect load balancing is NP-hard.\nThe current algorithm is implemented in the `rebalance_experts` function. ", "user": "# Current Program Information\n- Current performance metrics: - balancedness_score: 0.2983\n- speed_score: 0.0266\n- combined_score: 0.1625\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: speed_score, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0835, combined_score: 0.1909\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0845, combined_score: 0.1914\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0857, combined_score: 0.1920\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.1920)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0857), Performs well on combined_score (0.1920)\n\n\n### Program 2 (Score: 0.1914)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0845), Performs well on combined_score (0.1914)\n\n\n### Program 3 (Score: 0.1909)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0835), Performs well on combined_score (0.1909)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.1907)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n\n### Program D2 (Score: 0.1908)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.1920, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 2 (Score: 0.0939, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 23 lines with 236 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 3 (Score: 0.0000, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 61 lines with 46 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 4 (Score: 0.0000, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 5 (Score: 0.1595, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 6 lines with 8 lines\nChange 2: Replace 3 lines with 3 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n\n# Current Program\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort groups by weight in descending order.\n    # 'weight' is already float and on CPU from rebalance_experts,\n    # so no need for .float().cpu() here.\n    indices = weight.sort(-1, descending=True).indices\n\n    # Initialize pack_index and rank_in_pack.\n    # These will be fully populated, so `empty_like` is sufficient.\n    # They will be on the same device as 'weight' (i.e., CPU).\n    pack_index = torch.empty_like(weight, dtype=torch.int64)\n    rank_in_pack = torch.empty_like(pack_index)\n    \n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These tensors are on the same device as 'weight' (which is CPU).\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id in indices[i]:\n            # Get current state for this layer's packs.\n            current_pack_weights = pack_weights_per_layer[i]\n            current_pack_items = pack_items_per_layer[i]\n\n            # Find the pack with the minimum current weight among available ones.\n            # Use torch.where to effectively mask out full packs with infinity,\n            # then find the argmin. This avoids an explicit clone and subsequent\n            # masked assignment, potentially being more efficient.\n            pack = torch.argmin(\n                torch.where(current_pack_items < groups_per_pack,\n                            current_pack_weights,\n                            torch.full_like(current_pack_weights, float('inf')))\n            )\n\n            # Assert that a valid pack was found and it has space.\n            # Using pack directly as a 0-dim tensor for indexing.\n            assert current_pack_items[pack] < groups_per_pack, \\\n                f\"No available packs found, but expected to find one. Pack {pack} is full. \" \\\n                f\"Current pack items: {current_pack_items}, groups_per_pack: {groups_per_pack}\"\n\n            # Assign the group to the chosen pack.\n            # group_id is a 0-dim tensor, so it works directly for indexing.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            # weight[i, group_id] is the weight of the current group (0-dim tensor).\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    # Cast weight to float for calculations if not already, and keep on original device.\n    # Moving to CPU adds overhead if data is already on GPU and might slow down ops.\n    weight = weight.float()\n    \n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["The current performance metrics show a good `balancedness_score` but a poor `speed_score`, leading to a suboptimal `combined_score`. The goal is to improve the `speed_score` while maintaining or improving the `balancedness_score`.\n\nThe `rebalance_experts` function acts as an entry point and calls either `rebalance_experts_hierarchical` or a simplified version of it. The core logic for balancing seems to be within `balanced_packing` and `replicate_experts`.\n\nLooking at the `rebalance_experts_hierarchical` function, there are several tensor operations that could be optimized for speed. Specifically, the repeated use of `gather` and `view` operations, especially when combined with broadcasting and reshaping, can be computationally expensive.\n\nThe current implementation of `rebalance_experts_hierarchical` has a section where it calculates `tokens_per_mlog`:\n```python\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n```\nThis `gather` operation followed by `view` might be a bottleneck. We can potentially optimize this by directly reshaping or using more efficient indexing if possible.\n\nAnother area to consider is the logic for creating `log2phy`:\n```python\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n```\nThis part involves creating a large tensor and then scattering into it. While `scatter_` is generally efficient, the intermediate tensor creation and the indexing logic could be streamlined.\n\nLet's focus on optimizing `tokens_per_mlog` calculation and the `log2phy` construction.\n\nHere's a proposed change to optimize the `tokens_per_mlog` calculation:\n\nThe current code first flattens `weight` and then gathers. `mlog2log` is `[num_layers * num_nodes, num_logical_experts // num_nodes]`.\nThe `gather` operation `weight.gather(-1, mlog2log)` will result in a tensor of shape `[num_layers * num_nodes, num_logical_experts // num_nodes]`.\nThe subsequent `.view(-1, num_logical_experts // num_nodes)` is redundant as the shape is already correct. However, the intent might be to ensure the data is contiguous for the next step.\n\nA more direct approach might be to first unflatten `weight` to expose the node structure, then gather, and then flatten.\n\nLet's analyze the shapes more carefully:\n`weight`: `[num_layers, num_logical_experts]`\n`mlog2log`: `[num_layers * num_nodes, num_logical_experts // num_nodes]`\n\nThe goal of `tokens_per_mlog = weight.gather(-1, mlog2log).view(-1, num_logical_experts // num_nodes)` is to obtain a tensor where each row corresponds to a `(layer, node)` combination, and the columns are the logical experts assigned to that node.\n\nThe `weight.gather(-1, mlog2log)` operation expects `mlog2log` to have the same number of dimensions as `weight` (which is 2). However, `mlog2log` has 2 dimensions and `weight` has 2 dimensions. The gather will operate along the last dimension of `weight` using indices from `mlog2log`.\nThe result of `weight.gather(-1, mlog2log)` will have shape `[num_layers, num_groups, group_size]` if `mlog2log` was `[num_layers, num_groups, group_size]`.\nBut `mlog2log` is `[num_layers * num_nodes, num_logical_experts // num_nodes]`.\n\nLet's re-examine `log2mlog`:\n`group_pack_index`: `[num_layers, num_groups]`\n`group_rank_in_pack`: `[num_layers, num_groups]`\n`groups_per_node`: `num_groups // num_nodes`\n`group_size`: `num_logical_experts // num_groups`\n\n`log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) * group_size).unsqueeze(-1) + torch.arange(group_size, dtype=torch.int64, device=group_pack_index.device)).flatten(-2)`\n\nThe expression `(group_pack_index * groups_per_node + group_rank_in_pack)` produces a tensor of shape `[num_layers, num_groups]`, where each value is the index of the logical expert within the `num_logical_experts` space, but grouped by node.\nSpecifically, `group_pack_index * groups_per_node` distributes the groups across nodes. `group_rank_in_pack` assigns ranks within each node.\nThe result is `[num_layers, num_groups]`, mapping logical groups to physical nodes.\n\nThen, `( ... * group_size)` shifts these indices to represent the start of each logical group.\n`.unsqueeze(-1)` makes it `[num_layers, num_groups, 1]`.\n`+ torch.arange(group_size, ...)` adds the offset within each group.\nThis results in `[num_layers, num_groups, group_size]`, which correctly represents the mapping of logical experts to groups.\nFinally, `.flatten(-2)` flattens the last two dimensions, resulting in `log2mlog` of shape `[num_layers, num_logical_experts]`. This is incorrect based on the comment `[num_layers * num_nodes, num_logical_experts // num_nodes]`.\n\nLet's re-read the `rebalance_experts_hierarchical` logic.\nThe comment `[num_layers * num_nodes, num_logical_experts // num_nodes]` for `tokens_per_mlog` suggests that the intention is to operate on experts grouped by node.\n\nThe `group_pack_index` and `group_rank_in_pack` are computed for `num_groups`.\n`tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)`\nThis produces `[num_layers, num_groups]`.\n\nThe `balanced_packing` function is called with `tokens_per_group` (shape `[num_layers, num_groups]`) and `num_nodes`.\nThis means `balanced_packing` will be applied independently for each layer.\nThe output `group_pack_index` (shape `[num_layers, num_groups]`) indicates which node each group of logical experts belongs to.\nThe output `group_rank_in_pack` (shape `[num_layers, num_groups]`) indicates the rank of the group within its assigned node.\n\nThe calculation of `log2mlog` seems to be mapping logical experts to \"meta-logical\" experts, where experts are grouped by node.\n`log2mlog` should map `num_logical_experts` to `num_nodes * (num_logical_experts // num_nodes)`.\nThe shape `[num_layers * num_nodes, num_logical_experts // num_nodes]` suggests that `log2mlog` is being reshaped or interpreted as `[num_layers, num_nodes, num_logical_experts // num_nodes]`.\n\nLet's assume the comment is correct and `mlog2log` has the shape `[num_layers * num_nodes, num_logical_experts // num_nodes]`.\nThis would imply that `log2mlog` was calculated differently, or that the `flatten(-2)` operation needs to be re-evaluated.\n\nIf `log2mlog` is intended to be `[num_layers, num_nodes, num_logical_experts // num_nodes]`, then the current calculation is flawed.\nThe `balanced_packing` function returns `pack_index` and `rank_in_pack` of shape `[num_layers, num_groups]`.\n\nLet's trace `log2mlog` with the correct shapes:\n`group_pack_index`: `[num_layers, num_groups]`\n`group_rank_in_pack`: `[num_layers, num_groups]`\n`groups_per_node`: `num_groups // num_nodes`\n`group_size`: `num_logical_experts // num_groups`\n\nThe expression `group_pack_index * groups_per_node + group_rank_in_pack` creates an index representing the \"meta-group\" on a node.\nLet's call this `node_group_idx`: `[num_layers, num_groups]`.\nThe values range from `0` to `num_nodes * groups_per_node - 1` which is `num_groups - 1`.\n\nThen, `(node_group_idx * group_size)` shifts these indices.\nThis becomes `[num_layers, num_groups]`.\n\nThe `.unsqueeze(-1)` makes it `[num_layers, num_groups, 1]`.\nThe `+ torch.arange(group_size, ...)` adds the offset within the group.\nResult: `[num_layers, num_groups, group_size]`. This correctly maps logical experts to their `(layer, group, offset_in_group)`.\n\nThe `.flatten(-2)` flattens `num_groups` and `group_size` into `num_logical_experts`.\nSo `log2mlog` has shape `[num_layers, num_logical_experts]`.\n\nNow, let's look at `mlog2log = inverse(log2mlog)`. If `log2mlog` is `[num_layers, num_logical_experts]`, then `mlog2log` is also `[num_layers, num_logical_experts]`.\n\nThe comment `[num_layers * num_nodes, num_logical_experts // num_nodes]` for `tokens_per_mlog` is where the confusion lies. It seems the code is trying to treat experts as `[num_layers, num_nodes, experts_per_node]` structure.\n\nLet's assume the intent is to create a structure `[num_layers, num_nodes, experts_per_node]` where `experts_per_node = num_logical_experts // num_nodes`.\nThis implies that `num_groups` should be a multiple of `num_nodes`, and `group_size` should be `num_logical_experts // num_groups`.\n\nThe `tokens_per_group` calculation: `weight.unflatten(-1, (num_groups, group_size)).sum(-1)` gives `[num_layers, num_groups]`.\n`balanced_packing(tokens_per_group, num_nodes)` means we are packing `num_groups` into `num_nodes`.\nThis results in `group_pack_index` and `group_rank_in_pack`, both `[num_layers, num_groups]`.\n\nThe `log2mlog` calculation:\n`node_group_idx = group_pack_index * groups_per_node + group_rank_in_pack`  (shape `[num_layers, num_groups]`)\n`log2mlog = (node_group_idx * group_size).unsqueeze(-1) + torch.arange(group_size, ...)` (shape `[num_layers, num_groups, group_size]`)\n`log2mlog = log2mlog.flatten(-2)` (shape `[num_layers, num_logical_experts]`)\n\nThis `log2mlog` maps each logical expert to its original index. The `group_pack_index` and `group_rank_in_pack` are used to determine which node this logical expert belongs to.\nThe `inverse` function then creates a permutation.\n\nThe comment `[num_layers * num_nodes, num_logical_experts // num_nodes]` for `tokens_per_mlog` is likely a misunderstanding or a relic of a previous approach. If `mlog2log` is `[num_layers, num_logical_experts]`, then `weight.gather(-1, mlog2log)` would be `[num_layers, num_logical_experts]`. The `.view(-1, num_logical_experts // num_nodes)` would then reshape this into `[num_layers * num_nodes, num_logical_experts // num_nodes]`. This reshaping implies that `num_logical_experts` must be divisible by `num_nodes`.\n\nLet's assume this reshaping is intended. The gather operation itself is the main potential bottleneck.\n\nConsider the `replicate_experts` function.\nIt takes `weight` of shape `[X, num_log]` and replicates it to `num_phy`.\nThe loop iterates from `num_log` to `num_phy`.\nInside the loop, `(weight / logcnt).max(dim=-1).indices` is computed. This is a critical part for load balancing, but it involves division, max, and argmax.\n\nLet's propose changes to optimize the `tokens_per_mlog` calculation and the `log2phy` construction.\n\n**Optimization 1: Streamlining `tokens_per_mlog` calculation.**\n\nThe current calculation:\n```python\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n```\nAssuming `mlog2log` has shape `[num_layers, num_logical_experts]`.\n`weight.gather(-1, mlog2log)` results in `[num_layers, num_logical_experts]`.\nThen `.view(-1, num_logical_experts // num_nodes)` reshapes it. This implies `num_logical_experts` must be divisible by `num_nodes`.\n\nLet's directly construct the desired `[num_layers * num_nodes, num_logical_experts // num_nodes]` tensor without the intermediate `[num_layers, num_logical_experts]` gather.\n\nWe need to group logical experts by node.\n`group_pack_index`: `[num_layers, num_groups]`\n`groups_per_node`: `num_groups // num_nodes`\n`group_size`: `num_logical_experts // num_groups`\n\nThe logical experts for node `k` are those where `group_pack_index` is `k`.\nThe experts within node `k` are `group_size` experts from each of the `groups_per_node` groups assigned to node `k`.\n\nLet's re-think the `log2mlog` calculation and its purpose.\nThe goal is to create a new set of \"meta-logical experts\" per node.\nThere are `num_nodes` such meta-logical experts per layer.\nEach meta-logical expert should contain `num_logical_experts // num_nodes` logical experts.\n\nThe `group_pack_index` tells us which node each logical *group* belongs to.\nLet's reorganize `weight` based on nodes.\n\nThe current `log2mlog` calculation:\n1. `node_group_idx = group_pack_index * groups_per_node + group_rank_in_pack`\n   This maps logical groups to a linear index of groups within nodes.\n   `[num_layers, num_groups]`\n2. `log2mlog = (node_group_idx * group_size).unsqueeze(-1) + torch.arange(group_size, ...)`\n   This maps logical experts to their global index, but organized by `(layer, group, offset_in_group)`.\n   `[num_layers, num_groups, group_size]`\n3. `log2mlog = log2mlog.flatten(-2)`\n   This flattens `num_groups` and `group_size` into `num_logical_experts`.\n   `[num_layers, num_logical_experts]`\n\nThe `weight.gather(-1, mlog2log)` applies this mapping to the `weight` tensor.\nThe result is `[num_layers, num_logical_experts]`.\nThen `.view(-1, num_logical_experts // num_nodes)` reshapes it.\n\nThis implies `mlog2log` should have been `[num_layers, num_nodes, num_logical_experts // num_nodes]` or similar for the `view` to make sense.\n\nLet's try to directly construct the `tokens_per_mlog` tensor with the shape `[num_layers * num_nodes, num_logical_experts // num_nodes]`.\n\nWe can iterate through nodes, and for each node, gather the relevant logical experts.\nFor node `n_idx` from `0` to `num_nodes - 1`:\n  Identify groups assigned to `n_idx`: `group_indices = torch.where(group_pack_index == n_idx)[1]`\n  These `group_indices` will correspond to `groups_per_node` groups.\n  For each of these `groups_per_node` groups:\n    Get the logical experts: `(group_idx * group_size)` to `(group_idx * group_size) + group_size - 1`.\n    These experts need to be collected for node `n_idx`.\n\nThis is becoming complex and might not yield a significant speedup if the underlying gather is already optimized.\n\nLet's reconsider the `log2phy` construction.\nThe current approach:\n```python\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n```\nHere, `phy2log` is `[num_layers, num_physical_experts]` and `phyrank` is `[num_layers, num_physical_experts]`.\nThe goal is to create `log2phy[l, log_idx, rank]` which stores the physical expert ID.\n\nThe expression `phy2log * maxlogcnt + phyrank` creates the target index within the flattened `[num_layers, num_logical_experts * maxlogcnt]` tensor.\n`phy2log * maxlogcnt` gives the base index for each logical expert, considering its replicas.\n`phyrank` adds the offset for the specific replica.\n\nThe `scatter_` operation is filling this tensor.\nThe number of physical experts `num_replicas` is used to create the values to scatter.\n`torch.arange(num_replicas, ...).expand(num_layers, -1)` provides the physical expert IDs to be scattered.\n\nThis part seems logically sound for constructing the mapping. The potential inefficiency might be in the size of `log2phy` and the `expand` operation.\n\nLet's look at the `replicate_experts` function again:\n```python\n    for i in range(num_log, num_phy):\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n```\nThis loop is executed `num_phy - num_log` times. Inside the loop, `(weight / logcnt).max(dim=-1)` is computed. This is a dominant cost.\n\nThe current `replicate_experts` is called with `tokens_per_mlog` which has shape `[num_layers * num_nodes, num_logical_experts // num_nodes]`.\nLet `X = num_layers * num_nodes` and `num_log = num_logical_experts // num_nodes`.\nThe loop runs `num_phy // num_nodes - num_logical_experts // num_nodes` times.\nInside the loop, `(weight / logcnt).max(dim=-1)` operates on `[X, num_log]`.\n\n**Proposed Optimization 2: Optimizing `replicate_experts` within `rebalance_experts_hierarchical`.**\n\nThe `replicate_experts` function is a general-purpose function. However, in the context of `rebalance_experts_hierarchical`, it's applied to `tokens_per_mlog`.\nThe `weight` argument to `replicate_experts` is `tokens_per_mlog`.\nThe `num_log` is `num_logical_experts // num_nodes`.\nThe `num_phy` is `num_physical_experts // num_nodes`.\n\nThe core operation is `(weight / logcnt).max(dim=-1)`.\nThis is a greedy approach to assign replicas.\n\nConsider if we can use a more optimized method for this assignment or if the data structure can be improved.\n\nLet's focus on the `log2phy` construction first, as it seems more amenable to direct code changes without altering the core greedy logic of `replicate_experts`.\n\nThe `scatter_` operation is efficient, but creating the `full` tensor and then scattering might have overhead.\nCan we directly compute the `log2phy` without an intermediate large tensor?\n\nThe structure of `log2phy` is:\n`log2phy[layer, logical_expert, replica_rank]` = `physical_expert_id`.\n\nWe have `phy2log` (shape `[num_layers, num_replicas]`) and `phyrank` (shape `[num_layers, num_replicas]`).\n`phyrank[l, p]` is the replica rank of physical expert `p` in layer `l`.\n`phy2log[l, p]` is the logical expert ID for physical expert `p` in layer `l`.\n\nWe want to populate `log2phy[l, log_id, rank]` with the physical expert ID.\nFor each layer `l` and each physical expert `p`:\n  `log_id = phy2log[l, p]`\n  `rank = phyrank[l, p]`\n  `physical_expert_id = p`\n  Set `log2phy[l, log_id, rank] = physical_expert_id`.\n\nThis can be achieved by iterating through layers and physical experts, or by clever indexing.\n\nLet's try to construct the indices for scattering directly.\n`layers_idx = torch.arange(num_layers).repeat_interleave(num_replicas)`\n`physical_experts_idx = torch.arange(num_replicas).repeat(num_layers)`\n\nThen `log_indices = phy2log.flatten()` and `rank_indices = phyrank.flatten()`.\nWe need to scatter `physical_experts_idx` into `log2phy` at `(layers_idx, log_indices, rank_indices)`.\n\nThe current scatter indices are `phy2log * maxlogcnt + phyrank`.\nThis assumes `log2phy` is flattened along the last two dimensions.\n`log2phy.view(num_layers, -1)` flattens `num_logical_experts * maxlogcnt`.\nThe indices `phy2log * maxlogcnt + phyrank` are indices into this flattened tensor.\n`phy2log` is `[num_layers, num_replicas]`.\n`phy2log * maxlogcnt` is `[num_layers, num_replicas]`.\n`phyrank` is `[num_layers, num_replicas]`.\nThe sum is `[num_layers, num_replicas]`.\n\nThe scatter target is `log2phy.view(num_layers, -1)`. This has shape `[num_layers, num_logical_experts * maxlogcnt]`.\nThe indices should map `[num_layers, num_replicas]` to `[num_layers, num_logical_experts * maxlogcnt]`.\n\nLet's confirm the shapes and indices:\n`log2phy`: `[num_layers, num_logical_experts, maxlogcnt]`\n`view(num_layers, -1)` gives `[num_layers, num_logical_experts * maxlogcnt]`\n\nThe scatter indices need to be `[num_layers, num_replicas]`.\nThe current indices are `phy2log * maxlogcnt + phyrank`.\n`phy2log` is `[num_layers, num_replicas]`. `maxlogcnt` is a scalar.\n`phy2log * maxlogcnt` has shape `[num_layers, num_replicas]`.\n`phyrank` has shape `[num_layers, num_replicas]`.\nThe sum `phy2log * maxlogcnt + phyrank` is `[num_layers, num_replicas]`.\n\nThe `scatter_` operation:\n`log2phy.view(num_layers, -1).scatter_(-1, scatter_indices, values)`\nThe `scatter_indices` must have the same shape as `values`.\nHere, `values` is `torch.arange(num_replicas, ...).expand(num_layers, -1)`. Its shape is `[num_layers, num_replicas]`.\nSo, `scatter_indices` must also have shape `[num_layers, num_replicas]`.\nThe current `phy2log * maxlogcnt + phyrank` correctly produces this shape.\n\nThe problem is that `phy2log * maxlogcnt + phyrank` are indices into `[num_logical_experts * maxlogcnt]` space for each layer.\nThe scatter operation needs to be on the last dimension (`dim=-1`).\nSo, for each layer `l`, we are scattering into `log2phy[l, :, :]`.\nThe indices are `phy2log[l, :] * maxlogcnt + phyrank[l, :]`.\nThese indices must be valid for the dimension `num_logical_experts * maxlogcnt`.\n\nThe maximum value of `phy2log * maxlogcnt + phyrank` would be approximately:\n`(num_logical_experts - 1) * maxlogcnt + (maxlogcnt - 1)`\nThis is roughly `num_logical_experts * maxlogcnt - 1`.\nThis matches the size of the dimension `num_logical_experts * maxlogcnt`.\n\nThe `scatter_` operation is efficient. The primary overhead is creating the `log2phy` tensor.\n\n**Proposed Change: Eliminate the intermediate `log2phy` tensor by directly constructing the final output.**\n\nInstead of creating a large `log2phy` and filling it, we can construct the output using the information from `phy2log` and `phyrank`.\n\nThe desired output is `log2phy: [layers, num_logical_experts, X]`.\nThe current function returns `log2phy`, `phyrank`, and `logcnt`. The `phyrank` returned is actually the replica rank for physical experts, not the mapping from logical to physical. The naming is a bit confusing.\n\nLet's clarify the return values:\n1. `physical_to_logical_map`: `[layers, num_replicas]`. This is `phy2log`.\n2. `logical_to_physical_map`: `[layers, num_logical_experts, X]`. This is what we called `log2phy`. `X` is `maxlogcnt`.\n3. `expert_count`: `[layers, num_logical_experts]`. This is `logcnt`.\n\nThe current implementation calculates `logcnt` correctly. It also calculates `phy2log` correctly.\nThe part that needs to be constructed is `log2phy`.\n\nLet's rewrite the construction of `log2phy` using a more direct approach.\nWe can create a list of tensors, one for each layer, and then stack them.\n\nFor each layer `l`:\n  Create a mapping from `(logical_expert, rank)` to `physical_expert`.\n  `layer_log2phy = torch.full((num_logical_experts, maxlogcnt), -1, dtype=torch.int64, device=logcnt.device)`\n  Get the relevant `phy2log` and `phyrank` for this layer:\n    `current_phy2log = phy2log[l, :]`\n    `current_phyrank = phyrank[l, :]`\n  Iterate through `p` from `0` to `num_replicas - 1`:\n    `log_idx = current_phy2log[p]`\n    `rank = current_phyrank[p]`\n    If `log_idx != -1` and `rank < maxlogcnt`:\n      `layer_log2phy[log_idx, rank] = p`\n  Append `layer_log2phy` to a list.\nFinally, `torch.stack(list_of_tensors, dim=0)`.\n\nThis approach avoids the large intermediate tensor `log2phy.view(num_layers, -1)` and the potentially complex scatter indices. It uses a loop over layers, which might be acceptable if the number of layers is not excessively large.\n\nLet's try to optimize the loop over physical experts within a layer.\nWe can use `scatter_` on the layer-specific tensor.\n\nFor each layer `l`:\n  `layer_log2phy = torch.full((num_logical_experts, maxlogcnt), -1, dtype=torch.int64, device=logcnt.device)`\n  `current_phy2log = phy2log[l, :]`\n  `current_phyrank = phyrank[l, :]`\n  `valid_mask = (current_phy2log != -1) & (current_phyrank < maxlogcnt)`\n  `scatter_indices_flat = current_phy2log[valid_mask] * maxlogcnt + current_phyrank[valid_mask]`\n  `scatter_values = torch.arange(num_replicas, device=logcnt.device)[valid_mask]` # This needs careful indexing\n\nLet's rethink the indices for scatter.\nWe need to scatter `physical_expert_id` into `layer_log2phy` at `(log_idx, rank)`.\nThe indices for `scatter_` need to be of shape `(num_valid_assignments, 2)` for `dim=1` scatter.\n\nLet's try to construct `log2phy` more directly.\nThe output `log2phy` is `[num_layers, num_logical_experts, maxlogcnt]`.\nWe have `phy2log` `[num_layers, num_replicas]` and `phyrank` `[num_layers, num_replicas]`.\n\nConsider a single layer `l`:\n`current_phy2log = phy2log[l, :]`\n`current_phyrank = phyrank[l, :]`\nWe want to fill `log2phy[l, :, :]`.\n\nWe can create a tensor of physical expert IDs: `physical_expert_ids = torch.arange(num_replicas, device=logcnt.device)`\nWe need to map these `physical_expert_ids` to `(log_idx, rank)`.\n\nA more efficient way to construct `log2phy` might be to use `torch.scatter_` with appropriate indices.\n\nLet's analyze the indices for `scatter_` again.\n`log2phy.view(num_layers, -1).scatter_(-1, indices, values)`\n`indices` should be `[num_layers, num_replicas]`\n`values` should be `[num_layers, num_replicas]`\n\nThe `values` are `torch.arange(num_replicas, dtype=torch.int64, device=log2phy.device).expand(num_layers, -1)`.\nThe `indices` are `phy2log * maxlogcnt + phyrank`.\n\nLet's optimize the calculation of these indices.\nThe `phy2log * maxlogcnt + phyrank` calculation is fine. The issue is the intermediate tensor creation and the `scatter_` on a view.\n\nConsider an alternative:\n`log2phy = torch.zeros((num_layers, num_logical_experts, maxlogcnt), dtype=torch.int64, device=logcnt.device) - 1`\nThen iterate through `p` from 0 to `num_replicas - 1`:\n  `log_idx = phy2log[:, p]`\n  `rank = phyrank[:, p]`\n  `log2phy[torch.arange(num_layers), log_idx, rank] = p`\n\nThis uses `scatter_` implicitly with advanced indexing. This is likely more efficient than creating a view and scattering.\n\n**Proposed Change 1: Optimize `log2phy` construction.**\n\n```python\n# EVOLVE-BLOCK-START\n# ... (previous functions)\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    # Cast weight to float for calculations if not already, and keep on original device.\n    # Moving to CPU adds overhead if data is already on GPU and might slow down ops.\n    weight = weight.float()\n\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        # The original code calls hierarchical with num_groups=1, num_nodes=1.\n        # This is effectively a global balancing.\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    \n    # Construct logical_to_physical_map (log2phy) more efficiently.\n    # The current method creates a large tensor and scatters into a view.\n    # This new method uses advanced indexing for scatter, which is generally faster.\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    \n    # Initialize log2phy with -1.\n    log2phy = torch.full((num_layers, num_logical_experts, maxlogcnt), -1,\n                         dtype=torch.int64, device=logcnt.device)\n\n    # Efficiently populate log2phy using advanced indexing for scatter.\n    # Iterate through each physical expert and assign it to its logical expert and rank.\n    # This avoids creating the intermediate flattened tensor and scatter indices.\n    for p in range(num_replicas):\n        # Get the logical expert ID and its rank for the current physical expert p.\n        log_idx_for_p = phy2log[:, p]\n        rank_for_p = phyrank[:, p]\n\n        # Create a tensor of layer indices to use with advanced indexing.\n        layer_indices = torch.arange(num_layers, device=logcnt.device)\n\n        # Filter out assignments where log_idx is -1 (not assigned or invalid).\n        valid_mask = (log_idx_for_p != -1) & (rank_for_p < maxlogcnt)\n\n        # Apply scatter using advanced indexing.\n        # We scatter the physical expert index 'p' into the correct slot\n        # log2phy[layer_indices[valid_mask], log_idx_for_p[valid_mask], rank_for_p[valid_mask]]\n        log2phy[layer_indices[valid_mask], log_idx_for_p[valid_mask], rank_for_p[valid_mask]] = p\n\n    return phy2log, log2phy, logcnt\n\n# EVOLVE-BLOCK-END\n__all__ = [\"rebalance_experts\"]\n```\n\n**Reasoning for the change:**\n\nThe original construction of `log2phy` involved:\n1. Creating a large tensor `log2phy` of shape `(num_layers, num_logical_experts * maxlogcnt)`.\n2. Calculating scatter indices `phy2log * maxlogcnt + phyrank` of shape `(num_layers, num_replicas)`.\n3. Using `scatter_` on a view of `log2phy` with these indices.\n\nThe proposed change replaces this with a loop over the number of physical experts (`num_replicas`). Inside the loop:\n1. It retrieves the logical expert ID and rank for each layer for the current physical expert `p`.\n2. It uses `torch.arange(num_layers)` to create layer indices.\n3. It applies a `valid_mask` to filter out invalid assignments.\n4. It then uses advanced indexing directly on `log2phy` to place the physical expert ID `p` into the correct `(layer, logical_expert, rank)` slot.\n\nThis approach is generally more efficient in PyTorch for populating tensors with scattered values when the target indices can be computed directly. It avoids the overhead of creating a large intermediate tensor for scattering and potentially complex index calculations for `scatter_` on a view. The loop over `num_replicas` is a trade-off, but if `num_replicas` is manageable and the number of layers isn't excessively large, this can be faster. The key is that the scattering is done efficiently using PyTorch's advanced indexing capabilities, which are highly optimized.\n\nThis change focuses on improving the `speed_score` by optimizing the tensor manipulation for constructing the output mapping. It does not alter the core balancing logic within `rebalance_experts_hierarchical` or `replicate_experts`, so the `balancedness_score` should remain similar or improve slightly due to more precise indexing.\n\nThe change in `rebalance_experts` function:\n<<<<<<< SEARCH\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n=======\n    # Construct logical_to_physical_map (log2phy) more efficiently.\n    # The current method creates a large tensor and scatters into a view.\n    # This new method uses advanced indexing for scatter, which is generally faster.\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    \n    # Initialize log2phy with -1.\n    log2phy = torch.full((num_layers, num_logical_experts, maxlogcnt), -1,\n                         dtype=torch.int64, device=logcnt.device)\n\n    # Efficiently populate log2phy using advanced indexing for scatter.\n    # Iterate through each physical expert and assign it to its logical expert and rank.\n    # This avoids creating the intermediate flattened tensor and scatter indices.\n    for p in range(num_replicas):\n        # Get the logical expert ID and its rank for the current physical expert p.\n        log_idx_for_p = phy2log[:, p]\n        rank_for_p = phyrank[:, p]\n\n        # Create a tensor of layer indices to use with advanced indexing.\n        layer_indices = torch.arange(num_layers, device=logcnt.device)\n\n        # Filter out assignments where log_idx is -1 (not assigned or invalid).\n        valid_mask = (log_idx_for_p != -1) & (rank_for_p < maxlogcnt)\n\n        # Apply scatter using advanced indexing.\n        # We scatter the physical expert index 'p' into the correct slot\n        # log2phy[layer_indices[valid_mask], log_idx_for_p[valid_mask], rank_for_p[valid_mask]]\n        log2phy[layer_indices[valid_mask], log_idx_for_p[valid_mask], rank_for_p[valid_mask]] = p\n\n    return phy2log, log2phy, logcnt\n>>>>>>> REPLACE"]}}}