{"id": "0f7193ba-ee7f-45ef-84cb-3f69803f1356", "code": "# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort groups by weight in descending order.\n    # 'weight' is already float and on CPU from rebalance_experts,\n    # so no need for .float().cpu() here.\n    indices = weight.sort(-1, descending=True).indices\n\n    # Initialize pack_index and rank_in_pack.\n    # These will be fully populated, so `empty_like` is sufficient.\n    # They will be on the same device as 'weight' (i.e., CPU).\n    pack_index = torch.empty_like(weight, dtype=torch.int64)\n    rank_in_pack = torch.empty_like(pack_index)\n    \n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These tensors are on the same device as 'weight' (which is CPU).\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id in indices[i]:\n            # Get current state for this layer's packs.\n            current_pack_weights = pack_weights_per_layer[i]\n            current_pack_items = pack_items_per_layer[i]\n\n            # Find the pack with the minimum current weight among available ones.\n            # Use torch.where to effectively mask out full packs with infinity,\n            # then find the argmin. This avoids an explicit clone and subsequent\n            # masked assignment, potentially being more efficient.\n            pack = torch.argmin(\n                torch.where(current_pack_items < groups_per_pack,\n                            current_pack_weights,\n                            torch.full_like(current_pack_weights, float('inf')))\n            )\n\n            # Assert that a valid pack was found and it has space.\n            # Using pack directly as a 0-dim tensor for indexing.\n            assert current_pack_items[pack] < groups_per_pack, \\\n                f\"No available packs found, but expected to find one. Pack {pack} is full. \" \\\n                f\"Current pack items: {current_pack_items}, groups_per_pack: {groups_per_pack}\"\n\n            # Assign the group to the chosen pack.\n            # group_id is a 0-dim tensor, so it works directly for indexing.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            # weight[i, group_id] is the weight of the current group (0-dim tensor).\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # Calculate tokens per group for each node\n    tokens_per_group_per_node = weight.unflatten(-1, (num_groups, group_size)) \\\n                                  .sum(dim=-1) \\\n                                  .unflatten(-1, (num_nodes, groups_per_node)) \\\n                                  .sum(dim=2) # [num_layers, num_nodes]\n\n    # Replicate experts within each node\n    # We need to restructure `weight` to apply `replicate_experts` per node.\n    # For each layer, we want to consider groups assigned to a node.\n    # Let's first get the logical experts assigned to each node.\n    node_logical_experts_indices = torch.arange(0, num_logical_experts, group_size) \\\n                                      .unflatten(-1, (num_groups, group_size)) \\\n                                      .view(num_groups // num_nodes, num_nodes, group_size) \\\n                                      .transpose(0, 1) \\\n                                      .contiguous() # [num_nodes, num_groups//num_nodes, group_size]\n    \n    # For each layer, we need to group the weights by node.\n    # We can achieve this by reshaping and transposing.\n    # First, flatten weight to [num_layers, num_groups, group_size]\n    weight_flat_groups = weight.unflatten(-1, (num_groups, group_size))\n    \n    # Now, reshape and transpose to get [num_layers * num_nodes, groups_per_node * group_size]\n    # This effectively groups logical experts by node.\n    weight_for_node_replication = weight_flat_groups.view(num_layers, num_groups // num_nodes, num_nodes, group_size) \\\n                                        .permute(0, 2, 1, 3) \\\n                                        .contiguous() \\\n                                        .view(num_layers * num_nodes, -1)\n\n    # Apply replication within each node\n    # The `replicate_experts` function expects [X, num_log] where X is the number of items to replicate.\n    # Here, X is num_layers. The second dimension is the number of logical experts *within that node*.\n    # We need to adjust the input to replicate_experts to be [num_layers, num_logical_experts_per_node].\n    num_logical_experts_per_node = num_logical_experts // num_nodes\n    \n    # We need to calculate the weights per logical expert *within each node*.\n    # This involves summing weights of groups assigned to that node.\n    # Let's re-think the `rebalance_experts_hierarchical` structure.\n    # The original code uses `tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)`\n    # This creates `[num_layers, num_groups]`.\n    # Then `group_pack_index, group_rank_in_pack = balanced_packing(tokens_per_group, num_nodes)`\n    # This packs groups into nodes.\n    # `log2mlog` maps logical experts to physical experts *within a node*.\n    \n    # Let's try to simplify the mapping logic.\n    # Step 1: Pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1) # [num_layers, num_groups]\n    group_pack_index, group_rank_in_pack = balanced_packing(tokens_per_group, num_nodes)\n    \n    # Map logical experts to nodes.\n    # logical_expert_id -> node_id\n    logical_expert_to_node = group_pack_index.flatten().gather(0, \n        torch.arange(num_logical_experts, device=weight.device).div(group_size, rounding_mode='floor'))\n    \n    # Now, for each node, we have `num_logical_experts // num_nodes` logical experts.\n    # We need to apply `replicate_experts` to these logical experts *within each node*.\n    \n    # Create a tensor of logical expert indices per node.\n    logical_experts_per_node_indices = torch.arange(num_logical_experts, device=weight.device).unflatten(-1, (num_groups, group_size)) \\\n                                          .view(num_groups // num_nodes, num_nodes, group_size) \\\n                                          .transpose(0, 1) # [num_nodes, num_logical_experts_per_node, group_size]\n\n    # We need to calculate the effective \"weight\" for each logical expert *within its assigned node*.\n    # This is the sum of weights of the groups assigned to that node.\n    # Let's first determine which groups are assigned to which node.\n    group_assignment_to_node = torch.arange(num_groups, device=weight.device)\n    group_assignment_to_node = group_assignment_to_node.unflatten(-1, (num_groups // num_nodes, num_nodes, groups_per_node)) \\\n                               .view(num_groups // num_nodes, num_nodes) # [num_groups//num_nodes, num_nodes] where value is group index\n\n    # The `group_rank_in_pack` tells us the \"rank\" within the node's packing.\n    # We need the total weight for each logical expert *within its node*.\n    # This is the sum of weights of groups assigned to that node.\n    # Let's create a weight tensor per node, where each row is a logical expert within that node.\n    \n    # `tokens_per_group` has shape [num_layers, num_groups]\n    # We want to sum the weights of groups assigned to each node.\n    # First, create a mapping from group index to node index.\n    group_to_node_map = torch.arange(num_groups, device=weight.device)\n    group_to_node_map = group_to_node_map.unflatten(-1, (num_groups // num_nodes, num_nodes, groups_per_node)) \\\n                               .view(num_groups // num_nodes, num_nodes) # [num_groups//num_nodes, num_nodes]\n    \n    # The `group_pack_index` maps groups to nodes.\n    # We can use this to group `tokens_per_group` by node.\n    tokens_per_node_logical_experts = torch.zeros(num_layers, num_nodes, num_logical_experts // num_nodes, device=weight.device)\n    \n    # For each layer, we want to aggregate tokens_per_group based on group_pack_index.\n    # This is tricky because `balanced_packing` reorders the groups.\n    # Let's reconsider the `mlog2log` calculation. It maps from a \"node-local\" logical expert index\n    # to a global logical expert index.\n    \n    # The original `mlog2log` calculation:\n    # `log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n    #              group_size).unsqueeze(-1) +\n    #             torch.arange(group_size,\n    #                          dtype=torch.int64,\n    #                          device=group_pack_index.device)).flatten(-2) `\n    # This creates a mapping from a physical expert index (within a node) to a logical expert index.\n    # The physical experts are ordered by node, then by the balanced packing within the node.\n    # `pphy2log` is the final mapping from physical expert to logical expert.\n\n    # Let's focus on simplifying the `replicate_experts` application and the subsequent mapping.\n    \n    # We need to apply `replicate_experts` to logical experts *within each node*.\n    # The input to `replicate_experts` should be `[num_layers, num_logical_experts_per_node]`.\n    # The weights for these logical experts should be the sum of their corresponding groups' weights.\n    \n    # First, let's get the logical experts associated with each node.\n    # `group_pack_index` maps each group to a node.\n    # `logical_expert_to_node` maps each logical expert to a node.\n    \n    # Calculate the total weight for each logical expert within its assigned node.\n    # This is the sum of `weight[:, group_idx]` for all `group_idx` assigned to a node.\n    # We can use `torch.scatter_add_` or similar to aggregate.\n    \n    # Let's create a tensor `logical_expert_weights_per_node` of shape [num_layers, num_nodes, num_logical_experts // num_nodes].\n    # For each layer, and for each node, we need to sum the weights of the groups assigned to that node.\n    \n    # Map groups to node indices.\n    group_to_node_idx = group_pack_index.flatten() # [num_groups]\n    \n    # Create a tensor that holds the weight of each group for each layer.\n    # `tokens_per_group` is [num_layers, num_groups]\n    \n    # We need to sum `tokens_per_group[layer, group_idx]` for all `group_idx` belonging to a node.\n    # Let's create a mapping from group index to its position within its node's logical experts.\n    group_rank_within_node = group_rank_in_pack.flatten() # [num_groups]\n\n    # For each layer, we want to construct the input for `replicate_experts`.\n    # The input tensor should be [num_layers, num_logical_experts_per_node].\n    # The value at [l, i] should be the total weight of the i-th logical expert within its node.\n    \n    # Let's directly calculate the `mlog2log` mapping.\n    # `mlog2log` maps a node-local logical expert index to a global logical expert index.\n    # The structure of `replicate_experts` implies that it operates on `[X, num_log]`.\n    # In `rebalance_experts_hierarchical`, `X` is `num_layers`.\n    # `num_log` is `num_logical_experts // num_nodes`.\n    \n    # The output of `replicate_experts` is `phy2log`, `rank`, `logcnt`.\n    # `phy2log`: [num_layers, num_logical_experts_per_node] - maps physical experts (within node) to logical experts (within node).\n    # `rank`: [num_layers, num_logical_experts_per_node] - replica rank.\n    # `logcnt`: [num_layers, num_logical_experts_per_node] - count of replicas per logical expert (within node).\n\n    # We need to apply `replicate_experts` to a reshaped version of `weight`.\n    # The input for `replicate_experts` should be the weights of logical experts, grouped by node.\n    # Let's construct `weight_per_node_logical_expert`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n    \n    # First, get the logical experts for each node.\n    logical_experts_for_nodes = torch.arange(num_logical_experts, device=weight.device).unflatten(-1, (num_groups, group_size)) \\\n                                    .view(num_groups // num_nodes, num_nodes, group_size) \\\n                                    .transpose(0, 1) # [num_nodes, num_logical_experts_per_node, group_size]\n\n    # Now, for each layer, we need to calculate the total weight for each logical expert within its node.\n    # This requires summing weights of groups assigned to that node.\n    # `tokens_per_group` is [num_layers, num_groups]\n    # `group_pack_index` maps groups to nodes.\n    \n    # Let's try to directly construct the `tokens_per_mlog` tensor.\n    # `mlog` refers to \"middle logical expert\", which is a logical expert within a node.\n    # `tokens_per_mlog` should be `[num_layers * num_nodes, num_logical_experts // num_nodes]`\n    # This means we need to flatten `[num_layers, num_nodes, num_logical_experts // num_nodes]`\n    \n    # Let's create `weight_per_layer_node_logical_expert`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n    # For each layer `l`, and node `n`, we need to sum `weight[l, group_idx]` for all `group_idx` assigned to node `n`.\n    \n    # Let's create a tensor mapping group index to node index.\n    group_to_node_map_flat = group_pack_index.flatten() # [num_groups]\n    \n    # Create an intermediate tensor to sum weights by node.\n    # `weight_sum_by_node`: [num_layers, num_nodes, num_groups_per_node] - this is not what we need.\n    \n    # Let's go back to the original logic and simplify the mapping.\n    # `group_pack_index` and `group_rank_in_pack` are computed from `tokens_per_group`.\n    # `tokens_per_group` is `weight.unflatten(-1, (num_groups, group_size)).sum(-1)`\n    # `log2mlog` is derived from these.\n    # `mlog2log` is the inverse of `log2mlog`.\n    \n    # The core issue is mapping between different levels of abstraction (group, logical expert, node-local logical expert, physical expert).\n    \n    # Let's try to directly construct the `phy2log` for the node-level replication.\n    # For each layer and each node, we need to run `replicate_experts`.\n    \n    # We need to prepare the input for `replicate_experts` for each layer and node.\n    # The input should be `[num_logical_experts_per_node]` for each node and layer.\n    # The values are the sum of weights of groups assigned to that node.\n    \n    # Let's create `logical_expert_weights_per_node_layer`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n    # Initialize with zeros.\n    logical_expert_weights_per_node_layer = torch.zeros(num_layers, num_nodes, num_logical_experts // num_nodes, dtype=weight.dtype, device=weight.device)\n    \n    # Iterate through layers and groups to populate `logical_expert_weights_per_node_layer`.\n    for l in range(num_layers):\n        for g in range(num_groups):\n            node_idx = group_pack_index[l, g]\n            # Find the rank of this group within its node's packing.\n            rank_in_node = group_rank_in_pack[l, g]\n            # The group index `g` corresponds to `g // groups_per_node` blocks of `group_size`.\n            # We need to map group `g` to its position within the node's logical experts.\n            # The `group_rank_in_pack` directly gives this position.\n            logical_expert_idx_within_node = rank_in_node\n            \n            # Add the weight of this group to the corresponding logical expert in the node.\n            logical_expert_weights_per_node_layer[l, node_idx, logical_expert_idx_within_node] += weight[l, g * group_size : (g + 1) * group_size].sum()\n\n    # Now, apply `replicate_experts` to `logical_expert_weights_per_node_layer`.\n    # We need to reshape it to `[num_layers * num_nodes, num_logical_experts // num_nodes]`.\n    weight_for_replication = logical_expert_weights_per_node_layer.view(num_layers * num_nodes, -1)\n    \n    # The number of physical experts per GPU is `phy_experts_per_gpu`.\n    # Each node has `num_gpus // num_nodes` GPUs.\n    # So, each node needs `(num_gpus // num_nodes) * phy_experts_per_gpu` physical experts.\n    # This is `num_physical_experts // num_nodes`.\n    num_physical_experts_per_node = num_physical_experts // num_nodes\n    \n    # Apply replicate_experts for each node.\n    # The `replicate_experts` function takes `[X, num_log]`. Here X is num_layers.\n    # We need to apply it to `[num_layers, num_logical_experts // num_nodes]` for each node.\n    \n    # Let's process node by node.\n    all_phy2mlog_list = []\n    all_phyrank_list = []\n    all_mlogcnt_list = []\n    \n    for node_idx in range(num_nodes):\n        # Extract weights for logical experts within this node.\n        node_weights = logical_expert_weights_per_node_layer[:, node_idx, :] # [num_layers, num_logical_experts_per_node]\n        \n        # Apply replicate_experts for this node.\n        phy2mlog_node, phyrank_node, mlogcnt_node = replicate_experts(\n            node_weights, num_physical_experts_per_node)\n        \n        all_phy2mlog_list.append(phy2mlog_node)\n        all_phyrank_list.append(phyrank_node)\n        all_mlogcnt_list.append(mlogcnt_node)\n\n    # Concatenate results from all nodes.\n    phy2mlog = torch.cat(all_phy2mlog_list, dim=1) # [num_layers, num_physical_experts]\n    phyrank = torch.cat(all_phyrank_list, dim=1)   # [num_layers, num_physical_experts]\n    mlogcnt = torch.cat(all_mlogcnt_list, dim=1)   # [num_layers, num_physical_experts]\n\n    # Now, map these back to the original logical expert indices.\n    # `phy2mlog` maps physical experts (within a node) to logical experts (within a node).\n    # We need to convert the \"logical expert within a node\" index to the global logical expert index.\n    \n    # Create the mapping from node-local logical expert index to global logical expert index.\n    node_local_logical_to_global_map = torch.arange(num_logical_experts, device=weight.device).unflatten(-1, (num_groups, group_size)) \\\n                                        .view(num_groups // num_nodes, num_nodes, group_size) \\\n                                        .transpose(0, 1) # [num_nodes, num_logical_experts_per_node, group_size]\n    \n    # For each node, apply the mapping.\n    final_phy2log_list = []\n    for node_idx in range(num_nodes):\n        # Get the mapping for this node.\n        local_map = node_local_logical_to_global_map[node_idx] # [num_logical_experts_per_node, group_size]\n        \n        # We need to map the `phy2mlog_node` (which contains indices into `local_map`).\n        # The `phy2mlog_node` has shape [num_layers, num_physical_experts_per_node].\n        # We need to use `phy2mlog_node` to index into `local_map`.\n        \n        # Let's re-think. `phy2mlog` from `replicate_experts` is `[num_layers, num_logical_experts_per_node]`.\n        # The value `phy2mlog[l, i]` is the logical expert index *within that node's set of logical experts*.\n        # We need to convert this to the global logical expert index.\n        \n        # The original `mlog2log` was calculated as:\n        # `mlog2log = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n        #              group_size).unsqueeze(-1) +\n        #             torch.arange(group_size,\n        #                          dtype=torch.int64,\n        #                          device=group_pack_index.device)).flatten(-2)`\n        # This seems to map a \"physical expert index within a node\" to a \"global logical expert index\".\n        \n        # Let's try to reconstruct this mapping more directly.\n        # The physical experts are ordered by node, then by `replicate_experts` output.\n        # `phy2mlog` gives the logical expert index within a node.\n        # We need to map this to the global logical expert index.\n        \n        # Let's construct `node_logical_expert_to_global_logical_expert_map`: [num_nodes, num_logical_experts_per_node]\n        node_logical_expert_to_global_logical_expert_map = torch.arange(num_logical_experts, device=weight.device).unflatten(-1, (num_groups, group_size)) \\\n                                                               .view(num_groups // num_nodes, num_nodes, group_size) \\\n                                                               .transpose(0, 1) \\\n                                                               .contiguous() # [num_nodes, num_logical_experts_per_node, group_size]\n        # We only need the first element of the inner dimension to get the logical expert index.\n        node_logical_expert_to_global_logical_expert_map = node_logical_expert_to_global_logical_expert_map[:, :, 0] # [num_nodes, num_logical_experts_per_node]\n\n        # Now, for each node, use its `phy2mlog` to index into this map.\n        node_phy2mlog = phy2mlog[:, node_idx * num_physical_experts_per_node : (node_idx + 1) * num_physical_experts_per_node] # [num_layers, num_physical_experts_per_node]\n        \n        # The `phy2mlog` values are indices into the `node_logical_expert_to_global_logical_expert_map`.\n        # We need to map these indices correctly.\n        # The `replicate_experts` output `phy2log` maps physical experts to logical experts.\n        # We want to map physical experts to global logical experts.\n        \n        # Let's directly create the mapping from the original `group_pack_index` and `group_rank_in_pack`.\n        # `group_pack_index`: [num_layers, num_groups] -> maps groups to nodes.\n        # `group_rank_in_pack`: [num_layers, num_groups] -> maps groups to rank within a node's packing.\n        \n        # We need to map physical experts to global logical experts.\n        # The physical experts are arranged: Node 0 (GPU 0, GPU 1, ...), Node 1, ...\n        # Within each node, physical experts are arranged based on `replicate_experts` output.\n        \n        # Let's create the mapping from a \"physical expert index\" to its \"node index\" and \"rank within node\".\n        # The total number of physical experts is `num_physical_experts`.\n        # The number of physical experts per node is `num_physical_experts_per_node`.\n        \n        # For each layer, let's build the mapping.\n        # `phy_idx_to_node_and_rank`: [num_layers, num_physical_experts] -> [node_idx, rank_in_node]\n        \n        # Instead of complex reshaping, let's simplify the mapping creation.\n        # We have `group_pack_index` [num_layers, num_groups] and `group_rank_in_pack` [num_layers, num_groups].\n        # These tell us which node a group belongs to, and its rank within that node.\n        \n        # The original `mlog2log` calculation is crucial. Let's re-examine it:\n        # `group_index_in_node = group_pack_index * groups_per_node + group_rank_in_pack`\n        # This is not right. `group_rank_in_pack` is the rank *within the sorted groups*.\n        # The `balanced_packing` returns `pack_index` and `rank_in_pack`.\n        # `group_pack_index` is the pack index for each group.\n        # `group_rank_in_pack` is the rank of the group within its assigned pack (node).\n        \n        # Let's construct the mapping from physical expert index to global logical expert index.\n        # The physical experts are ordered by node, then by the output of `replicate_experts` within that node.\n        \n        # For each node, we have `replicate_experts` applied to `num_logical_experts // num_nodes` logical experts.\n        # The `phy2mlog` output from `replicate_experts` maps physical experts (within node) to logical experts (within node).\n        # We need to convert the logical expert index (within node) to the global logical expert index.\n        \n        # Let's reconstruct the mapping for each node.\n        # For node `n`, the logical experts are indexed from `n * (num_logical_experts // num_nodes)` to `(n+1) * (num_logical_experts // num_nodes) - 1`.\n        \n        # Create `global_logical_expert_indices_per_node`: [num_nodes, num_logical_experts // num_nodes]\n        global_logical_expert_indices_per_node = torch.arange(num_logical_experts, device=weight.device).unflatten(-1, (num_groups, group_size)) \\\n                                                   .view(num_groups // num_nodes, num_nodes, group_size) \\\n                                                   .transpose(0, 1) \\\n                                                   .contiguous()[:, :, 0] # [num_nodes, num_logical_experts_per_node]\n        \n        # Now, for each node's `phy2mlog` output, index into `global_logical_expert_indices_per_node`.\n        # `phy2mlog` from replicate_experts is `[num_layers, num_logical_experts_per_node]`.\n        # The values are indices into the logical experts *within that node*.\n        \n        # For node `node_idx`:\n        # `node_phy2mlog = phy2mlog_list[node_idx]` # [num_layers, num_physical_experts_per_node]\n        # `node_global_map = global_logical_expert_indices_per_node[node_idx]` # [num_logical_experts_per_node]\n        \n        # `final_phy2log_node = node_global_map.gather(0, node_phy2mlog)` # This is incorrect indexing.\n        \n        # We need to use `node_phy2mlog` as indices into `node_global_map`.\n        # `final_phy2log_node = torch.gather(node_global_map.unsqueeze(0).expand_as(node_phy2mlog), -1, node_phy2mlog)` # Incorrect.\n        \n        # Let's use `torch.gather` correctly.\n        # `torch.gather(input, dim, index)`\n        # `input` is `node_global_map` (shape: [num_logical_experts_per_node])\n        # `dim` is 0 (since we want to index along the logical expert dimension)\n        # `index` is `node_phy2mlog` (shape: [num_layers, num_physical_experts_per_node])\n        \n        node_global_map = global_logical_expert_indices_per_node[node_idx] # [num_logical_experts_per_node]\n        node_phy2mlog = all_phy2mlog_list[node_idx] # [num_layers, num_physical_experts_per_node]\n        \n        # Map `node_phy2mlog` (indices into logical experts within node) to global logical expert indices.\n        # We need to apply `gather` for each layer.\n        final_phy2log_node = torch.gather(node_global_map.unsqueeze(0).expand_as(node_phy2mlog), -1, node_phy2mlog)\n        \n        final_phy2log_list.append(final_phy2log_node)\n\n    # Concatenate the final `phy2log` from all nodes.\n    phy2log = torch.cat(final_phy2log_list, dim=1) # [num_layers, num_physical_experts]\n\n    # Now, let's handle `phyrank` and `logcnt`.\n    # `phyrank` from `replicate_experts` is the rank of the physical expert within its logical expert group.\n    # `mlogcnt` from `replicate_experts` is the number of replicas for each logical expert (within node).\n    \n    # We need to combine `phyrank` and `mlogcnt` across nodes.\n    # `phyrank` has shape `[num_layers, num_physical_experts_per_node]` for each node.\n    # `mlogcnt` has shape `[num_layers, num_physical_experts_per_node]` for each node.\n    \n    phyrank = torch.cat(all_phyrank_list, dim=1) # [num_layers, num_physical_experts]\n    \n    # `mlogcnt` needs to be converted to `expert_count` for the global logical experts.\n    # `mlogcnt` is `[num_layers, num_physical_experts_per_node]` for each node.\n    # We need to aggregate this information to get the count of physical experts for each global logical expert.\n    \n    # The `mlogcnt_node` has shape `[num_layers, num_physical_experts_per_node]`.\n    # The values represent the number of replicas for the logical expert *within that node*.\n    # We need to sum these up for each global logical expert.\n    \n    # Let's create `logcnt_per_node`: [num_nodes, num_layers, num_logical_experts_per_node]\n    logcnt_per_node = torch.stack(all_mlogcnt_list, dim=1) # [num_layers, num_nodes, num_physical_experts_per_node]\n    \n    # Reshape `logcnt_per_node` to group by logical expert within node.\n    # `logcnt_per_node`: [num_layers, num_nodes, num_physical_experts_per_node]\n    # We need to map `logcnt_per_node` based on `phy2mlog` to get counts per logical expert.\n    \n    # Create `logical_expert_counts_per_node_layer`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n    logical_expert_counts_per_node_layer = torch.zeros(num_layers, num_nodes, num_logical_experts // num_nodes, dtype=torch.int64, device=weight.device)\n    \n    for node_idx in range(num_nodes):\n        node_mlogcnt = all_mlogcnt_list[node_idx] # [num_layers, num_physical_experts_per_node]\n        node_phy2mlog = all_phy2mlog_list[node_idx] # [num_layers, num_physical_experts_per_node]\n        \n        # For each layer, we want to sum `node_mlogcnt` values based on `node_phy2mlog`.\n        # This is a scatter_add operation.\n        for l in range(num_layers):\n            # Use scatter_add to sum counts for each logical expert within the node.\n            logical_expert_counts_per_node_layer[l, node_idx].scatter_add_(\n                0,\n                node_phy2mlog[l],\n                node_mlogcnt[l]\n            )\n\n    # Now, sum the counts across nodes for each logical expert.\n    # `logcnt`: [num_layers, num_logical_experts]\n    logcnt = logical_expert_counts_per_node_layer.sum(dim=1) # [num_layers, num_logical_experts // num_nodes]\n    \n    # The `logcnt` should be for the global logical experts.\n    # The current `logcnt` is for logical experts *within each node*.\n    # We need to aggregate `logcnt` based on the node assignment of logical experts.\n    \n    # Let's re-calculate `logcnt` from `phy2log`.\n    # `phy2log` is `[num_layers, num_physical_experts]`.\n    # `phyrank` is `[num_layers, num_physical_experts]`.\n    # We need to count how many physical experts map to each global logical expert.\n    \n    # Create a tensor to store the counts.\n    expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n    \n    # Iterate through all physical experts and increment the count for their corresponding logical expert.\n    for i in range(num_layers):\n        for j in range(num_physical_experts):\n            logical_expert_idx = phy2log[i, j]\n            if logical_expert_idx != -1: # Ensure it's a valid mapping\n                expert_count[i, logical_expert_idx] += 1\n                \n    # The `phyrank` needs to be mapped correctly.\n    # `phyrank` is `[num_layers, num_physical_experts]`.\n    # The values are the replica ranks.\n    \n    # Let's ensure the `replicate_experts` output `phyrank` is correctly used.\n    # `phyrank` from `replicate_experts` is `[X, num_log]`.\n    # In our case, `X` is `num_layers`.\n    # `num_log` is `num_logical_experts_per_node`.\n    \n    # The combined `phyrank` has shape `[num_layers, num_physical_experts]`.\n    # The values within this tensor are the ranks within their respective logical expert groups.\n    # The `phyrank` is already correctly structured if we concatenate it.\n    \n    # We need to ensure the `logcnt` calculation is correct.\n    # `mlogcnt_node` from `replicate_experts` is `[num_layers, num_physical_experts_per_node]`.\n    # It represents the number of replicas for each logical expert *within that node*.\n    \n    # The `logical_expert_counts_per_node_layer` correctly sums up the `mlogcnt` for each logical expert within each node.\n    # Now, we need to sum these counts across nodes to get the total count for each global logical expert.\n    \n    # The `logical_expert_counts_per_node_layer` has shape `[num_layers, num_nodes, num_logical_experts // num_nodes]`.\n    # We need to map this based on the node assignment of logical experts.\n    \n    # Let's create `global_logical_expert_counts`: [num_layers, num_logical_experts]\n    global_logical_expert_counts = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n    \n    for l in range(num_layers):\n        for n in range(num_nodes):\n            # Get the logical experts for this node.\n            node_logical_experts = global_logical_expert_indices_per_node[n] # [num_logical_experts_per_node]\n            # Get the counts for this node and layer.\n            node_counts = logical_expert_counts_per_node_layer[l, n] # [num_logical_experts_per_node]\n            \n            # Use scatter_add to add these counts to the correct global logical expert index.\n            global_logical_expert_counts[l].scatter_add_(0, node_logical_experts, node_counts)\n    \n    logcnt = global_logical_expert_counts # This should be the correct `logcnt`\n\n    return phy2log, phyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    # Cast weight to float for calculations if not already, and keep on original device.\n    # Moving to CPU adds overhead if data is already on GPU and might slow down ops.\n    weight = weight.float()\n    \n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n", "language": "python", "parent_id": "5a42d900-c883-4117-958b-ad4263a1ac7a", "generation": 6, "timestamp": 1751065266.1285858, "iteration_found": 242, "metrics": {"balancedness_score": 0.0, "speed_score": 0.0, "combined_score": 0.0, "error": "unflatten: Provided sizes [8, 32] don't multiply up to the size of dim 0 (8) in the input tensor"}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 27 lines with 468 lines", "parent_metrics": {"balancedness_score": 0.29830835153138974, "speed_score": 0.026640255592653206, "combined_score": 0.16247430356202147}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve the Mixture-of-Expert models Expert Parallelism Load Balancer (MoE EPLB) expert rearrangement algorithm.\nThis algorithm will take the load metrics recorded by the vLLM server, and rearrange the experts to balance the load. It can make replicas of some experts to achieve better load balancing.\nYour goal will be two-fold: 1. Improve the algorithm to achieve better load balancing; while 2. Improve the algorithm to be more efficient, i.e. reduce the execution time of the algorithm itself, since perfect load balancing is NP-hard.\nThe current algorithm is implemented in the `rebalance_experts` function. ", "user": "# Current Program Information\n- Current performance metrics: - balancedness_score: 0.2983\n- speed_score: 0.0266\n- combined_score: 0.1625\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: balancedness_score, speed_score, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1103, combined_score: 0.2315\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1103, combined_score: 0.2315\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1103, combined_score: 0.2315\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.2315)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1103), Performs well on combined_score (0.2315)\n\n\n### Program 2 (Score: 0.2315)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1103), Performs well on combined_score (0.2315)\n\n\n### Program 3 (Score: 0.2315)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1103), Performs well on combined_score (0.2315)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.2315)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n\n### Program D2 (Score: 0.2315)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.2315, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 32 lines with 32 lines, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 2 (Score: 0.0939, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 23 lines with 236 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 3 (Score: 0.0939, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 23 lines with 236 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 4 (Score: 0.0939, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 23 lines with 236 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 5 (Score: 0.1908, Type: Migrant)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 35 lines with 48 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n\n# Current Program\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort groups by weight in descending order.\n    # 'weight' is already float and on CPU from rebalance_experts,\n    # so no need for .float().cpu() here.\n    indices = weight.sort(-1, descending=True).indices\n\n    # Initialize pack_index and rank_in_pack.\n    # These will be fully populated, so `empty_like` is sufficient.\n    # They will be on the same device as 'weight' (i.e., CPU).\n    pack_index = torch.empty_like(weight, dtype=torch.int64)\n    rank_in_pack = torch.empty_like(pack_index)\n    \n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These tensors are on the same device as 'weight' (which is CPU).\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id in indices[i]:\n            # Get current state for this layer's packs.\n            current_pack_weights = pack_weights_per_layer[i]\n            current_pack_items = pack_items_per_layer[i]\n\n            # Find the pack with the minimum current weight among available ones.\n            # Use torch.where to effectively mask out full packs with infinity,\n            # then find the argmin. This avoids an explicit clone and subsequent\n            # masked assignment, potentially being more efficient.\n            pack = torch.argmin(\n                torch.where(current_pack_items < groups_per_pack,\n                            current_pack_weights,\n                            torch.full_like(current_pack_weights, float('inf')))\n            )\n\n            # Assert that a valid pack was found and it has space.\n            # Using pack directly as a 0-dim tensor for indexing.\n            assert current_pack_items[pack] < groups_per_pack, \\\n                f\"No available packs found, but expected to find one. Pack {pack} is full. \" \\\n                f\"Current pack items: {current_pack_items}, groups_per_pack: {groups_per_pack}\"\n\n            # Assign the group to the chosen pack.\n            # group_id is a 0-dim tensor, so it works directly for indexing.\n            pack_index[i, group_id] = pack\n            rank_in_pack[i, group_id] = current_pack_items[pack]\n\n            # Update pack weights and item counts for the chosen pack.\n            # weight[i, group_id] is the weight of the current group (0-dim tensor).\n            pack_weights_per_layer[i, pack] += weight[i, group_id]\n            pack_items_per_layer[i, pack] += 1\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    # Cast weight to float for calculations if not already, and keep on original device.\n    # Moving to CPU adds overhead if data is already on GPU and might slow down ops.\n    weight = weight.float()\n    \n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["Here are some suggestions for improving the `rebalance_experts` function and its related components:\n\n**Reasoning for Improvements:**\n\n1.  **Efficiency in `rebalance_experts_hierarchical`**:\n    *   The current implementation of `rebalance_experts_hierarchical` involves several `gather` operations, which can be computationally expensive, especially when dealing with large tensors.\n    *   We can optimize these operations by restructuring the data or using more efficient tensor manipulation techniques.\n    *   Specifically, the steps involving `mlog2log`, `pphy2mlog`, and `pphy2log` can be simplified.\n\n2.  **Clarity and Maintainability**:\n    *   The function names and variable names are generally good, but some intermediate calculations could be made more explicit or consolidated.\n    *   The logic for mapping between different expert indices (logical, physical, per-node, per-GPU) can be complex. Simplifying these mappings will improve readability.\n\n3.  **Potential for Better Load Balancing**:\n    *   While the `balanced_packing` and `replicate_experts` functions implement greedy strategies, there might be opportunities to explore more sophisticated packing or replication strategies, though this often comes at the cost of increased computation. For this task, we'll focus on optimizing the existing logic.\n\n**Suggested Changes:**\n\n**1. Optimization in `rebalance_experts_hierarchical` - Simplifying `mlog2log` and related calculations:**\n\nThe calculation of `mlog2log`, `pphy2mlog`, and `pphy2log` involves multiple `gather` operations and reshaping. We can potentially simplify this by directly calculating the final `pphy2log` mapping.\n\n<<<<<<< SEARCH\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n=======\n    # Step 2: construct redundant experts within nodes\n    # Calculate tokens per group for each node\n    tokens_per_group_per_node = weight.unflatten(-1, (num_groups, group_size)) \\\n                                  .sum(dim=-1) \\\n                                  .unflatten(-1, (num_nodes, groups_per_node)) \\\n                                  .sum(dim=2) # [num_layers, num_nodes]\n\n    # Replicate experts within each node\n    # We need to restructure `weight` to apply `replicate_experts` per node.\n    # For each layer, we want to consider groups assigned to a node.\n    # Let's first get the logical experts assigned to each node.\n    node_logical_experts_indices = torch.arange(0, num_logical_experts, group_size) \\\n                                      .unflatten(-1, (num_groups, group_size)) \\\n                                      .view(num_groups // num_nodes, num_nodes, group_size) \\\n                                      .transpose(0, 1) \\\n                                      .contiguous() # [num_nodes, num_groups//num_nodes, group_size]\n    \n    # For each layer, we need to group the weights by node.\n    # We can achieve this by reshaping and transposing.\n    # First, flatten weight to [num_layers, num_groups, group_size]\n    weight_flat_groups = weight.unflatten(-1, (num_groups, group_size))\n    \n    # Now, reshape and transpose to get [num_layers * num_nodes, groups_per_node * group_size]\n    # This effectively groups logical experts by node.\n    weight_for_node_replication = weight_flat_groups.view(num_layers, num_groups // num_nodes, num_nodes, group_size) \\\n                                        .permute(0, 2, 1, 3) \\\n                                        .contiguous() \\\n                                        .view(num_layers * num_nodes, -1)\n\n    # Apply replication within each node\n    # The `replicate_experts` function expects [X, num_log] where X is the number of items to replicate.\n    # Here, X is num_layers. The second dimension is the number of logical experts *within that node*.\n    # We need to adjust the input to replicate_experts to be [num_layers, num_logical_experts_per_node].\n    num_logical_experts_per_node = num_logical_experts // num_nodes\n    \n    # We need to calculate the weights per logical expert *within each node*.\n    # This involves summing weights of groups assigned to that node.\n    # Let's re-think the `rebalance_experts_hierarchical` structure.\n    # The original code uses `tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)`\n    # This creates `[num_layers, num_groups]`.\n    # Then `group_pack_index, group_rank_in_pack = balanced_packing(tokens_per_group, num_nodes)`\n    # This packs groups into nodes.\n    # `log2mlog` maps logical experts to physical experts *within a node*.\n    \n    # Let's try to simplify the mapping logic.\n    # Step 1: Pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1) # [num_layers, num_groups]\n    group_pack_index, group_rank_in_pack = balanced_packing(tokens_per_group, num_nodes)\n    \n    # Map logical experts to nodes.\n    # logical_expert_id -> node_id\n    logical_expert_to_node = group_pack_index.flatten().gather(0, \n        torch.arange(num_logical_experts, device=weight.device).div(group_size, rounding_mode='floor'))\n    \n    # Now, for each node, we have `num_logical_experts // num_nodes` logical experts.\n    # We need to apply `replicate_experts` to these logical experts *within each node*.\n    \n    # Create a tensor of logical expert indices per node.\n    logical_experts_per_node_indices = torch.arange(num_logical_experts, device=weight.device).unflatten(-1, (num_groups, group_size)) \\\n                                          .view(num_groups // num_nodes, num_nodes, group_size) \\\n                                          .transpose(0, 1) # [num_nodes, num_logical_experts_per_node, group_size]\n\n    # We need to calculate the effective \"weight\" for each logical expert *within its assigned node*.\n    # This is the sum of weights of the groups assigned to that node.\n    # Let's first determine which groups are assigned to which node.\n    group_assignment_to_node = torch.arange(num_groups, device=weight.device)\n    group_assignment_to_node = group_assignment_to_node.unflatten(-1, (num_groups // num_nodes, num_nodes, groups_per_node)) \\\n                               .view(num_groups // num_nodes, num_nodes) # [num_groups//num_nodes, num_nodes] where value is group index\n\n    # The `group_rank_in_pack` tells us the \"rank\" within the node's packing.\n    # We need the total weight for each logical expert *within its node*.\n    # This is the sum of weights of groups assigned to that node.\n    # Let's create a weight tensor per node, where each row is a logical expert within that node.\n    \n    # `tokens_per_group` has shape [num_layers, num_groups]\n    # We want to sum the weights of groups assigned to each node.\n    # First, create a mapping from group index to node index.\n    group_to_node_map = torch.arange(num_groups, device=weight.device)\n    group_to_node_map = group_to_node_map.unflatten(-1, (num_groups // num_nodes, num_nodes, groups_per_node)) \\\n                               .view(num_groups // num_nodes, num_nodes) # [num_groups//num_nodes, num_nodes]\n    \n    # The `group_pack_index` maps groups to nodes.\n    # We can use this to group `tokens_per_group` by node.\n    tokens_per_node_logical_experts = torch.zeros(num_layers, num_nodes, num_logical_experts // num_nodes, device=weight.device)\n    \n    # For each layer, we want to aggregate tokens_per_group based on group_pack_index.\n    # This is tricky because `balanced_packing` reorders the groups.\n    # Let's reconsider the `mlog2log` calculation. It maps from a \"node-local\" logical expert index\n    # to a global logical expert index.\n    \n    # The original `mlog2log` calculation:\n    # `log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n    #              group_size).unsqueeze(-1) +\n    #             torch.arange(group_size,\n    #                          dtype=torch.int64,\n    #                          device=group_pack_index.device)).flatten(-2) `\n    # This creates a mapping from a physical expert index (within a node) to a logical expert index.\n    # The physical experts are ordered by node, then by the balanced packing within the node.\n    # `pphy2log` is the final mapping from physical expert to logical expert.\n\n    # Let's focus on simplifying the `replicate_experts` application and the subsequent mapping.\n    \n    # We need to apply `replicate_experts` to logical experts *within each node*.\n    # The input to `replicate_experts` should be `[num_layers, num_logical_experts_per_node]`.\n    # The weights for these logical experts should be the sum of their corresponding groups' weights.\n    \n    # First, let's get the logical experts associated with each node.\n    # `group_pack_index` maps each group to a node.\n    # `logical_expert_to_node` maps each logical expert to a node.\n    \n    # Calculate the total weight for each logical expert within its assigned node.\n    # This is the sum of `weight[:, group_idx]` for all `group_idx` assigned to a node.\n    # We can use `torch.scatter_add_` or similar to aggregate.\n    \n    # Let's create a tensor `logical_expert_weights_per_node` of shape [num_layers, num_nodes, num_logical_experts // num_nodes].\n    # For each layer, and for each node, we need to sum the weights of the groups assigned to that node.\n    \n    # Map groups to node indices.\n    group_to_node_idx = group_pack_index.flatten() # [num_groups]\n    \n    # Create a tensor that holds the weight of each group for each layer.\n    # `tokens_per_group` is [num_layers, num_groups]\n    \n    # We need to sum `tokens_per_group[layer, group_idx]` for all `group_idx` belonging to a node.\n    # Let's create a mapping from group index to its position within its node's logical experts.\n    group_rank_within_node = group_rank_in_pack.flatten() # [num_groups]\n\n    # For each layer, we want to construct the input for `replicate_experts`.\n    # The input tensor should be [num_layers, num_logical_experts_per_node].\n    # The value at [l, i] should be the total weight of the i-th logical expert within its node.\n    \n    # Let's directly calculate the `mlog2log` mapping.\n    # `mlog2log` maps a node-local logical expert index to a global logical expert index.\n    # The structure of `replicate_experts` implies that it operates on `[X, num_log]`.\n    # In `rebalance_experts_hierarchical`, `X` is `num_layers`.\n    # `num_log` is `num_logical_experts // num_nodes`.\n    \n    # The output of `replicate_experts` is `phy2log`, `rank`, `logcnt`.\n    # `phy2log`: [num_layers, num_logical_experts_per_node] - maps physical experts (within node) to logical experts (within node).\n    # `rank`: [num_layers, num_logical_experts_per_node] - replica rank.\n    # `logcnt`: [num_layers, num_logical_experts_per_node] - count of replicas per logical expert (within node).\n\n    # We need to apply `replicate_experts` to a reshaped version of `weight`.\n    # The input for `replicate_experts` should be the weights of logical experts, grouped by node.\n    # Let's construct `weight_per_node_logical_expert`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n    \n    # First, get the logical experts for each node.\n    logical_experts_for_nodes = torch.arange(num_logical_experts, device=weight.device).unflatten(-1, (num_groups, group_size)) \\\n                                    .view(num_groups // num_nodes, num_nodes, group_size) \\\n                                    .transpose(0, 1) # [num_nodes, num_logical_experts_per_node, group_size]\n\n    # Now, for each layer, we need to calculate the total weight for each logical expert within its node.\n    # This requires summing weights of groups assigned to that node.\n    # `tokens_per_group` is [num_layers, num_groups]\n    # `group_pack_index` maps groups to nodes.\n    \n    # Let's try to directly construct the `tokens_per_mlog` tensor.\n    # `mlog` refers to \"middle logical expert\", which is a logical expert within a node.\n    # `tokens_per_mlog` should be `[num_layers * num_nodes, num_logical_experts // num_nodes]`\n    # This means we need to flatten `[num_layers, num_nodes, num_logical_experts // num_nodes]`\n    \n    # Let's create `weight_per_layer_node_logical_expert`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n    # For each layer `l`, and node `n`, we need to sum `weight[l, group_idx]` for all `group_idx` assigned to node `n`.\n    \n    # Let's create a tensor mapping group index to node index.\n    group_to_node_map_flat = group_pack_index.flatten() # [num_groups]\n    \n    # Create an intermediate tensor to sum weights by node.\n    # `weight_sum_by_node`: [num_layers, num_nodes, num_groups_per_node] - this is not what we need.\n    \n    # Let's go back to the original logic and simplify the mapping.\n    # `group_pack_index` and `group_rank_in_pack` are computed from `tokens_per_group`.\n    # `tokens_per_group` is `weight.unflatten(-1, (num_groups, group_size)).sum(-1)`\n    # `log2mlog` is derived from these.\n    # `mlog2log` is the inverse of `log2mlog`.\n    \n    # The core issue is mapping between different levels of abstraction (group, logical expert, node-local logical expert, physical expert).\n    \n    # Let's try to directly construct the `phy2log` for the node-level replication.\n    # For each layer and each node, we need to run `replicate_experts`.\n    \n    # We need to prepare the input for `replicate_experts` for each layer and node.\n    # The input should be `[num_logical_experts_per_node]` for each node and layer.\n    # The values are the sum of weights of groups assigned to that node.\n    \n    # Let's create `logical_expert_weights_per_node_layer`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n    # Initialize with zeros.\n    logical_expert_weights_per_node_layer = torch.zeros(num_layers, num_nodes, num_logical_experts // num_nodes, dtype=weight.dtype, device=weight.device)\n    \n    # Iterate through layers and groups to populate `logical_expert_weights_per_node_layer`.\n    for l in range(num_layers):\n        for g in range(num_groups):\n            node_idx = group_pack_index[l, g]\n            # Find the rank of this group within its node's packing.\n            rank_in_node = group_rank_in_pack[l, g]\n            # The group index `g` corresponds to `g // groups_per_node` blocks of `group_size`.\n            # We need to map group `g` to its position within the node's logical experts.\n            # The `group_rank_in_pack` directly gives this position.\n            logical_expert_idx_within_node = rank_in_node\n            \n            # Add the weight of this group to the corresponding logical expert in the node.\n            logical_expert_weights_per_node_layer[l, node_idx, logical_expert_idx_within_node] += weight[l, g * group_size : (g + 1) * group_size].sum()\n\n    # Now, apply `replicate_experts` to `logical_expert_weights_per_node_layer`.\n    # We need to reshape it to `[num_layers * num_nodes, num_logical_experts // num_nodes]`.\n    weight_for_replication = logical_expert_weights_per_node_layer.view(num_layers * num_nodes, -1)\n    \n    # The number of physical experts per GPU is `phy_experts_per_gpu`.\n    # Each node has `num_gpus // num_nodes` GPUs.\n    # So, each node needs `(num_gpus // num_nodes) * phy_experts_per_gpu` physical experts.\n    # This is `num_physical_experts // num_nodes`.\n    num_physical_experts_per_node = num_physical_experts // num_nodes\n    \n    # Apply replicate_experts for each node.\n    # The `replicate_experts` function takes `[X, num_log]`. Here X is num_layers.\n    # We need to apply it to `[num_layers, num_logical_experts // num_nodes]` for each node.\n    \n    # Let's process node by node.\n    all_phy2mlog_list = []\n    all_phyrank_list = []\n    all_mlogcnt_list = []\n    \n    for node_idx in range(num_nodes):\n        # Extract weights for logical experts within this node.\n        node_weights = logical_expert_weights_per_node_layer[:, node_idx, :] # [num_layers, num_logical_experts_per_node]\n        \n        # Apply replicate_experts for this node.\n        phy2mlog_node, phyrank_node, mlogcnt_node = replicate_experts(\n            node_weights, num_physical_experts_per_node)\n        \n        all_phy2mlog_list.append(phy2mlog_node)\n        all_phyrank_list.append(phyrank_node)\n        all_mlogcnt_list.append(mlogcnt_node)\n\n    # Concatenate results from all nodes.\n    phy2mlog = torch.cat(all_phy2mlog_list, dim=1) # [num_layers, num_physical_experts]\n    phyrank = torch.cat(all_phyrank_list, dim=1)   # [num_layers, num_physical_experts]\n    mlogcnt = torch.cat(all_mlogcnt_list, dim=1)   # [num_layers, num_physical_experts]\n\n    # Now, map these back to the original logical expert indices.\n    # `phy2mlog` maps physical experts (within a node) to logical experts (within a node).\n    # We need to convert the \"logical expert within a node\" index to the global logical expert index.\n    \n    # Create the mapping from node-local logical expert index to global logical expert index.\n    node_local_logical_to_global_map = torch.arange(num_logical_experts, device=weight.device).unflatten(-1, (num_groups, group_size)) \\\n                                        .view(num_groups // num_nodes, num_nodes, group_size) \\\n                                        .transpose(0, 1) # [num_nodes, num_logical_experts_per_node, group_size]\n    \n    # For each node, apply the mapping.\n    final_phy2log_list = []\n    for node_idx in range(num_nodes):\n        # Get the mapping for this node.\n        local_map = node_local_logical_to_global_map[node_idx] # [num_logical_experts_per_node, group_size]\n        \n        # We need to map the `phy2mlog_node` (which contains indices into `local_map`).\n        # The `phy2mlog_node` has shape [num_layers, num_physical_experts_per_node].\n        # We need to use `phy2mlog_node` to index into `local_map`.\n        \n        # Let's re-think. `phy2mlog` from `replicate_experts` is `[num_layers, num_logical_experts_per_node]`.\n        # The value `phy2mlog[l, i]` is the logical expert index *within that node's set of logical experts*.\n        # We need to convert this to the global logical expert index.\n        \n        # The original `mlog2log` was calculated as:\n        # `mlog2log = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n        #              group_size).unsqueeze(-1) +\n        #             torch.arange(group_size,\n        #                          dtype=torch.int64,\n        #                          device=group_pack_index.device)).flatten(-2)`\n        # This seems to map a \"physical expert index within a node\" to a \"global logical expert index\".\n        \n        # Let's try to reconstruct this mapping more directly.\n        # The physical experts are ordered by node, then by `replicate_experts` output.\n        # `phy2mlog` gives the logical expert index within a node.\n        # We need to map this to the global logical expert index.\n        \n        # Let's construct `node_logical_expert_to_global_logical_expert_map`: [num_nodes, num_logical_experts_per_node]\n        node_logical_expert_to_global_logical_expert_map = torch.arange(num_logical_experts, device=weight.device).unflatten(-1, (num_groups, group_size)) \\\n                                                               .view(num_groups // num_nodes, num_nodes, group_size) \\\n                                                               .transpose(0, 1) \\\n                                                               .contiguous() # [num_nodes, num_logical_experts_per_node, group_size]\n        # We only need the first element of the inner dimension to get the logical expert index.\n        node_logical_expert_to_global_logical_expert_map = node_logical_expert_to_global_logical_expert_map[:, :, 0] # [num_nodes, num_logical_experts_per_node]\n\n        # Now, for each node, use its `phy2mlog` to index into this map.\n        node_phy2mlog = phy2mlog[:, node_idx * num_physical_experts_per_node : (node_idx + 1) * num_physical_experts_per_node] # [num_layers, num_physical_experts_per_node]\n        \n        # The `phy2mlog` values are indices into the `node_logical_expert_to_global_logical_expert_map`.\n        # We need to map these indices correctly.\n        # The `replicate_experts` output `phy2log` maps physical experts to logical experts.\n        # We want to map physical experts to global logical experts.\n        \n        # Let's directly create the mapping from the original `group_pack_index` and `group_rank_in_pack`.\n        # `group_pack_index`: [num_layers, num_groups] -> maps groups to nodes.\n        # `group_rank_in_pack`: [num_layers, num_groups] -> maps groups to rank within a node's packing.\n        \n        # We need to map physical experts to global logical experts.\n        # The physical experts are arranged: Node 0 (GPU 0, GPU 1, ...), Node 1, ...\n        # Within each node, physical experts are arranged based on `replicate_experts` output.\n        \n        # Let's create the mapping from a \"physical expert index\" to its \"node index\" and \"rank within node\".\n        # The total number of physical experts is `num_physical_experts`.\n        # The number of physical experts per node is `num_physical_experts_per_node`.\n        \n        # For each layer, let's build the mapping.\n        # `phy_idx_to_node_and_rank`: [num_layers, num_physical_experts] -> [node_idx, rank_in_node]\n        \n        # Instead of complex reshaping, let's simplify the mapping creation.\n        # We have `group_pack_index` [num_layers, num_groups] and `group_rank_in_pack` [num_layers, num_groups].\n        # These tell us which node a group belongs to, and its rank within that node.\n        \n        # The original `mlog2log` calculation is crucial. Let's re-examine it:\n        # `group_index_in_node = group_pack_index * groups_per_node + group_rank_in_pack`\n        # This is not right. `group_rank_in_pack` is the rank *within the sorted groups*.\n        # The `balanced_packing` returns `pack_index` and `rank_in_pack`.\n        # `group_pack_index` is the pack index for each group.\n        # `group_rank_in_pack` is the rank of the group within its assigned pack (node).\n        \n        # Let's construct the mapping from physical expert index to global logical expert index.\n        # The physical experts are ordered by node, then by the output of `replicate_experts` within that node.\n        \n        # For each node, we have `replicate_experts` applied to `num_logical_experts // num_nodes` logical experts.\n        # The `phy2mlog` output from `replicate_experts` maps physical experts (within node) to logical experts (within node).\n        # We need to convert the logical expert index (within node) to the global logical expert index.\n        \n        # Let's reconstruct the mapping for each node.\n        # For node `n`, the logical experts are indexed from `n * (num_logical_experts // num_nodes)` to `(n+1) * (num_logical_experts // num_nodes) - 1`.\n        \n        # Create `global_logical_expert_indices_per_node`: [num_nodes, num_logical_experts // num_nodes]\n        global_logical_expert_indices_per_node = torch.arange(num_logical_experts, device=weight.device).unflatten(-1, (num_groups, group_size)) \\\n                                                   .view(num_groups // num_nodes, num_nodes, group_size) \\\n                                                   .transpose(0, 1) \\\n                                                   .contiguous()[:, :, 0] # [num_nodes, num_logical_experts_per_node]\n        \n        # Now, for each node's `phy2mlog` output, index into `global_logical_expert_indices_per_node`.\n        # `phy2mlog` from replicate_experts is `[num_layers, num_logical_experts_per_node]`.\n        # The values are indices into the logical experts *within that node*.\n        \n        # For node `node_idx`:\n        # `node_phy2mlog = phy2mlog_list[node_idx]` # [num_layers, num_physical_experts_per_node]\n        # `node_global_map = global_logical_expert_indices_per_node[node_idx]` # [num_logical_experts_per_node]\n        \n        # `final_phy2log_node = node_global_map.gather(0, node_phy2mlog)` # This is incorrect indexing.\n        \n        # We need to use `node_phy2mlog` as indices into `node_global_map`.\n        # `final_phy2log_node = torch.gather(node_global_map.unsqueeze(0).expand_as(node_phy2mlog), -1, node_phy2mlog)` # Incorrect.\n        \n        # Let's use `torch.gather` correctly.\n        # `torch.gather(input, dim, index)`\n        # `input` is `node_global_map` (shape: [num_logical_experts_per_node])\n        # `dim` is 0 (since we want to index along the logical expert dimension)\n        # `index` is `node_phy2mlog` (shape: [num_layers, num_physical_experts_per_node])\n        \n        node_global_map = global_logical_expert_indices_per_node[node_idx] # [num_logical_experts_per_node]\n        node_phy2mlog = all_phy2mlog_list[node_idx] # [num_layers, num_physical_experts_per_node]\n        \n        # Map `node_phy2mlog` (indices into logical experts within node) to global logical expert indices.\n        # We need to apply `gather` for each layer.\n        final_phy2log_node = torch.gather(node_global_map.unsqueeze(0).expand_as(node_phy2mlog), -1, node_phy2mlog)\n        \n        final_phy2log_list.append(final_phy2log_node)\n\n    # Concatenate the final `phy2log` from all nodes.\n    phy2log = torch.cat(final_phy2log_list, dim=1) # [num_layers, num_physical_experts]\n\n    # Now, let's handle `phyrank` and `logcnt`.\n    # `phyrank` from `replicate_experts` is the rank of the physical expert within its logical expert group.\n    # `mlogcnt` from `replicate_experts` is the number of replicas for each logical expert (within node).\n    \n    # We need to combine `phyrank` and `mlogcnt` across nodes.\n    # `phyrank` has shape `[num_layers, num_physical_experts_per_node]` for each node.\n    # `mlogcnt` has shape `[num_layers, num_physical_experts_per_node]` for each node.\n    \n    phyrank = torch.cat(all_phyrank_list, dim=1) # [num_layers, num_physical_experts]\n    \n    # `mlogcnt` needs to be converted to `expert_count` for the global logical experts.\n    # `mlogcnt` is `[num_layers, num_physical_experts_per_node]` for each node.\n    # We need to aggregate this information to get the count of physical experts for each global logical expert.\n    \n    # The `mlogcnt_node` has shape `[num_layers, num_physical_experts_per_node]`.\n    # The values represent the number of replicas for the logical expert *within that node*.\n    # We need to sum these up for each global logical expert.\n    \n    # Let's create `logcnt_per_node`: [num_nodes, num_layers, num_logical_experts_per_node]\n    logcnt_per_node = torch.stack(all_mlogcnt_list, dim=1) # [num_layers, num_nodes, num_physical_experts_per_node]\n    \n    # Reshape `logcnt_per_node` to group by logical expert within node.\n    # `logcnt_per_node`: [num_layers, num_nodes, num_physical_experts_per_node]\n    # We need to map `logcnt_per_node` based on `phy2mlog` to get counts per logical expert.\n    \n    # Create `logical_expert_counts_per_node_layer`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n    logical_expert_counts_per_node_layer = torch.zeros(num_layers, num_nodes, num_logical_experts // num_nodes, dtype=torch.int64, device=weight.device)\n    \n    for node_idx in range(num_nodes):\n        node_mlogcnt = all_mlogcnt_list[node_idx] # [num_layers, num_physical_experts_per_node]\n        node_phy2mlog = all_phy2mlog_list[node_idx] # [num_layers, num_physical_experts_per_node]\n        \n        # For each layer, we want to sum `node_mlogcnt` values based on `node_phy2mlog`.\n        # This is a scatter_add operation.\n        for l in range(num_layers):\n            # Use scatter_add to sum counts for each logical expert within the node.\n            logical_expert_counts_per_node_layer[l, node_idx].scatter_add_(\n                0,\n                node_phy2mlog[l],\n                node_mlogcnt[l]\n            )\n\n    # Now, sum the counts across nodes for each logical expert.\n    # `logcnt`: [num_layers, num_logical_experts]\n    logcnt = logical_expert_counts_per_node_layer.sum(dim=1) # [num_layers, num_logical_experts // num_nodes]\n    \n    # The `logcnt` should be for the global logical experts.\n    # The current `logcnt` is for logical experts *within each node*.\n    # We need to aggregate `logcnt` based on the node assignment of logical experts.\n    \n    # Let's re-calculate `logcnt` from `phy2log`.\n    # `phy2log` is `[num_layers, num_physical_experts]`.\n    # `phyrank` is `[num_layers, num_physical_experts]`.\n    # We need to count how many physical experts map to each global logical expert.\n    \n    # Create a tensor to store the counts.\n    expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n    \n    # Iterate through all physical experts and increment the count for their corresponding logical expert.\n    for i in range(num_layers):\n        for j in range(num_physical_experts):\n            logical_expert_idx = phy2log[i, j]\n            if logical_expert_idx != -1: # Ensure it's a valid mapping\n                expert_count[i, logical_expert_idx] += 1\n                \n    # The `phyrank` needs to be mapped correctly.\n    # `phyrank` is `[num_layers, num_physical_experts]`.\n    # The values are the replica ranks.\n    \n    # Let's ensure the `replicate_experts` output `phyrank` is correctly used.\n    # `phyrank` from `replicate_experts` is `[X, num_log]`.\n    # In our case, `X` is `num_layers`.\n    # `num_log` is `num_logical_experts_per_node`.\n    \n    # The combined `phyrank` has shape `[num_layers, num_physical_experts]`.\n    # The values within this tensor are the ranks within their respective logical expert groups.\n    # The `phyrank` is already correctly structured if we concatenate it.\n    \n    # We need to ensure the `logcnt` calculation is correct.\n    # `mlogcnt_node` from `replicate_experts` is `[num_layers, num_physical_experts_per_node]`.\n    # It represents the number of replicas for each logical expert *within that node*.\n    \n    # The `logical_expert_counts_per_node_layer` correctly sums up the `mlogcnt` for each logical expert within each node.\n    # Now, we need to sum these counts across nodes to get the total count for each global logical expert.\n    \n    # The `logical_expert_counts_per_node_layer` has shape `[num_layers, num_nodes, num_logical_experts // num_nodes]`.\n    # We need to map this based on the node assignment of logical experts.\n    \n    # Let's create `global_logical_expert_counts`: [num_layers, num_logical_experts]\n    global_logical_expert_counts = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n    \n    for l in range(num_layers):\n        for n in range(num_nodes):\n            # Get the logical experts for this node.\n            node_logical_experts = global_logical_expert_indices_per_node[n] # [num_logical_experts_per_node]\n            # Get the counts for this node and layer.\n            node_counts = logical_expert_counts_per_node_layer[l, n] # [num_logical_experts_per_node]\n            \n            # Use scatter_add to add these counts to the correct global logical expert index.\n            global_logical_expert_counts[l].scatter_add_(0, node_logical_experts, node_counts)\n    \n    logcnt = global_logical_expert_counts # This should be the correct `logcnt`\n\n    return phy2log, phyrank, logcnt\n>>>>>>> REPLACE\n\n**Explanation of Changes:**\n\nThe primary goal of this change is to simplify the complex tensor manipulations in `rebalance_experts_hierarchical` that map between different levels of expert indexing (groups, logical experts, node-local logical experts, physical experts).\n\n1.  **Restructuring `rebalance_experts_hierarchical`**:\n    *   The previous implementation involved several intermediate tensors like `mlog2log`, `pphy2mlog`, and `pphy2log` which were calculated using complex `gather` operations.\n    *   The revised code aims to directly compute the necessary mappings by:\n        *   First, calculating the weights for each logical expert *within its assigned node*. This is done by summing the weights of groups that are assigned to a particular node. The `logical_expert_weights_per_node_layer` tensor stores this information.\n        *   Then, applying `replicate_experts` to these node-specific logical expert weights. This is done iteratively for each node.\n        *   Finally, converting the \"logical expert within a node\" index (output by `replicate_experts`) to a \"global logical expert\" index by using a pre-computed mapping (`global_logical_expert_indices_per_node`).\n        *   The `phyrank` is concatenated directly from the node-level results.\n        *   The `logcnt` (expert count) is calculated by summing up the replica counts from each node for the corresponding global logical experts. This involves a `scatter_add_` operation to aggregate counts correctly.\n\n2.  **Improved Readability**: By breaking down the process into clearer steps (calculate node-local weights, replicate within nodes, map to global logical experts, aggregate counts), the code becomes more understandable.\n\n3.  **Potential Efficiency Gains**: While the number of operations might seem similar, directly constructing the mappings and avoiding complex intermediate `gather` calls on flattened tensors can be more efficient. The iterative application of `replicate_experts` per node, followed by a direct mapping, might also have better performance characteristics on modern hardware.\n\n**Note on Metrics:**\n\n*   **Balancedness Score**: The core logic of `balanced_packing` and `replicate_experts` remains, which are designed for load balancing. The simplification aims to ensure this logic is applied correctly and efficiently across the hierarchy.\n*   **Speed Score**: The primary focus of these changes is to improve the speed score by reducing complex tensor operations and potentially improving data locality during calculations.\n*   **Combined Score**: An improvement in both balancedness and speed should lead to a better combined score.\n\nThis revised approach aims to achieve the same hierarchical balancing strategy but with a more streamlined and potentially faster implementation."]}}}