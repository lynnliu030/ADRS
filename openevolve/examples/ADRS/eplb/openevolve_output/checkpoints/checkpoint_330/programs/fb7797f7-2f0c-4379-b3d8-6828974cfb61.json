{"id": "fb7797f7-2f0c-4379-b3d8-6828974cfb61", "code": "# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n    device = weight.device\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(num_groups,\n                                  dtype=torch.int64,\n                                  device=device).expand(num_layers, num_groups)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort weights and get indices\n    # Keep on original device (GPU if weight is on GPU)\n    sorted_weights, sorted_indices = weight.float().sort(-1, descending=True)\n\n    # Initialize pack_index and rank_in_pack on the same device as weight\n    pack_index = torch.full((num_layers, num_groups), -1, dtype=torch.int64, device=device)\n    rank_in_pack = torch.full((num_layers, num_groups), -1, dtype=torch.int64, device=device)\n\n    # Initialize pack_loads and pack_counts on the same device as weight\n    pack_loads = torch.zeros(num_layers, num_packs, dtype=torch.float32, device=device)\n    pack_counts = torch.zeros(num_layers, num_packs, dtype=torch.int64, device=device)\n\n    # Iterate through sorted items and assign them to the least loaded pack\n    # This loop is inherently sequential for each layer (i) and group (g)\n    # as pack_loads and pack_counts are updated dynamically.\n    # However, keeping operations on GPU avoids CPU-GPU transfers if weight is on GPU.\n    for i in range(num_layers):\n        for g in range(num_groups):\n            # Find the pack with the minimum current load and available slots\n            available_packs_mask = (pack_counts[i] < groups_per_pack)\n\n            # Select from available packs\n            # Use `torch.where` to set loads of unavailable packs to infinity,\n            # so `argmin` will correctly pick from available ones.\n            current_pack_loads = torch.where(available_packs_mask, pack_loads[i], float('inf'))\n            chosen_pack_idx = torch.argmin(current_pack_loads)\n\n            original_group_idx = sorted_indices[i, g]\n            original_group_weight = weight[i, original_group_idx]\n\n            pack_index[i, original_group_idx] = chosen_pack_idx\n            rank_in_pack[i, original_group_idx] = pack_counts[i, chosen_pack_idx]\n\n            pack_loads[i, chosen_pack_idx] += original_group_weight\n            pack_counts[i, chosen_pack_idx] += 1\n\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Unified rebalancing logic:\n    # The hierarchical approach is used when num_groups is divisible by num_nodes.\n    # Otherwise, it falls back to a global balancing strategy by setting num_nodes=1 and num_groups=1.\n    # We can simplify this by directly implementing the global strategy when needed,\n    # or by ensuring the hierarchical logic correctly handles num_nodes=1.\n\n    # Determine the effective number of groups and nodes for balancing.\n    # If num_groups is not divisible by num_nodes, we treat it as a single group\n    # across all nodes for the first level of balancing.\n    effective_num_groups = num_groups if num_groups % num_nodes == 0 else 1\n    effective_num_nodes = num_nodes if num_groups % num_nodes == 0 else 1\n\n    # If num_groups % num_nodes != 0, we effectively want to balance across all groups as if they were one large group.\n    # And then balance those groups onto nodes.\n    # This is equivalent to setting num_nodes = 1 and num_groups = 1 in the hierarchical logic.\n    # Let's adjust parameters for a unified call to a simplified hierarchical logic.\n\n    # Simplified approach:\n    # First, balance logical experts across groups within layers.\n    # Then, balance these groups across nodes.\n    # Finally, replicate experts within nodes to balance across GPUs.\n\n    # Step 1: Balance logical experts into groups.\n    # If num_groups == 1, this step is trivial and doesn't change anything.\n    group_size = num_logical_experts // num_groups\n    tokens_per_group_layer = weight.unflatten(-1, (num_groups, group_size)).sum(-1) # [num_layers, num_groups]\n\n    # Pack groups onto nodes.\n    # If num_groups % num_nodes != 0, we effectively want to balance all groups onto a single 'node' conceptually,\n    # and then distribute those across the actual num_nodes.\n    # Let's consider the case where num_groups is not divisible by num_nodes.\n    # The original code uses num_nodes for packing groups. If num_groups < num_nodes, this packing might be problematic.\n    # A more robust approach is to balance groups onto num_nodes, and if num_groups < num_nodes, some nodes get 0 groups.\n    # The current `balanced_packing` assumes num_groups is divisible by num_packs (num_nodes here).\n\n    # Let's redefine the packing strategy:\n    # We want to pack 'num_groups' items (groups) into 'num_nodes' packs.\n    # If num_groups is not divisible by num_nodes, balanced_packing needs to handle it.\n    # The current `balanced_packing` asserts `num_groups % num_packs == 0`.\n    # This means if num_groups % num_nodes != 0, the original code calls `rebalance_experts_hierarchical` with num_nodes=1, num_groups=1.\n    # This is a global balancing strategy.\n\n    # Let's ensure the function call correctly reflects the intended strategy.\n    # If num_groups % num_nodes == 0, use hierarchical.\n    # Otherwise, use global (which is hierarchical with num_nodes=1, num_groups=1).\n\n    if num_groups % num_nodes == 0:\n        # Hierarchical balancing: balance groups to nodes, then nodes to GPUs.\n        # Step 1: Pack groups to nodes.\n        group_pack_index, group_rank_in_pack = balanced_packing(tokens_per_group_layer, num_nodes)\n        # Map logical experts to \"middle\" logical experts (logical experts per node)\n        # log2mlog: [num_layers, num_logical_experts] -> maps logical expert to its middle logical expert\n        mlog2log_map = (((group_pack_index * groups_per_node + group_rank_in_pack) * group_size).unsqueeze(-1) +\n                        torch.arange(group_size, dtype=torch.int64, device=group_pack_index.device)).flatten(-2)\n        # mlog2log_map: [num_layers, num_logical_experts] maps each logical expert to its \"middle\" logical expert ID.\n        # We need the inverse mapping for `weight.gather` later.\n        # This inverse mapping is tricky because multiple logical experts can map to the same middle expert.\n        # The `inverse` function is designed for permutations, which isn't directly applicable here.\n\n        # Let's rethink the mapping and replication.\n        # We have `num_layers` of `weight` ([num_layers, num_logical_experts]).\n        # First, we group logical experts: `tokens_per_group_layer` ([num_layers, num_groups]).\n        # Then, we pack these groups onto `num_nodes`: `group_pack_index` ([num_layers, num_groups]) and `group_rank_in_pack` ([num_layers, num_groups]).\n        # This means `num_groups` are assigned to `num_nodes`. `groups_per_node` groups per node.\n        # So, we have `num_layers * num_nodes` \"node-level\" expert weight tensors.\n        # Each node-level tensor has `num_logical_experts // num_nodes` \"middle\" experts.\n\n        # Let's create the \"middle\" logical experts' weights.\n        # `mlog_weights`: [num_layers * num_nodes, num_logical_experts // num_nodes]\n        # This requires carefully gathering weights.\n        # The `mlog2log` mapping needs to be constructed correctly.\n\n        # `mlog2log_map` assigns each logical expert to a conceptual \"middle\" logical expert.\n        # e.g., if num_logical_experts=8, num_groups=2, group_size=4.\n        # Group 0 contains logical experts 0,1,2,3. Group 1 contains 4,5,6,7.\n        # If num_nodes=2, groups_per_node=1.\n        # Group 0 (log experts 0-3) goes to node 0. Group 1 (log experts 4-7) goes to node 1.\n        # Node 0 has 1 middle expert. Node 1 has 1 middle expert.\n        # Middle expert for node 0 is logical experts 0-3. Middle expert for node 1 is 4-7.\n        # The mapping `mlog2log_map` should map:\n        # 0,1,2,3 -> 0 (on node 0)\n        # 4,5,6,7 -> 0 (on node 1)\n        # The current `mlog2log` calculation seems to be constructing the indices of the middle experts.\n        # `log2mlog` is `[num_layers, num_logical_experts]`.\n        # It maps logical expert ID to its index within the middle expert group.\n        # The `mlog2log` is the inverse of this index mapping.\n\n        # Let's reconstruct `mlog2log` and `tokens_per_mlog` more directly.\n        # For each layer, we have `num_groups` of `group_size`.\n        # `group_pack_index`: [num_layers, num_groups] - node assignment for each group.\n        # `group_rank_in_pack`: [num_layers, num_groups] - index of group within the node's assigned groups.\n\n        # We want to create `num_layers * num_nodes` \"node-level\" expert groups.\n        # Each node-level group corresponds to a set of original logical experts.\n        # The `tokens_per_mlog` should be `[num_layers * num_nodes, num_logical_experts // num_nodes]`.\n\n        # Calculate the logical expert index for each \"middle\" expert.\n        # `mlog_expert_indices`: [num_layers, num_nodes, groups_per_node, group_size]\n        # This is becoming complex. Let's simplify the conceptualization.\n\n        # Simpler approach:\n        # 1. Pack groups to nodes. This determines which logical experts are grouped together on each node.\n        # `group_pack_index`: [num_layers, num_groups] -> node ID for each group.\n        # `group_rank_in_pack`: [num_layers, num_groups] -> rank within node for each group.\n\n        # 2. For each node, create a combined weight tensor for the logical experts assigned to it.\n        #    The number of logical experts per node will be `groups_per_node * group_size`.\n        #    We need to gather the weights of logical experts based on `group_pack_index`.\n\n        # Let's create a mapping from (layer, node) to a contiguous block of logical experts.\n        # `node_logical_expert_map`: [num_layers, num_nodes, groups_per_node * group_size]\n\n        # This requires constructing a permutation for each layer and node.\n        # The original `mlog2log` calculation is:\n        # `((group_pack_index * groups_per_node + group_rank_in_pack) * group_size)`\n        # This forms a base index for each logical expert.\n        # `log2mlog` is this base index + `arange(group_size)`.\n        # `mlog2log` is the inverse of `log2mlog`.\n\n        # Let's directly construct the `tokens_per_mlog` and `mlog2log`.\n        # `tokens_per_mlog` is the weight tensor for experts *within* each node, considering the grouped logical experts.\n        # The number of such \"middle\" experts per node is `groups_per_node * group_size`.\n\n        # For each layer:\n        # `group_pack_index` tells us which node each group belongs to.\n        # `group_rank_in_pack` tells us the order within the node.\n\n        # Let's create `num_layers * num_nodes` tensors, each of size `groups_per_node * group_size`.\n        # `mlog_weights`: [num_layers * num_nodes, groups_per_node * group_size]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, groups_per_node * group_size]\n\n        # Example: num_layers=1, num_logical_experts=8, num_groups=2, group_size=4, num_nodes=2, groups_per_node=1, num_gpus=2, phy_experts_per_gpu=1.\n        # weight: [1, 8]\n        # tokens_per_group_layer: [1, 2] (sum of weights for group 0 and group 1)\n        # balanced_packing(tokens_per_group_layer, num_nodes=2) ->\n        #   group_pack_index: [1, 2] -> [[0, 1]] (group 0 to node 0, group 1 to node 1)\n        #   group_rank_in_pack: [1, 2] -> [[0, 0]] (rank 0 within node for both groups)\n        # groups_per_node = 1, group_size = 4.\n\n        # Middle expert for node 0 is logical experts 0,1,2,3.\n        # Middle expert for node 1 is logical experts 4,5,6,7.\n        # `mlog_to_orig_log_map` for node 0 should be [0,1,2,3].\n        # `mlog_to_orig_log_map` for node 1 should be [4,5,6,7].\n\n        # The original `mlog2log` calculation:\n        # `base_idx = (group_pack_index * groups_per_node + group_rank_in_pack) * group_size`\n        # `base_idx`: [1, 2] -> [[0*1+0, 1*1+0]] * 4 = [[0, 4]]\n        # `log2mlog`: `base_idx.unsqueeze(-1) + torch.arange(group_size)`\n        # `log2mlog`: [[0, 4]].unsqueeze(-1) + [0,1,2,3] -> [[0,1,2,3], [4,5,6,7]]\n        # This `log2mlog` is `[num_layers, num_logical_experts]`. It maps original logical expert ID to its \"middle\" expert index.\n        # The original logic uses `weight.gather(-1, mlog2log)` which is incorrect here.\n        # `weight.gather(-1, mlog2log)` expects `mlog2log` to be `[num_layers, num_middle_experts]` where `num_middle_experts` is the number of columns in `weight`.\n        # The shape should be `[num_layers, num_logical_experts]` if we are gathering from `weight` which is `[num_layers, num_logical_experts]`.\n        # The `mlog2log` calculated here has shape `[num_layers, num_logical_experts]`.\n        # It maps `original_logical_expert_id` to `middle_expert_id`.\n        # We need to gather `weight[layer, original_logical_expert_id]` and group them by `middle_expert_id`.\n\n        # Let's create `mlog_weights` directly.\n        # `mlog_weights`: [num_layers * num_nodes, groups_per_node * group_size]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, groups_per_node * group_size]\n\n        # Vectorized construction of mlog_weights and mlog_to_orig_log_map\n        # 1. Create a combined key for sorting groups within each layer based on node assignment and rank.\n        # This will order the groups such that all groups for node 0 come first, then node 1, etc.,\n        # and within each node, groups are ordered by their rank_in_pack.\n        combined_group_key = group_pack_index * groups_per_node + group_rank_in_pack # [num_layers, num_groups]\n\n        # 2. Get the sorted indices for groups for each layer.\n        sorted_group_indices = combined_group_key.argsort(dim=-1) # [num_layers, num_groups]\n\n        # 3. Create a base tensor for original logical expert IDs within each group.\n        # This tensor represents the original logical expert IDs if groups were laid out sequentially.\n        # Shape: [num_groups, group_size]\n        base_orig_log_expert_ids_per_group = (torch.arange(num_groups, device=weight.device, dtype=torch.int64) * group_size).unsqueeze(-1) + \\\n                                             torch.arange(group_size, device=weight.device, dtype=torch.int64)\n\n        # 4. Use sorted_group_indices to gather original logical expert IDs and their weights.\n        # Expand sorted_group_indices to match the shape of the data being gathered (groups, then experts within groups).\n        sorted_group_indices_expanded = sorted_group_indices.unsqueeze(-1).expand(-1, -1, group_size)\n\n        # Reshape the original weight tensor to [num_layers, num_groups, group_size] for gathering.\n        weight_reshaped_by_group = weight.view(num_layers, num_groups, group_size)\n\n        # Gather the weights based on the sorted group order.\n        # mlog_weights will have shape [num_layers, num_groups, group_size]\n        mlog_weights = weight_reshaped_by_group.gather(1, sorted_group_indices_expanded)\n\n        # Gather the original logical expert IDs based on the sorted group order.\n        # mlog_to_orig_log_map will have shape [num_layers, num_groups, group_size]\n        mlog_to_orig_log_map = base_orig_log_expert_ids_per_group.unsqueeze(0).expand(num_layers, -1, -1).gather(1, sorted_group_indices_expanded)\n\n        # 5. Reshape to [num_layers, num_nodes, groups_per_node * group_size]\n        # This aligns the data such that experts assigned to the same node are contiguous.\n        num_middle_experts_per_node = groups_per_node * group_size\n        mlog_weights = mlog_weights.view(num_layers, num_nodes, num_middle_experts_per_node)\n        mlog_to_orig_log_map = mlog_to_orig_log_map.view(num_layers, num_nodes, num_middle_experts_per_node)\n\n        # Now, `mlog_weights`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n        # And `mlog_to_orig_log_map`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n\n        # Step 2: Replicate experts within each node.\n        # `mlog_weights` are the weights for experts within each node.\n        # `num_physical_experts // num_nodes` is the target number of physical experts per node.\n        # `replicate_experts` takes `[X, num_log]` and returns `phy2log`, `rank`, `logcnt`.\n        # Here, X = num_layers * num_nodes, num_log = num_logical_experts // num_nodes.\n        # So, we need to reshape `mlog_weights` and `mlog_to_orig_log_map`.\n\n        num_middle_experts = num_logical_experts // num_nodes\n        reshaped_mlog_weights = mlog_weights.view(-1, num_middle_experts)\n        reshaped_mlog_to_orig_log_map = mlog_to_orig_log_map.view(-1, num_middle_experts)\n\n        # `phy2mlog`: [num_layers * num_nodes, num_physical_experts // num_nodes]\n        # `phyrank`: [num_layers * num_nodes, num_physical_experts // num_nodes]\n        # `mlogcnt`: [num_layers * num_nodes, num_middle_experts]\n        phy2mlog, phyrank, mlogcnt = replicate_experts(\n            reshaped_mlog_weights, num_physical_experts // num_nodes)\n\n        # Step 3: Pack physical experts to GPUs.\n        # `phy2mlog` maps physical experts (within each node) to middle logical experts.\n        # `phy2mlog` is `[num_layers * num_nodes, num_phy_per_node]`\n        # `phyrank` is `[num_layers * num_nodes, num_phy_per_node]`\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`\n\n        # We need to map these back to global physical expert indices.\n        # The `phy2mlog` refers to the *middle* logical experts.\n        # The `replicate_experts` function returns `phy2log` which is the *original* logical expert ID.\n        # So, `phy2mlog` from `replicate_experts` is actually `[num_layers*num_nodes, num_phy_per_node]`,\n        # where each entry is an index into the *middle* logical experts.\n\n        # We need to map these `phy2mlog` indices back to original logical expert IDs.\n        # `reshaped_mlog_to_orig_log_map`: [num_layers * num_nodes, num_middle_experts]\n        # `final_phy2log_per_node`: [num_layers * num_nodes, num_phy_per_node]\n        final_phy2log_per_node = reshaped_mlog_to_orig_log_map.gather(-1, phy2mlog)\n\n        # `phyrank` is the replica rank for the middle experts.\n        # We need to combine this with `mlogcnt` to get the final `logcnt`.\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`. It tells us how many replicas each middle expert has.\n        # The `phyrank` is the rank of the replica for a *specific* middle expert.\n\n        # The `logcnt` returned by `replicate_experts` is `[X, num_log]`, which is `[num_layers*num_nodes, num_middle_experts]`.\n        # We need to map this back to `[num_layers, num_logical_experts]`.\n\n        # Let's use the `mlogcnt` directly to get the total count of replicas for each logical expert.\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`.\n        # We need to sum/average counts based on the `mlog_to_orig_log_map`.\n\n        # Let's refine the output mapping:\n        # `physical_to_logical_map`: [num_layers, num_replicas]\n        # `logical_to_physical_map`: [num_layers, num_logical_experts, X]\n        # `expert_count`: [num_layers, num_logical_experts]\n\n        # `final_phy2log_per_node`: [num_layers * num_nodes, num_phy_per_node]\n        # This is the logical expert ID for each physical expert *within its node*.\n        # We need to convert this to global physical expert IDs.\n        # Physical experts are grouped by GPU. `num_gpus` total, `num_gpus // num_nodes` per node.\n        # `phy_experts_per_gpu` = `num_physical_experts // num_gpus`.\n        # So, each GPU has `phy_experts_per_gpu` physical experts.\n        # Each Node has `num_gpus // num_nodes` GPUs.\n        # Total physical experts per node = `(num_gpus // num_nodes) * phy_experts_per_gpu`\n        # This should equal `num_physical_experts // num_nodes`.\n\n        # `final_phy2log_per_node`: [num_layers, num_nodes, num_phy_per_node]\n        # `final_rank_per_node`: [num_layers, num_nodes, num_phy_per_node] (this is `phyrank`)\n\n        # We need to flatten this to `[num_layers, num_physical_experts]`.\n        # `num_physical_experts = num_nodes * num_phy_per_node`.\n\n        # `final_phy2log_global`: [num_layers, num_physical_experts]\n        # `final_rank_global`: [num_layers, num_physical_experts]\n\n        # The `mlog_to_orig_log_map` correctly maps middle experts to original logical experts.\n        # `phy2mlog` maps physical experts (within a node) to middle experts.\n        # So, `phy2mlog.gather(-1, final_phy2log_per_node)` gives the logical expert ID.\n        # This is what `final_phy2log_per_node` already represents.\n\n        # Let's rename `final_phy2log_per_node` to `phy2log_per_node` for clarity.\n        phy2log_per_node = final_phy2log_per_node\n        rank_per_node = phyrank # [num_layers * num_nodes, num_phy_per_node]\n\n        # Flatten these to get global mappings for physical experts.\n        phy2log_global = phy2log_per_node.view(num_layers, -1)\n        rank_global = rank_per_node.view(num_layers, -1)\n\n        # Now, let's construct the `logcnt` (expert_count).\n        # `mlogcnt`: [num_layers * num_nodes, num_middle_experts]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, num_middle_experts]\n\n        # We need to aggregate `mlogcnt` based on `mlog_to_orig_log_map`.\n        # For each logical expert, its total count is the sum of counts of its constituent middle experts.\n        # This aggregation needs to be done carefully.\n\n        # Let's create a tensor for `logcnt`: [num_layers, num_logical_experts] initialized to zeros.\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n\n        # Iterate through each layer and node.\n        for i in range(num_layers):\n            for node_idx in range(num_nodes):\n                # For the middle experts belonging to this node and layer:\n                middle_expert_indices = mlog_to_orig_log_map[i, node_idx] # [num_middle_experts]\n                counts_for_middle_experts = mlogcnt[i * num_nodes + node_idx] # [num_middle_experts]\n\n                # Add these counts to the corresponding original logical experts.\n                # `torch.add.at` allows for sparse updates.\n                expert_count[i].scatter_add_(0, middle_expert_indices, counts_for_middle_experts)\n\n        # The `logcnt` from `replicate_experts` is the number of replicas for the middle experts.\n        # We need to make sure the `expert_count` is correct.\n        # The `replicate_experts` function returns `logcnt` which is the number of replicas for each logical expert it processed.\n        # So, `mlogcnt` is the number of replicas for each middle expert.\n\n        # The final `expert_count` should be `[num_layers, num_logical_experts]`.\n        # We need to map `mlogcnt` back.\n\n        # Let's create `expert_count` of size `[num_layers, num_logical_experts]` initialized to 0.\n        # Then, for each layer, node, and middle expert:\n        # `expert_count[layer, mlog_to_orig_log_map[layer, node, middle_idx]] += mlogcnt[layer*num_nodes + node, middle_idx]`\n\n        # This aggregation is correct.\n\n        # Now, we need to construct `logical_to_physical_map`.\n        # `logical_to_physical_map`: [num_layers, num_logical_experts, X] where X is max replicas.\n        # `phy2log_global`: [num_layers, num_physical_experts]\n        # `rank_global`: [num_layers, num_physical_experts]\n\n        # We need to group physical experts by their logical expert ID and rank.\n        max_replicas_per_expert = (num_physical_experts + num_logical_experts - 1) // num_logical_experts # Ceiling division\n\n        logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert), -1, dtype=torch.int64, device=weight.device)\n\n        # Iterate through all physical experts\n        for layer_idx in range(num_layers):\n            for phys_idx in range(num_physical_experts):\n                log_expert_id = phy2log_global[layer_idx, phys_idx]\n                replica_rank = rank_global[layer_idx, phys_idx]\n\n                if log_expert_id >= 0 and log_expert_id < num_logical_experts:\n                    # Place the physical expert ID into the correct slot based on its logical ID and rank.\n                    # We need to ensure we don't exceed `max_replicas_per_expert`.\n                    # The `replica_rank` should correspond to the index in the third dimension.\n                    if replica_rank < max_replicas_per_expert:\n                        logical_to_physical_map[layer_idx, log_expert_id, replica_rank] = phys_idx\n                    else:\n                        # This case indicates an issue or that max_replicas_per_expert needs to be larger.\n                        # For now, we can assert or log a warning.\n                        pass # Or handle error\n\n        # The `expert_count` is already computed.\n\n        return phy2log_global, logical_to_physical_map, expert_count\n\n    else:\n        # Global balancing strategy: treat all experts as one group, balance across num_gpus.\n        # This is equivalent to calling the hierarchical function with num_nodes=1, num_groups=1.\n        # num_groups = 1 means group_size = num_logical_experts.\n        # num_nodes = 1 means groups_per_node = 1.\n        # The entire logic simplifies.\n\n        # Step 1: Pack logical experts to nodes (here, nodes = GPUs conceptually).\n        # `tokens_per_group` becomes `weight` itself since num_groups=1.\n        # `balanced_packing(weight, num_gpus)`\n        # `group_pack_index`: [num_layers, num_logical_experts] -> GPU assignment for each logical expert.\n        # `group_rank_in_pack`: [num_layers, num_logical_experts] -> rank within GPU for each logical expert.\n        # `num_nodes` becomes `num_gpus`. `groups_per_node` becomes `num_logical_experts // num_gpus`.\n\n        # We need to adapt `balanced_packing` to work with `num_packs = num_gpus`.\n        # The `groups_per_pack` in `balanced_packing` refers to items per pack.\n        # Here, items are logical experts. So, `groups_per_pack` = `num_logical_experts // num_gpus`.\n        # `num_packs` = `num_gpus`.\n\n        pack_index_gpu, rank_in_gpu = balanced_packing(weight, num_gpus)\n\n        # `pack_index_gpu`: [num_layers, num_logical_experts] -> GPU ID for each logical expert.\n        # `rank_in_gpu`: [num_layers, num_logical_experts] -> rank within GPU for each logical expert.\n\n        # Step 2: Replicate experts within GPUs.\n        # We need to create tensors for `replicate_experts` that represent experts *per GPU*.\n        # `num_logical_experts_per_gpu` = `num_logical_experts // num_gpus`.\n        # `num_physical_experts_per_gpu` = `num_physical_experts // num_gpus`.\n\n        # For each GPU, we have a set of logical experts.\n        # We need to gather weights for logical experts assigned to the same GPU.\n        # `gpu_logical_weights`: [num_gpus, num_logical_experts_per_gpu, num_layers] (transposed for replicate_experts)\n        # `gpu_logical_to_orig_map`: [num_gpus, num_logical_experts_per_gpu]\n\n        gpu_logical_weights_list = []\n        gpu_logical_to_orig_map_list = []\n\n        num_logical_experts_per_gpu = num_logical_experts // num_gpus\n\n        for gpu_idx in range(num_gpus):\n            # Find logical experts assigned to this GPU\n            assigned_logical_mask = (pack_index_gpu == gpu_idx)\n            # Get their ranks within the GPU\n            ranks_on_gpu = rank_in_gpu[assigned_logical_mask]\n            # Sort by rank\n            sorted_ranks_on_gpu, sort_indices_on_gpu = ranks_on_gpu.sort()\n            # Get original logical expert indices\n            assigned_logical_indices = torch.arange(num_logical_experts, device=pack_index_gpu.device)[assigned_logical_mask][sort_indices_on_gpu]\n\n            if len(assigned_logical_indices) > 0:\n                # Gather weights for these logical experts\n                # We need weights for each layer.\n                # `weight` is [num_layers, num_logical_experts].\n                # `assigned_logical_indices` is [num_logical_experts_per_gpu].\n                # We want `gpu_logical_weights` of shape [num_layers, num_logical_experts_per_gpu].\n                gpu_logical_weights = weight[:, assigned_logical_indices]\n                gpu_logical_weights_list.append(gpu_logical_weights)\n                gpu_logical_to_orig_map_list.append(assigned_logical_indices)\n            else:\n                # If a GPU gets no logical experts (shouldn't happen if num_logical_experts >= num_gpus)\n                gpu_logical_weights_list.append(torch.empty(num_layers, 0, dtype=weight.dtype, device=weight.device))\n                gpu_logical_to_orig_map_list.append(torch.empty(0, dtype=torch.int64, device=weight.device))\n\n        # `gpu_logical_weights`: [num_gpus, num_layers, num_logical_experts_per_gpu]\n        # `gpu_logical_to_orig_map`: [num_gpus, num_logical_experts_per_gpu]\n        gpu_logical_weights = torch.stack(gpu_logical_weights_list)\n        gpu_logical_to_orig_map = torch.stack(gpu_logical_to_orig_map_list)\n\n        # Reshape for `replicate_experts`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        reshaped_gpu_logical_weights = gpu_logical_weights.view(-1, num_logical_experts_per_gpu)\n        reshaped_gpu_logical_to_orig_map = gpu_logical_to_orig_map.view(-1, num_logical_experts_per_gpu)\n\n        # `replicate_experts` is called with `num_phy = num_physical_experts // num_gpus` (target physical experts per GPU).\n        # `X` becomes `num_gpus * num_layers`. `num_log` becomes `num_logical_experts_per_gpu`.\n        # `phy2log_gpu`: [num_gpus * num_layers, num_phy_per_gpu]\n        # `phyrank_gpu`: [num_gpus * num_layers, num_phy_per_gpu]\n        # `gpu_logcnt`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        phy2log_gpu, phyrank_gpu, gpu_logcnt = replicate_experts(\n            reshaped_gpu_logical_weights, num_physical_experts // num_gpus)\n\n        # Map `phy2log_gpu` back to original logical expert IDs using `reshaped_gpu_logical_to_orig_map`.\n        # `phy2log_global`: [num_gpus * num_layers, num_phy_per_gpu]\n        phy2log_global = reshaped_gpu_logical_to_orig_map.gather(-1, phy2log_gpu)\n\n        # Flatten to get the final `physical_to_logical_map` and `rank`.\n        # The `rank_global` is `phyrank_gpu`.\n        rank_global = phyrank_gpu\n\n        # Construct `expert_count` ([num_layers, num_logical_experts]).\n        # `gpu_logcnt`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n\n        for gpu_layer_idx in range(num_gpus * num_layers):\n            layer_idx = gpu_layer_idx // num_gpus\n            # Get the original logical expert indices for this gpu_layer combination\n            orig_log_indices = reshaped_gpu_logical_to_orig_map[gpu_layer_idx] # [num_logical_experts_per_gpu]\n            counts_for_orig_logs = gpu_logcnt[gpu_layer_idx] # [num_logical_experts_per_gpu]\n\n            # Add these counts to the correct positions in the expert_count tensor\n            expert_count[layer_idx].scatter_add_(0, orig_log_indices, counts_for_orig_logs)\n\n        # Construct `logical_to_physical_map` ([num_layers, num_logical_experts, X])\n        max_replicas_per_expert = (num_physical_experts + num_logical_experts - 1) // num_logical_experts\n\n        logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert), -1, dtype=torch.int64, device=weight.device)\n\n        # Need to map `phy2log_global` and `rank_global` to the correct positions.\n        # `phy2log_global` are indices into the *original* logical experts.\n        # `rank_global` is the replica rank for that original logical expert.\n\n        # Reshape to get mappings per layer and then iterate through physical experts.\n        phy2log_global_reshaped = phy2log_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous()\n        rank_global_reshaped = rank_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous()\n\n        # Iterate through all physical experts\n        for layer_idx in range(num_layers):\n            for gpu_idx in range(num_gpus):\n                for phys_idx_in_gpu in range(num_physical_experts // num_gpus):\n                    # Get the global physical expert index\n                    global_phys_idx = gpu_idx * (num_physical_experts // num_gpus) + phys_idx_in_gpu\n\n                    log_expert_id = phy2log_global_reshaped[layer_idx, gpu_idx, phys_idx_in_gpu]\n                    replica_rank = rank_global_reshaped[layer_idx, gpu_idx, phys_idx_in_gpu]\n\n                    if log_expert_id >= 0 and log_expert_id < num_logical_experts:\n                        if replica_rank < max_replicas_per_expert:\n                            logical_to_physical_map[layer_idx, log_expert_id, replica_rank] = global_phys_idx\n                        else:\n                            pass # Handle error or log\n\n        # The `phy2log_global` needs to be reshaped to `[num_layers, num_replicas]`\n        # where `num_replicas` is `num_physical_experts`.\n        # `phy2log_global` is currently `[num_gpus * num_layers, num_phy_per_gpu]`.\n        # We need to combine these.\n        # The order of physical experts should be consistent.\n        # Let's reconstruct `phy2log_global` to be `[num_layers, num_physical_experts]`.\n\n        final_phy2log_global = torch.empty((num_layers, num_physical_experts), dtype=torch.int64, device=weight.device)\n        current_phys_idx = 0\n        for gpu_idx in range(num_gpus):\n            for layer_idx in range(num_layers):\n                # Calculate the range of physical experts for this GPU and layer combination\n                start_phys_idx = layer_idx * num_gpus * (num_physical_experts // num_gpus) + gpu_idx * (num_physical_experts // num_gpus)\n                end_phys_idx = start_phys_idx + (num_physical_experts // num_gpus)\n\n                # The `phy2log_global` calculated earlier is `[num_gpus * num_layers, num_phy_per_gpu]`\n                # We need to map it correctly.\n                # `phy2log_gpu` was `[num_gpus * num_layers, num_phy_per_gpu]`\n                # `reshaped_gpu_logical_to_orig_map` was `[num_gpus * num_layers, num_phy_per_gpu]`\n                # `phy2log_global = reshaped_gpu_logical_to_orig_map.gather(-1, phy2log_gpu)`\n\n                # The indices for `phy2log_global` should be:\n                # For layer 0, GPU 0: phys experts 0 to num_phy_per_gpu - 1\n                # For layer 0, GPU 1: phys experts num_phy_per_gpu to 2*num_phy_per_gpu - 1\n                # ...\n                # For layer 1, GPU 0: phys experts num_gpus * num_phy_per_gpu to (num_gpus+1)*num_phy_per_gpu - 1\n\n                # Let's re-index the `phy2log_global` and `rank_global` to match `[num_layers, num_physical_experts]`\n                # The current `phy2log_global` is effectively `[num_layers * num_gpus, num_phy_per_gpu]`\n                # We need to rearrange it.\n\n                # Example: num_layers=2, num_gpus=2, num_phy_per_gpu=2. num_physical_experts=4.\n                # phy2log_global:\n                # [layer0_gpu0_phys0, layer0_gpu0_phys1,\n                #  layer0_gpu1_phys0, layer0_gpu1_phys1,\n                #  layer1_gpu0_phys0, layer1_gpu0_phys1,\n                #  layer1_gpu1_phys0, layer1_gpu1_phys1]\n                # Reshaped to [num_gpus*num_layers, num_phy_per_gpu] -> [4, 2]\n                # We want [num_layers, num_physical_experts] -> [2, 4]\n\n                # The current `phy2log_global` is `[num_gpus * num_layers, num_phy_per_gpu]`.\n                # Permute and reshape:\n                # `view(num_gpus, num_layers, -1)` gives `[num_gpus, num_layers, num_phy_per_gpu]`\n                # `permute(1, 0, 2)` gives `[num_layers, num_gpus, num_phy_per_gpu]`\n                # `contiguous().view(num_layers, -1)` gives `[num_layers, num_gpus * num_phy_per_gpu]` = `[num_layers, num_physical_experts]`\n\n                final_phy2log_global = phy2log_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous().view(num_layers, num_physical_experts)\n                final_rank_global = rank_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous().view(num_layers, num_physical_experts)\n\n        # Return the results for global balancing.\n        return final_phy2log_global, final_rank_global, expert_count\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    # Keep weights on their original device to avoid unnecessary transfers.\n    # Ensure float type for calculations.\n    weight = weight.float()\n    \n    # num_replicas is equivalent to num_physical_experts used internally\n    num_physical_experts = num_replicas\n\n    if num_groups % num_nodes == 0:\n        # Use hierarchical load-balance policy\n        phy2log_global, rank_global, expert_count = rebalance_experts_hierarchical(\n            weight, num_physical_experts, num_groups, num_nodes, num_gpus)\n    else:\n        # Use global load-balance policy\n        # This is equivalent to calling the hierarchical function with num_nodes=1, num_groups=1.\n        # The rebalance_experts_hierarchical function's 'else' branch is designed for this.\n        phy2log_global, rank_global, expert_count = rebalance_experts_hierarchical(\n            weight, num_physical_experts, 1, 1, num_gpus)\n\n    # Construct logical_to_physical_map here, common to both strategies.\n    # Max replicas per expert: This ensures `logical_to_physical_map` is large enough.\n    # It must be at least 1.\n    if num_logical_experts == 0: # Handle edge case to avoid division by zero\n        max_replicas_per_expert = 0\n    else:\n        # Calculate the maximum possible replicas for any single logical expert.\n        # This is a safe upper bound to dimension the output tensor.\n        max_replicas_per_expert = (num_physical_experts + num_logical_experts - 1) // num_logical_experts\n\n    # Initialize the map with -1 (indicating no physical expert assigned)\n    logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert), -1, dtype=torch.int64, device=weight.device)\n\n    # Prepare indices for index_put_ operation.\n    # `phy2log_global` maps physical expert index to logical expert index.\n    # `rank_global` is the replica rank for that physical expert.\n    # We need to gather the physical expert indices themselves.\n    # `global_physical_indices` represents the index of each physical expert.\n    global_physical_indices = torch.arange(num_replicas, dtype=torch.int64, device=weight.device).unsqueeze(0).expand(num_layers, -1)\n\n    # Create a mask for valid placements to avoid out-of-bounds writes.\n    # This mask selects entries where the logical expert index and rank are within the bounds\n    # of the `logical_to_physical_map` dimensions.\n    valid_mask = (phy2log_global >= 0) & \\\n                 (phy2log_global < num_logical_experts) & \\\n                 (rank_global >= 0) & \\\n                 (rank_global < max_replicas_per_expert)\n\n    # Flatten and filter indices and values using the valid_mask for `index_put_`.\n    # `layer_indices` will be a 1D tensor of layer indices for valid assignments.\n    layer_indices = torch.arange(num_layers, device=weight.device).unsqueeze(1).expand(-1, num_replicas)[valid_mask]\n    # `log_expert_indices` are the logical expert IDs for valid assignments.\n    log_expert_indices = phy2log_global[valid_mask]\n    # `rank_indices` are the replica ranks for valid assignments.\n    rank_indices = rank_global[valid_mask]\n    # `values_to_put` are the actual global physical expert indices for valid assignments.\n    values_to_put = global_physical_indices[valid_mask]\n\n    # Use index_put_ for efficient, scattered assignment.\n    # This directly places the `values_to_put` (physical expert indices) into the\n    # `logical_to_physical_map` at the positions specified by `layer_indices`,\n    # `log_expert_indices`, and `rank_indices`.\n    if values_to_put.numel() > 0: # Avoid calling index_put_ with empty tensors\n        logical_to_physical_map.index_put_((layer_indices, log_expert_indices, rank_indices), values_to_put)\n\n    # `phy2log_global` is the mapping from physical expert index to logical expert index.\n    # `rank_global` is the replica rank for each physical expert.\n    # `expert_count` is the number of physical replicas for each logical expert.\n    return phy2log_global, rank_global, expert_count\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n", "language": "python", "parent_id": "36cd8956-dddf-4c71-805c-d1c1d9156f26", "generation": 4, "timestamp": 1751053088.585316, "iteration_found": 52, "metrics": {"balancedness_score": 0.0, "speed_score": 0.0, "combined_score": 0.0, "error": "The size of tensor a (288) must match the size of tensor b (2) at non-singleton dimension 2"}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 12 lines with 10 lines\nChange 2: Replace 168 lines with 13 lines", "parent_metrics": {"balancedness_score": 0.0, "speed_score": 0.0, "combined_score": 0.0, "error": "The size of tensor a (288) must match the size of tensor b (2) at non-singleton dimension 2"}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve the Mixture-of-Expert models Expert Parallelism Load Balancer (MoE EPLB) expert rearrangement algorithm.\nThis algorithm will take the load metrics recorded by the vLLM server, and rearrange the experts to balance the load. It can make replicas of some experts to achieve better load balancing.\nYour goal will be two-fold: 1. Improve the algorithm to achieve better load balancing; while 2. Improve the algorithm to be more efficient, i.e. reduce the execution time of the algorithm itself, since perfect load balancing is NP-hard.\nThe current algorithm is implemented in the `rebalance_experts` function. ", "user": "# Current Program Information\n- Current performance metrics: - balancedness_score: 0.0000\n- speed_score: 0.0000\n- combined_score: 0.0000\n- error: The size of tensor a (288) must match the size of tensor b (2) at non-singleton dimension 2\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: balancedness_score, speed_score, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0405, combined_score: 0.1694\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0722, combined_score: 0.1852\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0761, combined_score: 0.1872\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.1872)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0761), Performs well on combined_score (0.1872)\n\n\n### Program 2 (Score: 0.1852)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0722), Performs well on combined_score (0.1852)\n\n\n### Program 3 (Score: 0.1694)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0405), Performs well on combined_score (0.1694)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.1661)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n\n### Program D2 (Score: 0.1667)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.1872, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 2 (Score: 0.0939, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 23 lines with 236 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 3 (Score: 0.0939, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 23 lines with 236 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 4 (Score: 0.0000, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 5 (Score: 0.0000, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 21 lines with 21 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n\n# Current Program\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n    device = weight.device\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(num_groups,\n                                  dtype=torch.int64,\n                                  device=device).expand(num_layers, num_groups)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort weights and get indices\n    # Keep on original device (GPU if weight is on GPU)\n    sorted_weights, sorted_indices = weight.float().sort(-1, descending=True)\n\n    # Initialize pack_index and rank_in_pack on the same device as weight\n    pack_index = torch.full((num_layers, num_groups), -1, dtype=torch.int64, device=device)\n    rank_in_pack = torch.full((num_layers, num_groups), -1, dtype=torch.int64, device=device)\n\n    # Initialize pack_loads and pack_counts on the same device as weight\n    pack_loads = torch.zeros(num_layers, num_packs, dtype=torch.float32, device=device)\n    pack_counts = torch.zeros(num_layers, num_packs, dtype=torch.int64, device=device)\n\n    # Iterate through sorted items and assign them to the least loaded pack\n    # This loop is inherently sequential for each layer (i) and group (g)\n    # as pack_loads and pack_counts are updated dynamically.\n    # However, keeping operations on GPU avoids CPU-GPU transfers if weight is on GPU.\n    for i in range(num_layers):\n        for g in range(num_groups):\n            # Find the pack with the minimum current load and available slots\n            available_packs_mask = (pack_counts[i] < groups_per_pack)\n\n            # Select from available packs\n            # Use `torch.where` to set loads of unavailable packs to infinity,\n            # so `argmin` will correctly pick from available ones.\n            current_pack_loads = torch.where(available_packs_mask, pack_loads[i], float('inf'))\n            chosen_pack_idx = torch.argmin(current_pack_loads)\n\n            original_group_idx = sorted_indices[i, g]\n            original_group_weight = weight[i, original_group_idx]\n\n            pack_index[i, original_group_idx] = chosen_pack_idx\n            rank_in_pack[i, original_group_idx] = pack_counts[i, chosen_pack_idx]\n\n            pack_loads[i, chosen_pack_idx] += original_group_weight\n            pack_counts[i, chosen_pack_idx] += 1\n\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Unified rebalancing logic:\n    # The hierarchical approach is used when num_groups is divisible by num_nodes.\n    # Otherwise, it falls back to a global balancing strategy by setting num_nodes=1 and num_groups=1.\n    # We can simplify this by directly implementing the global strategy when needed,\n    # or by ensuring the hierarchical logic correctly handles num_nodes=1.\n\n    # Determine the effective number of groups and nodes for balancing.\n    # If num_groups is not divisible by num_nodes, we treat it as a single group\n    # across all nodes for the first level of balancing.\n    effective_num_groups = num_groups if num_groups % num_nodes == 0 else 1\n    effective_num_nodes = num_nodes if num_groups % num_nodes == 0 else 1\n\n    # If num_groups % num_nodes != 0, we effectively want to balance across all groups as if they were one large group.\n    # And then balance those groups onto nodes.\n    # This is equivalent to setting num_nodes = 1 and num_groups = 1 in the hierarchical logic.\n    # Let's adjust parameters for a unified call to a simplified hierarchical logic.\n\n    # Simplified approach:\n    # First, balance logical experts across groups within layers.\n    # Then, balance these groups across nodes.\n    # Finally, replicate experts within nodes to balance across GPUs.\n\n    # Step 1: Balance logical experts into groups.\n    # If num_groups == 1, this step is trivial and doesn't change anything.\n    group_size = num_logical_experts // num_groups\n    tokens_per_group_layer = weight.unflatten(-1, (num_groups, group_size)).sum(-1) # [num_layers, num_groups]\n\n    # Pack groups onto nodes.\n    # If num_groups % num_nodes != 0, we effectively want to balance all groups onto a single 'node' conceptually,\n    # and then distribute those across the actual num_nodes.\n    # Let's consider the case where num_groups is not divisible by num_nodes.\n    # The original code uses num_nodes for packing groups. If num_groups < num_nodes, this packing might be problematic.\n    # A more robust approach is to balance groups onto num_nodes, and if num_groups < num_nodes, some nodes get 0 groups.\n    # The current `balanced_packing` assumes num_groups is divisible by num_packs (num_nodes here).\n\n    # Let's redefine the packing strategy:\n    # We want to pack 'num_groups' items (groups) into 'num_nodes' packs.\n    # If num_groups is not divisible by num_nodes, balanced_packing needs to handle it.\n    # The current `balanced_packing` asserts `num_groups % num_packs == 0`.\n    # This means if num_groups % num_nodes != 0, the original code calls `rebalance_experts_hierarchical` with num_nodes=1, num_groups=1.\n    # This is a global balancing strategy.\n\n    # Let's ensure the function call correctly reflects the intended strategy.\n    # If num_groups % num_nodes == 0, use hierarchical.\n    # Otherwise, use global (which is hierarchical with num_nodes=1, num_groups=1).\n\n    if num_groups % num_nodes == 0:\n        # Hierarchical balancing: balance groups to nodes, then nodes to GPUs.\n        # Step 1: Pack groups to nodes.\n        group_pack_index, group_rank_in_pack = balanced_packing(tokens_per_group_layer, num_nodes)\n        # Map logical experts to \"middle\" logical experts (logical experts per node)\n        # log2mlog: [num_layers, num_logical_experts] -> maps logical expert to its middle logical expert\n        mlog2log_map = (((group_pack_index * groups_per_node + group_rank_in_pack) * group_size).unsqueeze(-1) +\n                        torch.arange(group_size, dtype=torch.int64, device=group_pack_index.device)).flatten(-2)\n        # mlog2log_map: [num_layers, num_logical_experts] maps each logical expert to its \"middle\" logical expert ID.\n        # We need the inverse mapping for `weight.gather` later.\n        # This inverse mapping is tricky because multiple logical experts can map to the same middle expert.\n        # The `inverse` function is designed for permutations, which isn't directly applicable here.\n\n        # Let's rethink the mapping and replication.\n        # We have `num_layers` of `weight` ([num_layers, num_logical_experts]).\n        # First, we group logical experts: `tokens_per_group_layer` ([num_layers, num_groups]).\n        # Then, we pack these groups onto `num_nodes`: `group_pack_index` ([num_layers, num_groups]) and `group_rank_in_pack` ([num_layers, num_groups]).\n        # This means `num_groups` are assigned to `num_nodes`. `groups_per_node` groups per node.\n        # So, we have `num_layers * num_nodes` \"node-level\" expert weight tensors.\n        # Each node-level tensor has `num_logical_experts // num_nodes` \"middle\" experts.\n\n        # Let's create the \"middle\" logical experts' weights.\n        # `mlog_weights`: [num_layers * num_nodes, num_logical_experts // num_nodes]\n        # This requires carefully gathering weights.\n        # The `mlog2log` mapping needs to be constructed correctly.\n\n        # `mlog2log_map` assigns each logical expert to a conceptual \"middle\" logical expert.\n        # e.g., if num_logical_experts=8, num_groups=2, group_size=4.\n        # Group 0 contains logical experts 0,1,2,3. Group 1 contains 4,5,6,7.\n        # If num_nodes=2, groups_per_node=1.\n        # Group 0 (log experts 0-3) goes to node 0. Group 1 (log experts 4-7) goes to node 1.\n        # Node 0 has 1 middle expert. Node 1 has 1 middle expert.\n        # Middle expert for node 0 is logical experts 0-3. Middle expert for node 1 is 4-7.\n        # The mapping `mlog2log_map` should map:\n        # 0,1,2,3 -> 0 (on node 0)\n        # 4,5,6,7 -> 0 (on node 1)\n        # The current `mlog2log` calculation seems to be constructing the indices of the middle experts.\n        # `log2mlog` is `[num_layers, num_logical_experts]`.\n        # It maps logical expert ID to its index within the middle expert group.\n        # The `mlog2log` is the inverse of this index mapping.\n\n        # Let's reconstruct `mlog2log` and `tokens_per_mlog` more directly.\n        # For each layer, we have `num_groups` of `group_size`.\n        # `group_pack_index`: [num_layers, num_groups] - node assignment for each group.\n        # `group_rank_in_pack`: [num_layers, num_groups] - index of group within the node's assigned groups.\n\n        # We want to create `num_layers * num_nodes` \"node-level\" expert groups.\n        # Each node-level group corresponds to a set of original logical experts.\n        # The `tokens_per_mlog` should be `[num_layers * num_nodes, num_logical_experts // num_nodes]`.\n\n        # Calculate the logical expert index for each \"middle\" expert.\n        # `mlog_expert_indices`: [num_layers, num_nodes, groups_per_node, group_size]\n        # This is becoming complex. Let's simplify the conceptualization.\n\n        # Simpler approach:\n        # 1. Pack groups to nodes. This determines which logical experts are grouped together on each node.\n        # `group_pack_index`: [num_layers, num_groups] -> node ID for each group.\n        # `group_rank_in_pack`: [num_layers, num_groups] -> rank within node for each group.\n\n        # 2. For each node, create a combined weight tensor for the logical experts assigned to it.\n        #    The number of logical experts per node will be `groups_per_node * group_size`.\n        #    We need to gather the weights of logical experts based on `group_pack_index`.\n\n        # Let's create a mapping from (layer, node) to a contiguous block of logical experts.\n        # `node_logical_expert_map`: [num_layers, num_nodes, groups_per_node * group_size]\n\n        # This requires constructing a permutation for each layer and node.\n        # The original `mlog2log` calculation is:\n        # `((group_pack_index * groups_per_node + group_rank_in_pack) * group_size)`\n        # This forms a base index for each logical expert.\n        # `log2mlog` is this base index + `arange(group_size)`.\n        # `mlog2log` is the inverse of `log2mlog`.\n\n        # Let's directly construct the `tokens_per_mlog` and `mlog2log`.\n        # `tokens_per_mlog` is the weight tensor for experts *within* each node, considering the grouped logical experts.\n        # The number of such \"middle\" experts per node is `groups_per_node * group_size`.\n\n        # For each layer:\n        # `group_pack_index` tells us which node each group belongs to.\n        # `group_rank_in_pack` tells us the order within the node.\n\n        # Let's create `num_layers * num_nodes` tensors, each of size `groups_per_node * group_size`.\n        # `mlog_weights`: [num_layers * num_nodes, groups_per_node * group_size]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, groups_per_node * group_size]\n\n        # Example: num_layers=1, num_logical_experts=8, num_groups=2, group_size=4, num_nodes=2, groups_per_node=1, num_gpus=2, phy_experts_per_gpu=1.\n        # weight: [1, 8]\n        # tokens_per_group_layer: [1, 2] (sum of weights for group 0 and group 1)\n        # balanced_packing(tokens_per_group_layer, num_nodes=2) ->\n        #   group_pack_index: [1, 2] -> [[0, 1]] (group 0 to node 0, group 1 to node 1)\n        #   group_rank_in_pack: [1, 2] -> [[0, 0]] (rank 0 within node for both groups)\n        # groups_per_node = 1, group_size = 4.\n\n        # Middle expert for node 0 is logical experts 0,1,2,3.\n        # Middle expert for node 1 is logical experts 4,5,6,7.\n        # `mlog_to_orig_log_map` for node 0 should be [0,1,2,3].\n        # `mlog_to_orig_log_map` for node 1 should be [4,5,6,7].\n\n        # The original `mlog2log` calculation:\n        # `base_idx = (group_pack_index * groups_per_node + group_rank_in_pack) * group_size`\n        # `base_idx`: [1, 2] -> [[0*1+0, 1*1+0]] * 4 = [[0, 4]]\n        # `log2mlog`: `base_idx.unsqueeze(-1) + torch.arange(group_size)`\n        # `log2mlog`: [[0, 4]].unsqueeze(-1) + [0,1,2,3] -> [[0,1,2,3], [4,5,6,7]]\n        # This `log2mlog` is `[num_layers, num_logical_experts]`. It maps original logical expert ID to its \"middle\" expert index.\n        # The original logic uses `weight.gather(-1, mlog2log)` which is incorrect here.\n        # `weight.gather(-1, mlog2log)` expects `mlog2log` to be `[num_layers, num_middle_experts]` where `num_middle_experts` is the number of columns in `weight`.\n        # The shape should be `[num_layers, num_logical_experts]` if we are gathering from `weight` which is `[num_layers, num_logical_experts]`.\n        # The `mlog2log` calculated here has shape `[num_layers, num_logical_experts]`.\n        # It maps `original_logical_expert_id` to `middle_expert_id`.\n        # We need to gather `weight[layer, original_logical_expert_id]` and group them by `middle_expert_id`.\n\n        # Let's create `mlog_weights` directly.\n        # `mlog_weights`: [num_layers * num_nodes, groups_per_node * group_size]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, groups_per_node * group_size]\n\n        # Vectorized construction of mlog_weights and mlog_to_orig_log_map\n        # 1. Create a combined key for sorting groups within each layer based on node assignment and rank.\n        # This will order the groups such that all groups for node 0 come first, then node 1, etc.,\n        # and within each node, groups are ordered by their rank_in_pack.\n        combined_group_key = group_pack_index * groups_per_node + group_rank_in_pack # [num_layers, num_groups]\n\n        # 2. Get the sorted indices for groups for each layer.\n        sorted_group_indices = combined_group_key.argsort(dim=-1) # [num_layers, num_groups]\n\n        # 3. Create a base tensor for original logical expert IDs within each group.\n        # This tensor represents the original logical expert IDs if groups were laid out sequentially.\n        # Shape: [num_groups, group_size]\n        base_orig_log_expert_ids_per_group = (torch.arange(num_groups, device=weight.device, dtype=torch.int64) * group_size).unsqueeze(-1) + \\\n                                             torch.arange(group_size, device=weight.device, dtype=torch.int64)\n\n        # 4. Use sorted_group_indices to gather original logical expert IDs and their weights.\n        # Expand sorted_group_indices to match the shape of the data being gathered (groups, then experts within groups).\n        sorted_group_indices_expanded = sorted_group_indices.unsqueeze(-1).expand(-1, -1, group_size)\n\n        # Reshape the original weight tensor to [num_layers, num_groups, group_size] for gathering.\n        weight_reshaped_by_group = weight.view(num_layers, num_groups, group_size)\n\n        # Gather the weights based on the sorted group order.\n        # mlog_weights will have shape [num_layers, num_groups, group_size]\n        mlog_weights = weight_reshaped_by_group.gather(1, sorted_group_indices_expanded)\n\n        # Gather the original logical expert IDs based on the sorted group order.\n        # mlog_to_orig_log_map will have shape [num_layers, num_groups, group_size]\n        mlog_to_orig_log_map = base_orig_log_expert_ids_per_group.unsqueeze(0).expand(num_layers, -1, -1).gather(1, sorted_group_indices_expanded)\n\n        # 5. Reshape to [num_layers, num_nodes, groups_per_node * group_size]\n        # This aligns the data such that experts assigned to the same node are contiguous.\n        num_middle_experts_per_node = groups_per_node * group_size\n        mlog_weights = mlog_weights.view(num_layers, num_nodes, num_middle_experts_per_node)\n        mlog_to_orig_log_map = mlog_to_orig_log_map.view(num_layers, num_nodes, num_middle_experts_per_node)\n\n        # Now, `mlog_weights`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n        # And `mlog_to_orig_log_map`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n\n        # Step 2: Replicate experts within each node.\n        # `mlog_weights` are the weights for experts within each node.\n        # `num_physical_experts // num_nodes` is the target number of physical experts per node.\n        # `replicate_experts` takes `[X, num_log]` and returns `phy2log`, `rank`, `logcnt`.\n        # Here, X = num_layers * num_nodes, num_log = num_logical_experts // num_nodes.\n        # So, we need to reshape `mlog_weights` and `mlog_to_orig_log_map`.\n\n        num_middle_experts = num_logical_experts // num_nodes\n        reshaped_mlog_weights = mlog_weights.view(-1, num_middle_experts)\n        reshaped_mlog_to_orig_log_map = mlog_to_orig_log_map.view(-1, num_middle_experts)\n\n        # `phy2mlog`: [num_layers * num_nodes, num_physical_experts // num_nodes]\n        # `phyrank`: [num_layers * num_nodes, num_physical_experts // num_nodes]\n        # `mlogcnt`: [num_layers * num_nodes, num_middle_experts]\n        phy2mlog, phyrank, mlogcnt = replicate_experts(\n            reshaped_mlog_weights, num_physical_experts // num_nodes)\n\n        # Step 3: Pack physical experts to GPUs.\n        # `phy2mlog` maps physical experts (within each node) to middle logical experts.\n        # `phy2mlog` is `[num_layers * num_nodes, num_phy_per_node]`\n        # `phyrank` is `[num_layers * num_nodes, num_phy_per_node]`\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`\n\n        # We need to map these back to global physical expert indices.\n        # The `phy2mlog` refers to the *middle* logical experts.\n        # The `replicate_experts` function returns `phy2log` which is the *original* logical expert ID.\n        # So, `phy2mlog` from `replicate_experts` is actually `[num_layers*num_nodes, num_phy_per_node]`,\n        # where each entry is an index into the *middle* logical experts.\n\n        # We need to map these `phy2mlog` indices back to original logical expert IDs.\n        # `reshaped_mlog_to_orig_log_map`: [num_layers * num_nodes, num_middle_experts]\n        # `final_phy2log_per_node`: [num_layers * num_nodes, num_phy_per_node]\n        final_phy2log_per_node = reshaped_mlog_to_orig_log_map.gather(-1, phy2mlog)\n\n        # `phyrank` is the replica rank for the middle experts.\n        # We need to combine this with `mlogcnt` to get the final `logcnt`.\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`. It tells us how many replicas each middle expert has.\n        # The `phyrank` is the rank of the replica for a *specific* middle expert.\n\n        # The `logcnt` returned by `replicate_experts` is `[X, num_log]`, which is `[num_layers*num_nodes, num_middle_experts]`.\n        # We need to map this back to `[num_layers, num_logical_experts]`.\n\n        # Let's use the `mlogcnt` directly to get the total count of replicas for each logical expert.\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`.\n        # We need to sum/average counts based on the `mlog_to_orig_log_map`.\n\n        # Let's refine the output mapping:\n        # `physical_to_logical_map`: [num_layers, num_replicas]\n        # `logical_to_physical_map`: [num_layers, num_logical_experts, X]\n        # `expert_count`: [num_layers, num_logical_experts]\n\n        # `final_phy2log_per_node`: [num_layers * num_nodes, num_phy_per_node]\n        # This is the logical expert ID for each physical expert *within its node*.\n        # We need to convert this to global physical expert IDs.\n        # Physical experts are grouped by GPU. `num_gpus` total, `num_gpus // num_nodes` per node.\n        # `phy_experts_per_gpu` = `num_physical_experts // num_gpus`.\n        # So, each GPU has `phy_experts_per_gpu` physical experts.\n        # Each Node has `num_gpus // num_nodes` GPUs.\n        # Total physical experts per node = `(num_gpus // num_nodes) * phy_experts_per_gpu`\n        # This should equal `num_physical_experts // num_nodes`.\n\n        # `final_phy2log_per_node`: [num_layers, num_nodes, num_phy_per_node]\n        # `final_rank_per_node`: [num_layers, num_nodes, num_phy_per_node] (this is `phyrank`)\n\n        # We need to flatten this to `[num_layers, num_physical_experts]`.\n        # `num_physical_experts = num_nodes * num_phy_per_node`.\n\n        # `final_phy2log_global`: [num_layers, num_physical_experts]\n        # `final_rank_global`: [num_layers, num_physical_experts]\n\n        # The `mlog_to_orig_log_map` correctly maps middle experts to original logical experts.\n        # `phy2mlog` maps physical experts (within a node) to middle experts.\n        # So, `phy2mlog.gather(-1, final_phy2log_per_node)` gives the logical expert ID.\n        # This is what `final_phy2log_per_node` already represents.\n\n        # Let's rename `final_phy2log_per_node` to `phy2log_per_node` for clarity.\n        phy2log_per_node = final_phy2log_per_node\n        rank_per_node = phyrank # [num_layers * num_nodes, num_phy_per_node]\n\n        # Flatten these to get global mappings for physical experts.\n        phy2log_global = phy2log_per_node.view(num_layers, -1)\n        rank_global = rank_per_node.view(num_layers, -1)\n\n        # Now, let's construct the `logcnt` (expert_count).\n        # `mlogcnt`: [num_layers * num_nodes, num_middle_experts]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, num_middle_experts]\n\n        # We need to aggregate `mlogcnt` based on `mlog_to_orig_log_map`.\n        # For each logical expert, its total count is the sum of counts of its constituent middle experts.\n        # This aggregation needs to be done carefully.\n\n        # Let's create a tensor for `logcnt`: [num_layers, num_logical_experts] initialized to zeros.\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n\n        # Iterate through each layer and node.\n        for i in range(num_layers):\n            for node_idx in range(num_nodes):\n                # For the middle experts belonging to this node and layer:\n                middle_expert_indices = mlog_to_orig_log_map[i, node_idx] # [num_middle_experts]\n                counts_for_middle_experts = mlogcnt[i * num_nodes + node_idx] # [num_middle_experts]\n\n                # Add these counts to the corresponding original logical experts.\n                # `torch.add.at` allows for sparse updates.\n                expert_count[i].scatter_add_(0, middle_expert_indices, counts_for_middle_experts)\n\n        # The `logcnt` from `replicate_experts` is the number of replicas for the middle experts.\n        # We need to make sure the `expert_count` is correct.\n        # The `replicate_experts` function returns `logcnt` which is the number of replicas for each logical expert it processed.\n        # So, `mlogcnt` is the number of replicas for each middle expert.\n\n        # The final `expert_count` should be `[num_layers, num_logical_experts]`.\n        # We need to map `mlogcnt` back.\n\n        # Let's create `expert_count` of size `[num_layers, num_logical_experts]` initialized to 0.\n        # Then, for each layer, node, and middle expert:\n        # `expert_count[layer, mlog_to_orig_log_map[layer, node, middle_idx]] += mlogcnt[layer*num_nodes + node, middle_idx]`\n\n        # This aggregation is correct.\n\n        # Now, we need to construct `logical_to_physical_map`.\n        # `logical_to_physical_map`: [num_layers, num_logical_experts, X] where X is max replicas.\n        # `phy2log_global`: [num_layers, num_physical_experts]\n        # `rank_global`: [num_layers, num_physical_experts]\n\n        # We need to group physical experts by their logical expert ID and rank.\n        max_replicas_per_expert = (num_physical_experts + num_logical_experts - 1) // num_logical_experts # Ceiling division\n\n        logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert), -1, dtype=torch.int64, device=weight.device)\n\n        # Iterate through all physical experts\n        for layer_idx in range(num_layers):\n            for phys_idx in range(num_physical_experts):\n                log_expert_id = phy2log_global[layer_idx, phys_idx]\n                replica_rank = rank_global[layer_idx, phys_idx]\n\n                if log_expert_id >= 0 and log_expert_id < num_logical_experts:\n                    # Place the physical expert ID into the correct slot based on its logical ID and rank.\n                    # We need to ensure we don't exceed `max_replicas_per_expert`.\n                    # The `replica_rank` should correspond to the index in the third dimension.\n                    if replica_rank < max_replicas_per_expert:\n                        logical_to_physical_map[layer_idx, log_expert_id, replica_rank] = phys_idx\n                    else:\n                        # This case indicates an issue or that max_replicas_per_expert needs to be larger.\n                        # For now, we can assert or log a warning.\n                        pass # Or handle error\n\n        # The `expert_count` is already computed.\n\n        return phy2log_global, logical_to_physical_map, expert_count\n\n    else:\n        # Global balancing strategy: treat all experts as one group, balance across num_gpus.\n        # This is equivalent to calling the hierarchical function with num_nodes=1, num_groups=1.\n        # num_groups = 1 means group_size = num_logical_experts.\n        # num_nodes = 1 means groups_per_node = 1.\n        # The entire logic simplifies.\n\n        # Step 1: Pack logical experts to nodes (here, nodes = GPUs conceptually).\n        # `tokens_per_group` becomes `weight` itself since num_groups=1.\n        # `balanced_packing(weight, num_gpus)`\n        # `group_pack_index`: [num_layers, num_logical_experts] -> GPU assignment for each logical expert.\n        # `group_rank_in_pack`: [num_layers, num_logical_experts] -> rank within GPU for each logical expert.\n        # `num_nodes` becomes `num_gpus`. `groups_per_node` becomes `num_logical_experts // num_gpus`.\n\n        # We need to adapt `balanced_packing` to work with `num_packs = num_gpus`.\n        # The `groups_per_pack` in `balanced_packing` refers to items per pack.\n        # Here, items are logical experts. So, `groups_per_pack` = `num_logical_experts // num_gpus`.\n        # `num_packs` = `num_gpus`.\n\n        pack_index_gpu, rank_in_gpu = balanced_packing(weight, num_gpus)\n\n        # `pack_index_gpu`: [num_layers, num_logical_experts] -> GPU ID for each logical expert.\n        # `rank_in_gpu`: [num_layers, num_logical_experts] -> rank within GPU for each logical expert.\n\n        # Step 2: Replicate experts within GPUs.\n        # We need to create tensors for `replicate_experts` that represent experts *per GPU*.\n        # `num_logical_experts_per_gpu` = `num_logical_experts // num_gpus`.\n        # `num_physical_experts_per_gpu` = `num_physical_experts // num_gpus`.\n\n        # For each GPU, we have a set of logical experts.\n        # We need to gather weights for logical experts assigned to the same GPU.\n        # `gpu_logical_weights`: [num_gpus, num_logical_experts_per_gpu, num_layers] (transposed for replicate_experts)\n        # `gpu_logical_to_orig_map`: [num_gpus, num_logical_experts_per_gpu]\n\n        gpu_logical_weights_list = []\n        gpu_logical_to_orig_map_list = []\n\n        num_logical_experts_per_gpu = num_logical_experts // num_gpus\n\n        for gpu_idx in range(num_gpus):\n            # Find logical experts assigned to this GPU\n            assigned_logical_mask = (pack_index_gpu == gpu_idx)\n            # Get their ranks within the GPU\n            ranks_on_gpu = rank_in_gpu[assigned_logical_mask]\n            # Sort by rank\n            sorted_ranks_on_gpu, sort_indices_on_gpu = ranks_on_gpu.sort()\n            # Get original logical expert indices\n            assigned_logical_indices = torch.arange(num_logical_experts, device=pack_index_gpu.device)[assigned_logical_mask][sort_indices_on_gpu]\n\n            if len(assigned_logical_indices) > 0:\n                # Gather weights for these logical experts\n                # We need weights for each layer.\n                # `weight` is [num_layers, num_logical_experts].\n                # `assigned_logical_indices` is [num_logical_experts_per_gpu].\n                # We want `gpu_logical_weights` of shape [num_layers, num_logical_experts_per_gpu].\n                gpu_logical_weights = weight[:, assigned_logical_indices]\n                gpu_logical_weights_list.append(gpu_logical_weights)\n                gpu_logical_to_orig_map_list.append(assigned_logical_indices)\n            else:\n                # If a GPU gets no logical experts (shouldn't happen if num_logical_experts >= num_gpus)\n                gpu_logical_weights_list.append(torch.empty(num_layers, 0, dtype=weight.dtype, device=weight.device))\n                gpu_logical_to_orig_map_list.append(torch.empty(0, dtype=torch.int64, device=weight.device))\n\n        # `gpu_logical_weights`: [num_gpus, num_layers, num_logical_experts_per_gpu]\n        # `gpu_logical_to_orig_map`: [num_gpus, num_logical_experts_per_gpu]\n        gpu_logical_weights = torch.stack(gpu_logical_weights_list)\n        gpu_logical_to_orig_map = torch.stack(gpu_logical_to_orig_map_list)\n\n        # Reshape for `replicate_experts`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        reshaped_gpu_logical_weights = gpu_logical_weights.view(-1, num_logical_experts_per_gpu)\n        reshaped_gpu_logical_to_orig_map = gpu_logical_to_orig_map.view(-1, num_logical_experts_per_gpu)\n\n        # `replicate_experts` is called with `num_phy = num_physical_experts // num_gpus` (target physical experts per GPU).\n        # `X` becomes `num_gpus * num_layers`. `num_log` becomes `num_logical_experts_per_gpu`.\n        # `phy2log_gpu`: [num_gpus * num_layers, num_phy_per_gpu]\n        # `phyrank_gpu`: [num_gpus * num_layers, num_phy_per_gpu]\n        # `gpu_logcnt`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        phy2log_gpu, phyrank_gpu, gpu_logcnt = replicate_experts(\n            reshaped_gpu_logical_weights, num_physical_experts // num_gpus)\n\n        # Map `phy2log_gpu` back to original logical expert IDs using `reshaped_gpu_logical_to_orig_map`.\n        # `phy2log_global`: [num_gpus * num_layers, num_phy_per_gpu]\n        phy2log_global = reshaped_gpu_logical_to_orig_map.gather(-1, phy2log_gpu)\n\n        # Flatten to get the final `physical_to_logical_map` and `rank`.\n        # The `rank_global` is `phyrank_gpu`.\n        rank_global = phyrank_gpu\n\n        # Construct `expert_count` ([num_layers, num_logical_experts]).\n        # `gpu_logcnt`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n\n        for gpu_layer_idx in range(num_gpus * num_layers):\n            layer_idx = gpu_layer_idx // num_gpus\n            # Get the original logical expert indices for this gpu_layer combination\n            orig_log_indices = reshaped_gpu_logical_to_orig_map[gpu_layer_idx] # [num_logical_experts_per_gpu]\n            counts_for_orig_logs = gpu_logcnt[gpu_layer_idx] # [num_logical_experts_per_gpu]\n\n            # Add these counts to the correct positions in the expert_count tensor\n            expert_count[layer_idx].scatter_add_(0, orig_log_indices, counts_for_orig_logs)\n\n        # Construct `logical_to_physical_map` ([num_layers, num_logical_experts, X])\n        max_replicas_per_expert = (num_physical_experts + num_logical_experts - 1) // num_logical_experts\n\n        logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert), -1, dtype=torch.int64, device=weight.device)\n\n        # Need to map `phy2log_global` and `rank_global` to the correct positions.\n        # `phy2log_global` are indices into the *original* logical experts.\n        # `rank_global` is the replica rank for that original logical expert.\n\n        # Reshape to get mappings per layer and then iterate through physical experts.\n        phy2log_global_reshaped = phy2log_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous()\n        rank_global_reshaped = rank_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous()\n\n        # Iterate through all physical experts\n        for layer_idx in range(num_layers):\n            for gpu_idx in range(num_gpus):\n                for phys_idx_in_gpu in range(num_physical_experts // num_gpus):\n                    # Get the global physical expert index\n                    global_phys_idx = gpu_idx * (num_physical_experts // num_gpus) + phys_idx_in_gpu\n\n                    log_expert_id = phy2log_global_reshaped[layer_idx, gpu_idx, phys_idx_in_gpu]\n                    replica_rank = rank_global_reshaped[layer_idx, gpu_idx, phys_idx_in_gpu]\n\n                    if log_expert_id >= 0 and log_expert_id < num_logical_experts:\n                        if replica_rank < max_replicas_per_expert:\n                            logical_to_physical_map[layer_idx, log_expert_id, replica_rank] = global_phys_idx\n                        else:\n                            pass # Handle error or log\n\n        # The `phy2log_global` needs to be reshaped to `[num_layers, num_replicas]`\n        # where `num_replicas` is `num_physical_experts`.\n        # `phy2log_global` is currently `[num_gpus * num_layers, num_phy_per_gpu]`.\n        # We need to combine these.\n        # The order of physical experts should be consistent.\n        # Let's reconstruct `phy2log_global` to be `[num_layers, num_physical_experts]`.\n\n        final_phy2log_global = torch.empty((num_layers, num_physical_experts), dtype=torch.int64, device=weight.device)\n        current_phys_idx = 0\n        for gpu_idx in range(num_gpus):\n            for layer_idx in range(num_layers):\n                # Calculate the range of physical experts for this GPU and layer combination\n                start_phys_idx = layer_idx * num_gpus * (num_physical_experts // num_gpus) + gpu_idx * (num_physical_experts // num_gpus)\n                end_phys_idx = start_phys_idx + (num_physical_experts // num_gpus)\n\n                # The `phy2log_global` calculated earlier is `[num_gpus * num_layers, num_phy_per_gpu]`\n                # We need to map it correctly.\n                # `phy2log_gpu` was `[num_gpus * num_layers, num_phy_per_gpu]`\n                # `reshaped_gpu_logical_to_orig_map` was `[num_gpus * num_layers, num_phy_per_gpu]`\n                # `phy2log_global = reshaped_gpu_logical_to_orig_map.gather(-1, phy2log_gpu)`\n\n                # The indices for `phy2log_global` should be:\n                # For layer 0, GPU 0: phys experts 0 to num_phy_per_gpu - 1\n                # For layer 0, GPU 1: phys experts num_phy_per_gpu to 2*num_phy_per_gpu - 1\n                # ...\n                # For layer 1, GPU 0: phys experts num_gpus * num_phy_per_gpu to (num_gpus+1)*num_phy_per_gpu - 1\n\n                # Let's re-index the `phy2log_global` and `rank_global` to match `[num_layers, num_physical_experts]`\n                # The current `phy2log_global` is effectively `[num_layers * num_gpus, num_phy_per_gpu]`\n                # We need to rearrange it.\n\n                # Example: num_layers=2, num_gpus=2, num_phy_per_gpu=2. num_physical_experts=4.\n                # phy2log_global:\n                # [layer0_gpu0_phys0, layer0_gpu0_phys1,\n                #  layer0_gpu1_phys0, layer0_gpu1_phys1,\n                #  layer1_gpu0_phys0, layer1_gpu0_phys1,\n                #  layer1_gpu1_phys0, layer1_gpu1_phys1]\n                # Reshaped to [num_gpus*num_layers, num_phy_per_gpu] -> [4, 2]\n                # We want [num_layers, num_physical_experts] -> [2, 4]\n\n                # The current `phy2log_global` is `[num_gpus * num_layers, num_phy_per_gpu]`.\n                # Permute and reshape:\n                # `view(num_gpus, num_layers, -1)` gives `[num_gpus, num_layers, num_phy_per_gpu]`\n                # `permute(1, 0, 2)` gives `[num_layers, num_gpus, num_phy_per_gpu]`\n                # `contiguous().view(num_layers, -1)` gives `[num_layers, num_gpus * num_phy_per_gpu]` = `[num_layers, num_physical_experts]`\n\n                final_phy2log_global = phy2log_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous().view(num_layers, num_physical_experts)\n                final_rank_global = rank_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous().view(num_layers, num_physical_experts)\n\n        # Return the results for global balancing.\n        return final_phy2log_global, final_rank_global, expert_count\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    # Keep weights on their original device to avoid unnecessary transfers.\n    # Ensure float type for calculations.\n    weight = weight.float()\n    \n    # num_replicas is equivalent to num_physical_experts used internally\n    num_physical_experts = num_replicas\n\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        # The function now returns phy2log_global, rank_global, expert_count\n        phy2log_global, rank_global, expert_count = rebalance_experts_hierarchical(\n            weight, num_physical_experts, num_groups, num_nodes, num_gpus)\n    # use global load-balance policy\n    # This is equivalent to calling the hierarchical function with num_nodes=1, num_groups=1.\n    # The entire logic simplifies.\n    # We call rebalance_experts_hierarchical with num_groups=1, num_nodes=1 to ensure global balancing.\n    # This will correctly handle the case where num_gpus > num_logical_experts by distributing experts across available GPUs.\n    phy2log_global, rank_global, expert_count = rebalance_experts_hierarchical(\n        weight, num_physical_experts, 1, 1, num_gpus)\n\n    # Construct logical_to_physical_map here, common to both strategies.\n    # Max replicas per expert: This ensures `logical_to_physical_map` is large enough.\n    # It must be at least 1.\n    if num_logical_experts == 0: # Handle edge case to avoid division by zero\n        max_replicas_per_expert = 0\n    else:\n        # Calculate the maximum possible replicas for any single logical expert.\n        # This is a safe upper bound to dimension the output tensor.\n        max_replicas_per_expert = (num_physical_experts + num_logical_experts - 1) // num_logical_experts\n\n    # Initialize the map with -1 (indicating no physical expert assigned)\n    logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert), -1, dtype=torch.int64, device=weight.device)\n\n    # Prepare indices for index_put_ operation.\n    # `phy2log_global` maps physical expert index to logical expert index.\n    # `rank_global` is the replica rank for that physical expert.\n    # We need to gather the physical expert indices themselves.\n    # `global_physical_indices` represents the index of each physical expert.\n    global_physical_indices = torch.arange(num_replicas, dtype=torch.int64, device=weight.device).unsqueeze(0).expand(num_layers, -1)\n\n    # Create a mask for valid placements to avoid out-of-bounds writes.\n    # This mask selects entries where the logical expert index and rank are within the bounds\n    # of the `logical_to_physical_map` dimensions.\n    valid_mask = (phy2log_global >= 0) & \\\n                 (phy2log_global < num_logical_experts) & \\\n                 (rank_global >= 0) & \\\n                 (rank_global < max_replicas_per_expert)\n\n    # Flatten and filter indices and values using the valid_mask for `index_put_`.\n    # `layer_indices` will be a 1D tensor of layer indices for valid assignments.\n    layer_indices = torch.arange(num_layers, device=weight.device).unsqueeze(1).expand(-1, num_replicas)[valid_mask]\n    # `log_expert_indices` are the logical expert IDs for valid assignments.\n    log_expert_indices = phy2log_global[valid_mask]\n    # `rank_indices` are the replica ranks for valid assignments.\n    rank_indices = rank_global[valid_mask]\n    # `values_to_put` are the actual global physical expert indices for valid assignments.\n    values_to_put = global_physical_indices[valid_mask]\n\n    # Use index_put_ for efficient, scattered assignment.\n    # This directly places the `values_to_put` (physical expert indices) into the\n    # `logical_to_physical_map` at the positions specified by `layer_indices`,\n    # `log_expert_indices`, and `rank_indices`.\n    if values_to_put.numel() > 0: # Avoid calling index_put_ with empty tensors\n        logical_to_physical_map.index_put_((layer_indices, log_expert_indices, rank_indices), values_to_put)\n\n    # `phy2log_global` is the mapping from physical expert index to logical expert index.\n    # `rank_global` is the replica rank for each physical expert.\n    # `expert_count` is the number of physical replicas for each logical expert.\n    return phy2log_global, rank_global, expert_count\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["The current program has a critical bug that prevents it from running and achieving any performance scores. The error message \"The size of tensor a (288) must match the size of tensor b (2) at non-singleton dimension 2\" indicates a shape mismatch during tensor operations.\n\nUpon analysis, the root causes are:\n\n1.  **Incorrect `if/else` branching in `rebalance_experts`**: The `rebalance_experts` function unconditionally calls `rebalance_experts_hierarchical` with `num_groups=1, num_nodes=1` after potentially calling it with the original parameters. This means the global balancing strategy is *always* applied, overwriting any results from the hierarchical strategy. This must be fixed with a proper `else` block.\n\n2.  **Flawed global balancing strategy in `rebalance_experts_hierarchical` for `num_logical_experts < num_gpus`**: The `else` branch of `rebalance_experts_hierarchical` (intended for global balancing) attempts to use `balanced_packing` to distribute logical experts among GPUs. However, `balanced_packing` asserts that `num_groups % num_packs == 0`. In the global strategy, `num_groups` effectively becomes `num_logical_experts`, and `num_packs` becomes `num_gpus`. If `num_logical_experts` is not divisible by `num_gpus` (e.g., `2 % 288 != 0`), this assertion would fail. Even if the assertion were removed, `groups_per_pack` would become 0, leading to logical errors. More critically, when `num_logical_experts < num_gpus`, `num_logical_experts // num_gpus` becomes 0. This results in `replicate_experts` being called with `num_log=0` (the second dimension of its `weight` input is 0). Inside `replicate_experts`, operations like `(weight / logcnt).max(dim=-1).indices` on a tensor with a zero-sized last dimension cause the \"tensor with no elements\" runtime error, which manifests as the reported shape mismatch.\n\n**Proposed Improvements:**\n\n1.  **Fix `if/else` logic in `rebalance_experts`**: Ensure only one branch of the rebalancing logic is executed based on the `num_groups % num_nodes == 0` condition.\n2.  **Simplify and correct global balancing in `rebalance_experts_hierarchical`**: For the global balancing scenario (when `num_groups % num_nodes != 0`), the simplest and most robust approach is to directly call `replicate_experts` on the original `weight` tensor to scale `num_logical_experts` to `num_physical_experts`. `replicate_experts` is designed for this kind of replication and inherently handles the \"spreading\" of experts. This avoids the problematic `balanced_packing` call and the subsequent `num_log=0` issue in `replicate_experts`. This change will fix the runtime error and allow the program to function correctly.\n\nThese changes primarily focus on correcting the functional errors. The `balanced_packing` function, with its Python loops, remains a potential bottleneck for `speed_score` if `num_groups` is very large in the hierarchical case, but fixing the immediate error is paramount.\n\n**Detailed Diff:**\n\n<<<<<<< SEARCH\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        # The function now returns phy2log_global, rank_global, expert_count\n        phy2log_global, rank_global, expert_count = rebalance_experts_hierarchical(\n            weight, num_physical_experts, num_groups, num_nodes, num_gpus)\n    # use global load-balance policy\n    # This is equivalent to calling the hierarchical function with num_nodes=1, num_groups=1.\n    # The entire logic simplifies.\n    # We call rebalance_experts_hierarchical with num_groups=1, num_nodes=1 to ensure global balancing.\n    # This will correctly handle the case where num_gpus > num_logical_experts by distributing experts across available GPUs.\n    phy2log_global, rank_global, expert_count = rebalance_experts_hierarchical(\n        weight, num_physical_experts, 1, 1, num_gpus)\n=======\n    if num_groups % num_nodes == 0:\n        # Use hierarchical load-balance policy\n        phy2log_global, rank_global, expert_count = rebalance_experts_hierarchical(\n            weight, num_physical_experts, num_groups, num_nodes, num_gpus)\n    else:\n        # Use global load-balance policy\n        # This is equivalent to calling the hierarchical function with num_nodes=1, num_groups=1.\n        # The rebalance_experts_hierarchical function's 'else' branch is designed for this.\n        phy2log_global, rank_global, expert_count = rebalance_experts_hierarchical(\n            weight, num_physical_experts, 1, 1, num_gpus)\n>>>>>>> REPLACE\n<<<<<<< SEARCH\n    else:\n        # Global balancing strategy: treat all experts as one group, balance across num_gpus.\n        # This is equivalent to calling the hierarchical function with num_nodes=1, num_groups=1.\n        # num_groups = 1 means group_size = num_logical_experts.\n        # num_nodes = 1 means groups_per_node = 1.\n        # The entire logic simplifies.\n\n        # Step 1: Pack logical experts to nodes (here, nodes = GPUs conceptually).\n        # `tokens_per_group` becomes `weight` itself since num_groups=1.\n        # `balanced_packing(weight, num_gpus)`\n        # `group_pack_index`: [num_layers, num_logical_experts] -> GPU assignment for each logical expert.\n        # `rank_in_gpu`: [num_layers, num_logical_experts] -> rank within GPU for each logical expert.\n        # `num_nodes` becomes `num_gpus`. `groups_per_node` becomes `num_logical_experts // num_gpus`.\n\n        pack_index_gpu, rank_in_gpu = balanced_packing(weight, num_gpus)\n\n        # Step 2: Replicate experts within GPUs.\n        # We need to create tensors for `replicate_experts` that represent experts *per GPU*.\n        # `num_logical_experts_per_gpu` = `num_logical_experts // num_gpus`.\n        # `num_physical_experts_per_gpu` = `num_physical_experts // num_gpus`.\n\n        gpu_logical_weights_list = []\n        gpu_logical_to_orig_map_list = []\n\n        num_logical_experts_per_gpu = num_logical_experts // num_gpus\n\n        for gpu_idx in range(num_gpus):\n            # Find logical experts assigned to this GPU\n            assigned_logical_mask = (pack_index_gpu == gpu_idx)\n            # Get their ranks within the GPU\n            ranks_on_gpu = rank_in_gpu[assigned_logical_mask]\n            # Sort by rank\n            sorted_ranks_on_gpu, sort_indices_on_gpu = ranks_on_gpu.sort()\n            # Get original logical expert indices\n            assigned_logical_indices = torch.arange(num_logical_experts, device=pack_index_gpu.device)[assigned_logical_mask][sort_indices_on_gpu]\n\n            if len(assigned_logical_indices) > 0:\n                # Gather weights for these logical experts\n                # We need weights for each layer.\n                # `weight` is [num_layers, num_logical_experts].\n                # `assigned_logical_indices` is [num_logical_experts_per_gpu].\n                # We want `gpu_logical_weights` of shape [num_layers, num_logical_experts_per_gpu].\n                gpu_logical_weights = weight[:, assigned_logical_indices]\n                gpu_logical_weights_list.append(gpu_logical_weights)\n                gpu_logical_to_orig_map_list.append(assigned_logical_indices)\n            else:\n                # If a GPU gets no logical experts (shouldn't happen if num_logical_experts >= num_gpus)\n                gpu_logical_weights_list.append(torch.empty(num_layers, 0, dtype=weight.dtype, device=weight.device))\n                gpu_logical_to_orig_map_list.append(torch.empty(0, dtype=torch.int64, device=weight.device))\n\n        # `gpu_logical_weights`: [num_gpus, num_layers, num_logical_experts_per_gpu]\n        # `gpu_logical_to_orig_map`: [num_gpus, num_logical_experts_per_gpu]\n        gpu_logical_weights = torch.stack(gpu_logical_weights_list)\n        gpu_logical_to_orig_map = torch.stack(gpu_logical_to_orig_map_list)\n\n        # Reshape for `replicate_experts`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        reshaped_gpu_logical_weights = gpu_logical_weights.view(-1, num_logical_experts_per_gpu)\n        reshaped_gpu_logical_to_orig_map = gpu_logical_to_orig_map.view(-1, num_logical_experts_per_gpu)\n\n        # `replicate_experts` is called with `num_phy = num_physical_experts // num_gpus` (target physical experts per GPU).\n        # `X` becomes `num_gpus * num_layers`. `num_log` becomes `num_logical_experts_per_gpu`.\n        # `phy2log_gpu`: [num_gpus * num_layers, num_phy_per_gpu]\n        # `phyrank_gpu`: [num_gpus * num_layers, num_phy_per_gpu]\n        # `gpu_logcnt`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        phy2log_gpu, phyrank_gpu, gpu_logcnt = replicate_experts(\n            reshaped_gpu_logical_weights, num_physical_experts // num_gpus)\n\n        # Map `phy2log_gpu` back to original logical expert IDs using `reshaped_gpu_logical_to_orig_map`.\n        # `phy2log_global`: [num_gpus * num_layers, num_phy_per_gpu]\n        phy2log_global = reshaped_gpu_logical_to_orig_map.gather(-1, phy2log_gpu)\n\n        # Flatten to get the final `physical_to_logical_map` and `rank`.\n        # The `rank_global` is `phyrank_gpu`.\n        rank_global = phyrank_gpu\n\n        # Construct `expert_count` ([num_layers, num_logical_experts]).\n        # `gpu_logcnt`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n\n        for gpu_layer_idx in range(num_gpus * num_layers):\n            layer_idx = gpu_layer_idx // num_gpus\n            # Get the original logical expert indices for this gpu_layer combination\n            orig_log_indices = reshaped_gpu_logical_to_orig_map[gpu_layer_idx] # [num_logical_experts_per_gpu]\n            counts_for_orig_logs = gpu_logcnt[gpu_layer_idx] # [num_logical_experts_per_gpu]\n\n            # Add these counts to the correct positions in the expert_count tensor\n            expert_count[layer_idx].scatter_add_(0, orig_log_indices, counts_for_orig_logs)\n\n        # Construct `logical_to_physical_map` ([num_layers, num_logical_experts, X])\n        max_replicas_per_expert = (num_physical_experts + num_logical_experts - 1) // num_logical_experts\n\n        logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert), -1, dtype=torch.int64, device=weight.device)\n\n        # Need to map `phy2log_global` and `rank_global` to the correct positions.\n        # `phy2log_global` are indices into the *original* logical experts.\n        # `rank_global` is the replica rank for that original logical expert.\n\n        # Reshape to get mappings per layer and then iterate through physical experts.\n        phy2log_global_reshaped = phy2log_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous()\n        rank_global_reshaped = rank_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous()\n\n        # Iterate through all physical experts\n        for layer_idx in range(num_layers):\n            for gpu_idx in range(num_gpus):\n                for phys_idx_in_gpu in range(num_physical_experts // num_gpus):\n                    # Get the global physical expert index\n                    global_phys_idx = gpu_idx * (num_physical_experts // num_gpus) + phys_idx_in_gpu\n\n                    log_expert_id = phy2log_global_reshaped[layer_idx, gpu_idx, phys_idx_in_gpu]\n                    replica_rank = rank_global_reshaped[layer_idx, gpu_idx, phys_idx_in_gpu]\n\n                    if log_expert_id >= 0 and log_expert_id < num_logical_experts:\n                        if replica_rank < max_replicas_per_expert:\n                            logical_to_physical_map[layer_idx, log_expert_id, replica_rank] = global_phys_idx\n                        else:\n                            pass # Handle error or log\n\n        # The `phy2log_global` needs to be reshaped to `[num_layers, num_replicas]`\n        # where `num_replicas` is `num_physical_experts`.\n        # `phy2log_global` is currently `[num_gpus * num_layers, num_phy_per_gpu]`.\n        # We need to combine these.\n        # The order of physical experts should be consistent.\n        # Let's reconstruct `phy2log_global` to be `[num_layers, num_physical_experts]`.\n\n        final_phy2log_global = torch.empty((num_layers, num_physical_experts), dtype=torch.int64, device=weight.device)\n        current_phys_idx = 0\n        for gpu_idx in range(num_gpus):\n            for layer_idx in range(num_layers):\n                # Calculate the range of physical experts for this GPU and layer combination\n                start_phys_idx = layer_idx * num_gpus * (num_physical_experts // num_gpus) + gpu_idx * (num_physical_experts // num_gpus)\n                end_phys_idx = start_phys_idx + (num_physical_experts // num_gpus)\n\n                # The `phy2log_global` calculated earlier is `[num_gpus * num_layers, num_phy_per_gpu]`\n                # We need to map it correctly.\n                # `phy2log_gpu` was `[num_gpus * num_layers, num_phy_per_gpu]`\n                # `reshaped_gpu_logical_to_orig_map` was `[num_gpus * num_layers, num_phy_per_gpu]`\n                # `phy2log_global = reshaped_gpu_logical_to_orig_map.gather(-1, phy2log_gpu)`\n\n                # The indices for `phy2log_global` should be:\n                # For layer 0, GPU 0: phys experts 0 to num_phy_per_gpu - 1\n                # For layer 0, GPU 1: phys experts num_phy_per_gpu to 2*num_phy_per_gpu - 1\n                # ...\n                # For layer 1, GPU 0: phys experts num_gpus * num_phy_per_gpu to (num_gpus+1)*num_phy_per_gpu - 1\n\n                # Let's re-index the `phy2log_global` and `rank_global` to match `[num_layers, num_physical_experts]`\n                # The current `phy2log_global` is effectively `[num_layers * num_gpus, num_phy_per_gpu]`\n                # We need to rearrange it.\n\n                # Example: num_layers=2, num_gpus=2, num_phy_per_gpu=2. num_physical_experts=4.\n                # phy2log_global:\n                # [layer0_gpu0_phys0, layer0_gpu0_phys1,\n                #  layer0_gpu1_phys0, layer0_gpu1_phys1,\n                #  layer1_gpu0_phys0, layer1_gpu0_phys1,\n                #  layer1_gpu1_phys0, layer1_gpu1_phys1]\n                # Reshaped to [num_gpus*num_layers, num_phy_per_gpu] -> [4, 2]\n                # We want [num_layers, num_physical_experts] -> [2, 4]\n\n                # The current `phy2log_global` is `[num_gpus * num_layers, num_phy_per_gpu]`.\n                # Permute and reshape:\n                # `view(num_gpus, num_layers, -1)` gives `[num_gpus, num_layers, num_phy_per_gpu]`\n                # `permute(1, 0, 2)` gives `[num_layers, num_gpus, num_phy_per_gpu]`\n                # `contiguous().view(num_layers, -1)` gives `[num_layers, num_gpus * num_phy_per_gpu]` = `[num_layers, num_physical_experts]`\n\n                final_phy2log_global = phy2log_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous().view(num_layers, num_physical_experts)\n                final_rank_global = rank_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous().view(num_layers, num_physical_experts)\n\n        # Return the results for global balancing.\n        return final_phy2log_global, final_rank_global, expert_count\n=======\n    else:\n        # Global balancing strategy: When hierarchical grouping is not applicable (num_groups not divisible by num_nodes),\n        # or when a flat distribution is preferred.\n        # In this case, we directly replicate the logical experts to the desired number of physical experts.\n        # The `replicate_experts` function inherently handles the load balancing by creating replicas\n        # for the most loaded logical experts. The output is already in the global format.\n        phy2log_global, rank_global, expert_count = replicate_experts(weight, num_physical_experts)\n        \n        # The outputs from replicate_experts are directly the desired global mappings:\n        # phy2log_global: [num_layers, num_physical_experts] (physical_to_logical_map)\n        # rank_global: [num_layers, num_physical_experts] (replica rank for each physical expert)\n        # expert_count: [num_layers, num_logical_experts] (number of replicas for each logical expert)\n        return phy2log_global, rank_global, expert_count\n>>>>>>> REPLACE"]}}}