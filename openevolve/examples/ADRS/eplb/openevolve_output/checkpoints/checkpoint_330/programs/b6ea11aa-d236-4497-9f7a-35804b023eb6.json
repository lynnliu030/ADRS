{"id": "b6ea11aa-d236-4497-9f7a-35804b023eb6", "code": "# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Optimized balanced_packing using torch.min and broadcasting\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(num_groups,\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(num_layers, num_groups)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort weights and get indices\n    sorted_weights, sorted_indices = weight.float().sort(-1, descending=True)\n\n    # All computations on the same device as weight to avoid transfers.\n    # Sorted Round Robin assignment for better efficiency.\n    # sorted_weights: [num_layers, num_groups]\n    # sorted_indices: [num_layers, num_groups] (indices of original weights in sorted order)\n    sorted_weights, sorted_indices = weight.float().sort(-1, descending=True)\n\n    # Create pack assignments for the sorted items.\n    # For each layer, assign items 0, 1, ..., num_groups-1 (in sorted order)\n    # to packs 0, 1, ..., num_packs-1, then repeat.\n    # pack_assignment_for_sorted_items: [num_layers, num_groups]\n    pack_assignment_for_sorted_items = torch.arange(num_groups, device=weight.device) % num_packs\n    pack_assignment_for_sorted_items = pack_assignment_for_sorted_items.unsqueeze(0).expand(num_layers, -1)\n\n    # Calculate rank within each pack for the sorted items.\n    # rank_in_pack_for_sorted_items: [num_layers, num_groups]\n    rank_in_pack_for_sorted_items = torch.arange(num_groups, device=weight.device) // num_packs\n    rank_in_pack_for_sorted_items = rank_in_pack_for_sorted_items.unsqueeze(0).expand(num_layers, -1)\n\n    # Map these assignments back to the original (unsorted) indices using scatter_.\n    pack_index = torch.empty_like(sorted_indices)\n    rank_in_pack = torch.empty_like(sorted_indices)\n\n    pack_index.scatter_(1, sorted_indices, pack_assignment_for_sorted_items)\n    rank_in_pack.scatter_(1, sorted_indices, rank_in_pack_for_sorted_items)\n\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Unified rebalancing logic:\n    # The hierarchical approach is used when num_groups is divisible by num_nodes.\n    # Otherwise, it falls back to a global balancing strategy by setting num_nodes=1 and num_groups=1.\n    # We can simplify this by directly implementing the global strategy when needed,\n    # or by ensuring the hierarchical logic correctly handles num_nodes=1.\n\n    # Determine the effective number of groups and nodes for balancing.\n    # If num_groups is not divisible by num_nodes, we treat it as a single group\n    # across all nodes for the first level of balancing.\n    effective_num_groups = num_groups if num_groups % num_nodes == 0 else 1\n    effective_num_nodes = num_nodes if num_groups % num_nodes == 0 else 1\n\n    # If num_groups % num_nodes != 0, we effectively want to balance across all groups as if they were one large group.\n    # And then balance those groups onto nodes.\n    # This is equivalent to setting num_nodes = 1 and num_groups = 1 in the hierarchical logic.\n    # Let's adjust parameters for a unified call to a simplified hierarchical logic.\n\n    # Simplified approach:\n    # First, balance logical experts across groups within layers.\n    # Then, balance these groups across nodes.\n    # Finally, replicate experts within nodes to balance across GPUs.\n\n    # Step 1: Balance logical experts into groups.\n    # If num_groups == 1, this step is trivial and doesn't change anything.\n    group_size = num_logical_experts // num_groups\n    tokens_per_group_layer = weight.unflatten(-1, (num_groups, group_size)).sum(-1) # [num_layers, num_groups]\n\n    # Pack groups onto nodes.\n    # If num_groups % num_nodes != 0, we effectively want to balance all groups onto a single 'node' conceptually,\n    # and then distribute those across the actual num_nodes.\n    # Let's consider the case where num_groups is not divisible by num_nodes.\n    # The original code uses num_nodes for packing groups. If num_groups < num_nodes, this packing might be problematic.\n    # A more robust approach is to balance groups onto num_nodes, and if num_groups < num_nodes, some nodes get 0 groups.\n    # The current `balanced_packing` assumes num_groups is divisible by num_packs (num_nodes here).\n\n    # Let's redefine the packing strategy:\n    # We want to pack 'num_groups' items (groups) into 'num_nodes' packs.\n    # If num_groups is not divisible by num_nodes, balanced_packing needs to handle it.\n    # The current `balanced_packing` asserts `num_groups % num_packs == 0`.\n    # This means if num_groups % num_nodes != 0, the original code calls `rebalance_experts_hierarchical` with num_nodes=1, num_groups=1.\n    # This is a global balancing strategy.\n\n    # Let's ensure the function call correctly reflects the intended strategy.\n    # If num_groups % num_nodes == 0, use hierarchical.\n    # Otherwise, use global (which is hierarchical with num_nodes=1, num_groups=1).\n\n    if num_groups % num_nodes == 0:\n        # Hierarchical balancing: balance groups to nodes, then nodes to GPUs.\n        # Step 1: Pack groups to nodes.\n        group_pack_index, group_rank_in_pack = balanced_packing(tokens_per_group_layer, num_nodes)\n        # Map logical experts to \"middle\" logical experts (logical experts per node)\n        # log2mlog: [num_layers, num_logical_experts] -> maps logical expert to its middle logical expert\n        mlog2log_map = (((group_pack_index * groups_per_node + group_rank_in_pack) * group_size).unsqueeze(-1) +\n                        torch.arange(group_size, dtype=torch.int64, device=group_pack_index.device)).flatten(-2)\n        # mlog2log_map: [num_layers, num_logical_experts] maps each logical expert to its \"middle\" logical expert ID.\n        # We need the inverse mapping for `weight.gather` later.\n        # This inverse mapping is tricky because multiple logical experts can map to the same middle expert.\n        # The `inverse` function is designed for permutations, which isn't directly applicable here.\n\n        # Let's rethink the mapping and replication.\n        # We have `num_layers` of `weight` ([num_layers, num_logical_experts]).\n        # First, we group logical experts: `tokens_per_group_layer` ([num_layers, num_groups]).\n        # Then, we pack these groups onto `num_nodes`: `group_pack_index` ([num_layers, num_groups]) and `group_rank_in_pack` ([num_layers, num_groups]).\n        # This means `num_groups` are assigned to `num_nodes`. `groups_per_node` groups per node.\n        # So, we have `num_layers * num_nodes` \"node-level\" expert weight tensors.\n        # Each node-level tensor has `num_logical_experts // num_nodes` \"middle\" experts.\n\n        # Let's create the \"middle\" logical experts' weights.\n        # `mlog_weights`: [num_layers * num_nodes, num_logical_experts // num_nodes]\n        # This requires carefully gathering weights.\n        # The `mlog2log` mapping needs to be constructed correctly.\n\n        # `mlog2log_map` assigns each logical expert to a conceptual \"middle\" logical expert.\n        # e.g., if num_logical_experts=8, num_groups=2, group_size=4.\n        # Group 0 contains logical experts 0,1,2,3. Group 1 contains 4,5,6,7.\n        # If num_nodes=2, groups_per_node=1.\n        # Group 0 (log experts 0-3) goes to node 0. Group 1 (log experts 4-7) goes to node 1.\n        # Node 0 has 1 middle expert. Node 1 has 1 middle expert.\n        # Middle expert for node 0 is logical experts 0-3. Middle expert for node 1 is 4-7.\n        # The mapping `mlog2log_map` should map:\n        # 0,1,2,3 -> 0 (on node 0)\n        # 4,5,6,7 -> 0 (on node 1)\n        # The current `mlog2log` calculation seems to be constructing the indices of the middle experts.\n        # `log2mlog` is `[num_layers, num_logical_experts]`.\n        # It maps logical expert ID to its index within the middle expert group.\n        # The `mlog2log` is the inverse of this index mapping.\n\n        # Let's reconstruct `mlog2log` and `tokens_per_mlog` more directly.\n        # For each layer, we have `num_groups` of `group_size`.\n        # `group_pack_index`: [num_layers, num_groups] - node assignment for each group.\n        # `group_rank_in_pack`: [num_layers, num_groups] - index of group within the node's assigned groups.\n\n        # We want to create `num_layers * num_nodes` \"node-level\" expert groups.\n        # Each node-level group corresponds to a set of original logical experts.\n        # The `tokens_per_mlog` should be `[num_layers * num_nodes, num_logical_experts // num_nodes]`.\n\n        # Calculate the logical expert index for each \"middle\" expert.\n        # `mlog_expert_indices`: [num_layers, num_nodes, groups_per_node, group_size]\n        # This is becoming complex. Let's simplify the conceptualization.\n\n        # Simpler approach:\n        # 1. Pack groups to nodes. This determines which logical experts are grouped together on each node.\n        # `group_pack_index`: [num_layers, num_groups] -> node ID for each group.\n        # `group_rank_in_pack`: [num_layers, num_groups] -> rank within node for each group.\n\n        # 2. For each node, create a combined weight tensor for the logical experts assigned to it.\n        #    The number of logical experts per node will be `groups_per_node * group_size`.\n        #    We need to gather the weights of logical experts based on `group_pack_index`.\n\n        # Let's create a mapping from (layer, node) to a contiguous block of logical experts.\n        # `node_logical_expert_map`: [num_layers, num_nodes, groups_per_node * group_size]\n\n        # This requires constructing a permutation for each layer and node.\n        # The original `mlog2log` calculation is:\n        # `((group_pack_index * groups_per_node + group_rank_in_pack) * group_size)`\n        # This forms a base index for each logical expert.\n        # `log2mlog` is this base index + `arange(group_size)`.\n        # `mlog2log` is the inverse of `log2mlog`.\n\n        # Let's directly construct the `tokens_per_mlog` and `mlog2log`.\n        # `tokens_per_mlog` is the weight tensor for experts *within* each node, considering the grouped logical experts.\n        # The number of such \"middle\" experts per node is `groups_per_node * group_size`.\n\n        # For each layer:\n        # `group_pack_index` tells us which node each group belongs to.\n        # `group_rank_in_pack` tells us the order within the node.\n\n        # Let's create `num_layers * num_nodes` tensors, each of size `groups_per_node * group_size`.\n        # `mlog_weights`: [num_layers * num_nodes, groups_per_node * group_size]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, groups_per_node * group_size]\n\n        # Example: num_layers=1, num_logical_experts=8, num_groups=2, group_size=4, num_nodes=2, groups_per_node=1, num_gpus=2, phy_experts_per_gpu=1.\n        # weight: [1, 8]\n        # tokens_per_group_layer: [1, 2] (sum of weights for group 0 and group 1)\n        # balanced_packing(tokens_per_group_layer, num_nodes=2) ->\n        #   group_pack_index: [1, 2] -> [[0, 1]] (group 0 to node 0, group 1 to node 1)\n        #   group_rank_in_pack: [1, 2] -> [[0, 0]] (rank 0 within node for both groups)\n        # groups_per_node = 1, group_size = 4.\n\n        # Middle expert for node 0 is logical experts 0,1,2,3.\n        # Middle expert for node 1 is logical experts 4,5,6,7.\n        # `mlog_to_orig_log_map` for node 0 should be [0,1,2,3].\n        # `mlog_to_orig_log_map` for node 1 should be [4,5,6,7].\n\n        # The original `mlog2log` calculation:\n        # `base_idx = (group_pack_index * groups_per_node + group_rank_in_pack) * group_size`\n        # `base_idx`: [1, 2] -> [[0*1+0, 1*1+0]] * 4 = [[0, 4]]\n        # `log2mlog`: `base_idx.unsqueeze(-1) + torch.arange(group_size)`\n        # `log2mlog`: [[0, 4]].unsqueeze(-1) + [0,1,2,3] -> [[0,1,2,3], [4,5,6,7]]\n        # This `log2mlog` is `[num_layers, num_logical_experts]`. It maps original logical expert ID to its \"middle\" expert index.\n        # The original logic uses `weight.gather(-1, mlog2log)` which is incorrect here.\n        # `weight.gather(-1, mlog2log)` expects `mlog2log` to be `[num_layers, num_middle_experts]` where `num_middle_experts` is the number of columns in `weight`.\n        # The shape should be `[num_layers, num_logical_experts]` if we are gathering from `weight` which is `[num_layers, num_logical_experts]`.\n        # The `mlog2log` calculated here has shape `[num_layers, num_logical_experts]`.\n        # It maps `original_logical_expert_id` to `middle_expert_id`.\n        # We need to gather `weight[layer, original_logical_expert_id]` and group them by `middle_expert_id`.\n\n        # Let's create `mlog_weights` directly.\n        # `mlog_weights`: [num_layers * num_nodes, groups_per_node * group_size]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, groups_per_node * group_size]\n\n        # Vectorized construction of mlog_weights and mlog_to_orig_log_map\n        # This replaces the nested Python loops for better performance.\n\n        # Calculate the new \"middle expert group\" index for each original group.\n        # This new index combines node assignment and rank within the node.\n        # Shape: [num_layers, num_groups]\n        new_group_idx = group_pack_index * groups_per_node + group_rank_in_pack\n\n        # Create a tensor that maps new group index back to original group index.\n        # This is a permutation tensor for groups within each layer.\n        # Shape: [num_layers, num_nodes * groups_per_node] (which is num_groups)\n        group_permutation = torch.empty((num_layers, num_groups),\n                                        dtype=torch.int64,\n                                        device=weight.device)\n        group_permutation.scatter_(1, new_group_idx,\n                                   torch.arange(num_groups, device=weight.device).expand(num_layers, -1))\n\n        # Reshape original weights to [num_layers, num_groups, group_size]\n        weight_reshaped = weight.view(num_layers, num_groups, group_size)\n\n        # Gather weights based on the group_permutation\n        # The .gather() operation reorders the groups according to the permutation.\n        # Shape: [num_layers, num_groups, group_size]\n        mlog_weights_ordered = weight_reshaped.gather(1, group_permutation.unsqueeze(-1).expand(-1, -1, group_size))\n        \n        # Reshape to [num_layers, num_nodes, groups_per_node * group_size]\n        # This is the final mlog_weights.\n        mlog_weights = mlog_weights_ordered.view(num_layers, num_nodes, groups_per_node * group_size)\n\n        # Construct mlog_to_orig_log_map similarly\n        # Original logical expert IDs, reshaped to [num_groups, group_size]\n        orig_log_ids_reshaped = torch.arange(num_logical_experts,\n                                             dtype=torch.int64,\n                                             device=weight.device).view(num_groups, group_size)\n        \n        # Expand to [num_layers, num_groups, group_size] and gather\n        mlog_to_orig_log_map_ordered = orig_log_ids_reshaped.unsqueeze(0).expand(num_layers, -1, -1).gather(\n            1, group_permutation.unsqueeze(-1).expand(-1, -1, group_size))\n        \n        # Reshape to [num_layers, num_nodes, groups_per_node * group_size]\n        mlog_to_orig_log_map = mlog_to_orig_log_map_ordered.view(\n            num_layers, num_nodes, groups_per_node * group_size)\n\n        # Now, `mlog_weights`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n        # And `mlog_to_orig_log_map`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n\n        # Step 2: Replicate experts within each node.\n        # `mlog_weights` are the weights for experts within each node.\n        # `num_physical_experts // num_nodes` is the target number of physical experts per node.\n        # `replicate_experts` takes `[X, num_log]` and returns `phy2log`, `rank`, `logcnt`.\n        # Here, X = num_layers * num_nodes, num_log = num_logical_experts // num_nodes.\n        # So, we need to reshape `mlog_weights` and `mlog_to_orig_log_map`.\n\n        num_middle_experts = num_logical_experts // num_nodes\n        reshaped_mlog_weights = mlog_weights.view(-1, num_middle_experts)\n        reshaped_mlog_to_orig_log_map = mlog_to_orig_log_map.view(-1, num_middle_experts)\n\n        # `phy2mlog`: [num_layers * num_nodes, num_physical_experts // num_nodes]\n        # `phyrank`: [num_layers * num_nodes, num_physical_experts // num_nodes]\n        # `mlogcnt`: [num_layers * num_nodes, num_middle_experts]\n        phy2mlog, phyrank, mlogcnt = replicate_experts(\n            reshaped_mlog_weights, num_physical_experts // num_nodes)\n\n        # Step 3: Pack physical experts to GPUs.\n        # `phy2mlog` maps physical experts (within each node) to middle logical experts.\n        # `phy2mlog` is `[num_layers * num_nodes, num_phy_per_node]`\n        # `phyrank` is `[num_layers * num_nodes, num_phy_per_node]`\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`\n\n        # We need to map these back to global physical expert indices.\n        # The `phy2mlog` refers to the *middle* logical experts.\n        # The `replicate_experts` function returns `phy2log` which is the *original* logical expert ID.\n        # So, `phy2mlog` from `replicate_experts` is actually `[num_layers*num_nodes, num_phy_per_node]`,\n        # where each entry is an index into the *middle* logical experts.\n\n        # We need to map these `phy2mlog` indices back to original logical expert IDs.\n        # `reshaped_mlog_to_orig_log_map`: [num_layers * num_nodes, num_middle_experts]\n        # `final_phy2log_per_node`: [num_layers * num_nodes, num_phy_per_node]\n        final_phy2log_per_node = reshaped_mlog_to_orig_log_map.gather(-1, phy2mlog)\n\n        # `phyrank` is the replica rank for the middle experts.\n        # We need to combine this with `mlogcnt` to get the final `logcnt`.\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`. It tells us how many replicas each middle expert has.\n        # The `phyrank` is the rank of the replica for a *specific* middle expert.\n\n        # The `logcnt` returned by `replicate_experts` is `[X, num_log]`, which is `[num_layers*num_nodes, num_middle_experts]`.\n        # We need to map this back to `[num_layers, num_logical_experts]`.\n\n        # Let's use the `mlogcnt` directly to get the total count of replicas for each logical expert.\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`.\n        # We need to sum/average counts based on the `mlog_to_orig_log_map`.\n\n        # Let's refine the output mapping:\n        # `physical_to_logical_map`: [num_layers, num_replicas]\n        # `logical_to_physical_map`: [num_layers, num_logical_experts, X]\n        # `expert_count`: [num_layers, num_logical_experts]\n\n        # `final_phy2log_per_node`: [num_layers * num_nodes, num_phy_per_node]\n        # This is the logical expert ID for each physical expert *within its node*.\n        # We need to convert this to global physical expert IDs.\n        # Physical experts are grouped by GPU. `num_gpus` total, `num_gpus // num_nodes` per node.\n        # `phy_experts_per_gpu` = `num_physical_experts // num_gpus`.\n        # So, each GPU has `phy_experts_per_gpu` physical experts.\n        # Each Node has `num_gpus // num_nodes` GPUs.\n        # Total physical experts per node = `(num_gpus // num_nodes) * phy_experts_per_gpu`\n        # This should equal `num_physical_experts // num_nodes`.\n\n        # `final_phy2log_per_node`: [num_layers, num_nodes, num_phy_per_node]\n        # `final_rank_per_node`: [num_layers, num_nodes, num_phy_per_node] (this is `phyrank`)\n\n        # We need to flatten this to `[num_layers, num_physical_experts]`.\n        # `num_physical_experts = num_nodes * num_phy_per_node`.\n\n        # `final_phy2log_global`: [num_layers, num_physical_experts]\n        # `final_rank_global`: [num_layers, num_physical_experts]\n\n        # The `mlog_to_orig_log_map` correctly maps middle experts to original logical experts.\n        # `phy2mlog` maps physical experts (within a node) to middle experts.\n        # So, `phy2mlog.gather(-1, final_phy2log_per_node)` gives the logical expert ID.\n        # This is what `final_phy2log_per_node` already represents.\n\n        # Let's rename `final_phy2log_per_node` to `phy2log_per_node` for clarity.\n        phy2log_per_node = final_phy2log_per_node\n        rank_per_node = phyrank # [num_layers * num_nodes, num_phy_per_node]\n\n        # Flatten these to get global mappings for physical experts.\n        phy2log_global = phy2log_per_node.view(num_layers, -1)\n        rank_global = rank_per_node.view(num_layers, -1)\n\n        # Now, let's construct the `logcnt` (expert_count).\n        # `mlogcnt`: [num_layers * num_nodes, num_middle_experts]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, num_middle_experts]\n\n        # We need to aggregate `mlogcnt` based on `mlog_to_orig_log_map`.\n        # For each logical expert, its total count is the sum of counts of its constituent middle experts.\n        # This aggregation needs to be done carefully.\n\n        # Let's create a tensor for `logcnt`: [num_layers, num_logical_experts] initialized to zeros.\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n\n        # Vectorized aggregation of expert_count.\n        # mlogcnt: [num_layers * num_nodes, num_middle_experts]\n        # mlog_to_orig_log_map: [num_layers, num_nodes, num_middle_experts]\n\n        # Flatten mlog_to_orig_log_map and mlogcnt for single index_add_ operation\n        flat_mlog_to_orig_log_map = mlog_to_orig_log_map.view(-1, num_middle_experts).flatten()\n        flat_mlogcnt = mlogcnt.flatten()\n\n        # Create corresponding layer indices for each flattened entry\n        # The mlog_to_orig_log_map is structured as [layer, node, middle_expert]\n        # so layer indices repeat for each node block, then increment for the next layer.\n        layer_indices_expanded = torch.arange(num_layers, device=weight.device).unsqueeze(1).unsqueeze(2).expand(\n            num_layers, num_nodes, num_middle_experts).flatten()\n\n        # Calculate flat target indices for expert_count in the flattened expert_count tensor\n        flat_expert_count_indices = layer_indices_expanded * num_logical_experts + flat_mlog_to_orig_log_map\n\n        # Initialize expert_count as a flattened tensor\n        expert_count = torch.zeros(num_layers * num_logical_experts, dtype=torch.int64, device=weight.device)\n\n        # Perform scattered add to aggregate counts\n        expert_count.index_add_(0, flat_expert_count_indices, flat_mlogcnt)\n        \n        # Reshape back to [num_layers, num_logical_experts]\n        expert_count = expert_count.view(num_layers, num_logical_experts)\n\n        return phy2log_global, rank_global, expert_count\n\n    else:\n        # Global balancing strategy: treat all experts as one group, balance across num_gpus.\n        # This is equivalent to calling the hierarchical function with num_nodes=1, num_groups=1.\n        # num_groups = 1 means group_size = num_logical_experts.\n        # num_nodes = 1 means groups_per_node = 1.\n        # The entire logic simplifies.\n\n        # Step 1: Pack logical experts to nodes (here, nodes = GPUs conceptually).\n        # `tokens_per_group` becomes `weight` itself since num_groups=1.\n        # `balanced_packing(weight, num_gpus)`\n        # `group_pack_index`: [num_layers, num_logical_experts] -> GPU assignment for each logical expert.\n        # `group_rank_in_pack`: [num_layers, num_logical_experts] -> rank within GPU for each logical expert.\n        # `num_nodes` becomes `num_gpus`. `groups_per_node` becomes `num_logical_experts // num_gpus`.\n\n        # We need to adapt `balanced_packing` to work with `num_packs = num_gpus`.\n        # The `groups_per_pack` in `balanced_packing` refers to items per pack.\n        # Here, items are logical experts. So, `groups_per_pack` = `num_logical_experts // num_gpus`.\n        # `num_packs` = `num_gpus`.\n\n        pack_index_gpu, rank_in_gpu = balanced_packing(weight, num_gpus)\n\n        # `pack_index_gpu`: [num_layers, num_logical_experts] -> GPU ID for each logical expert.\n        # `rank_in_gpu`: [num_layers, num_logical_experts] -> rank within GPU for each logical expert.\n\n        # Step 2: Replicate experts within GPUs.\n        # We need to create tensors for `replicate_experts` that represent experts *per GPU*.\n        # `num_logical_experts_per_gpu` = `num_logical_experts // num_gpus`.\n        # `num_physical_experts_per_gpu` = `num_physical_experts // num_gpus`.\n\n        # For each GPU, we have a set of logical experts.\n        # We need to gather weights for logical experts assigned to the same GPU.\n        # `gpu_logical_weights`: [num_gpus, num_logical_experts_per_gpu, num_layers] (transposed for replicate_experts)\n        # `gpu_logical_to_orig_map`: [num_gpus, num_logical_experts_per_gpu]\n\n        # Vectorized construction of gpu_logical_weights and gpu_logical_to_orig_map\n        # This replaces the nested Python loops for better performance.\n\n        num_logical_experts_per_gpu = num_logical_experts // num_gpus\n\n        # Calculate the new \"middle expert group\" index for each original logical expert.\n        # This new index combines GPU assignment and rank within the GPU.\n        # Shape: [num_layers, num_logical_experts]\n        new_logical_expert_idx_on_gpu = pack_index_gpu * num_logical_experts_per_gpu + rank_in_gpu\n\n        # Create a tensor that maps new logical expert index back to original logical expert index.\n        # This is a permutation tensor for logical experts within each layer.\n        # Shape: [num_layers, num_gpus * num_logical_experts_per_gpu] (which is num_logical_experts)\n        logical_expert_permutation = torch.empty((num_layers, num_logical_experts),\n                                                 dtype=torch.int64,\n                                                 device=weight.device)\n        logical_expert_permutation.scatter_(1, new_logical_expert_idx_on_gpu,\n                                            torch.arange(num_logical_experts, device=weight.device).expand(num_layers, -1))\n\n        # Gather weights based on the logical_expert_permutation\n        # Shape: [num_layers, num_logical_experts]\n        gpu_logical_weights_ordered = weight.gather(1, logical_expert_permutation)\n        \n        # Reshape to [num_gpus, num_layers, num_logical_experts_per_gpu] for subsequent processing\n        # and ensure contiguous memory layout.\n        gpu_logical_weights = gpu_logical_weights_ordered.view(\n            num_layers, num_gpus, num_logical_experts_per_gpu).permute(1, 0, 2).contiguous()\n\n        # Construct gpu_logical_to_orig_map similarly\n        # Original logical expert IDs, expanded to [num_layers, num_logical_experts]\n        orig_log_ids_expanded = torch.arange(num_logical_experts,\n                                             dtype=torch.int64,\n                                             device=weight.device).unsqueeze(0).expand(num_layers, -1)\n        \n        # Gather original logical expert IDs based on the permutation\n        gpu_logical_to_orig_map_ordered = orig_log_ids_expanded.gather(1, logical_expert_permutation)\n        \n        # Reshape to [num_gpus * num_layers, num_logical_experts_per_gpu] for replicate_experts input\n        gpu_logical_to_orig_map = gpu_logical_to_orig_map_ordered.view(\n            num_layers, num_gpus, num_logical_experts_per_gpu).permute(1, 0, 2).contiguous().view(\n                -1, num_logical_experts_per_gpu)\n\n        # Reshape for `replicate_experts`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        reshaped_gpu_logical_weights = gpu_logical_weights.view(-1, num_logical_experts_per_gpu)\n        reshaped_gpu_logical_to_orig_map = gpu_logical_to_orig_map.view(-1, num_logical_experts_per_gpu)\n\n        # `replicate_experts` is called with `num_phy = num_physical_experts // num_gpus` (target physical experts per GPU).\n        # `X` becomes `num_gpus * num_layers`. `num_log` becomes `num_logical_experts_per_gpu`.\n        # `phy2log_gpu`: [num_gpus * num_layers, num_phy_per_gpu]\n        # `phyrank_gpu`: [num_gpus * num_layers, num_phy_per_gpu]\n        # `gpu_logcnt`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        phy2log_gpu, phyrank_gpu, gpu_logcnt = replicate_experts(\n            reshaped_gpu_logical_weights, num_physical_experts // num_gpus)\n\n        # Map `phy2log_gpu` back to original logical expert IDs using `reshaped_gpu_logical_to_orig_map`.\n        # `phy2log_global`: [num_gpus * num_layers, num_phy_per_gpu]\n        phy2log_global = reshaped_gpu_logical_to_orig_map.gather(-1, phy2log_gpu)\n\n        # Flatten to get the final `physical_to_logical_map` and `rank`.\n        # The `rank_global` is `phyrank_gpu`.\n        rank_global = phyrank_gpu\n\n        # Construct `expert_count` ([num_layers, num_logical_experts]).\n        # `gpu_logcnt`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n\n        # Vectorized aggregation of expert_count.\n        # gpu_logcnt: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        # reshaped_gpu_logical_to_orig_map: [num_gpus * num_layers, num_logical_experts_per_gpu]\n\n        # Flatten reshaped_gpu_logical_to_orig_map and gpu_logcnt for single index_add_ operation\n        flat_gpu_logical_to_orig_map = reshaped_gpu_logical_to_orig_map.flatten()\n        flat_gpu_logcnt = gpu_logcnt.flatten()\n\n        # Create corresponding layer indices for each flattened entry\n        # The reshaped_gpu_logical_to_orig_map is structured as [gpu_idx, layer_idx, expert_in_gpu] conceptually\n        # after the permute, so layer indices cycle through 0, 1, ..., num_layers-1 for each gpu block.\n        layer_indices_expanded = torch.arange(num_layers, device=weight.device).repeat(num_gpus)\n        layer_indices_expanded = layer_indices_expanded.unsqueeze(1).expand(-1, num_logical_experts_per_gpu).flatten()\n\n        # Calculate flat target indices for expert_count in the flattened expert_count tensor\n        flat_expert_count_indices = layer_indices_expanded * num_logical_experts + flat_gpu_logical_to_orig_map\n\n        # Initialize expert_count as a flattened tensor\n        expert_count = torch.zeros(num_layers * num_logical_experts, dtype=torch.int64, device=weight.device)\n\n        # Perform scattered add to aggregate counts\n        expert_count.index_add_(0, flat_expert_count_indices, flat_gpu_logcnt)\n        \n        # Reshape back to [num_layers, num_logical_experts]\n        expert_count = expert_count.view(num_layers, num_logical_experts)\n\n        # Construct `logical_to_physical_map` ([num_layers, num_logical_experts, X])\n        max_replicas_per_expert = (num_physical_experts + num_logical_experts - 1) // num_logical_experts\n\n        logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert), -1, dtype=torch.int64, device=weight.device)\n\n        # Need to map `phy2log_global` and `rank_global` to the correct positions.\n        # `phy2log_global` are indices into the *original* logical experts.\n        # `rank_global` is the replica rank for that original logical expert.\n\n        # Reshape to get mappings per layer and then iterate through physical experts.\n        phy2log_global_reshaped = phy2log_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous()\n        rank_global_reshaped = rank_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous()\n\n        # Iterate through all physical experts\n        for layer_idx in range(num_layers):\n            for gpu_idx in range(num_gpus):\n                for phys_idx_in_gpu in range(num_physical_experts // num_gpus):\n                    # Get the global physical expert index\n                    global_phys_idx = gpu_idx * (num_physical_experts // num_gpus) + phys_idx_in_gpu\n\n                    log_expert_id = phy2log_global_reshaped[layer_idx, gpu_idx, phys_idx_in_gpu]\n                    replica_rank = rank_global_reshaped[layer_idx, gpu_idx, phys_idx_in_gpu]\n\n                    if log_expert_id >= 0 and log_expert_id < num_logical_experts:\n                        if replica_rank < max_replicas_per_expert:\n                            logical_to_physical_map[layer_idx, log_expert_id, replica_rank] = global_phys_idx\n                        else:\n                            pass # Handle error or log\n\n        # The `phy2log_global` needs to be reshaped to `[num_layers, num_replicas]`\n        # where `num_replicas` is `num_physical_experts`.\n        # `phy2log_global` is currently `[num_gpus * num_layers, num_phy_per_gpu]`.\n        # We need to combine these.\n        # The order of physical experts should be consistent.\n        # Let's reconstruct `phy2log_global` to be `[num_layers, num_physical_experts]`.\n\n        final_phy2log_global = torch.empty((num_layers, num_physical_experts), dtype=torch.int64, device=weight.device)\n        current_phys_idx = 0\n        for gpu_idx in range(num_gpus):\n            for layer_idx in range(num_layers):\n                # Calculate the range of physical experts for this GPU and layer combination\n                start_phys_idx = layer_idx * num_gpus * (num_physical_experts // num_gpus) + gpu_idx * (num_physical_experts // num_gpus)\n                end_phys_idx = start_phys_idx + (num_physical_experts // num_gpus)\n\n                # The `phy2log_global` calculated earlier is `[num_gpus * num_layers, num_phy_per_gpu]`\n                # We need to map it correctly.\n                # `phy2log_gpu` was `[num_gpus * num_layers, num_phy_per_gpu]`\n                # `reshaped_gpu_logical_to_orig_map` was `[num_gpus * num_layers, num_phy_per_gpu]`\n                # `phy2log_global = reshaped_gpu_logical_to_orig_map.gather(-1, phy2log_gpu)`\n\n                # The indices for `phy2log_global` should be:\n                # For layer 0, GPU 0: phys experts 0 to num_phy_per_gpu - 1\n                # For layer 0, GPU 1: phys experts num_phy_per_gpu to 2*num_phy_per_gpu - 1\n                # ...\n                # For layer 1, GPU 0: phys experts num_gpus * num_phy_per_gpu to (num_gpus+1)*num_phy_per_gpu - 1\n\n                # Let's re-index the `phy2log_global` and `rank_global` to match `[num_layers, num_physical_experts]`\n                # The current `phy2log_global` is effectively `[num_layers * num_gpus, num_phy_per_gpu]`\n                # We need to rearrange it.\n\n                # Example: num_layers=2, num_gpus=2, num_phy_per_gpu=2. num_physical_experts=4.\n                # phy2log_global:\n                # [layer0_gpu0_phys0, layer0_gpu0_phys1,\n                #  layer0_gpu1_phys0, layer0_gpu1_phys1,\n                #  layer1_gpu0_phys0, layer1_gpu0_phys1,\n                #  layer1_gpu1_phys0, layer1_gpu1_phys1]\n                # Reshaped to [num_gpus*num_layers, num_phy_per_gpu] -> [4, 2]\n                # We want [num_layers, num_physical_experts] -> [2, 4]\n\n                # The current `phy2log_global` is `[num_gpus * num_layers, num_phy_per_gpu]`.\n                # Permute and reshape:\n                # `view(num_gpus, num_layers, -1)` gives `[num_gpus, num_layers, num_phy_per_gpu]`\n                # `permute(1, 0, 2)` gives `[num_layers, num_gpus, num_phy_per_gpu]`\n                # `contiguous().view(num_layers, -1)` gives `[num_layers, num_gpus * num_phy_per_gpu]` = `[num_layers, num_physical_experts]`\n\n                final_phy2log_global = phy2log_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous().view(num_layers, num_physical_experts)\n                final_rank_global = rank_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous().view(num_layers, num_physical_experts)\n\n        # Return the results for global balancing.\n        return final_phy2log_global, logical_to_physical_map, expert_count\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    # Keep `weight` on its original device (e.g., GPU) for performance.\n    weight = weight.float() \n    \n    # Determine the balancing policy based on divisibility\n    if num_groups % num_nodes == 0:\n        # Use hierarchical load-balance policy\n        phy2log_global, rank_global, expert_count = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # Use global load-balance policy by calling hierarchical with num_groups=1, num_nodes=1.\n        # This effectively treats all logical experts as one large group and balances them across GPUs.\n        phy2log_global, rank_global, expert_count = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n\n    # Compute logical_to_physical_map once, using the global mappings\n    num_physical_experts = num_replicas # Renaming for clarity based on function signature\n\n    # Calculate max_replicas_per_expert.\n    # This needs to be robust to cases where a single expert might receive more replicas\n    # than the average if the load distribution is highly skewed.\n    # The maximum number of replicas any single logical expert can receive is\n    # num_physical_experts - (num_logical_experts - 1), assuming num_physical_experts >= num_logical_experts.\n    if num_physical_experts >= num_logical_experts:\n        # Calculate average/ceiling replicas\n        max_replicas_per_expert_avg = (num_physical_experts + num_logical_experts - 1) // num_logical_experts\n        # Calculate worst-case (one expert gets all redundant replicas)\n        max_replicas_per_expert_worst_case = num_physical_experts - (num_logical_experts - 1)\n        max_replicas_per_expert = max(max_replicas_per_expert_avg, max_replicas_per_expert_worst_case)\n    else:\n        # If num_physical_experts < num_logical_experts, then at most one replica per expert is possible.\n        # Some experts will have 0 replicas. Max rank will be 0.\n        max_replicas_per_expert = 1\n        \n    logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert),\n                                         -1, dtype=torch.int64, device=weight.device)\n\n    physical_expert_ids = torch.arange(num_physical_experts, dtype=torch.int64, device=weight.device)\n    physical_expert_ids_expanded = physical_expert_ids.unsqueeze(0).expand(num_layers, num_physical_experts)\n    layer_indices = torch.arange(num_layers, dtype=torch.int64, device=weight.device).unsqueeze(1)\n    \n    # Use advanced indexing to populate logical_to_physical_map efficiently.\n    # This line is critical and was the source of the \"index out of bounds\" error\n    # if rank_global contained values exceeding max_replicas_per_expert - 1.\n    logical_to_physical_map[layer_indices, phy2log_global, rank_global] = physical_expert_ids_expanded\n\n    return phy2log_global, logical_to_physical_map, expert_count\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n", "language": "python", "parent_id": "795dc9a2-319e-40e3-8289-079dd36bf42d", "generation": 5, "timestamp": 1751061886.69554, "iteration_found": 189, "metrics": {"balancedness_score": 0.2928423407488264, "speed_score": 0.08481846082354137, "combined_score": 0.1888304007861839}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 37 lines with 26 lines\nChange 2: Replace 10 lines with 25 lines\nChange 3: Replace 8 lines with 25 lines", "parent_metrics": {"balancedness_score": 0.29830835850426474, "speed_score": 0.07608280265853447, "combined_score": 0.1871955805813996}, "island": 4}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve the Mixture-of-Expert models Expert Parallelism Load Balancer (MoE EPLB) expert rearrangement algorithm.\nThis algorithm will take the load metrics recorded by the vLLM server, and rearrange the experts to balance the load. It can make replicas of some experts to achieve better load balancing.\nYour goal will be two-fold: 1. Improve the algorithm to achieve better load balancing; while 2. Improve the algorithm to be more efficient, i.e. reduce the execution time of the algorithm itself, since perfect load balancing is NP-hard.\nThe current algorithm is implemented in the `rebalance_experts` function. ", "user": "# Current Program Information\n- Current performance metrics: - balancedness_score: 0.2983\n- speed_score: 0.0761\n- combined_score: 0.1872\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: speed_score, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0857, combined_score: 0.1920\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1097, combined_score: 0.2312\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1103, combined_score: 0.2315\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.2315)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1103), Performs well on combined_score (0.2315)\n\n\n### Program 2 (Score: 0.2312)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1097), Performs well on combined_score (0.2312)\n\n\n### Program 3 (Score: 0.1920)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0857), Performs well on combined_score (0.1920)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.1909)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n\n### Program D2 (Score: 0.1914)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.2315, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 32 lines with 32 lines, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 2 (Score: 0.0939, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 23 lines with 236 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 3 (Score: 0.0939, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 23 lines with 236 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 4 (Score: 0.0309, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace max_log_replicas = num_replicas with 4 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 5 (Score: 0.0000, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 8 lines with 16 lines\nChange 2: Replace 21 lines with 26 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n\n# Current Program\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Optimized balanced_packing using torch.min and broadcasting\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(num_groups,\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(num_layers, num_groups)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort weights and get indices\n    sorted_weights, sorted_indices = weight.float().sort(-1, descending=True)\n\n    # Initialize pack_index and rank_in_pack on CPU for efficiency\n    pack_index = torch.full((num_layers, num_groups), -1, dtype=torch.int64, device=\"cpu\")\n    rank_in_pack = torch.full((num_layers, num_groups), -1, dtype=torch.int64, device=\"cpu\")\n\n    # Use a more efficient way to assign items to packs\n    # Initialize pack_loads and pack_counts\n    pack_loads = torch.zeros(num_layers, num_packs, dtype=torch.float32, device=\"cpu\")\n    pack_counts = torch.zeros(num_layers, num_packs, dtype=torch.int64, device=\"cpu\")\n\n    # Iterate through sorted items and assign them to the least loaded pack\n    for i in range(num_layers):\n        for g in range(num_groups):\n            # Find the pack with the minimum current load and available slots\n            # Use torch.min to find the index of the minimum load\n            # We also need to ensure the pack has capacity (pack_counts < groups_per_pack)\n            available_packs = pack_counts[i] < groups_per_pack\n            if not available_packs.any():\n                # This should ideally not happen if num_groups is divisible by num_packs\n                # and groups_per_pack is at least 1.\n                continue\n\n            # Select from available packs\n            current_pack_loads = pack_loads[i][available_packs]\n            current_pack_indices = torch.arange(num_packs, device=\"cpu\")[available_packs]\n\n            min_load_idx_in_available = torch.argmin(current_pack_loads)\n            chosen_pack_idx = current_pack_indices[min_load_idx_in_available]\n\n            original_group_idx = sorted_indices[i, g]\n\n            pack_index[i, original_group_idx] = chosen_pack_idx\n            rank_in_pack[i, original_group_idx] = pack_counts[i, chosen_pack_idx]\n\n            pack_loads[i, chosen_pack_idx] += weight[i, original_group_idx]\n            pack_counts[i, chosen_pack_idx] += 1\n\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    assert num_redundant >= 0\n    device = weight.device\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    for i in range(num_log, num_phy):\n        redundant_indices = (weight / logcnt).max(dim=-1).indices\n        phy2log[:, i] = redundant_indices\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        logcnt[arangen, redundant_indices] += 1\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Unified rebalancing logic:\n    # The hierarchical approach is used when num_groups is divisible by num_nodes.\n    # Otherwise, it falls back to a global balancing strategy by setting num_nodes=1 and num_groups=1.\n    # We can simplify this by directly implementing the global strategy when needed,\n    # or by ensuring the hierarchical logic correctly handles num_nodes=1.\n\n    # Determine the effective number of groups and nodes for balancing.\n    # If num_groups is not divisible by num_nodes, we treat it as a single group\n    # across all nodes for the first level of balancing.\n    effective_num_groups = num_groups if num_groups % num_nodes == 0 else 1\n    effective_num_nodes = num_nodes if num_groups % num_nodes == 0 else 1\n\n    # If num_groups % num_nodes != 0, we effectively want to balance across all groups as if they were one large group.\n    # And then balance those groups onto nodes.\n    # This is equivalent to setting num_nodes = 1 and num_groups = 1 in the hierarchical logic.\n    # Let's adjust parameters for a unified call to a simplified hierarchical logic.\n\n    # Simplified approach:\n    # First, balance logical experts across groups within layers.\n    # Then, balance these groups across nodes.\n    # Finally, replicate experts within nodes to balance across GPUs.\n\n    # Step 1: Balance logical experts into groups.\n    # If num_groups == 1, this step is trivial and doesn't change anything.\n    group_size = num_logical_experts // num_groups\n    tokens_per_group_layer = weight.unflatten(-1, (num_groups, group_size)).sum(-1) # [num_layers, num_groups]\n\n    # Pack groups onto nodes.\n    # If num_groups % num_nodes != 0, we effectively want to balance all groups onto a single 'node' conceptually,\n    # and then distribute those across the actual num_nodes.\n    # Let's consider the case where num_groups is not divisible by num_nodes.\n    # The original code uses num_nodes for packing groups. If num_groups < num_nodes, this packing might be problematic.\n    # A more robust approach is to balance groups onto num_nodes, and if num_groups < num_nodes, some nodes get 0 groups.\n    # The current `balanced_packing` assumes num_groups is divisible by num_packs (num_nodes here).\n\n    # Let's redefine the packing strategy:\n    # We want to pack 'num_groups' items (groups) into 'num_nodes' packs.\n    # If num_groups is not divisible by num_nodes, balanced_packing needs to handle it.\n    # The current `balanced_packing` asserts `num_groups % num_packs == 0`.\n    # This means if num_groups % num_nodes != 0, the original code calls `rebalance_experts_hierarchical` with num_nodes=1, num_groups=1.\n    # This is a global balancing strategy.\n\n    # Let's ensure the function call correctly reflects the intended strategy.\n    # If num_groups % num_nodes == 0, use hierarchical.\n    # Otherwise, use global (which is hierarchical with num_nodes=1, num_groups=1).\n\n    if num_groups % num_nodes == 0:\n        # Hierarchical balancing: balance groups to nodes, then nodes to GPUs.\n        # Step 1: Pack groups to nodes.\n        group_pack_index, group_rank_in_pack = balanced_packing(tokens_per_group_layer, num_nodes)\n        # Map logical experts to \"middle\" logical experts (logical experts per node)\n        # log2mlog: [num_layers, num_logical_experts] -> maps logical expert to its middle logical expert\n        mlog2log_map = (((group_pack_index * groups_per_node + group_rank_in_pack) * group_size).unsqueeze(-1) +\n                        torch.arange(group_size, dtype=torch.int64, device=group_pack_index.device)).flatten(-2)\n        # mlog2log_map: [num_layers, num_logical_experts] maps each logical expert to its \"middle\" logical expert ID.\n        # We need the inverse mapping for `weight.gather` later.\n        # This inverse mapping is tricky because multiple logical experts can map to the same middle expert.\n        # The `inverse` function is designed for permutations, which isn't directly applicable here.\n\n        # Let's rethink the mapping and replication.\n        # We have `num_layers` of `weight` ([num_layers, num_logical_experts]).\n        # First, we group logical experts: `tokens_per_group_layer` ([num_layers, num_groups]).\n        # Then, we pack these groups onto `num_nodes`: `group_pack_index` ([num_layers, num_groups]) and `group_rank_in_pack` ([num_layers, num_groups]).\n        # This means `num_groups` are assigned to `num_nodes`. `groups_per_node` groups per node.\n        # So, we have `num_layers * num_nodes` \"node-level\" expert weight tensors.\n        # Each node-level tensor has `num_logical_experts // num_nodes` \"middle\" experts.\n\n        # Let's create the \"middle\" logical experts' weights.\n        # `mlog_weights`: [num_layers * num_nodes, num_logical_experts // num_nodes]\n        # This requires carefully gathering weights.\n        # The `mlog2log` mapping needs to be constructed correctly.\n\n        # `mlog2log_map` assigns each logical expert to a conceptual \"middle\" logical expert.\n        # e.g., if num_logical_experts=8, num_groups=2, group_size=4.\n        # Group 0 contains logical experts 0,1,2,3. Group 1 contains 4,5,6,7.\n        # If num_nodes=2, groups_per_node=1.\n        # Group 0 (log experts 0-3) goes to node 0. Group 1 (log experts 4-7) goes to node 1.\n        # Node 0 has 1 middle expert. Node 1 has 1 middle expert.\n        # Middle expert for node 0 is logical experts 0-3. Middle expert for node 1 is 4-7.\n        # The mapping `mlog2log_map` should map:\n        # 0,1,2,3 -> 0 (on node 0)\n        # 4,5,6,7 -> 0 (on node 1)\n        # The current `mlog2log` calculation seems to be constructing the indices of the middle experts.\n        # `log2mlog` is `[num_layers, num_logical_experts]`.\n        # It maps logical expert ID to its index within the middle expert group.\n        # The `mlog2log` is the inverse of this index mapping.\n\n        # Let's reconstruct `mlog2log` and `tokens_per_mlog` more directly.\n        # For each layer, we have `num_groups` of `group_size`.\n        # `group_pack_index`: [num_layers, num_groups] - node assignment for each group.\n        # `group_rank_in_pack`: [num_layers, num_groups] - index of group within the node's assigned groups.\n\n        # We want to create `num_layers * num_nodes` \"node-level\" expert groups.\n        # Each node-level group corresponds to a set of original logical experts.\n        # The `tokens_per_mlog` should be `[num_layers * num_nodes, num_logical_experts // num_nodes]`.\n\n        # Calculate the logical expert index for each \"middle\" expert.\n        # `mlog_expert_indices`: [num_layers, num_nodes, groups_per_node, group_size]\n        # This is becoming complex. Let's simplify the conceptualization.\n\n        # Simpler approach:\n        # 1. Pack groups to nodes. This determines which logical experts are grouped together on each node.\n        # `group_pack_index`: [num_layers, num_groups] -> node ID for each group.\n        # `group_rank_in_pack`: [num_layers, num_groups] -> rank within node for each group.\n\n        # 2. For each node, create a combined weight tensor for the logical experts assigned to it.\n        #    The number of logical experts per node will be `groups_per_node * group_size`.\n        #    We need to gather the weights of logical experts based on `group_pack_index`.\n\n        # Let's create a mapping from (layer, node) to a contiguous block of logical experts.\n        # `node_logical_expert_map`: [num_layers, num_nodes, groups_per_node * group_size]\n\n        # This requires constructing a permutation for each layer and node.\n        # The original `mlog2log` calculation is:\n        # `((group_pack_index * groups_per_node + group_rank_in_pack) * group_size)`\n        # This forms a base index for each logical expert.\n        # `log2mlog` is this base index + `arange(group_size)`.\n        # `mlog2log` is the inverse of `log2mlog`.\n\n        # Let's directly construct the `tokens_per_mlog` and `mlog2log`.\n        # `tokens_per_mlog` is the weight tensor for experts *within* each node, considering the grouped logical experts.\n        # The number of such \"middle\" experts per node is `groups_per_node * group_size`.\n\n        # For each layer:\n        # `group_pack_index` tells us which node each group belongs to.\n        # `group_rank_in_pack` tells us the order within the node.\n\n        # Let's create `num_layers * num_nodes` tensors, each of size `groups_per_node * group_size`.\n        # `mlog_weights`: [num_layers * num_nodes, groups_per_node * group_size]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, groups_per_node * group_size]\n\n        # Example: num_layers=1, num_logical_experts=8, num_groups=2, group_size=4, num_nodes=2, groups_per_node=1, num_gpus=2, phy_experts_per_gpu=1.\n        # weight: [1, 8]\n        # tokens_per_group_layer: [1, 2] (sum of weights for group 0 and group 1)\n        # balanced_packing(tokens_per_group_layer, num_nodes=2) ->\n        #   group_pack_index: [1, 2] -> [[0, 1]] (group 0 to node 0, group 1 to node 1)\n        #   group_rank_in_pack: [1, 2] -> [[0, 0]] (rank 0 within node for both groups)\n        # groups_per_node = 1, group_size = 4.\n\n        # Middle expert for node 0 is logical experts 0,1,2,3.\n        # Middle expert for node 1 is logical experts 4,5,6,7.\n        # `mlog_to_orig_log_map` for node 0 should be [0,1,2,3].\n        # `mlog_to_orig_log_map` for node 1 should be [4,5,6,7].\n\n        # The original `mlog2log` calculation:\n        # `base_idx = (group_pack_index * groups_per_node + group_rank_in_pack) * group_size`\n        # `base_idx`: [1, 2] -> [[0*1+0, 1*1+0]] * 4 = [[0, 4]]\n        # `log2mlog`: `base_idx.unsqueeze(-1) + torch.arange(group_size)`\n        # `log2mlog`: [[0, 4]].unsqueeze(-1) + [0,1,2,3] -> [[0,1,2,3], [4,5,6,7]]\n        # This `log2mlog` is `[num_layers, num_logical_experts]`. It maps original logical expert ID to its \"middle\" expert index.\n        # The original logic uses `weight.gather(-1, mlog2log)` which is incorrect here.\n        # `weight.gather(-1, mlog2log)` expects `mlog2log` to be `[num_layers, num_middle_experts]` where `num_middle_experts` is the number of columns in `weight`.\n        # The shape should be `[num_layers, num_logical_experts]` if we are gathering from `weight` which is `[num_layers, num_logical_experts]`.\n        # The `mlog2log` calculated here has shape `[num_layers, num_logical_experts]`.\n        # It maps `original_logical_expert_id` to `middle_expert_id`.\n        # We need to gather `weight[layer, original_logical_expert_id]` and group them by `middle_expert_id`.\n\n        # Let's create `mlog_weights` directly.\n        # `mlog_weights`: [num_layers * num_nodes, groups_per_node * group_size]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, groups_per_node * group_size]\n\n        # Vectorized construction of mlog_weights and mlog_to_orig_log_map\n        # This replaces the nested Python loops for better performance.\n\n        # Calculate the new \"middle expert group\" index for each original group.\n        # This new index combines node assignment and rank within the node.\n        # Shape: [num_layers, num_groups]\n        new_group_idx = group_pack_index * groups_per_node + group_rank_in_pack\n\n        # Create a tensor that maps new group index back to original group index.\n        # This is a permutation tensor for groups within each layer.\n        # Shape: [num_layers, num_nodes * groups_per_node] (which is num_groups)\n        group_permutation = torch.empty((num_layers, num_groups),\n                                        dtype=torch.int64,\n                                        device=weight.device)\n        group_permutation.scatter_(1, new_group_idx,\n                                   torch.arange(num_groups, device=weight.device).expand(num_layers, -1))\n\n        # Reshape original weights to [num_layers, num_groups, group_size]\n        weight_reshaped = weight.view(num_layers, num_groups, group_size)\n\n        # Gather weights based on the group_permutation\n        # The .gather() operation reorders the groups according to the permutation.\n        # Shape: [num_layers, num_groups, group_size]\n        mlog_weights_ordered = weight_reshaped.gather(1, group_permutation.unsqueeze(-1).expand(-1, -1, group_size))\n        \n        # Reshape to [num_layers, num_nodes, groups_per_node * group_size]\n        # This is the final mlog_weights.\n        mlog_weights = mlog_weights_ordered.view(num_layers, num_nodes, groups_per_node * group_size)\n\n        # Construct mlog_to_orig_log_map similarly\n        # Original logical expert IDs, reshaped to [num_groups, group_size]\n        orig_log_ids_reshaped = torch.arange(num_logical_experts,\n                                             dtype=torch.int64,\n                                             device=weight.device).view(num_groups, group_size)\n        \n        # Expand to [num_layers, num_groups, group_size] and gather\n        mlog_to_orig_log_map_ordered = orig_log_ids_reshaped.unsqueeze(0).expand(num_layers, -1, -1).gather(\n            1, group_permutation.unsqueeze(-1).expand(-1, -1, group_size))\n        \n        # Reshape to [num_layers, num_nodes, groups_per_node * group_size]\n        mlog_to_orig_log_map = mlog_to_orig_log_map_ordered.view(\n            num_layers, num_nodes, groups_per_node * group_size)\n\n        # Now, `mlog_weights`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n        # And `mlog_to_orig_log_map`: [num_layers, num_nodes, num_logical_experts // num_nodes]\n\n        # Step 2: Replicate experts within each node.\n        # `mlog_weights` are the weights for experts within each node.\n        # `num_physical_experts // num_nodes` is the target number of physical experts per node.\n        # `replicate_experts` takes `[X, num_log]` and returns `phy2log`, `rank`, `logcnt`.\n        # Here, X = num_layers * num_nodes, num_log = num_logical_experts // num_nodes.\n        # So, we need to reshape `mlog_weights` and `mlog_to_orig_log_map`.\n\n        num_middle_experts = num_logical_experts // num_nodes\n        reshaped_mlog_weights = mlog_weights.view(-1, num_middle_experts)\n        reshaped_mlog_to_orig_log_map = mlog_to_orig_log_map.view(-1, num_middle_experts)\n\n        # `phy2mlog`: [num_layers * num_nodes, num_physical_experts // num_nodes]\n        # `phyrank`: [num_layers * num_nodes, num_physical_experts // num_nodes]\n        # `mlogcnt`: [num_layers * num_nodes, num_middle_experts]\n        phy2mlog, phyrank, mlogcnt = replicate_experts(\n            reshaped_mlog_weights, num_physical_experts // num_nodes)\n\n        # Step 3: Pack physical experts to GPUs.\n        # `phy2mlog` maps physical experts (within each node) to middle logical experts.\n        # `phy2mlog` is `[num_layers * num_nodes, num_phy_per_node]`\n        # `phyrank` is `[num_layers * num_nodes, num_phy_per_node]`\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`\n\n        # We need to map these back to global physical expert indices.\n        # The `phy2mlog` refers to the *middle* logical experts.\n        # The `replicate_experts` function returns `phy2log` which is the *original* logical expert ID.\n        # So, `phy2mlog` from `replicate_experts` is actually `[num_layers*num_nodes, num_phy_per_node]`,\n        # where each entry is an index into the *middle* logical experts.\n\n        # We need to map these `phy2mlog` indices back to original logical expert IDs.\n        # `reshaped_mlog_to_orig_log_map`: [num_layers * num_nodes, num_middle_experts]\n        # `final_phy2log_per_node`: [num_layers * num_nodes, num_phy_per_node]\n        final_phy2log_per_node = reshaped_mlog_to_orig_log_map.gather(-1, phy2mlog)\n\n        # `phyrank` is the replica rank for the middle experts.\n        # We need to combine this with `mlogcnt` to get the final `logcnt`.\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`. It tells us how many replicas each middle expert has.\n        # The `phyrank` is the rank of the replica for a *specific* middle expert.\n\n        # The `logcnt` returned by `replicate_experts` is `[X, num_log]`, which is `[num_layers*num_nodes, num_middle_experts]`.\n        # We need to map this back to `[num_layers, num_logical_experts]`.\n\n        # Let's use the `mlogcnt` directly to get the total count of replicas for each logical expert.\n        # `mlogcnt` is `[num_layers * num_nodes, num_middle_experts]`.\n        # We need to sum/average counts based on the `mlog_to_orig_log_map`.\n\n        # Let's refine the output mapping:\n        # `physical_to_logical_map`: [num_layers, num_replicas]\n        # `logical_to_physical_map`: [num_layers, num_logical_experts, X]\n        # `expert_count`: [num_layers, num_logical_experts]\n\n        # `final_phy2log_per_node`: [num_layers * num_nodes, num_phy_per_node]\n        # This is the logical expert ID for each physical expert *within its node*.\n        # We need to convert this to global physical expert IDs.\n        # Physical experts are grouped by GPU. `num_gpus` total, `num_gpus // num_nodes` per node.\n        # `phy_experts_per_gpu` = `num_physical_experts // num_gpus`.\n        # So, each GPU has `phy_experts_per_gpu` physical experts.\n        # Each Node has `num_gpus // num_nodes` GPUs.\n        # Total physical experts per node = `(num_gpus // num_nodes) * phy_experts_per_gpu`\n        # This should equal `num_physical_experts // num_nodes`.\n\n        # `final_phy2log_per_node`: [num_layers, num_nodes, num_phy_per_node]\n        # `final_rank_per_node`: [num_layers, num_nodes, num_phy_per_node] (this is `phyrank`)\n\n        # We need to flatten this to `[num_layers, num_physical_experts]`.\n        # `num_physical_experts = num_nodes * num_phy_per_node`.\n\n        # `final_phy2log_global`: [num_layers, num_physical_experts]\n        # `final_rank_global`: [num_layers, num_physical_experts]\n\n        # The `mlog_to_orig_log_map` correctly maps middle experts to original logical experts.\n        # `phy2mlog` maps physical experts (within a node) to middle experts.\n        # So, `phy2mlog.gather(-1, final_phy2log_per_node)` gives the logical expert ID.\n        # This is what `final_phy2log_per_node` already represents.\n\n        # Let's rename `final_phy2log_per_node` to `phy2log_per_node` for clarity.\n        phy2log_per_node = final_phy2log_per_node\n        rank_per_node = phyrank # [num_layers * num_nodes, num_phy_per_node]\n\n        # Flatten these to get global mappings for physical experts.\n        phy2log_global = phy2log_per_node.view(num_layers, -1)\n        rank_global = rank_per_node.view(num_layers, -1)\n\n        # Now, let's construct the `logcnt` (expert_count).\n        # `mlogcnt`: [num_layers * num_nodes, num_middle_experts]\n        # `mlog_to_orig_log_map`: [num_layers * num_nodes, num_middle_experts]\n\n        # We need to aggregate `mlogcnt` based on `mlog_to_orig_log_map`.\n        # For each logical expert, its total count is the sum of counts of its constituent middle experts.\n        # This aggregation needs to be done carefully.\n\n        # Let's create a tensor for `logcnt`: [num_layers, num_logical_experts] initialized to zeros.\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n\n        # Iterate through each layer and node.\n        for i in range(num_layers):\n            for node_idx in range(num_nodes):\n                # For the middle experts belonging to this node and layer:\n                middle_expert_indices = mlog_to_orig_log_map[i, node_idx] # [num_middle_experts]\n                counts_for_middle_experts = mlogcnt[i * num_nodes + node_idx] # [num_middle_experts]\n\n                # Add these counts to the corresponding original logical experts.\n                # `torch.add.at` allows for sparse updates.\n                expert_count[i].index_add_(0, middle_expert_indices, counts_for_middle_experts)\n\n        return phy2log_global, rank_global, expert_count\n\n    else:\n        # Global balancing strategy: treat all experts as one group, balance across num_gpus.\n        # This is equivalent to calling the hierarchical function with num_nodes=1, num_groups=1.\n        # num_groups = 1 means group_size = num_logical_experts.\n        # num_nodes = 1 means groups_per_node = 1.\n        # The entire logic simplifies.\n\n        # Step 1: Pack logical experts to nodes (here, nodes = GPUs conceptually).\n        # `tokens_per_group` becomes `weight` itself since num_groups=1.\n        # `balanced_packing(weight, num_gpus)`\n        # `group_pack_index`: [num_layers, num_logical_experts] -> GPU assignment for each logical expert.\n        # `group_rank_in_pack`: [num_layers, num_logical_experts] -> rank within GPU for each logical expert.\n        # `num_nodes` becomes `num_gpus`. `groups_per_node` becomes `num_logical_experts // num_gpus`.\n\n        # We need to adapt `balanced_packing` to work with `num_packs = num_gpus`.\n        # The `groups_per_pack` in `balanced_packing` refers to items per pack.\n        # Here, items are logical experts. So, `groups_per_pack` = `num_logical_experts // num_gpus`.\n        # `num_packs` = `num_gpus`.\n\n        pack_index_gpu, rank_in_gpu = balanced_packing(weight, num_gpus)\n\n        # `pack_index_gpu`: [num_layers, num_logical_experts] -> GPU ID for each logical expert.\n        # `rank_in_gpu`: [num_layers, num_logical_experts] -> rank within GPU for each logical expert.\n\n        # Step 2: Replicate experts within GPUs.\n        # We need to create tensors for `replicate_experts` that represent experts *per GPU*.\n        # `num_logical_experts_per_gpu` = `num_logical_experts // num_gpus`.\n        # `num_physical_experts_per_gpu` = `num_physical_experts // num_gpus`.\n\n        # For each GPU, we have a set of logical experts.\n        # We need to gather weights for logical experts assigned to the same GPU.\n        # `gpu_logical_weights`: [num_gpus, num_logical_experts_per_gpu, num_layers] (transposed for replicate_experts)\n        # `gpu_logical_to_orig_map`: [num_gpus, num_logical_experts_per_gpu]\n\n        # Vectorized construction of gpu_logical_weights and gpu_logical_to_orig_map\n        # This replaces the nested Python loops for better performance.\n\n        num_logical_experts_per_gpu = num_logical_experts // num_gpus\n\n        # Calculate the new \"middle expert group\" index for each original logical expert.\n        # This new index combines GPU assignment and rank within the GPU.\n        # Shape: [num_layers, num_logical_experts]\n        new_logical_expert_idx_on_gpu = pack_index_gpu * num_logical_experts_per_gpu + rank_in_gpu\n\n        # Create a tensor that maps new logical expert index back to original logical expert index.\n        # This is a permutation tensor for logical experts within each layer.\n        # Shape: [num_layers, num_gpus * num_logical_experts_per_gpu] (which is num_logical_experts)\n        logical_expert_permutation = torch.empty((num_layers, num_logical_experts),\n                                                 dtype=torch.int64,\n                                                 device=weight.device)\n        logical_expert_permutation.scatter_(1, new_logical_expert_idx_on_gpu,\n                                            torch.arange(num_logical_experts, device=weight.device).expand(num_layers, -1))\n\n        # Gather weights based on the logical_expert_permutation\n        # Shape: [num_layers, num_logical_experts]\n        gpu_logical_weights_ordered = weight.gather(1, logical_expert_permutation)\n        \n        # Reshape to [num_gpus, num_layers, num_logical_experts_per_gpu] for subsequent processing\n        # and ensure contiguous memory layout.\n        gpu_logical_weights = gpu_logical_weights_ordered.view(\n            num_layers, num_gpus, num_logical_experts_per_gpu).permute(1, 0, 2).contiguous()\n\n        # Construct gpu_logical_to_orig_map similarly\n        # Original logical expert IDs, expanded to [num_layers, num_logical_experts]\n        orig_log_ids_expanded = torch.arange(num_logical_experts,\n                                             dtype=torch.int64,\n                                             device=weight.device).unsqueeze(0).expand(num_layers, -1)\n        \n        # Gather original logical expert IDs based on the permutation\n        gpu_logical_to_orig_map_ordered = orig_log_ids_expanded.gather(1, logical_expert_permutation)\n        \n        # Reshape to [num_gpus * num_layers, num_logical_experts_per_gpu] for replicate_experts input\n        gpu_logical_to_orig_map = gpu_logical_to_orig_map_ordered.view(\n            num_layers, num_gpus, num_logical_experts_per_gpu).permute(1, 0, 2).contiguous().view(\n                -1, num_logical_experts_per_gpu)\n\n        # Reshape for `replicate_experts`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        reshaped_gpu_logical_weights = gpu_logical_weights.view(-1, num_logical_experts_per_gpu)\n        reshaped_gpu_logical_to_orig_map = gpu_logical_to_orig_map.view(-1, num_logical_experts_per_gpu)\n\n        # `replicate_experts` is called with `num_phy = num_physical_experts // num_gpus` (target physical experts per GPU).\n        # `X` becomes `num_gpus * num_layers`. `num_log` becomes `num_logical_experts_per_gpu`.\n        # `phy2log_gpu`: [num_gpus * num_layers, num_phy_per_gpu]\n        # `phyrank_gpu`: [num_gpus * num_layers, num_phy_per_gpu]\n        # `gpu_logcnt`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        phy2log_gpu, phyrank_gpu, gpu_logcnt = replicate_experts(\n            reshaped_gpu_logical_weights, num_physical_experts // num_gpus)\n\n        # Map `phy2log_gpu` back to original logical expert IDs using `reshaped_gpu_logical_to_orig_map`.\n        # `phy2log_global`: [num_gpus * num_layers, num_phy_per_gpu]\n        phy2log_global = reshaped_gpu_logical_to_orig_map.gather(-1, phy2log_gpu)\n\n        # Flatten to get the final `physical_to_logical_map` and `rank`.\n        # The `rank_global` is `phyrank_gpu`.\n        rank_global = phyrank_gpu\n\n        # Construct `expert_count` ([num_layers, num_logical_experts]).\n        # `gpu_logcnt`: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        expert_count = torch.zeros(num_layers, num_logical_experts, dtype=torch.int64, device=weight.device)\n\n        for gpu_layer_idx in range(num_gpus * num_layers):\n            layer_idx = gpu_layer_idx // num_gpus\n            # Get the original logical expert indices for this gpu_layer combination\n            orig_log_indices = reshaped_gpu_logical_to_orig_map[gpu_layer_idx] # [num_logical_experts_per_gpu]\n            counts_for_orig_logs = gpu_logcnt[gpu_layer_idx] # [num_logical_experts_per_gpu]\n\n            # Add these counts to the correct positions in the expert_count tensor\n            expert_count[layer_idx].index_add_(0, orig_log_indices, counts_for_orig_logs)\n\n        # Construct `logical_to_physical_map` ([num_layers, num_logical_experts, X])\n        max_replicas_per_expert = (num_physical_experts + num_logical_experts - 1) // num_logical_experts\n\n        logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert), -1, dtype=torch.int64, device=weight.device)\n\n        # Need to map `phy2log_global` and `rank_global` to the correct positions.\n        # `phy2log_global` are indices into the *original* logical experts.\n        # `rank_global` is the replica rank for that original logical expert.\n\n        # Reshape to get mappings per layer and then iterate through physical experts.\n        phy2log_global_reshaped = phy2log_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous()\n        rank_global_reshaped = rank_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous()\n\n        # Iterate through all physical experts\n        for layer_idx in range(num_layers):\n            for gpu_idx in range(num_gpus):\n                for phys_idx_in_gpu in range(num_physical_experts // num_gpus):\n                    # Get the global physical expert index\n                    global_phys_idx = gpu_idx * (num_physical_experts // num_gpus) + phys_idx_in_gpu\n\n                    log_expert_id = phy2log_global_reshaped[layer_idx, gpu_idx, phys_idx_in_gpu]\n                    replica_rank = rank_global_reshaped[layer_idx, gpu_idx, phys_idx_in_gpu]\n\n                    if log_expert_id >= 0 and log_expert_id < num_logical_experts:\n                        if replica_rank < max_replicas_per_expert:\n                            logical_to_physical_map[layer_idx, log_expert_id, replica_rank] = global_phys_idx\n                        else:\n                            pass # Handle error or log\n\n        # The `phy2log_global` needs to be reshaped to `[num_layers, num_replicas]`\n        # where `num_replicas` is `num_physical_experts`.\n        # `phy2log_global` is currently `[num_gpus * num_layers, num_phy_per_gpu]`.\n        # We need to combine these.\n        # The order of physical experts should be consistent.\n        # Let's reconstruct `phy2log_global` to be `[num_layers, num_physical_experts]`.\n\n        final_phy2log_global = torch.empty((num_layers, num_physical_experts), dtype=torch.int64, device=weight.device)\n        current_phys_idx = 0\n        for gpu_idx in range(num_gpus):\n            for layer_idx in range(num_layers):\n                # Calculate the range of physical experts for this GPU and layer combination\n                start_phys_idx = layer_idx * num_gpus * (num_physical_experts // num_gpus) + gpu_idx * (num_physical_experts // num_gpus)\n                end_phys_idx = start_phys_idx + (num_physical_experts // num_gpus)\n\n                # The `phy2log_global` calculated earlier is `[num_gpus * num_layers, num_phy_per_gpu]`\n                # We need to map it correctly.\n                # `phy2log_gpu` was `[num_gpus * num_layers, num_phy_per_gpu]`\n                # `reshaped_gpu_logical_to_orig_map` was `[num_gpus * num_layers, num_phy_per_gpu]`\n                # `phy2log_global = reshaped_gpu_logical_to_orig_map.gather(-1, phy2log_gpu)`\n\n                # The indices for `phy2log_global` should be:\n                # For layer 0, GPU 0: phys experts 0 to num_phy_per_gpu - 1\n                # For layer 0, GPU 1: phys experts num_phy_per_gpu to 2*num_phy_per_gpu - 1\n                # ...\n                # For layer 1, GPU 0: phys experts num_gpus * num_phy_per_gpu to (num_gpus+1)*num_phy_per_gpu - 1\n\n                # Let's re-index the `phy2log_global` and `rank_global` to match `[num_layers, num_physical_experts]`\n                # The current `phy2log_global` is effectively `[num_layers * num_gpus, num_phy_per_gpu]`\n                # We need to rearrange it.\n\n                # Example: num_layers=2, num_gpus=2, num_phy_per_gpu=2. num_physical_experts=4.\n                # phy2log_global:\n                # [layer0_gpu0_phys0, layer0_gpu0_phys1,\n                #  layer0_gpu1_phys0, layer0_gpu1_phys1,\n                #  layer1_gpu0_phys0, layer1_gpu0_phys1,\n                #  layer1_gpu1_phys0, layer1_gpu1_phys1]\n                # Reshaped to [num_gpus*num_layers, num_phy_per_gpu] -> [4, 2]\n                # We want [num_layers, num_physical_experts] -> [2, 4]\n\n                # The current `phy2log_global` is `[num_gpus * num_layers, num_phy_per_gpu]`.\n                # Permute and reshape:\n                # `view(num_gpus, num_layers, -1)` gives `[num_gpus, num_layers, num_phy_per_gpu]`\n                # `permute(1, 0, 2)` gives `[num_layers, num_gpus, num_phy_per_gpu]`\n                # `contiguous().view(num_layers, -1)` gives `[num_layers, num_gpus * num_phy_per_gpu]` = `[num_layers, num_physical_experts]`\n\n                final_phy2log_global = phy2log_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous().view(num_layers, num_physical_experts)\n                final_rank_global = rank_global.view(num_gpus, num_layers, -1).permute(1, 0, 2).contiguous().view(num_layers, num_physical_experts)\n\n        # Return the results for global balancing.\n        return final_phy2log_global, logical_to_physical_map, expert_count\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    # Keep `weight` on its original device (e.g., GPU) for performance.\n    weight = weight.float() \n    \n    # Determine the balancing policy based on divisibility\n    if num_groups % num_nodes == 0:\n        # Use hierarchical load-balance policy\n        phy2log_global, rank_global, expert_count = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # Use global load-balance policy by calling hierarchical with num_groups=1, num_nodes=1.\n        # This effectively treats all logical experts as one large group and balances them across GPUs.\n        phy2log_global, rank_global, expert_count = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n\n    # Compute logical_to_physical_map once, using the global mappings\n    num_physical_experts = num_replicas # Renaming for clarity based on function signature\n\n    # Calculate max_replicas_per_expert.\n    # This needs to be robust to cases where a single expert might receive more replicas\n    # than the average if the load distribution is highly skewed.\n    # The maximum number of replicas any single logical expert can receive is\n    # num_physical_experts - (num_logical_experts - 1), assuming num_physical_experts >= num_logical_experts.\n    if num_physical_experts >= num_logical_experts:\n        # Calculate average/ceiling replicas\n        max_replicas_per_expert_avg = (num_physical_experts + num_logical_experts - 1) // num_logical_experts\n        # Calculate worst-case (one expert gets all redundant replicas)\n        max_replicas_per_expert_worst_case = num_physical_experts - (num_logical_experts - 1)\n        max_replicas_per_expert = max(max_replicas_per_expert_avg, max_replicas_per_expert_worst_case)\n    else:\n        # If num_physical_experts < num_logical_experts, then at most one replica per expert is possible.\n        # Some experts will have 0 replicas. Max rank will be 0.\n        max_replicas_per_expert = 1\n        \n    logical_to_physical_map = torch.full((num_layers, num_logical_experts, max_replicas_per_expert),\n                                         -1, dtype=torch.int64, device=weight.device)\n\n    physical_expert_ids = torch.arange(num_physical_experts, dtype=torch.int64, device=weight.device)\n    physical_expert_ids_expanded = physical_expert_ids.unsqueeze(0).expand(num_layers, num_physical_experts)\n    layer_indices = torch.arange(num_layers, dtype=torch.int64, device=weight.device).unsqueeze(1)\n    \n    # Use advanced indexing to populate logical_to_physical_map efficiently.\n    # This line is critical and was the source of the \"index out of bounds\" error\n    # if rank_global contained values exceeding max_replicas_per_expert - 1.\n    logical_to_physical_map[layer_indices, phy2log_global, rank_global] = physical_expert_ids_expanded\n\n    return phy2log_global, logical_to_physical_map, expert_count\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["The current performance metrics show a regression in `speed_score` and `combined_score` compared to previous attempts. This often points to inefficiencies like Python loops or unnecessary device transfers within performance-critical sections. The `balancedness_score` is also lower, which might be a side effect of less efficient algorithms or simply room for improvement.\n\nMy analysis identifies the following key areas for improvement:\n\n1.  **`balanced_packing` function:**\n    *   The current implementation uses nested Python loops (`for i in range(num_layers): for g in range(num_groups):`) to assign items to packs. This is a significant bottleneck for larger inputs, as Python loops are slow compared to vectorized PyTorch operations.\n    *   It explicitly moves intermediate tensors (`pack_index`, `rank_in_pack`, `pack_loads`, `pack_counts`) to CPU for these loops, even though the input `weight` might be on a GPU. This introduces unnecessary device transfer overhead if `weight` is on a GPU.\n    *   **Improvement:** Replace the greedy (LPT-like) assignment with a vectorized \"Sorted Round-Robin\" strategy. While potentially offering a slightly less optimal theoretical balance than a perfect greedy approach, its vectorized nature will dramatically improve speed. Crucially, all operations will remain on the original device of the `weight` tensor, eliminating device transfer overheads.\n\n2.  **`expert_count` aggregation loops:**\n    *   Both the hierarchical and global balancing paths within `rebalance_experts_hierarchical` and `rebalance_experts` (respectively) contain Python loops for aggregating `expert_count` using `index_add_`. While `index_add_` itself is efficient, the outer Python loop `for gpu_layer_idx in range(...)` or `for i in range(num_layers): for node_idx in range(num_nodes):` can still be a bottleneck if `num_layers * num_nodes` or `num_gpus * num_layers` is large.\n    *   **Improvement:** Vectorize these aggregation loops by flattening the input tensors and computing the target indices for a single `index_add_` call across all layers/nodes/GPUs simultaneously.\n\nThese changes prioritize `speed_score` by eliminating Python loops and reducing device transfers, which should also positively impact the `combined_score`. While the `balanced_packing` heuristic changes from LPT to Sorted Round-Robin, the efficiency gains are expected to outweigh any minor theoretical degradation in balancedness, potentially even leading to better practical balancedness due to faster execution allowing for more iterations or more frequent rebalancing in a live system.\n\n---\n\nHere are the proposed changes in the specified diff format:\n\n**Change 1: Vectorize and optimize `balanced_packing`**\nThis change replaces the slow Python loops and CPU transfers with a fully vectorized Sorted Round-Robin assignment performed entirely on the original device.\n\n<<<<<<< SEARCH\n    # Initialize pack_index and rank_in_pack on CPU for efficiency\n    pack_index = torch.full((num_layers, num_groups), -1, dtype=torch.int64, device=\"cpu\")\n    rank_in_pack = torch.full((num_layers, num_groups), -1, dtype=torch.int64, device=\"cpu\")\n\n    # Use a more efficient way to assign items to packs\n    # Initialize pack_loads and pack_counts\n    pack_loads = torch.zeros(num_layers, num_packs, dtype=torch.float32, device=\"cpu\")\n    pack_counts = torch.zeros(num_layers, num_packs, dtype=torch.int64, device=\"cpu\")\n\n    # Iterate through sorted items and assign them to the least loaded pack\n    for i in range(num_layers):\n        for g in range(num_groups):\n            # Find the pack with the minimum current load and available slots\n            # Use torch.min to find the index of the minimum load\n            # We also need to ensure the pack has capacity (pack_counts < groups_per_pack)\n            available_packs = pack_counts[i] < groups_per_pack\n            if not available_packs.any():\n                # This should ideally not happen if num_groups is divisible by num_packs\n                # and groups_per_pack is at least 1.\n                continue\n\n            # Select from available packs\n            current_pack_loads = pack_loads[i][available_packs]\n            current_pack_indices = torch.arange(num_packs, device=\"cpu\")[available_packs]\n\n            min_load_idx_in_available = torch.argmin(current_pack_loads)\n            chosen_pack_idx = current_pack_indices[min_load_idx_in_available]\n\n            original_group_idx = sorted_indices[i, g]\n\n            pack_index[i, original_group_idx] = chosen_pack_idx\n            rank_in_pack[i, original_group_idx] = pack_counts[i, chosen_pack_idx]\n\n            pack_loads[i, chosen_pack_idx] += weight[i, original_group_idx]\n            pack_counts[i, chosen_pack_idx] += 1\n\n    return pack_index, rank_in_pack\n=======\n    # All computations on the same device as weight to avoid transfers.\n    # Sorted Round Robin assignment for better efficiency.\n    # sorted_weights: [num_layers, num_groups]\n    # sorted_indices: [num_layers, num_groups] (indices of original weights in sorted order)\n    sorted_weights, sorted_indices = weight.float().sort(-1, descending=True)\n\n    # Create pack assignments for the sorted items.\n    # For each layer, assign items 0, 1, ..., num_groups-1 (in sorted order)\n    # to packs 0, 1, ..., num_packs-1, then repeat.\n    # pack_assignment_for_sorted_items: [num_layers, num_groups]\n    pack_assignment_for_sorted_items = torch.arange(num_groups, device=weight.device) % num_packs\n    pack_assignment_for_sorted_items = pack_assignment_for_sorted_items.unsqueeze(0).expand(num_layers, -1)\n\n    # Calculate rank within each pack for the sorted items.\n    # rank_in_pack_for_sorted_items: [num_layers, num_groups]\n    rank_in_pack_for_sorted_items = torch.arange(num_groups, device=weight.device) // num_packs\n    rank_in_pack_for_sorted_items = rank_in_pack_for_sorted_items.unsqueeze(0).expand(num_layers, -1)\n\n    # Map these assignments back to the original (unsorted) indices using scatter_.\n    pack_index = torch.empty_like(sorted_indices)\n    rank_in_pack = torch.empty_like(sorted_indices)\n\n    pack_index.scatter_(1, sorted_indices, pack_assignment_for_sorted_items)\n    rank_in_pack.scatter_(1, sorted_indices, rank_in_pack_for_sorted_items)\n\n    return pack_index, rank_in_pack\n>>>>>>> REPLACE\n\n**Change 2: Vectorize `expert_count` aggregation in `rebalance_experts_hierarchical`**\nThis change replaces the nested Python loops for `expert_count` aggregation with a single vectorized `index_add_` operation.\n\n<<<<<<< SEARCH\n        # Iterate through each layer and node.\n        for i in range(num_layers):\n            for node_idx in range(num_nodes):\n                # For the middle experts belonging to this node and layer:\n                middle_expert_indices = mlog_to_orig_log_map[i, node_idx] # [num_middle_experts]\n                counts_for_middle_experts = mlogcnt[i * num_nodes + node_idx] # [num_middle_experts]\n\n                # Add these counts to the corresponding original logical experts.\n                # `torch.add.at` allows for sparse updates.\n                expert_count[i].index_add_(0, middle_expert_indices, counts_for_middle_experts)\n=======\n        # Vectorized aggregation of expert_count.\n        # mlogcnt: [num_layers * num_nodes, num_middle_experts]\n        # mlog_to_orig_log_map: [num_layers, num_nodes, num_middle_experts]\n\n        # Flatten mlog_to_orig_log_map and mlogcnt for single index_add_ operation\n        flat_mlog_to_orig_log_map = mlog_to_orig_log_map.view(-1, num_middle_experts).flatten()\n        flat_mlogcnt = mlogcnt.flatten()\n\n        # Create corresponding layer indices for each flattened entry\n        # The mlog_to_orig_log_map is structured as [layer, node, middle_expert]\n        # so layer indices repeat for each node block, then increment for the next layer.\n        layer_indices_expanded = torch.arange(num_layers, device=weight.device).unsqueeze(1).unsqueeze(2).expand(\n            num_layers, num_nodes, num_middle_experts).flatten()\n\n        # Calculate flat target indices for expert_count in the flattened expert_count tensor\n        flat_expert_count_indices = layer_indices_expanded * num_logical_experts + flat_mlog_to_orig_log_map\n\n        # Initialize expert_count as a flattened tensor\n        expert_count = torch.zeros(num_layers * num_logical_experts, dtype=torch.int64, device=weight.device)\n\n        # Perform scattered add to aggregate counts\n        expert_count.index_add_(0, flat_expert_count_indices, flat_mlogcnt)\n        \n        # Reshape back to [num_layers, num_logical_experts]\n        expert_count = expert_count.view(num_layers, num_logical_experts)\n>>>>>>> REPLACE\n\n**Change 3: Vectorize `expert_count` aggregation in `rebalance_experts` (global path)**\nThis change replaces the Python loop for `expert_count` aggregation in the global balancing path with a single vectorized `index_add_` operation.\n\n<<<<<<< SEARCH\n        for gpu_layer_idx in range(num_gpus * num_layers):\n            layer_idx = gpu_layer_idx // num_gpus\n            # Get the original logical expert indices for this gpu_layer combination\n            orig_log_indices = reshaped_gpu_logical_to_orig_map[gpu_layer_idx] # [num_logical_experts_per_gpu]\n            counts_for_orig_logs = gpu_logcnt[gpu_layer_idx] # [num_logical_experts_per_gpu]\n\n            # Add these counts to the correct positions in the expert_count tensor\n            expert_count[layer_idx].index_add_(0, orig_log_indices, counts_for_orig_logs)\n=======\n        # Vectorized aggregation of expert_count.\n        # gpu_logcnt: [num_gpus * num_layers, num_logical_experts_per_gpu]\n        # reshaped_gpu_logical_to_orig_map: [num_gpus * num_layers, num_logical_experts_per_gpu]\n\n        # Flatten reshaped_gpu_logical_to_orig_map and gpu_logcnt for single index_add_ operation\n        flat_gpu_logical_to_orig_map = reshaped_gpu_logical_to_orig_map.flatten()\n        flat_gpu_logcnt = gpu_logcnt.flatten()\n\n        # Create corresponding layer indices for each flattened entry\n        # The reshaped_gpu_logical_to_orig_map is structured as [gpu_idx, layer_idx, expert_in_gpu] conceptually\n        # after the permute, so layer indices cycle through 0, 1, ..., num_layers-1 for each gpu block.\n        layer_indices_expanded = torch.arange(num_layers, device=weight.device).repeat(num_gpus)\n        layer_indices_expanded = layer_indices_expanded.unsqueeze(1).expand(-1, num_logical_experts_per_gpu).flatten()\n\n        # Calculate flat target indices for expert_count in the flattened expert_count tensor\n        flat_expert_count_indices = layer_indices_expanded * num_logical_experts + flat_gpu_logical_to_orig_map\n\n        # Initialize expert_count as a flattened tensor\n        expert_count = torch.zeros(num_layers * num_logical_experts, dtype=torch.int64, device=weight.device)\n\n        # Perform scattered add to aggregate counts\n        expert_count.index_add_(0, flat_expert_count_indices, flat_gpu_logcnt)\n        \n        # Reshape back to [num_layers, num_logical_experts]\n        expert_count = expert_count.view(num_layers, num_logical_experts)\n>>>>>>> REPLACE"]}}}