{"id": "d6c94179-e4fe-4774-9834-6da09b4da3d2", "code": "# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort groups by weight in descending order.\n    # 'weight' is already float and on CPU from rebalance_experts,\n    # so no need for .float().cpu() here.\n    indices = weight.sort(-1, descending=True).indices\n\n    # Initialize pack_index and rank_in_pack.\n    # These will be fully populated, so `empty_like` is sufficient.\n    # They will be on the same device as 'weight' (i.e., CPU).\n    pack_index = torch.empty_like(weight, dtype=torch.int64)\n    rank_in_pack = torch.empty_like(pack_index)\n    \n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These tensors are on the same device as 'weight' (which is CPU).\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Define an infinity tensor once to avoid repeated creation inside the loops.\n    inf_tensor = torch.full((), float('inf'), dtype=weight.dtype, device=weight.device)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # Get current state for this layer's packs as views.\n        current_pack_weights = pack_weights_per_layer[i]\n        current_pack_items = pack_items_per_layer[i]\n\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id_tensor in indices[i]:\n            # Find the pack with the minimum current weight among available ones.\n            # Set weights of full packs to infinity to exclude them from argmin.\n            # The result 'pack_idx_tensor' will be a 0-dim tensor (scalar tensor).\n            pack_idx_tensor = torch.argmin(\n                torch.where(current_pack_items == groups_per_pack,\n                            inf_tensor, # Use pre-defined infinity tensor\n                            current_pack_weights))\n\n            # Assert that a valid pack was found and it has space.\n            # Use tensor indexing directly, avoiding .item() call.\n            assert current_pack_items[pack_idx_tensor] < groups_per_pack\n\n            # Assign the group to the chosen pack using tensor indexing.\n            # group_id_tensor is a 0-dim tensor, pack_idx_tensor is a 0-dim tensor.\n            pack_index[i, group_id_tensor] = pack_idx_tensor\n            rank_in_pack[i, group_id_tensor] = current_pack_items[pack_idx_tensor]\n\n            # Update pack weights and item counts for the chosen pack using tensor indexing.\n            pack_weights_per_layer[i, pack_idx_tensor] += weight[i, group_id_tensor]\n            pack_items_per_layer[i, pack_idx_tensor] += 1\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    # Ensure that replication is actually possible (i.e., num_phy >= num_log)\n    assert num_redundant >= 0, f\"num_phy ({num_phy}) must be >= num_log ({num_log}) for replication.\"\n    \n    device = weight.device\n    \n    # Initialize phy2log: maps physical expert index to logical expert index.\n    # Initially, physical expert j maps to logical expert j.\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    \n    # Initialize rank: tracks the replica rank within each logical expert group.\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    \n    # logcnt: tracks the number of physical replicas for each logical expert.\n    # Initially, each logical expert has 1 physical replica.\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    \n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    \n    # Initialize sum of weights for each logical expert.\n    # Initially, this is just the original weight of each logical expert.\n    # We will update this sum as replicas are added.\n    current_weight_sums = weight.clone()\n\n    # Iterate to add redundant experts one by one.\n    for i in range(num_log, num_phy):\n        # Calculate current average loads for all logical experts.\n        # This is done by dividing the current total weight sum by the current replica count.\n        # We use .to(weight.dtype) to ensure float division.\n        current_avg_loads = current_weight_sums / logcnt.to(weight.dtype)\n\n        # Find the logical expert (for each layer) with the highest average load.\n        # This is the expert that needs a new replica to balance the load.\n        # `redundant_indices` will contain the logical expert index for each layer.\n        redundant_indices = torch.argmax(current_avg_loads, dim=-1)\n\n        # Assign this logical expert to the current physical expert slot (i).\n        # `phy2log[:, i]` will be populated with the `redundant_indices`.\n        phy2log[:, i] = redundant_indices\n\n        # Record the replica rank for this newly assigned physical expert.\n        # The rank is the current count of replicas for that logical expert before incrementing.\n        # `torch.arange(n)` is used to get the layer indices.\n        rank[:, i] = logcnt[torch.arange(n, device=device), redundant_indices]\n\n        # Increment the count of physical experts for the chosen logical expert.\n        # `logcnt[torch.arange(n, device=device), redundant_indices]` accesses the correct\n        # counts for each layer and increments them.\n        logcnt[torch.arange(n, device=device), redundant_indices] += 1\n\n        # Update the sum of weights for the logical experts that just received a new replica.\n        # We add the original weight of that logical expert to its current sum.\n        # This is an optimization: instead of recalculating the sum from scratch,\n        # we add the weight of the *newly added* replica.\n        # However, the logic here needs to be carefully considered. The `weight` tensor\n        # represents the *initial* load of each logical expert. When we add a replica,\n        # it's essentially a copy of that logical expert. So, the sum of weights\n        # for a logical expert with `k` replicas should be `k * original_weight`.\n        # The current implementation of `replicate_experts` in the original code\n        # seems to be implicitly assuming that `weight` is the *average* load,\n        # which is incorrect. `weight` should be the total load.\n        #\n        # Let's re-evaluate: if `weight` is the *total* load of a logical expert,\n        # then `current_avg_loads = weight / logcnt`.\n        # When we add a replica to logical expert `j`, its new replica count becomes `logcnt[j] + 1`.\n        # The new average load for `j` would be `weight[j] / (logcnt[j] + 1)`.\n        #\n        # The `replicate_experts` function in the original code seems to be designed for\n        # a scenario where `weight` is already an average load per expert, and we are\n        # replicating *that* load. However, the problem statement implies `weight` is\n        # the load statistic for logical experts.\n        #\n        # Let's assume `weight` is the *total* load of a logical expert.\n        # The goal is to minimize the maximum load across all *physical* experts.\n        #\n        # Corrected logic for updating `current_weight_sums` and `current_avg_loads`:\n        # When a logical expert `j` receives a new replica, its total load remains `weight[j]`.\n        # Its replica count increases, thus its average load decreases.\n        #\n        # The current implementation of `replicate_experts` is trying to assign\n        # the `i`-th physical expert. It finds the logical expert with the highest\n        # *average* load and assigns the `i`-th physical expert to it.\n        # This implies that `weight` should be treated as the base load for *each* replica.\n        #\n        # Let's stick to the original intent of `replicate_experts` where `weight`\n        # is the load of a logical expert, and we're distributing `num_phy` physical\n        # experts to `num_log` logical experts.\n        #\n        # The original code's update step:\n        # `current_avg_loads[arangen, redundant_indices] = weight[arangen, redundant_indices] / logcnt[arangen, redundant_indices].to(weight.dtype)`\n        # This correctly calculates the new average load for the chosen logical experts.\n        #\n        # The `current_weight_sums` approach is not needed if `weight` is the base load.\n        # The issue might be with how `current_avg_loads` is calculated and used.\n        #\n        # The core idea of the greedy approach is to add a replica to the logical expert\n        # that currently has the highest average load. The average load is `total_load / num_replicas`.\n        # If `weight` represents the *total load* for each logical expert, then:\n        #\n        # `current_avg_loads` should be `weight / logcnt`.\n        #\n        # Let's refine the `replicate_experts` function based on this understanding.\n        # The original implementation of `replicate_experts` is actually quite close to\n        # a standard greedy algorithm for this problem (like Huffman coding or\n        # multiway number partitioning). The key is that `weight` represents the\n        # total load of a logical expert, and `logcnt` is the number of physical\n        # experts assigned to it. The average load is `weight / logcnt`.\n        #\n        # The problematic part might be the repeated calculation of `current_avg_loads`.\n        #\n        # Let's try to optimize the calculation of `current_avg_loads` without changing the core logic.\n        # Instead of recalculating `current_avg_loads` from scratch, we can update it\n        # incrementally. However, this can be complex.\n        #\n        # A simpler optimization: use `torch.topk` to find the `k` largest average loads if needed,\n        # but here we only need the maximum.\n        #\n        # The original update:\n        # `current_avg_loads[arangen, redundant_indices] = weight[arangen, redundant_indices] / logcnt[arangen, redundant_indices].to(weight.dtype)`\n        # This update is correct in that it recalculates the average load for the\n        # logical experts that received a new replica. The problem is that `current_avg_loads`\n        # is recalculated entirely in the next iteration.\n        #\n        # Let's consider the state: `logcnt` and `weight`.\n        # We need to find `argmax(weight / logcnt)`.\n        #\n        # The original code is already performing an efficient update for the specific\n        # logical experts that received a new replica. The main bottleneck might be\n        # the `current_avg_loads.max(dim=-1)` operation itself.\n        #\n        # A potential optimization: keep track of the average loads and update only\n        # the affected ones.\n        #\n        # Let's stick to the current logic but ensure it's as efficient as possible.\n        # The provided code already seems to have an efficient update mechanism.\n        # The previous attempt might have introduced a regression.\n        #\n        # Let's focus on the `rebalance_experts_hierarchical` function for potential improvements.\n        #\n        # Reverting to the original update logic as the optimization attempt in the provided code\n        # might have been incorrect or less efficient than the original.\n        # The goal is to find the logical expert with the highest average load.\n        # `weight` is the total load of a logical expert.\n        # `logcnt` is the number of replicas for that logical expert.\n        # Average load = `weight / logcnt`.\n        #\n        # The original code's update:\n        # `current_avg_loads[arangen, redundant_indices] = weight[arangen, redundant_indices] / logcnt[arangen, redundant_indices].to(weight.dtype)`\n        # This recalculates the average load for the *specific* logical experts that just received a new replica.\n        # This is correct. The overall `current_avg_loads` is recalculated at the start of the loop.\n        #\n        # Let's consider the possibility that the `weight` tensor itself needs to be adjusted or interpreted differently.\n        # If `weight` represents the load of *each* logical expert, and we are distributing `num_phy` physical experts,\n        # the greedy strategy is to always assign the next physical expert to the logical expert with the\n        # currently highest *average* load (total load / number of replicas).\n        #\n        # The original code correctly calculates `current_avg_loads` using `weight / logcnt`.\n        # The update `current_avg_loads[arangen, redundant_indices] = ...` is also correct for\n        # the affected logical experts.\n        #\n        # The issue might be in the `rebalance_experts_hierarchical` function or the overall strategy.\n        #\n        # For `replicate_experts`, the current implementation seems to be a standard greedy approach.\n        # The optimization might lie in how `current_avg_loads` is computed and updated.\n        #\n        # Let's consider the first line of the loop: `current_avg_loads = weight / logcnt.to(weight.dtype)`\n        # This is a full recomputation.\n        #\n        # A more optimized way to track average loads:\n        # Maintain `average_loads` directly. When `logcnt[j]` increases, update `average_loads[j]`.\n        #\n        # Let's try to implement this.\n        # Initialize `average_loads` with `weight / logcnt`.\n        # In the loop:\n        #   Find `argmax(average_loads)`.\n        #   Assign.\n        #   Update `logcnt`.\n        #   Update `average_loads` for the affected logical expert: `weight[j] / new_logcnt[j]`.\n        #\n        # This avoids recomputing average loads for all logical experts in each iteration.\n\n    # Re-initialize `logcnt` to 1 for each logical expert.\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n\n    # Initialize `average_loads` based on initial `logcnt`.\n    average_loads = weight / logcnt.to(weight.dtype)\n\n    # Iterate to add redundant experts one by one.\n    for i in range(num_log, num_phy):\n        # Find the logical expert (for each layer) with the highest average load.\n        # `redundant_indices` will contain the logical expert index for each layer.\n        redundant_indices = torch.argmax(average_loads, dim=-1)\n\n        # Assign this logical expert to the current physical expert slot (i).\n        phy2log[:, i] = redundant_indices\n\n        # Record the replica rank for this newly assigned physical expert.\n        rank[:, i] = logcnt[torch.arange(n, device=device), redundant_indices]\n\n        # Increment the count of physical experts for the chosen logical expert.\n        logcnt[torch.arange(n, device=device), redundant_indices] += 1\n\n        # Update the average load ONLY for the logical experts that just received a new replica.\n        # This avoids recomputing the entire average_loads tensor, significantly\n        # improving performance when num_redundant is large.\n        # The new average load is the original total weight divided by the new replica count.\n        average_loads[torch.arange(n, device=device), redundant_indices] = \\\n            weight[torch.arange(n, device=device), redundant_indices] / logcnt[torch.arange(n, device=device), redundant_indices].to(weight.dtype)\n\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: Pack logical experts into nodes based on their token counts.\n    # Calculate token counts per group first.\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    # Use balanced_packing to assign groups to nodes.\n    group_to_node_assignment, group_rank_within_node = balanced_packing(\n        tokens_per_group, num_nodes)\n\n    # Map logical experts to nodes.\n    # `logical_expert_to_node_map` indicates which node each logical expert belongs to.\n    # `logical_expert_rank_in_node` indicates the rank of a logical expert within its node group.\n    logical_expert_to_node_map = (group_to_node_assignment * groups_per_node +\n                                  group_rank_within_node)\n    # Expand to cover all experts within a group.\n    logical_expert_to_node_map = logical_expert_to_node_map.unsqueeze(-1).expand(-1, -1, group_size)\n    logical_expert_to_node_map = logical_expert_to_node_map.flatten(1, 2)\n\n    # Create the inverse mapping: node to logical experts.\n    node_to_logical_experts_map = inverse(logical_expert_to_node_map)\n\n    # Step 2: Replicate logical experts within each node to create physical experts.\n    # We need the load distribution for logical experts per node.\n    # Reshape weight to group by nodes: [num_layers, num_nodes, groups_per_node * group_size]\n    logical_experts_per_node = num_logical_experts // num_nodes\n    # `weight_per_node` contains the loads of logical experts, grouped by node.\n    # Shape: [num_layers * num_nodes, logical_experts_per_node]\n    weight_per_node = weight.gather(-1, logical_expert_to_node_map).view(\n        num_layers * num_nodes, logical_experts_per_node)\n\n    # Determine the number of physical experts per node.\n    physical_experts_per_node = num_physical_experts // num_gpus * (num_gpus // num_nodes) # This calculation seems off. Let's use the provided param `num_physical_experts // num_gpus` and scale by `num_gpus // num_nodes`.\n    physical_experts_per_node = (num_physical_experts // num_gpus) * (num_gpus // num_nodes)\n\n    # Replicate logical experts within each node to `physical_experts_per_node`.\n    # `logical_to_physical_map_per_node`: maps physical experts within a node to logical experts.\n    # `physical_rank_per_node`: rank of physical experts within their logical group.\n    # `logical_replica_count_per_node`: number of replicas for each logical expert within the node.\n    logical_to_physical_map_per_node, physical_rank_per_node, logical_replica_count_per_node = replicate_experts(\n        weight_per_node, physical_experts_per_node)\n\n    # Step 3: Pack physical experts across nodes to GPUs.\n    # Calculate the load per physical expert within each node.\n    # `load_per_physical_expert_in_node` is `weight_per_node / logical_replica_count_per_node`.\n    # Shape: [num_layers * num_nodes, physical_experts_per_node]\n    load_per_physical_expert_in_node = (weight_per_node / logical_replica_count_per_node.to(weight_per_node.dtype)).gather(-1, logical_to_physical_map_per_node)\n\n    # Use balanced_packing to assign physical experts (grouped by node) to GPUs.\n    # `gpu_assignment_per_node`: which GPU each physical expert (within its node group) goes to.\n    # `gpu_rank_within_assignment`: rank of physical expert within its GPU assignment.\n    gpu_assignment_per_node, gpu_rank_within_assignment = balanced_packing(\n        load_per_physical_expert_in_node, num_gpus // num_nodes)\n\n    # Create the final mapping from physical expert to logical expert.\n    # `final_physical_to_logical_map`: maps each physical expert across all GPUs to its logical expert.\n    # Shape: [num_layers, num_physical_experts]\n    # First, map physical experts within a node to their logical experts.\n    # `logical_expert_for_physical_in_node` maps physical experts within a node to their logical counterparts.\n    # Shape: [num_layers * num_nodes, physical_experts_per_node]\n    logical_expert_for_physical_in_node = logical_to_physical_map_per_node.gather(-1, node_to_logical_experts_map)\n\n    # Now, map these to the final physical expert indices across all GPUs.\n    # The `gpu_assignment_per_node` tells us which GPU (within the node's group of GPUs)\n    # a physical expert should go to.\n    # `physical_expert_index_on_gpu` is the index of a physical expert on its assigned GPU.\n    physical_expert_index_on_gpu = (gpu_assignment_per_node *\n                                     (num_physical_experts // num_gpus) +\n                                     gpu_rank_within_assignment)\n\n    # Map the logical experts to the final physical expert indices.\n    # `final_physical_to_logical_map` is constructed by gathering `logical_expert_for_physical_in_node`\n    # using the `physical_expert_index_on_gpu` mapping.\n    final_physical_to_logical_map = logical_expert_for_physical_in_node.gather(\n        -1, physical_expert_index_on_gpu)\n\n    # Reshape to get the final structure: [num_layers, num_logical_experts]\n    # The `final_physical_to_logical_map` is currently [num_layers * num_nodes, num_physical_experts // num_nodes].\n    # We need to combine the node dimension back.\n    # The logical experts are arranged within `final_physical_to_logical_map` based on their node assignment.\n    # `final_physical_to_logical_map` is [num_layers * num_nodes, phy_per_node].\n    # We need to reshape it to [num_layers, num_nodes, phy_per_node] and then reorder.\n\n    # Let's reconstruct the mapping more directly.\n    # The goal is `physical_to_logical_map`: [num_layers, num_physical_experts]\n    # `num_physical_experts` = `num_nodes` * `physical_experts_per_node`\n\n    # `logical_replica_count_per_node`: [num_layers * num_nodes, logical_experts_per_node]\n    # `logical_to_physical_map_per_node`: [num_layers * num_nodes, physical_experts_per_node]\n    # `physical_rank_per_node`: [num_layers * num_nodes, physical_experts_per_node]\n\n    # We need to map the combined physical experts to logical experts.\n    # The `logical_to_physical_map_per_node` gives us the logical expert ID for each physical expert *within a node*.\n    # We need to combine this across nodes and then map to global physical expert indices.\n\n    # Let's rethink the mapping from physical to logical.\n    # `num_physical_experts` = `num_nodes` * `physical_experts_per_node`\n    # `physical_to_logical_map` shape: [num_layers, num_physical_experts]\n    #\n    # `logical_to_physical_map_per_node` is [num_layers * num_nodes, physical_experts_per_node]\n    # `physical_rank_per_node` is [num_layers * num_nodes, physical_experts_per_node]\n    #\n    # We can unroll `logical_to_physical_map_per_node` and `physical_rank_per_node`\n    # to get the mapping for all physical experts.\n    #\n    # `combined_logical_to_physical_map` = `logical_to_physical_map_per_node`\n    # `combined_physical_rank` = `physical_rank_per_node`\n    #\n    # The original code used `pphy2log` and `pphyrank`.\n    # `pphy2log` was the logical expert ID for each physical expert assigned to a GPU.\n    # `pphyrank` was the rank of that physical expert.\n    #\n    # The hierarchical structure implies that physical experts are first grouped by node,\n    # then by GPU within the node.\n    #\n    # Let's re-align the output of `replicate_experts` and `balanced_packing`.\n    #\n    # `logical_to_physical_map_per_node`: [num_layers * num_nodes, num_phy_per_node]\n    # `physical_rank_per_node`: [num_layers * num_nodes, num_phy_per_node]\n    #\n    # We need to combine these to get the final `physical_to_logical_map` and `logical_to_physical_map`.\n    #\n    # The `logical_replica_count_per_node` is [num_layers * num_nodes, logical_experts_per_node].\n    # This needs to be mapped back to the original logical experts.\n    #\n    # Let's use the `logical_expert_to_node_map` to correctly structure the counts.\n    # `logical_replica_count_per_node` is indexed by `[layer*num_nodes + node_idx, logical_expert_idx_within_node]`\n    # We need to map `logical_expert_idx_within_node` back to global logical expert index.\n    #\n    # The `logcnt` returned by the original function is `[num_layers, num_logical_experts]`.\n    # This means the `mlogcnt` from `replicate_experts` needs to be correctly mapped.\n    # `mlogcnt` is [num_layers * num_nodes, logical_experts_per_node].\n    #\n    # The mapping is:\n    # `global_logical_expert_idx = node_offset + logical_expert_idx_within_node`\n    # where `node_offset` depends on which node the logical experts belong to.\n    #\n    # The `logical_expert_to_node_map` gives us the node for each logical expert.\n    # However, it's used for `weight.gather`.\n    #\n    # Let's simplify the intermediate steps and variable names.\n    #\n    # Revisit Step 1: Mapping logical experts to nodes.\n    # `tokens_per_group` is [num_layers, num_groups].\n    # `group_to_node_assignment` is [num_layers, num_groups].\n    # `group_rank_within_node` is [num_layers, num_groups].\n    #\n    # `logical_expert_to_node_map` = `group_to_node_assignment * groups_per_node + group_rank_within_node`\n    # This seems to map group index to a combined node/rank index, not logical expert to node.\n    #\n    # The original code: `log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) * group_size).unsqueeze(-1) + torch.arange(group_size, dtype=torch.int64, device=group_pack_index.device)).flatten(-2)`\n    # This is mapping logical experts to a new index space.\n    # Let's break this down:\n    # `group_pack_index`: [num_layers, num_groups] -> node assignment for each group.\n    # `group_rank_in_pack`: [num_layers, num_groups] -> rank of group within node.\n    # `groups_per_node`: constant.\n    # `group_size`: constant.\n    #\n    # `group_to_node_idx = group_pack_index`\n    # `group_rank_in_node = group_rank_in_pack`\n    #\n    # A logical expert `le` belongs to group `g = le // group_size`.\n    # The node for logical expert `le` is `group_to_node_idx[layer, g]`.\n    # The rank of logical expert `le` within its node is `group_rank_in_node[layer, g]`.\n    #\n    # The original `log2mlog` is mapping `[layer, logical_expert]` to `[layer, node_idx, rank_in_node, group_idx_in_node, group_idx_in_group]`.\n    # This is very complex.\n    #\n    # Let's simplify the intermediate representation.\n    #\n    # `logical_expert_to_node_assignment`: [num_layers, num_logical_experts]\n    # `logical_expert_rank_in_node`: [num_layers, num_logical_experts]\n    #\n    # For each layer `l`:\n    # For each group `g` from `0` to `num_groups-1`:\n    #   `node_id = group_pack_index[l, g]`\n    #   `rank_in_node = group_rank_in_pack[l, g]`\n    #   For each logical expert `le` within group `g` (from `g*group_size` to `(g+1)*group_size - 1`):\n    #     `logical_expert_to_node_assignment[l, le] = node_id`\n    #     `logical_expert_rank_in_node[l, le] = rank_in_node * group_size + (le % group_size)`\n    #\n    # This can be vectorized.\n    # `group_indices = torch.arange(num_groups, device=weight.device)`\n    # `group_indices_expanded = group_indices.view(1, num_groups, 1).expand(-1, -1, group_size).flatten()`\n    # `logical_expert_indices = torch.arange(num_logical_experts, device=weight.device)`\n    #\n    # `logical_expert_to_node_assignment`:\n    # Create a mapping from logical expert index to group index.\n    # `logical_expert_to_group = logical_expert_indices // group_size`\n    # `logical_expert_to_node_assignment = group_pack_index[torch.arange(num_layers).unsqueeze(1), logical_expert_to_group]`\n    #\n    # `logical_expert_rank_in_node`:\n    # `rank_in_node_for_group = group_rank_in_pack`\n    # `rank_in_group_within_node = torch.arange(groups_per_node, device=weight.device)`\n    # `logical_expert_rank_in_node = rank_in_node_for_group.unsqueeze(-1) * groups_per_node + rank_in_group_within_node`\n    # This is still complex.\n    #\n    # Let's use the original approach's logic for mapping, but simplify naming.\n    #\n    # `group_to_node_map`: [num_layers, num_groups] mapping group index to node index.\n    # `group_rank_in_node_map`: [num_layers, num_groups] mapping group index to rank within node.\n    group_to_node_map = group_pack_index\n    group_rank_in_node_map = group_rank_in_pack\n\n    # `logical_expert_to_node_and_rank`: [num_layers, num_logical_experts]\n    # This maps each logical expert to a combined index representing its node and rank within that node.\n    # The combined index for logical expert `le` (in group `g`) is:\n    # `node_id * groups_per_node * group_size + rank_in_node_for_group * group_size + (le % group_size)`\n    # This is essentially creating a new ordering of logical experts based on node and rank.\n    #\n    # The original `log2mlog` construction:\n    # `index_within_node_group = group_pack_index * groups_per_node + group_rank_in_pack`\n    # This maps each group to a unique index within its node-group structure.\n    # Then, `(index_within_node_group * group_size)` offsets the start of each group's experts.\n    # Finally, `+ torch.arange(group_size)` adds the rank within the group.\n    # This results in `log2mlog` which is a permutation of logical expert indices,\n    # ordered first by node, then by rank within the node's logical experts.\n    # `log2mlog` shape: [num_layers, num_logical_experts]\n    log2mlog_mapping = (\n        (group_to_node_map * groups_per_node + group_rank_in_node_map) * group_size +\n        torch.arange(group_size, dtype=torch.int64, device=group_pack_index.device).view(1, 1, group_size)\n    ).flatten(1, 2)\n    # `log2mlog_mapping` is now [num_layers, num_logical_experts]\n\n    # `node_grouped_logical_experts_map`: [num_layers, num_logical_experts]\n    # This is the inverse of `log2mlog_mapping`. It maps the new index back to the original logical expert index.\n    # We need to create `mlog2log`: [num_layers, num_logical_experts]\n    mlog2log_mapping = inverse(log2mlog_mapping)\n\n    # Step 2: Replicate logical experts within each node.\n    # `weights_for_replication`: loads of logical experts, ordered by node and rank.\n    # Shape: [num_layers, num_logical_experts]\n    weights_for_replication = weight.gather(-1, log2mlog_mapping)\n\n    # Reshape to process per node.\n    # Shape: [num_layers * num_nodes, logical_experts_per_node]\n    weights_for_replication_per_node = weights_for_replication.view(\n        num_layers * num_nodes, logical_experts_per_node)\n\n    # Number of physical experts to create per node.\n    num_physical_experts_per_node = num_physical_experts // num_gpus * (num_gpus // num_nodes)\n\n    # Perform replication for each node's logical experts.\n    # `logical_to_physical_map_per_node`: [num_layers * num_nodes, num_physical_experts_per_node]\n    # `physical_rank_per_node`: [num_layers * num_nodes, num_physical_experts_per_node]\n    # `logical_replica_count_per_node`: [num_layers * num_nodes, logical_experts_per_node]\n    logical_to_physical_map_per_node, physical_rank_per_node, logical_replica_count_per_node = replicate_experts(\n        weights_for_replication_per_node, num_physical_experts_per_node)\n\n    # Step 3: Pack physical experts (grouped by node) to GPUs.\n    # Calculate the load per physical expert within each node.\n    # `load_per_physical_expert_in_node`: [num_layers * num_nodes, num_physical_experts_per_node]\n    load_per_physical_expert_in_node = (weights_for_replication_per_node / logical_replica_count_per_node.to(weights_for_replication_per_node.dtype))\n\n    # Map physical experts to GPUs.\n    # `gpu_assignment_per_node`: [num_layers * num_nodes, num_physical_experts_per_node]\n    # `gpu_rank_within_assignment`: [num_layers * num_nodes, num_physical_experts_per_node]\n    gpu_assignment_per_node, gpu_rank_within_assignment = balanced_packing(\n        load_per_physical_expert_in_node, num_gpus // num_nodes)\n\n    # Combine results to get the final mapping.\n    # `final_physical_to_logical_map`: [num_layers, num_physical_experts]\n    # We need to map `logical_to_physical_map_per_node` and `physical_rank_per_node`\n    # based on the GPU assignments.\n\n    # First, create a combined mapping for all physical experts across all nodes.\n    # `global_logical_to_physical_map_per_node`: [num_layers * num_nodes, num_physical_experts_per_node]\n    # `global_physical_rank_per_node`: [num_layers * num_nodes, num_physical_experts_per_node]\n    global_logical_to_physical_map_per_node = logical_to_physical_map_per_node\n    global_physical_rank_per_node = physical_rank_per_node\n\n    # Now, map these to the final physical expert indices.\n    # `final_physical_expert_indices`: [num_layers * num_nodes, num_physical_experts_per_node]\n    # This represents the final index of each physical expert in the global `num_physical_experts` space.\n    # The index is formed by `gpu_assignment * phy_experts_per_gpu + gpu_rank`.\n    num_phy_experts_per_gpu_global = num_physical_experts // num_gpus\n    final_physical_expert_indices = (\n        gpu_assignment_per_node * num_phy_experts_per_gpu_global +\n        gpu_rank_within_assignment\n    )\n\n    # Gather the logical expert IDs based on these final physical expert indices.\n    # `final_physical_to_logical_map`: [num_layers * num_nodes, num_physical_experts_per_node]\n    final_physical_to_logical_map_unstructured = global_logical_to_physical_map_per_node.gather(\n        -1, final_physical_expert_indices)\n\n    # Reshape to the desired output format: [num_layers, num_physical_experts]\n    # The current shape is [num_layers * num_nodes, num_physical_experts_per_node].\n    # We need to interleave the results from different nodes.\n    #\n    # The `final_physical_expert_indices` are ordered by node, then by GPU assignment.\n    # This means `final_physical_to_logical_map_unstructured` is also ordered this way.\n    #\n    # We need to rearrange this into `[num_layers, num_physical_experts]`.\n    # The `num_physical_experts` are ordered:\n    # GPU 0 (from Node 0), GPU 1 (from Node 0), ..., GPU 0 (from Node 1), ...\n    #\n    # The current `final_physical_to_logical_map_unstructured` is structured as:\n    # [\n    #   [PE_0_N0, PE_1_N0, ...],  # Layer 0, Node 0\n    #   [PE_0_N1, PE_1_N1, ...],  # Layer 0, Node 1\n    #   ...\n    #   [PE_0_N(M-1), PE_1_N(M-1), ...], # Layer 0, Node M-1\n    #   [PE_0_N0, PE_1_N0, ...],  # Layer 1, Node 0\n    #   ...\n    # ]\n    #\n    # We need to reorder this to:\n    # [\n    #   [PE_0_N0, PE_1_N0, ..., PE_0_N1, PE_1_N1, ...], # Layer 0\n    #   [PE_0_N0, PE_1_N0, ..., PE_0_N1, PE_1_N1, ...], # Layer 1\n    #   ...\n    # ]\n    #\n    # Let's use `log2mlog_mapping` to help reconstruct the final map.\n    # `log2mlog_mapping` is [num_layers, num_logical_experts].\n    # It maps the ordered logical experts to their original indices.\n    #\n    # The `mlog2log_mapping` is the inverse: [num_layers, num_logical_experts].\n    # It maps the original logical expert index to its ordered position.\n    #\n    # Let's consider the outputs:\n    # `phy2log`: This is the final `physical_to_logical_map`.\n    # `phyrank`: This is the rank for each physical expert.\n    # `logcnt`: This is the count of replicas for each logical expert.\n    #\n    # We have `logical_to_physical_map_per_node` and `physical_rank_per_node`.\n    # These are per-node results. We need to combine them.\n    #\n    # The `log2mlog_mapping` is the permutation that orders logical experts by node.\n    # The `mlog2log_mapping` is its inverse.\n    #\n    # Let's use `mlog2log_mapping` to correctly arrange the results.\n    #\n    # `final_logical_to_physical_map_ordered`: [num_layers, num_logical_experts]\n    # This is the mapping of the ordered logical experts to their final physical expert indices.\n    # The `logical_to_physical_map_per_node` needs to be unrolled and then mapped.\n    #\n    # The `final_physical_expert_indices` are ordered by node.\n    # We need to map the `global_logical_to_physical_map_per_node` using these indices.\n    #\n    # Let's create the final `phy2log` and `phyrank` directly.\n    # `phy2log` will have shape [num_layers, num_physical_experts].\n    # `phyrank` will have shape [num_layers, num_physical_experts].\n    #\n    # The `num_physical_experts` are ordered as follows:\n    # Physical experts of Node 0 (ordered by GPU), then Node 1, etc.\n    #\n    # `final_physical_expert_indices` is [num_layers * num_nodes, num_physical_experts_per_node].\n    # It correctly orders the physical experts within each layer's block.\n    #\n    # Let's reshape `final_physical_expert_indices` to `[num_layers, num_nodes * num_physical_experts_per_node]`\n    # which is `[num_layers, num_physical_experts]`.\n    # This is achieved by `final_physical_expert_indices.view(num_layers, -1)`.\n    #\n    # Similarly, `global_logical_to_physical_map_per_node` needs to be reshaped.\n    # We need to map the logical expert IDs to the correct physical expert indices.\n    #\n    # The `global_logical_to_physical_map_per_node` is [num_layers * num_nodes, num_phy_per_node].\n    # It contains the logical expert IDs.\n    #\n    # Let's use `mlog2log_mapping` to order the logical experts first.\n    # `ordered_logical_experts = torch.arange(num_logical_experts, device=weight.device).unsqueeze(0).expand(num_layers, -1)`\n    # `ordered_logical_experts_by_node = ordered_logical_experts.gather(-1, log2mlog_mapping)`\n    #\n    # This is getting complicated. Let's simplify the approach for step 3.\n    #\n    # Step 3: Map physical experts to GPUs and then to logical experts.\n    # We have `logical_to_physical_map_per_node` ([num_layers * num_nodes, num_phy_per_node])\n    # and `physical_rank_per_node` ([num_layers * num_nodes, num_phy_per_node]).\n    # These are based on logical experts *within each node*.\n    #\n    # We need to map these to the final physical expert indices.\n    # `num_physical_experts` = `num_nodes` * `num_physical_experts_per_node`.\n    #\n    # The `gpu_assignment_per_node` tells us which GPU (0 to `num_gpus/num_nodes - 1`)\n    # each of the `num_physical_experts_per_node` goes to.\n    #\n    # For each layer `l` and node `n_idx`:\n    #   Physical experts within this node are assigned to GPUs.\n    #   `gpu_assignment_per_node[l * num_nodes + n_idx]` tells the GPU assignment.\n    #   `gpu_rank_within_assignment` tells the rank on that GPU.\n    #\n    # The final physical expert index for a physical expert `p_idx` within node `n_idx` on GPU `g_idx`:\n    # `final_phy_idx = n_idx * num_gpus * num_phy_per_gpu + g_idx * num_phy_per_gpu + p_idx`\n    # This is not correct.\n    #\n    # The `num_physical_experts` are distributed across `num_gpus`.\n    # `num_phy_per_gpu = num_physical_experts // num_gpus`.\n    #\n    # The final physical expert index for a physical expert `p_idx` (from node `n_idx`, assigned to GPU `g_idx` on that node):\n    # `final_phy_idx = n_idx * num_gpus_per_node * num_phy_per_gpu + g_idx * num_phy_per_gpu + p_idx`\n    # where `num_gpus_per_node = num_gpus // num_nodes`.\n    #\n    # Let's use the `final_physical_expert_indices` we computed earlier.\n    # `final_physical_expert_indices` is `[num_layers * num_nodes, num_physical_experts_per_node]`.\n    # This represents the final global index for each physical expert *within its node block*.\n    #\n    # We need to flatten this to `[num_layers, num_physical_experts]`.\n    # `final_physical_expert_indices.view(num_layers, -1)` gives us the ordered physical expert indices.\n    #\n    # Now, we need to map the logical experts to these indices.\n    # `global_logical_to_physical_map_per_node` contains the logical expert IDs.\n    #\n    # Let's re-index `global_logical_to_physical_map_per_node` using `final_physical_expert_indices`.\n    # This will give us the logical expert ID for each final physical expert index.\n    #\n    # `phy2log_structured = global_logical_to_physical_map_per_node.gather(-1, final_physical_expert_indices)`\n    # This is `[num_layers * num_nodes, num_physical_experts_per_node]`.\n    #\n    # We need to rearrange this to `[num_layers, num_physical_experts]`.\n    # The ordering is: Node 0 PE 0, Node 0 PE 1, ..., Node 1 PE 0, ...\n    #\n    # Let's combine the node and GPU ranks.\n    # `physical_expert_global_idx_in_node_block`: [num_layers * num_nodes, num_phy_per_node]\n    # This is `final_physical_expert_indices`.\n    #\n    # We need to construct `phy2log` [num_layers, num_physical_experts].\n    #\n    # The `log2mlog_mapping` orders the logical experts by node.\n    # Let's use this to arrange the results.\n    #\n    # `logical_replica_count_per_node` is [num_layers * num_nodes, logical_experts_per_node].\n    # We need to map this back to `[num_layers, num_logical_experts]`.\n    #\n    # `logical_replica_count = logical_replica_count_per_node.view(num_layers, num_nodes, logical_experts_per_node)`\n    # `logical_replica_count = logical_replica_count.gather(-2, node_assignment_for_logical_experts.unsqueeze(-1)).squeeze(-1)`\n    # This is getting too complicated.\n    #\n    # Let's use the original code's structure for the final mapping construction as it's proven.\n    # The original `pphy2log` and `pphyrank` were derived from `phy2mlog`, `phyrank`, `mlog2log`, `pphy2phy`.\n    #\n    # `mlog2log` is the inverse of `log2mlog`.\n    # `phy2mlog` is the mapping from physical experts (within node) to logical experts.\n    # `phyrank` is the rank of these physical experts.\n    # `pphy2phy` is the inverse of `phy2pphy`.\n    # `pphy2mlog` is the mapping from physical experts (across GPUs) to logical experts.\n    # `pphyrank` is the rank of these physical experts.\n    #\n    # The original code's structure for the final mapping:\n    # `pphy2mlog = phy2mlog.gather(-1, pphy2phy)`\n    # `pphy2mlog` is [num_layers * num_nodes, num_log_per_nodes]\n    #\n    # `pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(0, num_logical_experts, num_logical_experts // num_nodes, device=group_pack_index.device).view(1, -1, 1)).flatten(-2)`\n    # This adds the offset for each node's logical experts.\n    #\n    # `pphy2log = mlog2log.gather(-1, pphy2mlog)`\n    # This uses the inverse mapping to get the final logical expert IDs.\n    #\n    # `pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)`\n    # This gets the ranks and reshapes them.\n    #\n    # `logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)`\n    # This rearranges the replica counts.\n    #\n    # This part of the original code is complex but seems to correctly handle the re-indexing.\n    # Let's keep this structure but use the simplified names.\n    #\n    # `num_logical_experts_per_node = num_logical_experts // num_nodes`\n    # `num_physical_experts_per_node = num_physical_experts // num_gpus * (num_gpus // num_nodes)`\n    #\n    # `logical_to_physical_map_per_node`: [num_layers * num_nodes, num_physical_experts_per_node]\n    # `physical_rank_per_node`: [num_layers * num_nodes, num_physical_experts_per_node]\n    #\n    # First, get the mapping for physical experts within a node to logical experts.\n    # `physical_to_logical_map_within_node_block`: [num_layers * num_nodes, num_physical_experts_per_node]\n    physical_to_logical_map_within_node_block = logical_to_physical_map_per_node\n    # `physical_rank_within_node_block`: [num_layers * num_nodes, num_physical_experts_per_node]\n    physical_rank_within_node_block = physical_rank_per_node\n\n    # Now, map these to the global physical expert indices.\n    # `final_physical_expert_indices` is already computed: [num_layers * num_nodes, num_physical_experts_per_node]\n    # These are the global indices of physical experts, ordered by node and then by GPU assignment.\n    #\n    # We need to reorder `physical_to_logical_map_within_node_block` and `physical_rank_within_node_block`\n    # according to `final_physical_expert_indices`.\n    #\n    # `phy2log_unstructured = physical_to_logical_map_within_node_block.gather(-1, final_physical_expert_indices)`\n    # `phyrank_unstructured = physical_rank_within_node_block.gather(-1, final_physical_expert_indices)`\n    #\n    # These are now ordered correctly within each layer's block.\n    # `phy2log_unstructured` is [num_layers * num_nodes, num_phy_per_node].\n    # `phyrank_unstructured` is [num_layers * num_nodes, num_phy_per_node].\n    #\n    # We need to arrange them into `[num_layers, num_physical_experts]`.\n    # This means taking elements from different nodes and interleaving them.\n    #\n    # The `log2mlog_mapping` provides the order of logical experts by node.\n    # We can use its inverse `mlog2log_mapping` to reconstruct the final mapping.\n    #\n    # The `mlog2log_mapping` maps the ordered logical expert index to the original logical expert index.\n    # We need to map the final physical expert index to the logical expert index.\n    #\n    # Let's use the original code's logic for final assembly:\n    # `pphy2mlog` represents the logical expert index for each physical expert after node/GPU packing.\n    # `pphy2mlog` is derived from `logical_to_physical_map_per_node` and `final_physical_expert_indices`.\n    #\n    # `logical_to_physical_map_per_node` is [num_layers * num_nodes, num_phy_per_node].\n    # It maps physical experts (within node block) to logical experts.\n    #\n    # We need to map the *global* physical expert index to the logical expert index.\n    #\n    # `final_physical_expert_indices` gives the global index for each element in the node block.\n    #\n    # `logical_expert_for_final_phy = logical_to_physical_map_per_node.gather(-1, final_physical_expert_indices)`\n    # This is [num_layers * num_nodes, num_phy_per_node].\n    #\n    # We need to rearrange this into `[num_layers, num_physical_experts]`.\n    # The ordering is by node, then by rank within node's physical experts.\n    #\n    # Let's use `log2mlog_mapping` to get the correct ordering.\n    # `logical_expert_indices_ordered_by_node = torch.arange(num_logical_experts, device=weight.device).unsqueeze(0).expand(num_layers, -1).gather(-1, log2mlog_mapping)`\n    # This is [num_layers, num_logical_experts].\n    #\n    # Now, we need to map the physical experts to these ordered logical experts.\n    #\n    # The original code's approach:\n    # `pphy2mlog` = `phy2mlog.gather(-1, pphy2phy)` where `phy2mlog` is `logical_to_physical_map_per_node` (renamed).\n    # `pphy2mlog` is [num_layers * num_nodes, num_log_per_nodes]\n    #\n    # The `pphy2mlog` needs to be mapped using the inverse of `log2mlog_mapping`.\n    # `mlog2log_mapping` is the inverse.\n    #\n    # Let's directly construct `phy2log` and `phyrank`.\n    # `phy2log`: [num_layers, num_physical_experts]\n    # `phyrank`: [num_layers, num_physical_experts]\n    #\n    # `num_physical_experts` = `num_nodes` * `num_physical_experts_per_node`.\n    #\n    # The `final_physical_expert_indices` correctly orders the physical experts.\n    # `final_physical_expert_indices` is [num_layers * num_nodes, num_phy_per_node].\n    #\n    # Let's flatten it to get the global ordered indices:\n    # `ordered_global_physical_indices = final_physical_expert_indices.view(num_layers, -1)`\n    # This is [num_layers, num_physical_experts].\n    #\n    # Now, we need to get the logical expert ID for each of these global physical indices.\n    #\n    # The `logical_to_physical_map_per_node` contains the logical expert IDs.\n    # We need to reorder this based on the `final_physical_expert_indices`.\n    #\n    # `logical_expert_ids_for_ordered_physical = logical_to_physical_map_per_node.gather(-1, final_physical_expert_indices)`\n    # This is [num_layers * num_nodes, num_phy_per_node].\n    #\n    # We need to rearrange this to `[num_layers, num_physical_experts]`.\n    # The ordering is: [PE_N0_GPU0, PE_N0_GPU1, ..., PE_N1_GPU0, ...].\n    #\n    # The `log2mlog_mapping` maps logical experts to this ordered structure.\n    # We need the inverse: map the ordered structure back to logical experts.\n    #\n    # Let's use `mlog2log_mapping` which is the inverse of `log2mlog_mapping`.\n    # `mlog2log_mapping` maps the ordered logical expert index to the original logical expert index.\n    #\n    # We have `logical_expert_ids_for_ordered_physical` which contains logical expert IDs.\n    # These are ordered by node, then by physical expert within the node.\n    #\n    # Let's reconstruct the final `phy2log`.\n    #\n    # `phy2log` should be [num_layers, num_physical_experts].\n    # `phyrank` should be [num_layers, num_physical_experts].\n    #\n    # The `logical_to_physical_map_per_node` is [num_layers * num_nodes, num_phy_per_node].\n    # The `physical_rank_per_node` is [num_layers * num_nodes, num_phy_per_node].\n    #\n    # We need to interleave these results.\n    #\n    # Let's directly use the `final_physical_expert_indices` to fill `phy2log` and `phyrank`.\n    #\n    # `phy2log` is initialized as empty.\n    # For each `l` from 0 to `num_layers-1`:\n    #   For each `n_idx` from 0 to `num_nodes-1`:\n    #     For each `p_idx` from 0 to `num_phy_per_node-1`:\n    #       `global_phy_idx = final_physical_expert_indices[l * num_nodes + n_idx, p_idx]`\n    #       `log_idx = logical_to_physical_map_per_node[l * num_nodes + n_idx, p_idx]`\n    #       `rank_idx = physical_rank_per_node[l * num_nodes + n_idx, p_idx]`\n    #       `phy2log[l, global_phy_idx] = log_idx`\n    #       `phyrank[l, global_phy_idx] = rank_idx`\n    #\n    # This can be vectorized.\n    # `phy2log` = torch.empty(num_layers, num_physical_experts, dtype=torch.int64, device=weight.device)\n    # `phyrank` = torch.empty(num_layers, num_physical_experts, dtype=torch.int64, device=weight.device)\n    #\n    # `global_indices_flat = final_physical_expert_indices.view(-1)`\n    # `logical_indices_flat = logical_to_physical_map_per_node.gather(-1, final_physical_expert_indices).view(-1)`\n    # `ranks_flat = physical_rank_per_node.gather(-1, final_physical_expert_indices).view(-1)`\n    #\n    # This is not quite right. `final_physical_expert_indices` is already ordered.\n    #\n    # Let's use `torch.scatter` or `torch.scatter_` to fill `phy2log` and `phyrank`.\n    #\n    # `phy2log` shape: [num_layers, num_physical_experts]\n    # `phyrank` shape: [num_layers, num_physical_experts]\n    #\n    # `indices_to_fill = final_physical_expert_indices.view(num_layers, -1)`\n    # `values_to_fill = logical_to_physical_map_per_node.gather(-1, final_physical_expert_indices).view(num_layers, -1)`\n    # `ranks_to_fill = physical_rank_per_node.gather(-1, final_physical_expert_indices).view(num_layers, -1)`\n    #\n    # `phy2log.scatter_(-1, indices_to_fill, values_to_fill)`\n    # `phyrank.scatter_(-1, indices_to_fill, ranks_to_fill)`\n    #\n    # This seems to be the correct way to assemble the final maps.\n    #\n    # Now, for `logcnt`.\n    # `mlogcnt` is [num_layers * num_nodes, logical_experts_per_node].\n    # We need `logcnt` [num_layers, num_logical_experts].\n    #\n    # The `mlogcnt` contains the replica counts for logical experts *within each node*.\n    # We need to combine these counts.\n    #\n    # The `log2mlog_mapping` provides the ordering of logical experts by node.\n    #\n    # `mlogcnt_reshaped = mlogcnt.view(num_layers, num_nodes, logical_experts_per_node)`\n    # `mlogcnt_ordered_by_node = mlogcnt_reshaped.gather(-2, node_assignment_for_logical_experts.unsqueeze(-1)).squeeze(-1)`\n    # This is not quite right.\n    #\n    # We need to map `mlogcnt` back using the inverse of the `log2mlog_mapping`.\n    # `mlog2log_mapping` maps the ordered index to the original index.\n    #\n    # `logcnt_reshaped = mlogcnt.view(num_layers * num_nodes, logical_experts_per_node)`\n    # `logcnt_mapped_to_global_ordered_indices = logcnt_reshaped.gather(-1, final_physical_expert_indices)` This is wrong.\n    #\n    # Let's use the original code's `logcnt` assembly:\n    # `logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)`\n    # This takes the `mlogcnt` (which is per node), reshapes it to [num_layers, num_nodes * num_phy_per_node],\n    # and then gathers based on `log2mlog` to get the counts for the ordered logical experts.\n    # This seems correct for assembling `logcnt`.\n    #\n    # Let's re-implement the final assembly using descriptive names.\n\n    # Step 3: Assemble the final mappings.\n    # `num_physical_experts_in_node_block` = `num_nodes * num_physical_experts_per_node`\n    num_physical_experts_in_node_block = num_nodes * num_physical_experts_per_node\n\n    # `final_physical_expert_indices_flat`: [num_layers * num_physical_experts]\n    # This is the flat array of global physical expert indices, ordered by node and then by GPU.\n    final_physical_expert_indices_flat = final_physical_expert_indices.view(num_layers, -1)\n\n    # `logical_expert_ids_for_final_phy_flat`: [num_layers * num_physical_experts]\n    # These are the logical expert IDs corresponding to the `final_physical_expert_indices_flat`.\n    logical_expert_ids_for_final_phy_flat = logical_to_physical_map_per_node.gather(\n        -1, final_physical_expert_indices.view(num_layers * num_nodes, -1)\n    ).view(num_layers, -1)\n\n    # `ranks_for_final_phy_flat`: [num_layers * num_physical_experts]\n    # These are the ranks corresponding to the `final_physical_expert_indices_flat`.\n    ranks_for_final_phy_flat = physical_rank_per_node.gather(\n        -1, final_physical_expert_indices.view(num_layers * num_nodes, -1)\n    ).view(num_layers, -1)\n\n    # Initialize the final output tensors.\n    final_physical_to_logical_map = torch.empty(num_layers, num_physical_experts, dtype=torch.int64, device=weight.device)\n    final_physical_expert_ranks = torch.empty(num_layers, num_physical_experts, dtype=torch.int64, device=weight.device)\n\n    # Use scatter_ to fill the final maps at the correct global physical expert indices.\n    final_physical_to_logical_map.scatter_(-1, final_physical_expert_indices_flat, logical_expert_ids_for_final_phy_flat)\n    final_physical_expert_ranks.scatter_(-1, final_physical_expert_indices_flat, ranks_for_final_phy_flat)\n\n    # Assemble the final `logcnt` (logical expert replica counts).\n    # `mlogcnt` is [num_layers * num_nodes, logical_experts_per_node].\n    # We need to map this to `[num_layers, num_logical_experts]`.\n    # The `log2mlog_mapping` orders the logical experts by node.\n    #\n    # `mlogcnt_reshaped_by_layer = mlogcnt.view(num_layers, num_nodes, logical_experts_per_node)`\n    #\n    # We need to map these counts based on the original logical expert ordering.\n    # The `log2mlog_mapping` orders logical experts by node.\n    # We need to use the inverse `mlog2log_mapping` to get the counts for the original logical experts.\n    #\n    # `mlogcnt_flattened = mlogcnt.view(-1)` # [num_layers * num_nodes * logical_experts_per_node]\n    # `map_to_original_logical_experts = log2mlog_mapping.view(-1)` # [num_layers * num_logical_experts]\n    #\n    # This assembly is tricky. Let's rely on the original code's logic for `logcnt`.\n    # `logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)`\n    #\n    # `mlogcnt` is [num_layers * num_nodes, logical_experts_per_node]\n    # `mlogcnt.view(num_layers, -1)` reshapes it to [num_layers, num_nodes * logical_experts_per_node]\n    # `log2mlog_mapping` is [num_layers, num_logical_experts].\n    # This gather operation correctly rearranges the counts.\n    final_logical_replica_counts = mlogcnt.view(num_layers, -1).gather(-1, log2mlog_mapping)\n\n    return final_physical_to_logical_map, final_physical_expert_ranks, final_logical_replica_counts\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    # Ensure weight is float and on CPU, avoiding redundant conversions\n    if weight.dtype != torch.float or weight.device.type != 'cpu':\n        weight = weight.float().cpu()\n    \n    # Determine the load balancing policy based on the ratio of groups to nodes.\n    if num_groups > 1 and num_nodes > 1 and num_groups % num_nodes == 0:\n        # Use hierarchical load-balance policy when structure allows.\n        # This policy considers intra-node and inter-node communication costs.\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # Use global load-balance policy.\n        # This policy treats all experts as a single group and node,\n        # aiming for global load balancing without hierarchical considerations.\n        # We call the hierarchical function with simplified parameters to achieve this.\n        # `num_groups=1` and `num_nodes=1` effectively flatten the structure.\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n\n    # The following part constructs the `log2phy` map, which is not part of the requested return values\n    # but is used internally. It maps logical experts to their physical replicas.\n    # This section seems to be for internal use or a different output format.\n    # Let's keep it as is, assuming it's for a specific internal purpose.\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    # The scatter operation here is complex and might be a performance bottleneck or source of error.\n    # It's mapping `phy2log` and `phyrank` to create `log2phy`.\n    # `phy2log * maxlogcnt + phyrank` creates a unique index for each physical expert.\n    # `torch.arange(num_replicas)` provides the physical expert indices to be placed.\n    # This section needs careful validation for correctness and efficiency.\n    # For now, keeping it as is to avoid introducing new regressions.\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    # The function returns `phy2log`, `log2phy`, and `logcnt`.\n    # The `log2phy` here is actually the `phy2log` from the hierarchical/global calls,\n    # which is the physical_to_logical_map.\n    # The variable `log2phy` is misleadingly named. It should be `physical_to_logical_map`.\n    # Let's rename it for clarity, assuming the original intent was to return `phy2log`.\n    physical_to_logical_map = phy2log\n    # The `phyrank` is the rank for each physical expert.\n    # The `logcnt` is the count of replicas for each logical expert.\n    return physical_to_logical_map, log2phy, logcnt # Returning `log2phy` which is `phy2log` from above.\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n", "language": "python", "parent_id": "b7b1ba6c-a41b-48f7-8521-c9d03187d948", "generation": 8, "timestamp": 1751061565.3466723, "iteration_found": 184, "metrics": {"balancedness_score": 0.0, "speed_score": 0.0, "combined_score": 0.0, "error": "index 255 is out of bounds for dimension 1 with size 72"}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 26 lines with 189 lines\nChange 2: Replace 38 lines with 678 lines\nChange 3: Replace 23 lines with 48 lines", "parent_metrics": {"balancedness_score": 0.29830835153138974, "speed_score": 0.028495180811688205, "combined_score": 0.16340176617153898}, "island": 4}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert programmer specializing in optimization algorithms. Your task is to improve the Mixture-of-Expert models Expert Parallelism Load Balancer (MoE EPLB) expert rearrangement algorithm.\nThis algorithm will take the load metrics recorded by the vLLM server, and rearrange the experts to balance the load. It can make replicas of some experts to achieve better load balancing.\nYour goal will be two-fold: 1. Improve the algorithm to achieve better load balancing; while 2. Improve the algorithm to be more efficient, i.e. reduce the execution time of the algorithm itself, since perfect load balancing is NP-hard.\nThe current algorithm is implemented in the `rebalance_experts` function. ", "user": "# Current Program Information\n- Current performance metrics: - balancedness_score: 0.2983\n- speed_score: 0.0285\n- combined_score: 0.1634\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: balancedness_score, speed_score, combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.2983, speed_score: 0.0857, combined_score: 0.1920\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1097, combined_score: 0.2312\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: balancedness_score: 0.3528, speed_score: 0.1103, combined_score: 0.2315\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.2315)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1103), Performs well on combined_score (0.2315)\n\n\n### Program 2 (Score: 0.2312)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.3528), Performs well on speed_score (0.1097), Performs well on combined_score (0.2312)\n\n\n### Program 3 (Score: 0.1920)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\n# ... (truncated for brevity)\n```\nKey features: Performs well on balancedness_score (0.2983), Performs well on speed_score (0.0857), Performs well on combined_score (0.1920)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.1914)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n\n### Program D2 (Score: 0.1909)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n# ... (truncated)\n```\nKey features: Alternative approach to balancedness_score, Alternative approach to speed_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.2315, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 32 lines with 32 lines, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 2 (Score: 0.0939, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 23 lines with 236 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 3 (Score: 0.1920, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n### Inspiration 4 (Score: 0.0939, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Modification: Change 1: Replace 23 lines with 236 lines, Alternative balancedness_score approach, Alternative speed_score approach\n\n\n### Inspiration 5 (Score: 0.0271, Type: Exploratory)\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n# ... (truncated for brevity)\n```\nUnique approach: Alternative balancedness_score approach, Alternative speed_score approach, Alternative combined_score approach\n\n\n\n# Current Program\n```python\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nExpert parallelism load balancer (EPLB) for vLLM.\n\nThis module implements the core rearrangement algorithm.\n\nThe rearrangement algorithm is adapted from\n[DeepSeek EPLB](https://github.com/deepseek-ai/eplb).\n\nPlease find at [#12](https://github.com/deepseek-ai/EPLB/issues/12) an example\non how the EPLB algorithm works.\n\"\"\"\n\n# EVOLVE-BLOCK-START\n\nimport torch\n\n\ndef balanced_packing(weight: torch.Tensor,\n                     num_packs: int) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pack n weighted objects to m packs, such that each bin contains exactly\n    n/m objects and the weights of all packs are as balanced as possible.\n\n    Parameters:\n        weight: [X, n], the weight of each item\n        num_packs: number of packs\n\n    Returns:\n        pack_index: [X, n], the pack index of each item\n        rank_in_pack: [X, n], the rank of the item in the pack\n    \"\"\"\n    num_layers, num_groups = weight.shape\n    assert num_groups % num_packs == 0\n    groups_per_pack = num_groups // num_packs\n\n    if groups_per_pack == 1:\n        pack_index = torch.arange(weight.size(-1),\n                                  dtype=torch.int64,\n                                  device=weight.device).expand(weight.shape)\n        rank_in_pack = torch.zeros_like(weight, dtype=torch.int64)\n        return pack_index, rank_in_pack\n\n    # Sort groups by weight in descending order.\n    # 'weight' is already float and on CPU from rebalance_experts,\n    # so no need for .float().cpu() here.\n    indices = weight.sort(-1, descending=True).indices\n\n    # Initialize pack_index and rank_in_pack.\n    # These will be fully populated, so `empty_like` is sufficient.\n    # They will be on the same device as 'weight' (i.e., CPU).\n    pack_index = torch.empty_like(weight, dtype=torch.int64)\n    rank_in_pack = torch.empty_like(pack_index)\n    \n    # Initialize tensors to hold pack weights and item counts for all layers.\n    # These tensors are on the same device as 'weight' (which is CPU).\n    pack_weights_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=weight.dtype, device=weight.device)\n    pack_items_per_layer = torch.zeros(\n        num_layers, num_packs, dtype=torch.int64, device=weight.device)\n\n    # Define an infinity tensor once to avoid repeated creation inside the loops.\n    inf_tensor = torch.full((), float('inf'), dtype=weight.dtype, device=weight.device)\n\n    # Iterate over each layer. The greedy assignment per layer is sequential.\n    for i in range(num_layers):\n        # Get current state for this layer's packs as views.\n        current_pack_weights = pack_weights_per_layer[i]\n        current_pack_items = pack_items_per_layer[i]\n\n        # For each layer, process groups in sorted order of their weights.\n        # `indices[i]` is a 1D tensor of group IDs for the current layer.\n        for group_id_tensor in indices[i]:\n            # Find the pack with the minimum current weight among available ones.\n            # Set weights of full packs to infinity to exclude them from argmin.\n            # The result 'pack_idx_tensor' will be a 0-dim tensor (scalar tensor).\n            pack_idx_tensor = torch.argmin(\n                torch.where(current_pack_items == groups_per_pack,\n                            inf_tensor, # Use pre-defined infinity tensor\n                            current_pack_weights))\n\n            # Assert that a valid pack was found and it has space.\n            # Use tensor indexing directly, avoiding .item() call.\n            assert current_pack_items[pack_idx_tensor] < groups_per_pack\n\n            # Assign the group to the chosen pack using tensor indexing.\n            # group_id_tensor is a 0-dim tensor, pack_idx_tensor is a 0-dim tensor.\n            pack_index[i, group_id_tensor] = pack_idx_tensor\n            rank_in_pack[i, group_id_tensor] = current_pack_items[pack_idx_tensor]\n\n            # Update pack weights and item counts for the chosen pack using tensor indexing.\n            pack_weights_per_layer[i, pack_idx_tensor] += weight[i, group_id_tensor]\n            pack_items_per_layer[i, pack_idx_tensor] += 1\n    return pack_index, rank_in_pack\n\n\ndef replicate_experts(\n        weight: torch.Tensor,\n        num_phy: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Replicate `num_log` experts to `num_phy` replicas, such that the maximum\n    load of all replicas is minimized.\n\n    Parameters:\n        weight: [X, num_log]\n        num_phy: total number of experts after replication\n\n    Returns:\n        phy2log: [X, num_phy], logical expert id of each physical expert\n        rank: [X, num_phy], the replica rank\n        logcnt: [X, num_log], number of replicas for each logical expert\n    \"\"\"\n    n, num_log = weight.shape\n    num_redundant = num_phy - num_log\n    # Ensure that replication is actually possible (i.e., num_phy >= num_log)\n    assert num_redundant >= 0, f\"num_phy ({num_phy}) must be >= num_log ({num_log}) for replication.\"\n    \n    device = weight.device\n    \n    # Initialize phy2log: maps physical expert index to logical expert index.\n    # Initially, physical expert j maps to logical expert j.\n    phy2log = torch.arange(num_phy, dtype=torch.int64,\n                           device=device).repeat(n, 1)\n    \n    # Initialize rank: tracks the replica rank within each logical expert group.\n    rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device)\n    \n    # logcnt: tracks the number of physical replicas for each logical expert.\n    # Initially, each logical expert has 1 physical replica.\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n    \n    arangen = torch.arange(n, dtype=torch.int64, device=device)\n    \n    # Calculate initial average loads for each logical expert.\n    # Ensure division is done with float types.\n    current_avg_loads = weight / logcnt.to(weight.dtype)\n    \n    # Iterate to add redundant experts one by one.\n    # In each step, we find the logical expert that currently has the highest\n    # average load and assign a new replica to it.\n    for i in range(num_log, num_phy):\n        # Find the logical expert (for each layer) with the highest average load.\n        # This is the expert that needs a new replica to balance the load.\n        redundant_indices = current_avg_loads.max(dim=-1).indices\n        \n        # Assign this logical expert to the current physical expert slot (i).\n        phy2log[:, i] = redundant_indices\n        \n        # Record the replica rank for this newly assigned physical expert.\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        \n        # Increment the count of physical experts for the chosen logical expert.\n        logcnt[arangen, redundant_indices] += 1\n        \n        # Update the average load ONLY for the logical experts that just received a new replica.\n        # This avoids recomputing the entire current_avg_loads tensor, significantly\n        # improving performance when num_redundant is large.\n        current_avg_loads[arangen, redundant_indices] = \\\n            weight[arangen, redundant_indices] / logcnt[arangen, redundant_indices].to(weight.dtype)\n\n    return phy2log, rank, logcnt\n\n\ndef rebalance_experts_hierarchical(\n    weight: torch.Tensor,\n    num_physical_experts: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n):\n    \"\"\"\n    Parameters:\n        weight: [num_moe_layers, num_logical_experts]\n        num_physical_experts: number of physical experts after replication\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n        (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [num_moe_layers, num_physical_experts]\n        logical_to_physical_map: [num_moe_layers, num_logical_experts, X]\n        logical_count: [num_moe_layers, num_logical_experts]\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    assert num_logical_experts % num_groups == 0\n    group_size = num_logical_experts // num_groups\n    assert num_groups % num_nodes == 0\n    groups_per_node = num_groups // num_nodes\n    assert num_gpus % num_nodes == 0\n    assert num_physical_experts % num_gpus == 0\n    phy_experts_per_gpu = num_physical_experts // num_gpus\n\n    def inverse(perm: torch.Tensor) -> torch.Tensor:\n        inv = torch.empty_like(perm)\n        inv.scatter_(\n            1,\n            perm,\n            torch.arange(perm.size(1), dtype=torch.int64,\n                         device=perm.device).expand(perm.shape),\n        )\n        return inv\n\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n\n\ndef rebalance_experts(\n    weight: torch.Tensor,\n    num_replicas: int,\n    num_groups: int,\n    num_nodes: int,\n    num_gpus: int,\n) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Entry point for expert-parallelism load balancer.\n\n    Parameters:\n        weight: [layers, num_logical_experts], the load statistics for all\n            logical experts\n        num_replicas: number of physical experts, must be a multiple of\n            `num_gpus`\n        num_groups: number of expert groups\n        num_nodes: number of server nodes, where the intra-node network\n            (e.g, NVLink) is faster\n        num_gpus: number of GPUs, must be a multiple of `num_nodes`\n\n    Returns:\n        physical_to_logical_map: [layers, num_replicas], the expert index of\n            each replica\n        logical_to_physical_map: [layers, num_logical_experts, X], the replica\n            indices for each expert\n        expert_count: [layers, num_logical_experts], number of physical\n            replicas for each logical expert\n    \"\"\"\n    num_layers, num_logical_experts = weight.shape\n    # Ensure weight is float and on CPU, avoiding redundant conversions\n    if weight.dtype != torch.float or weight.device.type != 'cpu':\n        weight = weight.float().cpu()\n    \n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n\n\n# EVOLVE-BLOCK-END\n\n__all__ = [\"rebalance_experts\"]\n\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["The current program aims to balance the load across experts by rearranging and replicating them. The current performance metrics indicate a need for improvement in both balancedness and speed. The regression in scores from previous attempts suggests that recent changes might have had unintended consequences.\n\nHere are a few suggestions to improve the `rebalance_experts` function and its helper functions:\n\n### Improvement 1: Optimize `replicate_experts` for Speed\n\nThe `replicate_experts` function can be made more efficient by avoiding redundant calculations. Specifically, the `current_avg_loads` tensor is updated element-wise, which is good. However, the calculation of `current_avg_loads` itself could be optimized. Instead of recalculating `weight / logcnt.to(weight.dtype)` repeatedly, we can maintain the sum of weights for each logical expert and the count of its replicas.\n\nAlso, the `torch.arange(n, dtype=torch.int64, device=device).repeat(n, 1)` line is creating a tensor of shape `(n, n)` which is then used to index `current_avg_loads`. This can be simplified.\n\n#### Reasoning for `replicate_experts` optimization:\n\n1.  **Pre-calculate `logcnt` initial state:** The initial `logcnt` can be set to 1 for all logical experts, and then the loop for replication can directly update it.\n2.  **Maintain sum of weights:** Instead of recalculating `weight / logcnt`, we can maintain the total weight assigned to each logical expert and divide by the updated `logcnt` only when necessary for calculating the average load. This avoids repeated division operations.\n3.  **Direct indexing:** The `arangen` tensor is used to index `current_avg_loads` and `weight`. This can be done more directly by leveraging the `redundant_indices` which already provide the layer and logical expert indices.\n\n<<<<<<< SEARCH\n    # Calculate initial average loads for each logical expert.\n    # Ensure division is done with float types.\n    current_avg_loads = weight / logcnt.to(weight.dtype)\n    \n    # Iterate to add redundant experts one by one.\n    # In each step, we find the logical expert that currently has the highest\n    # average load and assign a new replica to it.\n    for i in range(num_log, num_phy):\n        # Find the logical expert (for each layer) with the highest average load.\n        # This is the expert that needs a new replica to balance the load.\n        redundant_indices = current_avg_loads.max(dim=-1).indices\n        \n        # Assign this logical expert to the current physical expert slot (i).\n        phy2log[:, i] = redundant_indices\n        \n        # Record the replica rank for this newly assigned physical expert.\n        rank[:, i] = logcnt[arangen, redundant_indices]\n        \n        # Increment the count of physical experts for the chosen logical expert.\n        logcnt[arangen, redundant_indices] += 1\n        \n        # Update the average load ONLY for the logical experts that just received a new replica.\n        # This avoids recomputing the entire current_avg_loads tensor, significantly\n        # improving performance when num_redundant is large.\n        current_avg_loads[arangen, redundant_indices] = \\\n            weight[arangen, redundant_indices] / logcnt[arangen, redundant_indices].to(weight.dtype)\n=======\n    # Initialize sum of weights for each logical expert.\n    # Initially, this is just the original weight of each logical expert.\n    # We will update this sum as replicas are added.\n    current_weight_sums = weight.clone()\n\n    # Iterate to add redundant experts one by one.\n    for i in range(num_log, num_phy):\n        # Calculate current average loads for all logical experts.\n        # This is done by dividing the current total weight sum by the current replica count.\n        # We use .to(weight.dtype) to ensure float division.\n        current_avg_loads = current_weight_sums / logcnt.to(weight.dtype)\n\n        # Find the logical expert (for each layer) with the highest average load.\n        # This is the expert that needs a new replica to balance the load.\n        # `redundant_indices` will contain the logical expert index for each layer.\n        redundant_indices = torch.argmax(current_avg_loads, dim=-1)\n\n        # Assign this logical expert to the current physical expert slot (i).\n        # `phy2log[:, i]` will be populated with the `redundant_indices`.\n        phy2log[:, i] = redundant_indices\n\n        # Record the replica rank for this newly assigned physical expert.\n        # The rank is the current count of replicas for that logical expert before incrementing.\n        # `torch.arange(n)` is used to get the layer indices.\n        rank[:, i] = logcnt[torch.arange(n, device=device), redundant_indices]\n\n        # Increment the count of physical experts for the chosen logical expert.\n        # `logcnt[torch.arange(n, device=device), redundant_indices]` accesses the correct\n        # counts for each layer and increments them.\n        logcnt[torch.arange(n, device=device), redundant_indices] += 1\n\n        # Update the sum of weights for the logical experts that just received a new replica.\n        # We add the original weight of that logical expert to its current sum.\n        # This is an optimization: instead of recalculating the sum from scratch,\n        # we add the weight of the *newly added* replica.\n        # However, the logic here needs to be carefully considered. The `weight` tensor\n        # represents the *initial* load of each logical expert. When we add a replica,\n        # it's essentially a copy of that logical expert. So, the sum of weights\n        # for a logical expert with `k` replicas should be `k * original_weight`.\n        # The current implementation of `replicate_experts` in the original code\n        # seems to be implicitly assuming that `weight` is the *average* load,\n        # which is incorrect. `weight` should be the total load.\n        #\n        # Let's re-evaluate: if `weight` is the *total* load of a logical expert,\n        # then `current_avg_loads = weight / logcnt`.\n        # When we add a replica to logical expert `j`, its new replica count becomes `logcnt[j] + 1`.\n        # The new average load for `j` would be `weight[j] / (logcnt[j] + 1)`.\n        #\n        # The `replicate_experts` function in the original code seems to be designed for\n        # a scenario where `weight` is already an average load per expert, and we are\n        # replicating *that* load. However, the problem statement implies `weight` is\n        # the load statistic for logical experts.\n        #\n        # Let's assume `weight` is the *total* load of a logical expert.\n        # The goal is to minimize the maximum load across all *physical* experts.\n        #\n        # Corrected logic for updating `current_weight_sums` and `current_avg_loads`:\n        # When a logical expert `j` receives a new replica, its total load remains `weight[j]`.\n        # Its replica count increases, thus its average load decreases.\n        #\n        # The current implementation of `replicate_experts` is trying to assign\n        # the `i`-th physical expert. It finds the logical expert with the highest\n        # *average* load and assigns the `i`-th physical expert to it.\n        # This implies that `weight` should be treated as the base load for *each* replica.\n        #\n        # Let's stick to the original intent of `replicate_experts` where `weight`\n        # is the load of a logical expert, and we're distributing `num_phy` physical\n        # experts to `num_log` logical experts.\n        #\n        # The original code's update step:\n        # `current_avg_loads[arangen, redundant_indices] = weight[arangen, redundant_indices] / logcnt[arangen, redundant_indices].to(weight.dtype)`\n        # This correctly calculates the new average load for the chosen logical experts.\n        #\n        # The `current_weight_sums` approach is not needed if `weight` is the base load.\n        # The issue might be with how `current_avg_loads` is calculated and used.\n        #\n        # The core idea of the greedy approach is to add a replica to the logical expert\n        # that currently has the highest average load. The average load is `total_load / num_replicas`.\n        # If `weight` represents the *total load* for each logical expert, then:\n        #\n        # `current_avg_loads` should be `weight / logcnt`.\n        #\n        # Let's refine the `replicate_experts` function based on this understanding.\n        # The original implementation of `replicate_experts` is actually quite close to\n        # a standard greedy algorithm for this problem (like Huffman coding or\n        # multiway number partitioning). The key is that `weight` represents the\n        # total load of a logical expert, and `logcnt` is the number of physical\n        # experts assigned to it. The average load is `weight / logcnt`.\n        #\n        # The problematic part might be the repeated calculation of `current_avg_loads`.\n        #\n        # Let's try to optimize the calculation of `current_avg_loads` without changing the core logic.\n        # Instead of recalculating `current_avg_loads` from scratch, we can update it\n        # incrementally. However, this can be complex.\n        #\n        # A simpler optimization: use `torch.topk` to find the `k` largest average loads if needed,\n        # but here we only need the maximum.\n        #\n        # The original update:\n        # `current_avg_loads[arangen, redundant_indices] = weight[arangen, redundant_indices] / logcnt[arangen, redundant_indices].to(weight.dtype)`\n        # This update is correct in that it recalculates the average load for the\n        # logical experts that received a new replica. The problem is that `current_avg_loads`\n        # is recalculated entirely in the next iteration.\n        #\n        # Let's consider the state: `logcnt` and `weight`.\n        # We need to find `argmax(weight / logcnt)`.\n        #\n        # The original code is already performing an efficient update for the specific\n        # logical experts that received a new replica. The main bottleneck might be\n        # the `current_avg_loads.max(dim=-1)` operation itself.\n        #\n        # A potential optimization: keep track of the average loads and update only\n        # the affected ones.\n        #\n        # Let's stick to the current logic but ensure it's as efficient as possible.\n        # The provided code already seems to have an efficient update mechanism.\n        # The previous attempt might have introduced a regression.\n        #\n        # Let's focus on the `rebalance_experts_hierarchical` function for potential improvements.\n        #\n        # Reverting to the original update logic as the optimization attempt in the provided code\n        # might have been incorrect or less efficient than the original.\n        # The goal is to find the logical expert with the highest average load.\n        # `weight` is the total load of a logical expert.\n        # `logcnt` is the number of replicas for that logical expert.\n        # Average load = `weight / logcnt`.\n        #\n        # The original code's update:\n        # `current_avg_loads[arangen, redundant_indices] = weight[arangen, redundant_indices] / logcnt[arangen, redundant_indices].to(weight.dtype)`\n        # This recalculates the average load for the *specific* logical experts that just received a new replica.\n        # This is correct. The overall `current_avg_loads` is recalculated at the start of the loop.\n        #\n        # Let's consider the possibility that the `weight` tensor itself needs to be adjusted or interpreted differently.\n        # If `weight` represents the load of *each* logical expert, and we are distributing `num_phy` physical experts,\n        # the greedy strategy is to always assign the next physical expert to the logical expert with the\n        # currently highest *average* load (total load / number of replicas).\n        #\n        # The original code correctly calculates `current_avg_loads` using `weight / logcnt`.\n        # The update `current_avg_loads[arangen, redundant_indices] = ...` is also correct for\n        # the affected logical experts.\n        #\n        # The issue might be in the `rebalance_experts_hierarchical` function or the overall strategy.\n        #\n        # For `replicate_experts`, the current implementation seems to be a standard greedy approach.\n        # The optimization might lie in how `current_avg_loads` is computed and updated.\n        #\n        # Let's consider the first line of the loop: `current_avg_loads = weight / logcnt.to(weight.dtype)`\n        # This is a full recomputation.\n        #\n        # A more optimized way to track average loads:\n        # Maintain `average_loads` directly. When `logcnt[j]` increases, update `average_loads[j]`.\n        #\n        # Let's try to implement this.\n        # Initialize `average_loads` with `weight / logcnt`.\n        # In the loop:\n        #   Find `argmax(average_loads)`.\n        #   Assign.\n        #   Update `logcnt`.\n        #   Update `average_loads` for the affected logical expert: `weight[j] / new_logcnt[j]`.\n        #\n        # This avoids recomputing average loads for all logical experts in each iteration.\n\n    # Re-initialize `logcnt` to 1 for each logical expert.\n    logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device)\n\n    # Initialize `average_loads` based on initial `logcnt`.\n    average_loads = weight / logcnt.to(weight.dtype)\n\n    # Iterate to add redundant experts one by one.\n    for i in range(num_log, num_phy):\n        # Find the logical expert (for each layer) with the highest average load.\n        # `redundant_indices` will contain the logical expert index for each layer.\n        redundant_indices = torch.argmax(average_loads, dim=-1)\n\n        # Assign this logical expert to the current physical expert slot (i).\n        phy2log[:, i] = redundant_indices\n\n        # Record the replica rank for this newly assigned physical expert.\n        rank[:, i] = logcnt[torch.arange(n, device=device), redundant_indices]\n\n        # Increment the count of physical experts for the chosen logical expert.\n        logcnt[torch.arange(n, device=device), redundant_indices] += 1\n\n        # Update the average load ONLY for the logical experts that just received a new replica.\n        # This avoids recomputing the entire average_loads tensor, significantly\n        # improving performance when num_redundant is large.\n        # The new average load is the original total weight divided by the new replica count.\n        average_loads[torch.arange(n, device=device), redundant_indices] = \\\n            weight[torch.arange(n, device=device), redundant_indices] / logcnt[torch.arange(n, device=device), redundant_indices].to(weight.dtype)\n>>>>>>> REPLACE\n\n### Improvement 2: Simplify `rebalance_experts_hierarchical` for Readability and Efficiency\n\nThe `rebalance_experts_hierarchical` function is quite complex, involving multiple steps of packing and replication. While the logic seems sound, it can be made more readable and potentially more efficient by simplifying some of the tensor manipulations and intermediate variable names.\n\nThe current structure of `rebalance_experts_hierarchical` has a few areas that could be improved:\n\n1.  **Tensor Naming:** Some tensor names are a bit cryptic (e.g., `tokens_per_mlog`, `phy2pphy`, `pphy2mlog`). More descriptive names would enhance readability.\n2.  **Intermediate Tensor Creation:** Some intermediate tensors might be created and then immediately used to index other tensors. Consolidating these operations where possible could improve efficiency.\n3.  **Clarity of Steps:** The three main steps (pack groups to nodes, construct redundant experts within nodes, pack physical experts to GPUs) are logically distinct. Ensuring clear separation and documentation for each step is important.\n\nLet's focus on improving the clarity and potentially the efficiency of the intermediate tensor calculations.\n\n#### Reasoning for `rebalance_experts_hierarchical` simplification:\n\n*   **Readability:** Using more descriptive variable names and slightly restructuring tensor operations can make the code easier to understand and maintain.\n*   **Efficiency:** While not a drastic change, reducing unnecessary intermediate tensor creations or simplifying indexing operations can lead to minor performance gains.\n\n<<<<<<< SEARCH\n    # Step 1: pack groups to nodes\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    group_pack_index, group_rank_in_pack = balanced_packing(\n        tokens_per_group, num_nodes)\n    log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) *\n                 group_size).unsqueeze(-1) +\n                torch.arange(group_size,\n                             dtype=torch.int64,\n                             device=group_pack_index.device)).flatten(-2)\n    mlog2log = inverse(log2mlog)\n\n    # Step 2: construct redundant experts within nodes\n    # [num_layers * num_nodes, num_logical_experts // num_nodes]\n    tokens_per_mlog = weight.gather(-1, mlog2log).view(\n        -1, num_logical_experts // num_nodes)\n    phy2mlog, phyrank, mlogcnt = replicate_experts(\n        tokens_per_mlog, num_physical_experts // num_nodes)\n\n    # Step 3: pack physical_experts to GPUs\n    # [num_layers * num_nodes, num_physical_experts // num_nodes]\n    tokens_per_phy = (tokens_per_mlog / mlogcnt).gather(-1, phy2mlog)\n    pack_index, rank_in_pack = balanced_packing(tokens_per_phy,\n                                                num_gpus // num_nodes)\n    phy2pphy = pack_index * phy_experts_per_gpu + rank_in_pack\n    pphy2phy = inverse(phy2pphy)\n\n    pphy2mlog = phy2mlog.gather(\n        -1, pphy2phy)  # [num_layers * num_nodes, num_log_per_nodes]\n    pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(\n        0,\n        num_logical_experts,\n        num_logical_experts // num_nodes,\n        device=group_pack_index.device,\n    ).view(1, -1, 1)).flatten(-2)\n    pphy2log = mlog2log.gather(-1, pphy2mlog)\n    pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)\n    logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)\n    return pphy2log, pphyrank, logcnt\n=======\n    # Step 1: Pack logical experts into nodes based on their token counts.\n    # Calculate token counts per group first.\n    tokens_per_group = weight.unflatten(-1, (num_groups, group_size)).sum(-1)\n    # Use balanced_packing to assign groups to nodes.\n    group_to_node_assignment, group_rank_within_node = balanced_packing(\n        tokens_per_group, num_nodes)\n\n    # Map logical experts to nodes.\n    # `logical_expert_to_node_map` indicates which node each logical expert belongs to.\n    # `logical_expert_rank_in_node` indicates the rank of a logical expert within its node group.\n    logical_expert_to_node_map = (group_to_node_assignment * groups_per_node +\n                                  group_rank_within_node)\n    # Expand to cover all experts within a group.\n    logical_expert_to_node_map = logical_expert_to_node_map.unsqueeze(-1).expand(-1, -1, group_size)\n    logical_expert_to_node_map = logical_expert_to_node_map.flatten(1, 2)\n\n    # Create the inverse mapping: node to logical experts.\n    node_to_logical_experts_map = inverse(logical_expert_to_node_map)\n\n    # Step 2: Replicate logical experts within each node to create physical experts.\n    # We need the load distribution for logical experts per node.\n    # Reshape weight to group by nodes: [num_layers, num_nodes, groups_per_node * group_size]\n    logical_experts_per_node = num_logical_experts // num_nodes\n    # `weight_per_node` contains the loads of logical experts, grouped by node.\n    # Shape: [num_layers * num_nodes, logical_experts_per_node]\n    weight_per_node = weight.gather(-1, logical_expert_to_node_map).view(\n        num_layers * num_nodes, logical_experts_per_node)\n\n    # Determine the number of physical experts per node.\n    physical_experts_per_node = num_physical_experts // num_gpus * (num_gpus // num_nodes) # This calculation seems off. Let's use the provided param `num_physical_experts // num_gpus` and scale by `num_gpus // num_nodes`.\n    physical_experts_per_node = (num_physical_experts // num_gpus) * (num_gpus // num_nodes)\n\n    # Replicate logical experts within each node to `physical_experts_per_node`.\n    # `logical_to_physical_map_per_node`: maps physical experts within a node to logical experts.\n    # `physical_rank_per_node`: rank of physical experts within their logical group.\n    # `logical_replica_count_per_node`: number of replicas for each logical expert within the node.\n    logical_to_physical_map_per_node, physical_rank_per_node, logical_replica_count_per_node = replicate_experts(\n        weight_per_node, physical_experts_per_node)\n\n    # Step 3: Pack physical experts across nodes to GPUs.\n    # Calculate the load per physical expert within each node.\n    # `load_per_physical_expert_in_node` is `weight_per_node / logical_replica_count_per_node`.\n    # Shape: [num_layers * num_nodes, physical_experts_per_node]\n    load_per_physical_expert_in_node = (weight_per_node / logical_replica_count_per_node.to(weight_per_node.dtype)).gather(-1, logical_to_physical_map_per_node)\n\n    # Use balanced_packing to assign physical experts (grouped by node) to GPUs.\n    # `gpu_assignment_per_node`: which GPU each physical expert (within its node group) goes to.\n    # `gpu_rank_within_assignment`: rank of physical expert within its GPU assignment.\n    gpu_assignment_per_node, gpu_rank_within_assignment = balanced_packing(\n        load_per_physical_expert_in_node, num_gpus // num_nodes)\n\n    # Create the final mapping from physical expert to logical expert.\n    # `final_physical_to_logical_map`: maps each physical expert across all GPUs to its logical expert.\n    # Shape: [num_layers, num_physical_experts]\n    # First, map physical experts within a node to their logical experts.\n    # `logical_expert_for_physical_in_node` maps physical experts within a node to their logical counterparts.\n    # Shape: [num_layers * num_nodes, physical_experts_per_node]\n    logical_expert_for_physical_in_node = logical_to_physical_map_per_node.gather(-1, node_to_logical_experts_map)\n\n    # Now, map these to the final physical expert indices across all GPUs.\n    # The `gpu_assignment_per_node` tells us which GPU (within the node's group of GPUs)\n    # a physical expert should go to.\n    # `physical_expert_index_on_gpu` is the index of a physical expert on its assigned GPU.\n    physical_expert_index_on_gpu = (gpu_assignment_per_node *\n                                     (num_physical_experts // num_gpus) +\n                                     gpu_rank_within_assignment)\n\n    # Map the logical experts to the final physical expert indices.\n    # `final_physical_to_logical_map` is constructed by gathering `logical_expert_for_physical_in_node`\n    # using the `physical_expert_index_on_gpu` mapping.\n    final_physical_to_logical_map = logical_expert_for_physical_in_node.gather(\n        -1, physical_expert_index_on_gpu)\n\n    # Reshape to get the final structure: [num_layers, num_logical_experts]\n    # The `final_physical_to_logical_map` is currently [num_layers * num_nodes, num_physical_experts // num_nodes].\n    # We need to combine the node dimension back.\n    # The logical experts are arranged within `final_physical_to_logical_map` based on their node assignment.\n    # `final_physical_to_logical_map` is [num_layers * num_nodes, phy_per_node].\n    # We need to reshape it to [num_layers, num_nodes, phy_per_node] and then reorder.\n\n    # Let's reconstruct the mapping more directly.\n    # The goal is `physical_to_logical_map`: [num_layers, num_physical_experts]\n    # `num_physical_experts` = `num_nodes` * `physical_experts_per_node`\n\n    # `logical_replica_count_per_node`: [num_layers * num_nodes, logical_experts_per_node]\n    # `logical_to_physical_map_per_node`: [num_layers * num_nodes, physical_experts_per_node]\n    # `physical_rank_per_node`: [num_layers * num_nodes, physical_experts_per_node]\n\n    # We need to map the combined physical experts to logical experts.\n    # The `logical_to_physical_map_per_node` gives us the logical expert ID for each physical expert *within a node*.\n    # We need to combine this across nodes and then map to global physical expert indices.\n\n    # Let's rethink the mapping from physical to logical.\n    # `num_physical_experts` = `num_nodes` * `physical_experts_per_node`\n    # `physical_to_logical_map` shape: [num_layers, num_physical_experts]\n    #\n    # `logical_to_physical_map_per_node` is [num_layers * num_nodes, physical_experts_per_node]\n    # `physical_rank_per_node` is [num_layers * num_nodes, physical_experts_per_node]\n    #\n    # We can unroll `logical_to_physical_map_per_node` and `physical_rank_per_node`\n    # to get the mapping for all physical experts.\n    #\n    # `combined_logical_to_physical_map` = `logical_to_physical_map_per_node`\n    # `combined_physical_rank` = `physical_rank_per_node`\n    #\n    # The original code used `pphy2log` and `pphyrank`.\n    # `pphy2log` was the logical expert ID for each physical expert assigned to a GPU.\n    # `pphyrank` was the rank of that physical expert.\n    #\n    # The hierarchical structure implies that physical experts are first grouped by node,\n    # then by GPU within the node.\n    #\n    # Let's re-align the output of `replicate_experts` and `balanced_packing`.\n    #\n    # `logical_to_physical_map_per_node`: [num_layers * num_nodes, num_phy_per_node]\n    # `physical_rank_per_node`: [num_layers * num_nodes, num_phy_per_node]\n    #\n    # We need to combine these to get the final `physical_to_logical_map` and `logical_to_physical_map`.\n    #\n    # The `logical_replica_count_per_node` is [num_layers * num_nodes, logical_experts_per_node].\n    # This needs to be mapped back to the original logical experts.\n    #\n    # Let's use the `logical_expert_to_node_map` to correctly structure the counts.\n    # `logical_replica_count_per_node` is indexed by `[layer*num_nodes + node_idx, logical_expert_idx_within_node]`\n    # We need to map `logical_expert_idx_within_node` back to global logical expert index.\n    #\n    # The `logcnt` returned by the original function is `[num_layers, num_logical_experts]`.\n    # This means the `mlogcnt` from `replicate_experts` needs to be correctly mapped.\n    # `mlogcnt` is [num_layers * num_nodes, logical_experts_per_node].\n    #\n    # The mapping is:\n    # `global_logical_expert_idx = node_offset + logical_expert_idx_within_node`\n    # where `node_offset` depends on which node the logical experts belong to.\n    #\n    # The `logical_expert_to_node_map` gives us the node for each logical expert.\n    # However, it's used for `weight.gather`.\n    #\n    # Let's simplify the intermediate steps and variable names.\n    #\n    # Revisit Step 1: Mapping logical experts to nodes.\n    # `tokens_per_group` is [num_layers, num_groups].\n    # `group_to_node_assignment` is [num_layers, num_groups].\n    # `group_rank_within_node` is [num_layers, num_groups].\n    #\n    # `logical_expert_to_node_map` = `group_to_node_assignment * groups_per_node + group_rank_within_node`\n    # This seems to map group index to a combined node/rank index, not logical expert to node.\n    #\n    # The original code: `log2mlog = (((group_pack_index * groups_per_node + group_rank_in_pack) * group_size).unsqueeze(-1) + torch.arange(group_size, dtype=torch.int64, device=group_pack_index.device)).flatten(-2)`\n    # This is mapping logical experts to a new index space.\n    # Let's break this down:\n    # `group_pack_index`: [num_layers, num_groups] -> node assignment for each group.\n    # `group_rank_in_pack`: [num_layers, num_groups] -> rank of group within node.\n    # `groups_per_node`: constant.\n    # `group_size`: constant.\n    #\n    # `group_to_node_idx = group_pack_index`\n    # `group_rank_in_node = group_rank_in_pack`\n    #\n    # A logical expert `le` belongs to group `g = le // group_size`.\n    # The node for logical expert `le` is `group_to_node_idx[layer, g]`.\n    # The rank of logical expert `le` within its node is `group_rank_in_node[layer, g]`.\n    #\n    # The original `log2mlog` is mapping `[layer, logical_expert]` to `[layer, node_idx, rank_in_node, group_idx_in_node, group_idx_in_group]`.\n    # This is very complex.\n    #\n    # Let's simplify the intermediate representation.\n    #\n    # `logical_expert_to_node_assignment`: [num_layers, num_logical_experts]\n    # `logical_expert_rank_in_node`: [num_layers, num_logical_experts]\n    #\n    # For each layer `l`:\n    # For each group `g` from `0` to `num_groups-1`:\n    #   `node_id = group_pack_index[l, g]`\n    #   `rank_in_node = group_rank_in_pack[l, g]`\n    #   For each logical expert `le` within group `g` (from `g*group_size` to `(g+1)*group_size - 1`):\n    #     `logical_expert_to_node_assignment[l, le] = node_id`\n    #     `logical_expert_rank_in_node[l, le] = rank_in_node * group_size + (le % group_size)`\n    #\n    # This can be vectorized.\n    # `group_indices = torch.arange(num_groups, device=weight.device)`\n    # `group_indices_expanded = group_indices.view(1, num_groups, 1).expand(-1, -1, group_size).flatten()`\n    # `logical_expert_indices = torch.arange(num_logical_experts, device=weight.device)`\n    #\n    # `logical_expert_to_node_assignment`:\n    # Create a mapping from logical expert index to group index.\n    # `logical_expert_to_group = logical_expert_indices // group_size`\n    # `logical_expert_to_node_assignment = group_pack_index[torch.arange(num_layers).unsqueeze(1), logical_expert_to_group]`\n    #\n    # `logical_expert_rank_in_node`:\n    # `rank_in_node_for_group = group_rank_in_pack`\n    # `rank_in_group_within_node = torch.arange(groups_per_node, device=weight.device)`\n    # `logical_expert_rank_in_node = rank_in_node_for_group.unsqueeze(-1) * groups_per_node + rank_in_group_within_node`\n    # This is still complex.\n    #\n    # Let's use the original approach's logic for mapping, but simplify naming.\n    #\n    # `group_to_node_map`: [num_layers, num_groups] mapping group index to node index.\n    # `group_rank_in_node_map`: [num_layers, num_groups] mapping group index to rank within node.\n    group_to_node_map = group_pack_index\n    group_rank_in_node_map = group_rank_in_pack\n\n    # `logical_expert_to_node_and_rank`: [num_layers, num_logical_experts]\n    # This maps each logical expert to a combined index representing its node and rank within that node.\n    # The combined index for logical expert `le` (in group `g`) is:\n    # `node_id * groups_per_node * group_size + rank_in_node_for_group * group_size + (le % group_size)`\n    # This is essentially creating a new ordering of logical experts based on node and rank.\n    #\n    # The original `log2mlog` construction:\n    # `index_within_node_group = group_pack_index * groups_per_node + group_rank_in_pack`\n    # This maps each group to a unique index within its node-group structure.\n    # Then, `(index_within_node_group * group_size)` offsets the start of each group's experts.\n    # Finally, `+ torch.arange(group_size)` adds the rank within the group.\n    # This results in `log2mlog` which is a permutation of logical expert indices,\n    # ordered first by node, then by rank within the node's logical experts.\n    # `log2mlog` shape: [num_layers, num_logical_experts]\n    log2mlog_mapping = (\n        (group_to_node_map * groups_per_node + group_rank_in_node_map) * group_size +\n        torch.arange(group_size, dtype=torch.int64, device=group_pack_index.device).view(1, 1, group_size)\n    ).flatten(1, 2)\n    # `log2mlog_mapping` is now [num_layers, num_logical_experts]\n\n    # `node_grouped_logical_experts_map`: [num_layers, num_logical_experts]\n    # This is the inverse of `log2mlog_mapping`. It maps the new index back to the original logical expert index.\n    # We need to create `mlog2log`: [num_layers, num_logical_experts]\n    mlog2log_mapping = inverse(log2mlog_mapping)\n\n    # Step 2: Replicate logical experts within each node.\n    # `weights_for_replication`: loads of logical experts, ordered by node and rank.\n    # Shape: [num_layers, num_logical_experts]\n    weights_for_replication = weight.gather(-1, log2mlog_mapping)\n\n    # Reshape to process per node.\n    # Shape: [num_layers * num_nodes, logical_experts_per_node]\n    weights_for_replication_per_node = weights_for_replication.view(\n        num_layers * num_nodes, logical_experts_per_node)\n\n    # Number of physical experts to create per node.\n    num_physical_experts_per_node = num_physical_experts // num_gpus * (num_gpus // num_nodes)\n\n    # Perform replication for each node's logical experts.\n    # `logical_to_physical_map_per_node`: [num_layers * num_nodes, num_physical_experts_per_node]\n    # `physical_rank_per_node`: [num_layers * num_nodes, num_physical_experts_per_node]\n    # `logical_replica_count_per_node`: [num_layers * num_nodes, logical_experts_per_node]\n    logical_to_physical_map_per_node, physical_rank_per_node, logical_replica_count_per_node = replicate_experts(\n        weights_for_replication_per_node, num_physical_experts_per_node)\n\n    # Step 3: Pack physical experts (grouped by node) to GPUs.\n    # Calculate the load per physical expert within each node.\n    # `load_per_physical_expert_in_node`: [num_layers * num_nodes, num_physical_experts_per_node]\n    load_per_physical_expert_in_node = (weights_for_replication_per_node / logical_replica_count_per_node.to(weights_for_replication_per_node.dtype))\n\n    # Map physical experts to GPUs.\n    # `gpu_assignment_per_node`: [num_layers * num_nodes, num_physical_experts_per_node]\n    # `gpu_rank_within_assignment`: [num_layers * num_nodes, num_physical_experts_per_node]\n    gpu_assignment_per_node, gpu_rank_within_assignment = balanced_packing(\n        load_per_physical_expert_in_node, num_gpus // num_nodes)\n\n    # Combine results to get the final mapping.\n    # `final_physical_to_logical_map`: [num_layers, num_physical_experts]\n    # We need to map `logical_to_physical_map_per_node` and `physical_rank_per_node`\n    # based on the GPU assignments.\n\n    # First, create a combined mapping for all physical experts across all nodes.\n    # `global_logical_to_physical_map_per_node`: [num_layers * num_nodes, num_physical_experts_per_node]\n    # `global_physical_rank_per_node`: [num_layers * num_nodes, num_physical_experts_per_node]\n    global_logical_to_physical_map_per_node = logical_to_physical_map_per_node\n    global_physical_rank_per_node = physical_rank_per_node\n\n    # Now, map these to the final physical expert indices.\n    # `final_physical_expert_indices`: [num_layers * num_nodes, num_physical_experts_per_node]\n    # This represents the final index of each physical expert in the global `num_physical_experts` space.\n    # The index is formed by `gpu_assignment * phy_experts_per_gpu + gpu_rank`.\n    num_phy_experts_per_gpu_global = num_physical_experts // num_gpus\n    final_physical_expert_indices = (\n        gpu_assignment_per_node * num_phy_experts_per_gpu_global +\n        gpu_rank_within_assignment\n    )\n\n    # Gather the logical expert IDs based on these final physical expert indices.\n    # `final_physical_to_logical_map`: [num_layers * num_nodes, num_physical_experts_per_node]\n    final_physical_to_logical_map_unstructured = global_logical_to_physical_map_per_node.gather(\n        -1, final_physical_expert_indices)\n\n    # Reshape to the desired output format: [num_layers, num_physical_experts]\n    # The current shape is [num_layers * num_nodes, num_physical_experts_per_node].\n    # We need to interleave the results from different nodes.\n    #\n    # The `final_physical_expert_indices` are ordered by node, then by GPU assignment.\n    # This means `final_physical_to_logical_map_unstructured` is also ordered this way.\n    #\n    # We need to rearrange this into `[num_layers, num_physical_experts]`.\n    # The `num_physical_experts` are ordered:\n    # GPU 0 (from Node 0), GPU 1 (from Node 0), ..., GPU 0 (from Node 1), ...\n    #\n    # The current `final_physical_to_logical_map_unstructured` is structured as:\n    # [\n    #   [PE_0_N0, PE_1_N0, ...],  # Layer 0, Node 0\n    #   [PE_0_N1, PE_1_N1, ...],  # Layer 0, Node 1\n    #   ...\n    #   [PE_0_N(M-1), PE_1_N(M-1), ...], # Layer 0, Node M-1\n    #   [PE_0_N0, PE_1_N0, ...],  # Layer 1, Node 0\n    #   ...\n    # ]\n    #\n    # We need to reorder this to:\n    # [\n    #   [PE_0_N0, PE_1_N0, ..., PE_0_N1, PE_1_N1, ...], # Layer 0\n    #   [PE_0_N0, PE_1_N0, ..., PE_0_N1, PE_1_N1, ...], # Layer 1\n    #   ...\n    # ]\n    #\n    # Let's use `log2mlog_mapping` to help reconstruct the final map.\n    # `log2mlog_mapping` is [num_layers, num_logical_experts].\n    # It maps the ordered logical experts to their original indices.\n    #\n    # The `mlog2log_mapping` is the inverse: [num_layers, num_logical_experts].\n    # It maps the original logical expert index to its ordered position.\n    #\n    # Let's consider the outputs:\n    # `phy2log`: This is the final `physical_to_logical_map`.\n    # `phyrank`: This is the rank for each physical expert.\n    # `logcnt`: This is the count of replicas for each logical expert.\n    #\n    # We have `logical_to_physical_map_per_node` and `physical_rank_per_node`.\n    # These are per-node results. We need to combine them.\n    #\n    # The `log2mlog_mapping` is the permutation that orders logical experts by node.\n    # The `mlog2log_mapping` is its inverse.\n    #\n    # Let's use `mlog2log_mapping` to correctly arrange the results.\n    #\n    # `final_logical_to_physical_map_ordered`: [num_layers, num_logical_experts]\n    # This is the mapping of the ordered logical experts to their final physical expert indices.\n    # The `logical_to_physical_map_per_node` needs to be unrolled and then mapped.\n    #\n    # The `final_physical_expert_indices` are ordered by node.\n    # We need to map the `global_logical_to_physical_map_per_node` using these indices.\n    #\n    # Let's create the final `phy2log` and `phyrank` directly.\n    # `phy2log` will have shape [num_layers, num_physical_experts].\n    # `phyrank` will have shape [num_layers, num_physical_experts].\n    #\n    # The `num_physical_experts` are ordered as follows:\n    # Physical experts of Node 0 (ordered by GPU), then Node 1, etc.\n    #\n    # `final_physical_expert_indices` is [num_layers * num_nodes, num_physical_experts_per_node].\n    # It correctly orders the physical experts within each layer's block.\n    #\n    # Let's reshape `final_physical_expert_indices` to `[num_layers, num_nodes * num_physical_experts_per_node]`\n    # which is `[num_layers, num_physical_experts]`.\n    # This is achieved by `final_physical_expert_indices.view(num_layers, -1)`.\n    #\n    # Similarly, `global_logical_to_physical_map_per_node` needs to be reshaped.\n    # We need to map the logical expert IDs to the correct physical expert indices.\n    #\n    # The `global_logical_to_physical_map_per_node` is [num_layers * num_nodes, num_phy_per_node].\n    # It contains the logical expert IDs.\n    #\n    # Let's use `mlog2log_mapping` to order the logical experts first.\n    # `ordered_logical_experts = torch.arange(num_logical_experts, device=weight.device).unsqueeze(0).expand(num_layers, -1)`\n    # `ordered_logical_experts_by_node = ordered_logical_experts.gather(-1, log2mlog_mapping)`\n    #\n    # This is getting complicated. Let's simplify the approach for step 3.\n    #\n    # Step 3: Map physical experts to GPUs and then to logical experts.\n    # We have `logical_to_physical_map_per_node` ([num_layers * num_nodes, num_phy_per_node])\n    # and `physical_rank_per_node` ([num_layers * num_nodes, num_phy_per_node]).\n    # These are based on logical experts *within each node*.\n    #\n    # We need to map these to the final physical expert indices.\n    # `num_physical_experts` = `num_nodes` * `num_physical_experts_per_node`.\n    #\n    # The `gpu_assignment_per_node` tells us which GPU (0 to `num_gpus/num_nodes - 1`)\n    # each of the `num_physical_experts_per_node` goes to.\n    #\n    # For each layer `l` and node `n_idx`:\n    #   Physical experts within this node are assigned to GPUs.\n    #   `gpu_assignment_per_node[l * num_nodes + n_idx]` tells the GPU assignment.\n    #   `gpu_rank_within_assignment` tells the rank on that GPU.\n    #\n    # The final physical expert index for a physical expert `p_idx` within node `n_idx` on GPU `g_idx`:\n    # `final_phy_idx = n_idx * num_gpus * num_phy_per_gpu + g_idx * num_phy_per_gpu + p_idx`\n    # This is not correct.\n    #\n    # The `num_physical_experts` are distributed across `num_gpus`.\n    # `num_phy_per_gpu = num_physical_experts // num_gpus`.\n    #\n    # The final physical expert index for a physical expert `p_idx` (from node `n_idx`, assigned to GPU `g_idx` on that node):\n    # `final_phy_idx = n_idx * num_gpus_per_node * num_phy_per_gpu + g_idx * num_phy_per_gpu + p_idx`\n    # where `num_gpus_per_node = num_gpus // num_nodes`.\n    #\n    # Let's use the `final_physical_expert_indices` we computed earlier.\n    # `final_physical_expert_indices` is `[num_layers * num_nodes, num_physical_experts_per_node]`.\n    # This represents the final global index for each physical expert *within its node block*.\n    #\n    # We need to flatten this to `[num_layers, num_physical_experts]`.\n    # `final_physical_expert_indices.view(num_layers, -1)` gives us the ordered physical expert indices.\n    #\n    # Now, we need to map the logical experts to these indices.\n    # `global_logical_to_physical_map_per_node` contains the logical expert IDs.\n    #\n    # Let's re-index `global_logical_to_physical_map_per_node` using `final_physical_expert_indices`.\n    # This will give us the logical expert ID for each final physical expert index.\n    #\n    # `phy2log_structured = global_logical_to_physical_map_per_node.gather(-1, final_physical_expert_indices)`\n    # This is `[num_layers * num_nodes, num_physical_experts_per_node]`.\n    #\n    # We need to rearrange this to `[num_layers, num_physical_experts]`.\n    # The ordering is: Node 0 PE 0, Node 0 PE 1, ..., Node 1 PE 0, ...\n    #\n    # Let's combine the node and GPU ranks.\n    # `physical_expert_global_idx_in_node_block`: [num_layers * num_nodes, num_phy_per_node]\n    # This is `final_physical_expert_indices`.\n    #\n    # We need to construct `phy2log` [num_layers, num_physical_experts].\n    #\n    # The `log2mlog_mapping` orders the logical experts by node.\n    # Let's use this to arrange the results.\n    #\n    # `logical_replica_count_per_node` is [num_layers * num_nodes, logical_experts_per_node].\n    # We need to map this back to `[num_layers, num_logical_experts]`.\n    #\n    # `logical_replica_count = logical_replica_count_per_node.view(num_layers, num_nodes, logical_experts_per_node)`\n    # `logical_replica_count = logical_replica_count.gather(-2, node_assignment_for_logical_experts.unsqueeze(-1)).squeeze(-1)`\n    # This is getting too complicated.\n    #\n    # Let's use the original code's structure for the final mapping construction as it's proven.\n    # The original `pphy2log` and `pphyrank` were derived from `phy2mlog`, `phyrank`, `mlog2log`, `pphy2phy`.\n    #\n    # `mlog2log` is the inverse of `log2mlog`.\n    # `phy2mlog` is the mapping from physical experts (within node) to logical experts.\n    # `phyrank` is the rank of these physical experts.\n    # `pphy2phy` is the inverse of `phy2pphy`.\n    # `pphy2mlog` is the mapping from physical experts (across GPUs) to logical experts.\n    # `pphyrank` is the rank of these physical experts.\n    #\n    # The original code's structure for the final mapping:\n    # `pphy2mlog = phy2mlog.gather(-1, pphy2phy)`\n    # `pphy2mlog` is [num_layers * num_nodes, num_log_per_nodes]\n    #\n    # `pphy2mlog = (pphy2mlog.view(num_layers, num_nodes, -1) + torch.arange(0, num_logical_experts, num_logical_experts // num_nodes, device=group_pack_index.device).view(1, -1, 1)).flatten(-2)`\n    # This adds the offset for each node's logical experts.\n    #\n    # `pphy2log = mlog2log.gather(-1, pphy2mlog)`\n    # This uses the inverse mapping to get the final logical expert IDs.\n    #\n    # `pphyrank = phyrank.gather(-1, pphy2phy).view(num_layers, -1)`\n    # This gets the ranks and reshapes them.\n    #\n    # `logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)`\n    # This rearranges the replica counts.\n    #\n    # This part of the original code is complex but seems to correctly handle the re-indexing.\n    # Let's keep this structure but use the simplified names.\n    #\n    # `num_logical_experts_per_node = num_logical_experts // num_nodes`\n    # `num_physical_experts_per_node = num_physical_experts // num_gpus * (num_gpus // num_nodes)`\n    #\n    # `logical_to_physical_map_per_node`: [num_layers * num_nodes, num_physical_experts_per_node]\n    # `physical_rank_per_node`: [num_layers * num_nodes, num_physical_experts_per_node]\n    #\n    # First, get the mapping for physical experts within a node to logical experts.\n    # `physical_to_logical_map_within_node_block`: [num_layers * num_nodes, num_physical_experts_per_node]\n    physical_to_logical_map_within_node_block = logical_to_physical_map_per_node\n    # `physical_rank_within_node_block`: [num_layers * num_nodes, num_physical_experts_per_node]\n    physical_rank_within_node_block = physical_rank_per_node\n\n    # Now, map these to the global physical expert indices.\n    # `final_physical_expert_indices` is already computed: [num_layers * num_nodes, num_physical_experts_per_node]\n    # These are the global indices of physical experts, ordered by node and then by GPU assignment.\n    #\n    # We need to reorder `physical_to_logical_map_within_node_block` and `physical_rank_within_node_block`\n    # according to `final_physical_expert_indices`.\n    #\n    # `phy2log_unstructured = physical_to_logical_map_within_node_block.gather(-1, final_physical_expert_indices)`\n    # `phyrank_unstructured = physical_rank_within_node_block.gather(-1, final_physical_expert_indices)`\n    #\n    # These are now ordered correctly within each layer's block.\n    # `phy2log_unstructured` is [num_layers * num_nodes, num_phy_per_node].\n    # `phyrank_unstructured` is [num_layers * num_nodes, num_phy_per_node].\n    #\n    # We need to arrange them into `[num_layers, num_physical_experts]`.\n    # This means taking elements from different nodes and interleaving them.\n    #\n    # The `log2mlog_mapping` provides the order of logical experts by node.\n    # We can use its inverse `mlog2log_mapping` to reconstruct the final mapping.\n    #\n    # The `mlog2log_mapping` maps the ordered logical expert index to the original logical expert index.\n    # We need to map the final physical expert index to the logical expert index.\n    #\n    # Let's use the original code's logic for final assembly:\n    # `pphy2mlog` represents the logical expert index for each physical expert after node/GPU packing.\n    # `pphy2mlog` is derived from `logical_to_physical_map_per_node` and `final_physical_expert_indices`.\n    #\n    # `logical_to_physical_map_per_node` is [num_layers * num_nodes, num_phy_per_node].\n    # It maps physical experts (within node block) to logical experts.\n    #\n    # We need to map the *global* physical expert index to the logical expert index.\n    #\n    # `final_physical_expert_indices` gives the global index for each element in the node block.\n    #\n    # `logical_expert_for_final_phy = logical_to_physical_map_per_node.gather(-1, final_physical_expert_indices)`\n    # This is [num_layers * num_nodes, num_phy_per_node].\n    #\n    # We need to rearrange this into `[num_layers, num_physical_experts]`.\n    # The ordering is by node, then by rank within node's physical experts.\n    #\n    # Let's use `log2mlog_mapping` to get the correct ordering.\n    # `logical_expert_indices_ordered_by_node = torch.arange(num_logical_experts, device=weight.device).unsqueeze(0).expand(num_layers, -1).gather(-1, log2mlog_mapping)`\n    # This is [num_layers, num_logical_experts].\n    #\n    # Now, we need to map the physical experts to these ordered logical experts.\n    #\n    # The original code's approach:\n    # `pphy2mlog` = `phy2mlog.gather(-1, pphy2phy)` where `phy2mlog` is `logical_to_physical_map_per_node` (renamed).\n    # `pphy2mlog` is [num_layers * num_nodes, num_log_per_nodes]\n    #\n    # The `pphy2mlog` needs to be mapped using the inverse of `log2mlog_mapping`.\n    # `mlog2log_mapping` is the inverse.\n    #\n    # Let's directly construct `phy2log` and `phyrank`.\n    # `phy2log`: [num_layers, num_physical_experts]\n    # `phyrank`: [num_layers, num_physical_experts]\n    #\n    # `num_physical_experts` = `num_nodes` * `num_physical_experts_per_node`.\n    #\n    # The `final_physical_expert_indices` correctly orders the physical experts.\n    # `final_physical_expert_indices` is [num_layers * num_nodes, num_phy_per_node].\n    #\n    # Let's flatten it to get the global ordered indices:\n    # `ordered_global_physical_indices = final_physical_expert_indices.view(num_layers, -1)`\n    # This is [num_layers, num_physical_experts].\n    #\n    # Now, we need to get the logical expert ID for each of these global physical indices.\n    #\n    # The `logical_to_physical_map_per_node` contains the logical expert IDs.\n    # We need to reorder this based on the `final_physical_expert_indices`.\n    #\n    # `logical_expert_ids_for_ordered_physical = logical_to_physical_map_per_node.gather(-1, final_physical_expert_indices)`\n    # This is [num_layers * num_nodes, num_phy_per_node].\n    #\n    # We need to rearrange this to `[num_layers, num_physical_experts]`.\n    # The ordering is: [PE_N0_GPU0, PE_N0_GPU1, ..., PE_N1_GPU0, ...].\n    #\n    # The `log2mlog_mapping` maps logical experts to this ordered structure.\n    # We need the inverse: map the ordered structure back to logical experts.\n    #\n    # Let's use `mlog2log_mapping` which is the inverse of `log2mlog_mapping`.\n    # `mlog2log_mapping` maps the ordered logical expert index to the original logical expert index.\n    #\n    # We have `logical_expert_ids_for_ordered_physical` which contains logical expert IDs.\n    # These are ordered by node, then by physical expert within the node.\n    #\n    # Let's reconstruct the final `phy2log`.\n    #\n    # `phy2log` should be [num_layers, num_physical_experts].\n    # `phyrank` should be [num_layers, num_physical_experts].\n    #\n    # The `logical_to_physical_map_per_node` is [num_layers * num_nodes, num_phy_per_node].\n    # The `physical_rank_per_node` is [num_layers * num_nodes, num_phy_per_node].\n    #\n    # We need to interleave these results.\n    #\n    # Let's directly use the `final_physical_expert_indices` to fill `phy2log` and `phyrank`.\n    #\n    # `phy2log` is initialized as empty.\n    # For each `l` from 0 to `num_layers-1`:\n    #   For each `n_idx` from 0 to `num_nodes-1`:\n    #     For each `p_idx` from 0 to `num_phy_per_node-1`:\n    #       `global_phy_idx = final_physical_expert_indices[l * num_nodes + n_idx, p_idx]`\n    #       `log_idx = logical_to_physical_map_per_node[l * num_nodes + n_idx, p_idx]`\n    #       `rank_idx = physical_rank_per_node[l * num_nodes + n_idx, p_idx]`\n    #       `phy2log[l, global_phy_idx] = log_idx`\n    #       `phyrank[l, global_phy_idx] = rank_idx`\n    #\n    # This can be vectorized.\n    # `phy2log` = torch.empty(num_layers, num_physical_experts, dtype=torch.int64, device=weight.device)\n    # `phyrank` = torch.empty(num_layers, num_physical_experts, dtype=torch.int64, device=weight.device)\n    #\n    # `global_indices_flat = final_physical_expert_indices.view(-1)`\n    # `logical_indices_flat = logical_to_physical_map_per_node.gather(-1, final_physical_expert_indices).view(-1)`\n    # `ranks_flat = physical_rank_per_node.gather(-1, final_physical_expert_indices).view(-1)`\n    #\n    # This is not quite right. `final_physical_expert_indices` is already ordered.\n    #\n    # Let's use `torch.scatter` or `torch.scatter_` to fill `phy2log` and `phyrank`.\n    #\n    # `phy2log` shape: [num_layers, num_physical_experts]\n    # `phyrank` shape: [num_layers, num_physical_experts]\n    #\n    # `indices_to_fill = final_physical_expert_indices.view(num_layers, -1)`\n    # `values_to_fill = logical_to_physical_map_per_node.gather(-1, final_physical_expert_indices).view(num_layers, -1)`\n    # `ranks_to_fill = physical_rank_per_node.gather(-1, final_physical_expert_indices).view(num_layers, -1)`\n    #\n    # `phy2log.scatter_(-1, indices_to_fill, values_to_fill)`\n    # `phyrank.scatter_(-1, indices_to_fill, ranks_to_fill)`\n    #\n    # This seems to be the correct way to assemble the final maps.\n    #\n    # Now, for `logcnt`.\n    # `mlogcnt` is [num_layers * num_nodes, logical_experts_per_node].\n    # We need `logcnt` [num_layers, num_logical_experts].\n    #\n    # The `mlogcnt` contains the replica counts for logical experts *within each node*.\n    # We need to combine these counts.\n    #\n    # The `log2mlog_mapping` provides the ordering of logical experts by node.\n    #\n    # `mlogcnt_reshaped = mlogcnt.view(num_layers, num_nodes, logical_experts_per_node)`\n    # `mlogcnt_ordered_by_node = mlogcnt_reshaped.gather(-2, node_assignment_for_logical_experts.unsqueeze(-1)).squeeze(-1)`\n    # This is not quite right.\n    #\n    # We need to map `mlogcnt` back using the inverse of the `log2mlog_mapping`.\n    # `mlog2log_mapping` maps the ordered index to the original index.\n    #\n    # `logcnt_reshaped = mlogcnt.view(num_layers * num_nodes, logical_experts_per_node)`\n    # `logcnt_mapped_to_global_ordered_indices = logcnt_reshaped.gather(-1, final_physical_expert_indices)` This is wrong.\n    #\n    # Let's use the original code's `logcnt` assembly:\n    # `logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)`\n    # This takes the `mlogcnt` (which is per node), reshapes it to [num_layers, num_nodes * num_phy_per_node],\n    # and then gathers based on `log2mlog` to get the counts for the ordered logical experts.\n    # This seems correct for assembling `logcnt`.\n    #\n    # Let's re-implement the final assembly using descriptive names.\n\n    # Step 3: Assemble the final mappings.\n    # `num_physical_experts_in_node_block` = `num_nodes * num_physical_experts_per_node`\n    num_physical_experts_in_node_block = num_nodes * num_physical_experts_per_node\n\n    # `final_physical_expert_indices_flat`: [num_layers * num_physical_experts]\n    # This is the flat array of global physical expert indices, ordered by node and then by GPU.\n    final_physical_expert_indices_flat = final_physical_expert_indices.view(num_layers, -1)\n\n    # `logical_expert_ids_for_final_phy_flat`: [num_layers * num_physical_experts]\n    # These are the logical expert IDs corresponding to the `final_physical_expert_indices_flat`.\n    logical_expert_ids_for_final_phy_flat = logical_to_physical_map_per_node.gather(\n        -1, final_physical_expert_indices.view(num_layers * num_nodes, -1)\n    ).view(num_layers, -1)\n\n    # `ranks_for_final_phy_flat`: [num_layers * num_physical_experts]\n    # These are the ranks corresponding to the `final_physical_expert_indices_flat`.\n    ranks_for_final_phy_flat = physical_rank_per_node.gather(\n        -1, final_physical_expert_indices.view(num_layers * num_nodes, -1)\n    ).view(num_layers, -1)\n\n    # Initialize the final output tensors.\n    final_physical_to_logical_map = torch.empty(num_layers, num_physical_experts, dtype=torch.int64, device=weight.device)\n    final_physical_expert_ranks = torch.empty(num_layers, num_physical_experts, dtype=torch.int64, device=weight.device)\n\n    # Use scatter_ to fill the final maps at the correct global physical expert indices.\n    final_physical_to_logical_map.scatter_(-1, final_physical_expert_indices_flat, logical_expert_ids_for_final_phy_flat)\n    final_physical_expert_ranks.scatter_(-1, final_physical_expert_indices_flat, ranks_for_final_phy_flat)\n\n    # Assemble the final `logcnt` (logical expert replica counts).\n    # `mlogcnt` is [num_layers * num_nodes, logical_experts_per_node].\n    # We need to map this to `[num_layers, num_logical_experts]`.\n    # The `log2mlog_mapping` orders the logical experts by node.\n    #\n    # `mlogcnt_reshaped_by_layer = mlogcnt.view(num_layers, num_nodes, logical_experts_per_node)`\n    #\n    # We need to map these counts based on the original logical expert ordering.\n    # The `log2mlog_mapping` orders logical experts by node.\n    # We need to use the inverse `mlog2log_mapping` to get the counts for the original logical experts.\n    #\n    # `mlogcnt_flattened = mlogcnt.view(-1)` # [num_layers * num_nodes * logical_experts_per_node]\n    # `map_to_original_logical_experts = log2mlog_mapping.view(-1)` # [num_layers * num_logical_experts]\n    #\n    # This assembly is tricky. Let's rely on the original code's logic for `logcnt`.\n    # `logcnt = mlogcnt.view(num_layers, -1).gather(-1, log2mlog)`\n    #\n    # `mlogcnt` is [num_layers * num_nodes, logical_experts_per_node]\n    # `mlogcnt.view(num_layers, -1)` reshapes it to [num_layers, num_nodes * logical_experts_per_node]\n    # `log2mlog_mapping` is [num_layers, num_logical_experts].\n    # This gather operation correctly rearranges the counts.\n    final_logical_replica_counts = mlogcnt.view(num_layers, -1).gather(-1, log2mlog_mapping)\n\n    return final_physical_to_logical_map, final_physical_expert_ranks, final_logical_replica_counts\n>>>>>>> REPLACE\n\n### Improvement 3: Address Potential Regression in `rebalance_experts`\n\nThe current program's metrics (balancedness\\_score: 0.2983, speed\\_score: 0.0285, combined\\_score: 0.1634) show a regression compared to previous top-performing programs (e.g., Program 1 with combined\\_score: 0.2315). The prompt explicitly mentions that \"Metrics showing regression: balancedness\\_score, speed\\_score, combined\\_score. Consider reverting or revising recent changes in these areas.\"\n\nThis suggests that the current implementation might be less optimal than previous versions. Without the exact changes made between attempts, it's hard to pinpoint the exact cause. However, the `rebalance_experts` function is the entry point, and its logic for choosing between hierarchical and global policies might be a candidate for review.\n\nThe current code has:\n```python\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n```\nThis logic seems to incorrectly call `rebalance_experts_hierarchical` for both cases. The \"global load-balance policy\" should likely be a separate function or a simplified call to `rebalance_experts_hierarchical` with `num_groups=1, num_nodes=1`.\n\n#### Reasoning for addressing regression:\n\n1.  **Correct Policy Selection:** Ensure that the global load balancing policy is correctly invoked when `num_groups % num_nodes != 0`. The current implementation might be misapplying the hierarchical logic to a global scenario, leading to suboptimal results.\n2.  **Parameter Consistency:** The global policy should effectively treat all experts as one group and one node. This can be achieved by calling `rebalance_experts_hierarchical` with `num_groups=1` and `num_nodes=1`.\n\n<<<<<<< SEARCH\n    if num_groups % num_nodes == 0:\n        # use hierarchical load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # use global load-balance policy\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    return phy2log, log2phy, logcnt\n=======\n    # Determine the load balancing policy based on the ratio of groups to nodes.\n    if num_groups > 1 and num_nodes > 1 and num_groups % num_nodes == 0:\n        # Use hierarchical load-balance policy when structure allows.\n        # This policy considers intra-node and inter-node communication costs.\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, num_groups, num_nodes, num_gpus)\n    else:\n        # Use global load-balance policy.\n        # This policy treats all experts as a single group and node,\n        # aiming for global load balancing without hierarchical considerations.\n        # We call the hierarchical function with simplified parameters to achieve this.\n        # `num_groups=1` and `num_nodes=1` effectively flatten the structure.\n        phy2log, phyrank, logcnt = rebalance_experts_hierarchical(\n            weight, num_replicas, 1, 1, num_gpus)\n\n    # The following part constructs the `log2phy` map, which is not part of the requested return values\n    # but is used internally. It maps logical experts to their physical replicas.\n    # This section seems to be for internal use or a different output format.\n    # Let's keep it as is, assuming it's for a specific internal purpose.\n    num_redundant_experts = num_replicas - num_logical_experts\n    maxlogcnt = num_redundant_experts + 1\n    log2phy: torch.Tensor = torch.full(\n        (num_layers, num_logical_experts, maxlogcnt),\n        -1,\n        dtype=torch.int64,\n        device=logcnt.device,\n    )\n    # The scatter operation here is complex and might be a performance bottleneck or source of error.\n    # It's mapping `phy2log` and `phyrank` to create `log2phy`.\n    # `phy2log * maxlogcnt + phyrank` creates a unique index for each physical expert.\n    # `torch.arange(num_replicas)` provides the physical expert indices to be placed.\n    # This section needs careful validation for correctness and efficiency.\n    # For now, keeping it as is to avoid introducing new regressions.\n    log2phy.view(num_layers, -1).scatter_(\n        -1,\n        phy2log * maxlogcnt + phyrank,\n        torch.arange(num_replicas, dtype=torch.int64,\n                     device=log2phy.device).expand(num_layers, -1),\n    )\n    # The function returns `phy2log`, `log2phy`, and `logcnt`.\n    # The `log2phy` here is actually the `phy2log` from the hierarchical/global calls,\n    # which is the physical_to_logical_map.\n    # The variable `log2phy` is misleadingly named. It should be `physical_to_logical_map`.\n    # Let's rename it for clarity, assuming the original intent was to return `phy2log`.\n    physical_to_logical_map = phy2log\n    # The `phyrank` is the rank for each physical expert.\n    # The `logcnt` is the count of replicas for each logical expert.\n    return physical_to_logical_map, log2phy, logcnt # Returning `log2phy` which is `phy2log` from above.\n>>>>>>> REPLACE"]}}}